I0908 21:10:38.645979      23 e2e.go:126] Starting e2e run "3af7af1c-08fb-48ad-95c3-714ce808d119" on Ginkgo node 1
Sep  8 21:10:38.662: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1694207438 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  8 21:10:38.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:10:38.848: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0908 21:10:38.855727      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Sep  8 21:10:38.891: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  8 21:10:39.000: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  8 21:10:39.000: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Sep  8 21:10:39.000: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  8 21:10:39.019: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Sep  8 21:10:39.020: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  8 21:10:39.020: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-vip-ds' (0 seconds elapsed)
Sep  8 21:10:39.020: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Sep  8 21:10:39.020: INFO: e2e test version: v1.26.5
Sep  8 21:10:39.022: INFO: kube-apiserver version: v1.26.5
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Sep  8 21:10:39.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:10:39.039: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.193 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  8 21:10:38.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:10:38.848: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0908 21:10:38.855727      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Sep  8 21:10:38.891: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Sep  8 21:10:39.000: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Sep  8 21:10:39.000: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
    Sep  8 21:10:39.000: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Sep  8 21:10:39.019: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Sep  8 21:10:39.020: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Sep  8 21:10:39.020: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-vip-ds' (0 seconds elapsed)
    Sep  8 21:10:39.020: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
    Sep  8 21:10:39.020: INFO: e2e test version: v1.26.5
    Sep  8 21:10:39.022: INFO: kube-apiserver version: v1.26.5
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Sep  8 21:10:39.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:10:39.039: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:10:39.074
Sep  8 21:10:39.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:10:39.075
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:10:39.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:10:39.118
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-1062bf5d-21fc-4ae4-bafa-03e393c5bf49 09/08/23 21:10:39.122
STEP: Creating a pod to test consume configMaps 09/08/23 21:10:39.139
Sep  8 21:10:39.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259" in namespace "projected-770" to be "Succeeded or Failed"
Sep  8 21:10:39.175: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 11.358081ms
Sep  8 21:10:41.201: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037157418s
Sep  8 21:10:43.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021759968s
Sep  8 21:10:45.193: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029682039s
Sep  8 21:10:47.281: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117157099s
Sep  8 21:10:49.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021101364s
Sep  8 21:10:51.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021237639s
Sep  8 21:10:53.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021188458s
Sep  8 21:10:55.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021246227s
Sep  8 21:10:57.189: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025508282s
Sep  8 21:10:59.186: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.022376865s
STEP: Saw pod success 09/08/23 21:10:59.186
Sep  8 21:10:59.186: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259" satisfied condition "Succeeded or Failed"
Sep  8 21:10:59.204: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:10:59.255
Sep  8 21:10:59.306: INFO: Waiting for pod pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259 to disappear
Sep  8 21:10:59.327: INFO: Pod pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:10:59.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-770" for this suite. 09/08/23 21:10:59.34
------------------------------
â€¢ [SLOW TEST] [20.294 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:10:39.074
    Sep  8 21:10:39.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:10:39.075
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:10:39.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:10:39.118
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-1062bf5d-21fc-4ae4-bafa-03e393c5bf49 09/08/23 21:10:39.122
    STEP: Creating a pod to test consume configMaps 09/08/23 21:10:39.139
    Sep  8 21:10:39.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259" in namespace "projected-770" to be "Succeeded or Failed"
    Sep  8 21:10:39.175: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 11.358081ms
    Sep  8 21:10:41.201: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037157418s
    Sep  8 21:10:43.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021759968s
    Sep  8 21:10:45.193: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029682039s
    Sep  8 21:10:47.281: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117157099s
    Sep  8 21:10:49.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021101364s
    Sep  8 21:10:51.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021237639s
    Sep  8 21:10:53.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021188458s
    Sep  8 21:10:55.185: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 16.021246227s
    Sep  8 21:10:57.189: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Pending", Reason="", readiness=false. Elapsed: 18.025508282s
    Sep  8 21:10:59.186: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.022376865s
    STEP: Saw pod success 09/08/23 21:10:59.186
    Sep  8 21:10:59.186: INFO: Pod "pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259" satisfied condition "Succeeded or Failed"
    Sep  8 21:10:59.204: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:10:59.255
    Sep  8 21:10:59.306: INFO: Waiting for pod pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259 to disappear
    Sep  8 21:10:59.327: INFO: Pod pod-projected-configmaps-350175af-a5b5-4adb-9e12-4b27b604a259 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:10:59.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-770" for this suite. 09/08/23 21:10:59.34
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:10:59.368
Sep  8 21:10:59.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename endpointslice 09/08/23 21:10:59.37
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:10:59.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:10:59.412
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  8 21:10:59.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3127" for this suite. 09/08/23 21:10:59.595
------------------------------
â€¢ [0.243 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:10:59.368
    Sep  8 21:10:59.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename endpointslice 09/08/23 21:10:59.37
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:10:59.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:10:59.412
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:10:59.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3127" for this suite. 09/08/23 21:10:59.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:10:59.616
Sep  8 21:10:59.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:10:59.617
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:10:59.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:10:59.661
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/08/23 21:10:59.668
Sep  8 21:10:59.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:11:07.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:11:24.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2084" for this suite. 09/08/23 21:11:24.563
------------------------------
â€¢ [SLOW TEST] [24.970 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:10:59.616
    Sep  8 21:10:59.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:10:59.617
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:10:59.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:10:59.661
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 09/08/23 21:10:59.668
    Sep  8 21:10:59.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:11:07.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:11:24.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2084" for this suite. 09/08/23 21:11:24.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:11:24.59
Sep  8 21:11:24.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 21:11:24.591
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:11:24.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:11:24.628
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 21:12:24.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4957" for this suite. 09/08/23 21:12:24.688
------------------------------
â€¢ [SLOW TEST] [60.129 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:11:24.59
    Sep  8 21:11:24.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 21:11:24.591
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:11:24.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:11:24.628
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:12:24.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4957" for this suite. 09/08/23 21:12:24.688
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:12:24.72
Sep  8 21:12:24.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:12:24.722
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:12:24.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:12:24.764
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:12:24.771
Sep  8 21:12:24.810: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b" in namespace "downward-api-7866" to be "Succeeded or Failed"
Sep  8 21:12:24.827: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.439743ms
Sep  8 21:12:26.852: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042276268s
Sep  8 21:12:28.844: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034622005s
Sep  8 21:12:30.844: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034463193s
STEP: Saw pod success 09/08/23 21:12:30.844
Sep  8 21:12:30.845: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b" satisfied condition "Succeeded or Failed"
Sep  8 21:12:30.853: INFO: Trying to get logs from node node-3 pod downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b container client-container: <nil>
STEP: delete the pod 09/08/23 21:12:30.893
Sep  8 21:12:30.949: INFO: Waiting for pod downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b to disappear
Sep  8 21:12:30.957: INFO: Pod downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 21:12:30.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7866" for this suite. 09/08/23 21:12:30.967
------------------------------
â€¢ [SLOW TEST] [6.266 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:12:24.72
    Sep  8 21:12:24.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:12:24.722
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:12:24.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:12:24.764
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:12:24.771
    Sep  8 21:12:24.810: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b" in namespace "downward-api-7866" to be "Succeeded or Failed"
    Sep  8 21:12:24.827: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.439743ms
    Sep  8 21:12:26.852: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042276268s
    Sep  8 21:12:28.844: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034622005s
    Sep  8 21:12:30.844: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034463193s
    STEP: Saw pod success 09/08/23 21:12:30.844
    Sep  8 21:12:30.845: INFO: Pod "downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b" satisfied condition "Succeeded or Failed"
    Sep  8 21:12:30.853: INFO: Trying to get logs from node node-3 pod downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b container client-container: <nil>
    STEP: delete the pod 09/08/23 21:12:30.893
    Sep  8 21:12:30.949: INFO: Waiting for pod downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b to disappear
    Sep  8 21:12:30.957: INFO: Pod downwardapi-volume-4ebc248b-d08d-40af-a444-8f333b450c4b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:12:30.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7866" for this suite. 09/08/23 21:12:30.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:12:30.987
Sep  8 21:12:30.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:12:30.989
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:12:31.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:12:31.03
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-39384b0d-53e8-446f-a0d2-9b74d67ebe1b 09/08/23 21:12:31.035
STEP: Creating a pod to test consume configMaps 09/08/23 21:12:31.046
Sep  8 21:12:31.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3" in namespace "projected-9101" to be "Succeeded or Failed"
Sep  8 21:12:31.090: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.861266ms
Sep  8 21:12:33.107: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032691617s
Sep  8 21:12:35.103: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029039635s
Sep  8 21:12:37.106: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031924334s
Sep  8 21:12:39.101: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026809462s
Sep  8 21:12:41.102: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02786191s
Sep  8 21:12:43.101: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026865766s
Sep  8 21:12:45.101: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.027140964s
STEP: Saw pod success 09/08/23 21:12:45.101
Sep  8 21:12:45.102: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3" satisfied condition "Succeeded or Failed"
Sep  8 21:12:45.109: INFO: Trying to get logs from node node-4 pod pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3 container projected-configmap-volume-test: <nil>
STEP: delete the pod 09/08/23 21:12:45.157
Sep  8 21:12:45.215: INFO: Waiting for pod pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3 to disappear
Sep  8 21:12:45.229: INFO: Pod pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:12:45.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9101" for this suite. 09/08/23 21:12:45.246
------------------------------
â€¢ [SLOW TEST] [14.275 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:12:30.987
    Sep  8 21:12:30.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:12:30.989
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:12:31.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:12:31.03
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-39384b0d-53e8-446f-a0d2-9b74d67ebe1b 09/08/23 21:12:31.035
    STEP: Creating a pod to test consume configMaps 09/08/23 21:12:31.046
    Sep  8 21:12:31.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3" in namespace "projected-9101" to be "Succeeded or Failed"
    Sep  8 21:12:31.090: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.861266ms
    Sep  8 21:12:33.107: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032691617s
    Sep  8 21:12:35.103: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029039635s
    Sep  8 21:12:37.106: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031924334s
    Sep  8 21:12:39.101: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026809462s
    Sep  8 21:12:41.102: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02786191s
    Sep  8 21:12:43.101: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026865766s
    Sep  8 21:12:45.101: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.027140964s
    STEP: Saw pod success 09/08/23 21:12:45.101
    Sep  8 21:12:45.102: INFO: Pod "pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3" satisfied condition "Succeeded or Failed"
    Sep  8 21:12:45.109: INFO: Trying to get logs from node node-4 pod pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 09/08/23 21:12:45.157
    Sep  8 21:12:45.215: INFO: Waiting for pod pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3 to disappear
    Sep  8 21:12:45.229: INFO: Pod pod-projected-configmaps-e3f4f2fe-e228-4c24-8e69-d270cc4378b3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:12:45.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9101" for this suite. 09/08/23 21:12:45.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:12:45.269
Sep  8 21:12:45.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename cronjob 09/08/23 21:12:45.271
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:12:45.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:12:45.328
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 09/08/23 21:12:45.334
STEP: Ensuring more than one job is running at a time 09/08/23 21:12:45.345
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/08/23 21:14:01.364
STEP: Removing cronjob 09/08/23 21:14:01.38
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  8 21:14:01.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-59" for this suite. 09/08/23 21:14:01.441
------------------------------
â€¢ [SLOW TEST] [76.239 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:12:45.269
    Sep  8 21:12:45.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename cronjob 09/08/23 21:12:45.271
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:12:45.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:12:45.328
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 09/08/23 21:12:45.334
    STEP: Ensuring more than one job is running at a time 09/08/23 21:12:45.345
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 09/08/23 21:14:01.364
    STEP: Removing cronjob 09/08/23 21:14:01.38
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:14:01.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-59" for this suite. 09/08/23 21:14:01.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:14:01.512
Sep  8 21:14:01.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:14:01.513
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:01.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:01.607
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-48c6f3f1-367b-44d3-a060-f0e30622862f 09/08/23 21:14:01.617
STEP: Creating a pod to test consume configMaps 09/08/23 21:14:01.632
Sep  8 21:14:01.669: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99" in namespace "projected-1158" to be "Succeeded or Failed"
Sep  8 21:14:01.685: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99": Phase="Pending", Reason="", readiness=false. Elapsed: 15.793561ms
Sep  8 21:14:03.704: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035000167s
Sep  8 21:14:05.694: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025239172s
STEP: Saw pod success 09/08/23 21:14:05.694
Sep  8 21:14:05.695: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99" satisfied condition "Succeeded or Failed"
Sep  8 21:14:05.713: INFO: Trying to get logs from node node-4 pod pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:14:05.736
Sep  8 21:14:05.779: INFO: Waiting for pod pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99 to disappear
Sep  8 21:14:05.787: INFO: Pod pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:14:05.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1158" for this suite. 09/08/23 21:14:05.799
------------------------------
â€¢ [4.305 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:14:01.512
    Sep  8 21:14:01.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:14:01.513
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:01.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:01.607
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-48c6f3f1-367b-44d3-a060-f0e30622862f 09/08/23 21:14:01.617
    STEP: Creating a pod to test consume configMaps 09/08/23 21:14:01.632
    Sep  8 21:14:01.669: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99" in namespace "projected-1158" to be "Succeeded or Failed"
    Sep  8 21:14:01.685: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99": Phase="Pending", Reason="", readiness=false. Elapsed: 15.793561ms
    Sep  8 21:14:03.704: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035000167s
    Sep  8 21:14:05.694: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025239172s
    STEP: Saw pod success 09/08/23 21:14:05.694
    Sep  8 21:14:05.695: INFO: Pod "pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99" satisfied condition "Succeeded or Failed"
    Sep  8 21:14:05.713: INFO: Trying to get logs from node node-4 pod pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:14:05.736
    Sep  8 21:14:05.779: INFO: Waiting for pod pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99 to disappear
    Sep  8 21:14:05.787: INFO: Pod pod-projected-configmaps-95f7e7f4-235e-4c0b-b73d-031021e6ed99 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:14:05.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1158" for this suite. 09/08/23 21:14:05.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:14:05.818
Sep  8 21:14:05.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 21:14:05.82
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:05.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:05.88
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 09/08/23 21:14:05.884
Sep  8 21:14:05.905: INFO: Waiting up to 5m0s for pod "pod-klmrh" in namespace "pods-6495" to be "running"
Sep  8 21:14:05.916: INFO: Pod "pod-klmrh": Phase="Pending", Reason="", readiness=false. Elapsed: 11.714186ms
Sep  8 21:14:07.934: INFO: Pod "pod-klmrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.029224004s
Sep  8 21:14:07.934: INFO: Pod "pod-klmrh" satisfied condition "running"
STEP: patching /status 09/08/23 21:14:07.934
Sep  8 21:14:07.960: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 21:14:07.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6495" for this suite. 09/08/23 21:14:08.007
------------------------------
â€¢ [2.209 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:14:05.818
    Sep  8 21:14:05.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 21:14:05.82
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:05.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:05.88
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 09/08/23 21:14:05.884
    Sep  8 21:14:05.905: INFO: Waiting up to 5m0s for pod "pod-klmrh" in namespace "pods-6495" to be "running"
    Sep  8 21:14:05.916: INFO: Pod "pod-klmrh": Phase="Pending", Reason="", readiness=false. Elapsed: 11.714186ms
    Sep  8 21:14:07.934: INFO: Pod "pod-klmrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.029224004s
    Sep  8 21:14:07.934: INFO: Pod "pod-klmrh" satisfied condition "running"
    STEP: patching /status 09/08/23 21:14:07.934
    Sep  8 21:14:07.960: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:14:07.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6495" for this suite. 09/08/23 21:14:08.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:14:08.028
Sep  8 21:14:08.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename csiinlinevolumes 09/08/23 21:14:08.029
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:08.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:08.085
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 09/08/23 21:14:08.097
STEP: getting 09/08/23 21:14:08.166
STEP: listing 09/08/23 21:14:08.19
STEP: deleting 09/08/23 21:14:08.21
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:14:08.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9435" for this suite. 09/08/23 21:14:08.3
------------------------------
â€¢ [0.299 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:14:08.028
    Sep  8 21:14:08.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename csiinlinevolumes 09/08/23 21:14:08.029
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:08.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:08.085
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 09/08/23 21:14:08.097
    STEP: getting 09/08/23 21:14:08.166
    STEP: listing 09/08/23 21:14:08.19
    STEP: deleting 09/08/23 21:14:08.21
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:14:08.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9435" for this suite. 09/08/23 21:14:08.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:14:08.329
Sep  8 21:14:08.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:14:08.331
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:08.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:08.381
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 09/08/23 21:14:08.424
Sep  8 21:14:08.454: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf" in namespace "emptydir-2000" to be "running"
Sep  8 21:14:08.478: INFO: Pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.764442ms
Sep  8 21:14:10.488: INFO: Pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf": Phase="Running", Reason="", readiness=true. Elapsed: 2.033446527s
Sep  8 21:14:10.488: INFO: Pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf" satisfied condition "running"
STEP: Reading file content from the nginx-container 09/08/23 21:14:10.488
Sep  8 21:14:10.489: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2000 PodName:pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:14:10.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:14:10.489: INFO: ExecWithOptions: Clientset creation
Sep  8 21:14:10.489: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-2000/pods/pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Sep  8 21:14:10.655: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:14:10.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2000" for this suite. 09/08/23 21:14:10.674
------------------------------
â€¢ [2.372 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:14:08.329
    Sep  8 21:14:08.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:14:08.331
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:08.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:08.381
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 09/08/23 21:14:08.424
    Sep  8 21:14:08.454: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf" in namespace "emptydir-2000" to be "running"
    Sep  8 21:14:08.478: INFO: Pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.764442ms
    Sep  8 21:14:10.488: INFO: Pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf": Phase="Running", Reason="", readiness=true. Elapsed: 2.033446527s
    Sep  8 21:14:10.488: INFO: Pod "pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf" satisfied condition "running"
    STEP: Reading file content from the nginx-container 09/08/23 21:14:10.488
    Sep  8 21:14:10.489: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2000 PodName:pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:14:10.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:14:10.489: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:14:10.489: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-2000/pods/pod-sharedvolume-0b42016e-623b-4b21-ae8d-b7e4a8bf3adf/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Sep  8 21:14:10.655: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:14:10.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2000" for this suite. 09/08/23 21:14:10.674
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:14:10.701
Sep  8 21:14:10.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:14:10.704
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:10.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:10.802
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/08/23 21:14:10.809
Sep  8 21:14:10.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/08/23 21:14:26.443
Sep  8 21:14:26.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:14:34.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:14:50.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8987" for this suite. 09/08/23 21:14:50.173
------------------------------
â€¢ [SLOW TEST] [39.486 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:14:10.701
    Sep  8 21:14:10.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:14:10.704
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:10.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:10.802
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 09/08/23 21:14:10.809
    Sep  8 21:14:10.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 09/08/23 21:14:26.443
    Sep  8 21:14:26.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:14:34.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:14:50.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8987" for this suite. 09/08/23 21:14:50.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:14:50.193
Sep  8 21:14:50.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 21:14:50.194
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:50.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:50.25
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Sep  8 21:14:50.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:14:55.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9022" for this suite. 09/08/23 21:14:55.906
------------------------------
â€¢ [SLOW TEST] [5.775 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:14:50.193
    Sep  8 21:14:50.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 21:14:50.194
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:50.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:50.25
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Sep  8 21:14:50.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:14:55.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9022" for this suite. 09/08/23 21:14:55.906
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:14:55.97
Sep  8 21:14:55.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 21:14:55.984
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:56.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:56.039
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Sep  8 21:14:56.045: INFO: Creating simple deployment test-new-deployment
Sep  8 21:14:56.105: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Sep  8 21:14:58.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:00.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:02.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:04.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:06.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:08.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:10.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:12.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:14.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:15:16.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 09/08/23 21:15:18.147
STEP: updating a scale subresource 09/08/23 21:15:18.167
STEP: verifying the deployment Spec.Replicas was modified 09/08/23 21:15:18.189
STEP: Patch a scale subresource 09/08/23 21:15:18.204
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 21:15:18.363: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5007  cf13e4e3-fab2-4048-ad07-3fd75c0675cb 9432 3 2023-09-08 21:14:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-08 21:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004efa978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-08 21:15:16 +0000 UTC,LastTransitionTime:2023-09-08 21:14:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-08 21:15:18 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  8 21:15:18.380: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5007  d8dfd789-f820-4554-a96f-403a3c919a86 9430 3 2023-09-08 21:14:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment cf13e4e3-fab2-4048-ad07-3fd75c0675cb 0xc004efadb0 0xc004efadb1}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf13e4e3-fab2-4048-ad07-3fd75c0675cb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004efae38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:15:18.453: INFO: Pod "test-new-deployment-7f5969cbc7-6kkkj" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-6kkkj test-new-deployment-7f5969cbc7- deployment-5007  480cc9ca-a1b9-4f8b-b108-474266282bed 9429 0 2023-09-08 21:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb220 0xc004efb221}] [] [{kube-controller-manager Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz9g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz9g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 21:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 21:15:18.454: INFO: Pod "test-new-deployment-7f5969cbc7-7h4xq" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-7h4xq test-new-deployment-7f5969cbc7- deployment-5007  01cabb62-3260-492d-a695-a590dbec3fdf 9441 0 2023-09-08 21:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb3e7 0xc004efb3e8}] [] [{kube-controller-manager Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddqp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddqp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 21:15:18.454: INFO: Pod "test-new-deployment-7f5969cbc7-j8chx" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-j8chx test-new-deployment-7f5969cbc7- deployment-5007  5a8ef045-4526-40f3-9e8b-437e77ce377b 9408 0 2023-09-08 21:14:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c8191e70b753c33798084a62c2295457c639233a0fcb42c9d9e375e588a15b56 cni.projectcalico.org/podIP:10.233.75.83/32 cni.projectcalico.org/podIPs:10.233.75.83/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb570 0xc004efb571}] [] [{calico Update v1 2023-09-08 21:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 21:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:15:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxzb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxzb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:14:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:14:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.83,StartTime:2023-09-08 21:14:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:15:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f3946db078bbe16926e617232261cbd49b0330d8a3810ff7edbd9dbe690bb52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 21:15:18.454: INFO: Pod "test-new-deployment-7f5969cbc7-l7glj" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-l7glj test-new-deployment-7f5969cbc7- deployment-5007  532902c3-a63d-4c40-a356-7def5a385624 9440 0 2023-09-08 21:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb770 0xc004efb771}] [] [{kube-controller-manager Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fqhkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fqhkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 21:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 21:15:18.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5007" for this suite. 09/08/23 21:15:18.472
------------------------------
â€¢ [SLOW TEST] [22.535 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:14:55.97
    Sep  8 21:14:55.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 21:14:55.984
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:14:56.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:14:56.039
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Sep  8 21:14:56.045: INFO: Creating simple deployment test-new-deployment
    Sep  8 21:14:56.105: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    Sep  8 21:14:58.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:00.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:02.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:04.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:06.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:08.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:10.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:12.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:14.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:15:16.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 14, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 09/08/23 21:15:18.147
    STEP: updating a scale subresource 09/08/23 21:15:18.167
    STEP: verifying the deployment Spec.Replicas was modified 09/08/23 21:15:18.189
    STEP: Patch a scale subresource 09/08/23 21:15:18.204
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 21:15:18.363: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-5007  cf13e4e3-fab2-4048-ad07-3fd75c0675cb 9432 3 2023-09-08 21:14:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-09-08 21:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004efa978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-09-08 21:15:16 +0000 UTC,LastTransitionTime:2023-09-08 21:14:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-08 21:15:18 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  8 21:15:18.380: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-5007  d8dfd789-f820-4554-a96f-403a3c919a86 9430 3 2023-09-08 21:14:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment cf13e4e3-fab2-4048-ad07-3fd75c0675cb 0xc004efadb0 0xc004efadb1}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf13e4e3-fab2-4048-ad07-3fd75c0675cb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004efae38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:15:18.453: INFO: Pod "test-new-deployment-7f5969cbc7-6kkkj" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-6kkkj test-new-deployment-7f5969cbc7- deployment-5007  480cc9ca-a1b9-4f8b-b108-474266282bed 9429 0 2023-09-08 21:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb220 0xc004efb221}] [] [{kube-controller-manager Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz9g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz9g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 21:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 21:15:18.454: INFO: Pod "test-new-deployment-7f5969cbc7-7h4xq" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-7h4xq test-new-deployment-7f5969cbc7- deployment-5007  01cabb62-3260-492d-a695-a590dbec3fdf 9441 0 2023-09-08 21:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb3e7 0xc004efb3e8}] [] [{kube-controller-manager Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddqp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddqp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 21:15:18.454: INFO: Pod "test-new-deployment-7f5969cbc7-j8chx" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-j8chx test-new-deployment-7f5969cbc7- deployment-5007  5a8ef045-4526-40f3-9e8b-437e77ce377b 9408 0 2023-09-08 21:14:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c8191e70b753c33798084a62c2295457c639233a0fcb42c9d9e375e588a15b56 cni.projectcalico.org/podIP:10.233.75.83/32 cni.projectcalico.org/podIPs:10.233.75.83/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb570 0xc004efb571}] [] [{calico Update v1 2023-09-08 21:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 21:14:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:15:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxzb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxzb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:14:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:14:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.83,StartTime:2023-09-08 21:14:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:15:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f3946db078bbe16926e617232261cbd49b0330d8a3810ff7edbd9dbe690bb52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 21:15:18.454: INFO: Pod "test-new-deployment-7f5969cbc7-l7glj" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-l7glj test-new-deployment-7f5969cbc7- deployment-5007  532902c3-a63d-4c40-a356-7def5a385624 9440 0 2023-09-08 21:15:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 d8dfd789-f820-4554-a96f-403a3c919a86 0xc004efb770 0xc004efb771}] [] [{kube-controller-manager Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8dfd789-f820-4554-a96f-403a3c919a86\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:15:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fqhkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fqhkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:15:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 21:15:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:15:18.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5007" for this suite. 09/08/23 21:15:18.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:15:18.507
Sep  8 21:15:18.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:15:18.508
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:15:18.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:15:18.576
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Sep  8 21:15:18.668: INFO: Waiting up to 5m0s for pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5" in namespace "svcaccounts-4713" to be "running"
Sep  8 21:15:18.679: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.131078ms
Sep  8 21:15:20.696: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027946269s
Sep  8 21:15:22.690: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5": Phase="Running", Reason="", readiness=true. Elapsed: 4.022182897s
Sep  8 21:15:22.690: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5" satisfied condition "running"
STEP: reading a file in the container 09/08/23 21:15:22.69
Sep  8 21:15:22.690: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4713 pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 09/08/23 21:15:23.06
Sep  8 21:15:23.061: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4713 pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 09/08/23 21:15:23.343
Sep  8 21:15:23.343: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4713 pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Sep  8 21:15:23.684: INFO: Got root ca configmap in namespace "svcaccounts-4713"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  8 21:15:23.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4713" for this suite. 09/08/23 21:15:23.721
------------------------------
â€¢ [SLOW TEST] [5.240 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:15:18.507
    Sep  8 21:15:18.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:15:18.508
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:15:18.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:15:18.576
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Sep  8 21:15:18.668: INFO: Waiting up to 5m0s for pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5" in namespace "svcaccounts-4713" to be "running"
    Sep  8 21:15:18.679: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.131078ms
    Sep  8 21:15:20.696: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027946269s
    Sep  8 21:15:22.690: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5": Phase="Running", Reason="", readiness=true. Elapsed: 4.022182897s
    Sep  8 21:15:22.690: INFO: Pod "pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5" satisfied condition "running"
    STEP: reading a file in the container 09/08/23 21:15:22.69
    Sep  8 21:15:22.690: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4713 pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 09/08/23 21:15:23.06
    Sep  8 21:15:23.061: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4713 pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 09/08/23 21:15:23.343
    Sep  8 21:15:23.343: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4713 pod-service-account-fcd02fb8-5632-4c56-b324-39445e5bd9f5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Sep  8 21:15:23.684: INFO: Got root ca configmap in namespace "svcaccounts-4713"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:15:23.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4713" for this suite. 09/08/23 21:15:23.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:15:23.749
Sep  8 21:15:23.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:15:23.75
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:15:23.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:15:23.808
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-0929f198-9641-4c38-9015-308c8997e412 09/08/23 21:15:23.831
STEP: Creating configMap with name cm-test-opt-upd-6143e291-454c-40e7-add5-4d7931d8f7cd 09/08/23 21:15:23.853
STEP: Creating the pod 09/08/23 21:15:23.869
Sep  8 21:15:23.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132" in namespace "configmap-7538" to be "running and ready"
Sep  8 21:15:23.912: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132": Phase="Pending", Reason="", readiness=false. Elapsed: 22.326555ms
Sep  8 21:15:23.912: INFO: The phase of Pod pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:15:25.923: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03350639s
Sep  8 21:15:25.924: INFO: The phase of Pod pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:15:27.960: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132": Phase="Running", Reason="", readiness=true. Elapsed: 4.069714179s
Sep  8 21:15:27.960: INFO: The phase of Pod pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 is Running (Ready = true)
Sep  8 21:15:27.960: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0929f198-9641-4c38-9015-308c8997e412 09/08/23 21:15:28.089
STEP: Updating configmap cm-test-opt-upd-6143e291-454c-40e7-add5-4d7931d8f7cd 09/08/23 21:15:28.116
STEP: Creating configMap with name cm-test-opt-create-bd18f676-447d-44ea-b36b-4593ff75058d 09/08/23 21:15:28.127
STEP: waiting to observe update in volume 09/08/23 21:15:28.139
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:16:53.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7538" for this suite. 09/08/23 21:16:53.335
------------------------------
â€¢ [SLOW TEST] [89.603 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:15:23.749
    Sep  8 21:15:23.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:15:23.75
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:15:23.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:15:23.808
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-0929f198-9641-4c38-9015-308c8997e412 09/08/23 21:15:23.831
    STEP: Creating configMap with name cm-test-opt-upd-6143e291-454c-40e7-add5-4d7931d8f7cd 09/08/23 21:15:23.853
    STEP: Creating the pod 09/08/23 21:15:23.869
    Sep  8 21:15:23.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132" in namespace "configmap-7538" to be "running and ready"
    Sep  8 21:15:23.912: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132": Phase="Pending", Reason="", readiness=false. Elapsed: 22.326555ms
    Sep  8 21:15:23.912: INFO: The phase of Pod pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:15:25.923: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03350639s
    Sep  8 21:15:25.924: INFO: The phase of Pod pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:15:27.960: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132": Phase="Running", Reason="", readiness=true. Elapsed: 4.069714179s
    Sep  8 21:15:27.960: INFO: The phase of Pod pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 is Running (Ready = true)
    Sep  8 21:15:27.960: INFO: Pod "pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0929f198-9641-4c38-9015-308c8997e412 09/08/23 21:15:28.089
    STEP: Updating configmap cm-test-opt-upd-6143e291-454c-40e7-add5-4d7931d8f7cd 09/08/23 21:15:28.116
    STEP: Creating configMap with name cm-test-opt-create-bd18f676-447d-44ea-b36b-4593ff75058d 09/08/23 21:15:28.127
    STEP: waiting to observe update in volume 09/08/23 21:15:28.139
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:16:53.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7538" for this suite. 09/08/23 21:16:53.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:16:53.353
Sep  8 21:16:53.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 21:16:53.355
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:16:53.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:16:53.408
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 09/08/23 21:16:53.416
Sep  8 21:16:53.417: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5456 proxy --unix-socket=/tmp/kubectl-proxy-unix3854335318/test'
STEP: retrieving proxy /api/ output 09/08/23 21:16:53.488
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 21:16:53.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5456" for this suite. 09/08/23 21:16:53.5
------------------------------
â€¢ [0.187 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:16:53.353
    Sep  8 21:16:53.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 21:16:53.355
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:16:53.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:16:53.408
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 09/08/23 21:16:53.416
    Sep  8 21:16:53.417: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5456 proxy --unix-socket=/tmp/kubectl-proxy-unix3854335318/test'
    STEP: retrieving proxy /api/ output 09/08/23 21:16:53.488
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:16:53.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5456" for this suite. 09/08/23 21:16:53.5
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:16:53.541
Sep  8 21:16:53.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-pred 09/08/23 21:16:53.543
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:16:53.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:16:53.607
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  8 21:16:53.612: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  8 21:16:53.641: INFO: Waiting for terminating namespaces to be deleted...
Sep  8 21:16:53.655: INFO: 
Logging pods the apiserver thinks is on node node-3 before test
Sep  8 21:16:53.681: INFO: cadvisor-kbl2r from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 21:16:53.682: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-bd2m5 from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  8 21:16:53.682: INFO: kube-prometheus-node-exporter-fpqfh from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 21:16:53.682: INFO: netchecker-agent-4vxnw from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:16:53.682: INFO: netchecker-agent-hostnet-hpshq from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:16:53.682: INFO: openebs-localpv-provisioner-5d6756bcd8-xfthd from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 21:16:53.682: INFO: ingress-nginx-controller-tf5xn from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:16:53.682: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 21:16:53.682: INFO: coredns-6c86d97486-8w8fz from kube-system started at 2023-09-08 20:57:17 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container coredns ready: true, restart count 0
Sep  8 21:16:53.682: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 21:16:53.682: INFO: kubernetes-dashboard-68b4f76cdb-tkz5s from kube-system started at 2023-09-08 20:52:27 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep  8 21:16:53.682: INFO: kubernetes-dashboard-admin-79b7cc94c8-xw79r from kube-system started at 2023-09-08 20:52:30 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container kubernetes-dashboard-admin ready: true, restart count 0
Sep  8 21:16:53.682: INFO: kubernetes-metrics-scraper-6585f876d5-xbkgd from kube-system started at 2023-09-08 20:52:28 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
Sep  8 21:16:53.682: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 21:16:53.682: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 21:16:53.682: INFO: metallb-controller-77b687f97-969st from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:16:53.682: INFO: metallb-speaker-gfxvl from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container speaker ready: true, restart count 0
Sep  8 21:16:53.682: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  8 21:16:53.682: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 21:16:53.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:16:53.682: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  8 21:16:53.682: INFO: 
Logging pods the apiserver thinks is on node node-4 before test
Sep  8 21:16:53.710: INFO: pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 from configmap-7538 started at 2023-09-08 21:15:23 +0000 UTC (3 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container createcm-volume-test ready: true, restart count 0
Sep  8 21:16:53.710: INFO: 	Container delcm-volume-test ready: true, restart count 0
Sep  8 21:16:53.710: INFO: 	Container updcm-volume-test ready: true, restart count 0
Sep  8 21:16:53.710: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 21:16:53.710: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  8 21:16:53.710: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 21:16:53.710: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:16:53.710: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:16:53.710: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container etcd ready: true, restart count 0
Sep  8 21:16:53.710: INFO: 	Container netchecker-server ready: true, restart count 1
Sep  8 21:16:53.710: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 21:16:53.710: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:16:53.710: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  8 21:16:53.710: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 21:16:53.710: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 21:16:53.710: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container metrics-server ready: true, restart count 0
Sep  8 21:16:53.710: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 21:16:53.710: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 21:16:53.710: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container metallb-monitor ready: true, restart count 0
Sep  8 21:16:53.710: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container speaker ready: true, restart count 0
Sep  8 21:16:53.710: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container e2e ready: true, restart count 0
Sep  8 21:16:53.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:16:53.710: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 21:16:53.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:16:53.710: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/08/23 21:16:53.71
Sep  8 21:16:53.739: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2534" to be "running"
Sep  8 21:16:53.748: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.403302ms
Sep  8 21:16:55.764: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024813844s
Sep  8 21:16:57.773: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.034104642s
Sep  8 21:16:57.773: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/08/23 21:16:57.786
STEP: Trying to apply a random label on the found node. 09/08/23 21:16:57.84
STEP: verifying the node has the label kubernetes.io/e2e-bb1d70f6-c1d5-4024-928e-cde5977651a6 42 09/08/23 21:16:57.884
STEP: Trying to relaunch the pod, now with labels. 09/08/23 21:16:57.901
Sep  8 21:16:57.923: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2534" to be "not pending"
Sep  8 21:16:57.933: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848634ms
Sep  8 21:16:59.951: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.028329365s
Sep  8 21:16:59.951: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-bb1d70f6-c1d5-4024-928e-cde5977651a6 off the node node-3 09/08/23 21:16:59.964
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bb1d70f6-c1d5-4024-928e-cde5977651a6 09/08/23 21:17:00.015
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:00.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2534" for this suite. 09/08/23 21:17:00.052
------------------------------
â€¢ [SLOW TEST] [6.531 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:16:53.541
    Sep  8 21:16:53.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-pred 09/08/23 21:16:53.543
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:16:53.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:16:53.607
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  8 21:16:53.612: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  8 21:16:53.641: INFO: Waiting for terminating namespaces to be deleted...
    Sep  8 21:16:53.655: INFO: 
    Logging pods the apiserver thinks is on node node-3 before test
    Sep  8 21:16:53.681: INFO: cadvisor-kbl2r from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-bd2m5 from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: kube-prometheus-node-exporter-fpqfh from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: netchecker-agent-4vxnw from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: netchecker-agent-hostnet-hpshq from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: openebs-localpv-provisioner-5d6756bcd8-xfthd from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: ingress-nginx-controller-tf5xn from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: coredns-6c86d97486-8w8fz from kube-system started at 2023-09-08 20:57:17 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container coredns ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: kubernetes-dashboard-68b4f76cdb-tkz5s from kube-system started at 2023-09-08 20:52:27 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: kubernetes-dashboard-admin-79b7cc94c8-xw79r from kube-system started at 2023-09-08 20:52:30 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container kubernetes-dashboard-admin ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: kubernetes-metrics-scraper-6585f876d5-xbkgd from kube-system started at 2023-09-08 20:52:28 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: metallb-controller-77b687f97-969st from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: metallb-speaker-gfxvl from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 21:16:53.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  8 21:16:53.682: INFO: 
    Logging pods the apiserver thinks is on node node-4 before test
    Sep  8 21:16:53.710: INFO: pod-configmaps-6d7ccfce-5094-4806-9b7f-047569c1c132 from configmap-7538 started at 2023-09-08 21:15:23 +0000 UTC (3 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container createcm-volume-test ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: 	Container delcm-volume-test ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: 	Container updcm-volume-test ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container etcd ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: 	Container netchecker-server ready: true, restart count 1
    Sep  8 21:16:53.710: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container metallb-monitor ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container e2e ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 21:16:53.710: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:16:53.710: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/08/23 21:16:53.71
    Sep  8 21:16:53.739: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2534" to be "running"
    Sep  8 21:16:53.748: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 9.403302ms
    Sep  8 21:16:55.764: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024813844s
    Sep  8 21:16:57.773: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.034104642s
    Sep  8 21:16:57.773: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/08/23 21:16:57.786
    STEP: Trying to apply a random label on the found node. 09/08/23 21:16:57.84
    STEP: verifying the node has the label kubernetes.io/e2e-bb1d70f6-c1d5-4024-928e-cde5977651a6 42 09/08/23 21:16:57.884
    STEP: Trying to relaunch the pod, now with labels. 09/08/23 21:16:57.901
    Sep  8 21:16:57.923: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2534" to be "not pending"
    Sep  8 21:16:57.933: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848634ms
    Sep  8 21:16:59.951: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.028329365s
    Sep  8 21:16:59.951: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-bb1d70f6-c1d5-4024-928e-cde5977651a6 off the node node-3 09/08/23 21:16:59.964
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-bb1d70f6-c1d5-4024-928e-cde5977651a6 09/08/23 21:17:00.015
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:00.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2534" for this suite. 09/08/23 21:17:00.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:00.073
Sep  8 21:17:00.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename gc 09/08/23 21:17:00.074
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:00.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:00.139
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Sep  8 21:17:00.255: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fb19838f-37b3-41b8-ac80-86d70eb15266", Controller:(*bool)(0xc005ffc0a6), BlockOwnerDeletion:(*bool)(0xc005ffc0a7)}}
Sep  8 21:17:00.295: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1036fd11-be3c-4b75-bcbd-ea777191d21f", Controller:(*bool)(0xc005ffc2de), BlockOwnerDeletion:(*bool)(0xc005ffc2df)}}
Sep  8 21:17:00.316: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1b79542a-aff0-4c20-b938-50512eafb348", Controller:(*bool)(0xc0080af376), BlockOwnerDeletion:(*bool)(0xc0080af377)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:05.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5676" for this suite. 09/08/23 21:17:05.373
------------------------------
â€¢ [SLOW TEST] [5.342 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:00.073
    Sep  8 21:17:00.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename gc 09/08/23 21:17:00.074
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:00.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:00.139
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Sep  8 21:17:00.255: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fb19838f-37b3-41b8-ac80-86d70eb15266", Controller:(*bool)(0xc005ffc0a6), BlockOwnerDeletion:(*bool)(0xc005ffc0a7)}}
    Sep  8 21:17:00.295: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1036fd11-be3c-4b75-bcbd-ea777191d21f", Controller:(*bool)(0xc005ffc2de), BlockOwnerDeletion:(*bool)(0xc005ffc2df)}}
    Sep  8 21:17:00.316: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1b79542a-aff0-4c20-b938-50512eafb348", Controller:(*bool)(0xc0080af376), BlockOwnerDeletion:(*bool)(0xc0080af377)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:05.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5676" for this suite. 09/08/23 21:17:05.373
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:05.415
Sep  8 21:17:05.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replication-controller 09/08/23 21:17:05.417
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:05.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:05.481
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Sep  8 21:17:05.487: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/08/23 21:17:05.516
STEP: Checking rc "condition-test" has the desired failure condition set 09/08/23 21:17:05.533
STEP: Scaling down rc "condition-test" to satisfy pod quota 09/08/23 21:17:06.563
Sep  8 21:17:06.600: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 09/08/23 21:17:06.6
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:06.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5200" for this suite. 09/08/23 21:17:06.636
------------------------------
â€¢ [1.242 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:05.415
    Sep  8 21:17:05.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replication-controller 09/08/23 21:17:05.417
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:05.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:05.481
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Sep  8 21:17:05.487: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 09/08/23 21:17:05.516
    STEP: Checking rc "condition-test" has the desired failure condition set 09/08/23 21:17:05.533
    STEP: Scaling down rc "condition-test" to satisfy pod quota 09/08/23 21:17:06.563
    Sep  8 21:17:06.600: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 09/08/23 21:17:06.6
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:06.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5200" for this suite. 09/08/23 21:17:06.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:06.66
Sep  8 21:17:06.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename lease-test 09/08/23 21:17:06.662
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:06.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:06.714
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:06.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7445" for this suite. 09/08/23 21:17:06.98
------------------------------
â€¢ [0.343 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:06.66
    Sep  8 21:17:06.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename lease-test 09/08/23 21:17:06.662
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:06.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:06.714
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:06.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7445" for this suite. 09/08/23 21:17:06.98
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:07.005
Sep  8 21:17:07.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:17:07.009
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:07.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:07.063
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 09/08/23 21:17:07.073
Sep  8 21:17:07.108: INFO: Waiting up to 5m0s for pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283" in namespace "downward-api-5845" to be "Succeeded or Failed"
Sep  8 21:17:07.128: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Pending", Reason="", readiness=false. Elapsed: 20.108825ms
Sep  8 21:17:09.145: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036697888s
Sep  8 21:17:11.140: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031957435s
Sep  8 21:17:13.148: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040131368s
STEP: Saw pod success 09/08/23 21:17:13.148
Sep  8 21:17:13.149: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283" satisfied condition "Succeeded or Failed"
Sep  8 21:17:13.174: INFO: Trying to get logs from node node-3 pod downward-api-67eb14df-627d-478e-9331-fa6005a05283 container dapi-container: <nil>
STEP: delete the pod 09/08/23 21:17:13.231
Sep  8 21:17:13.292: INFO: Waiting for pod downward-api-67eb14df-627d-478e-9331-fa6005a05283 to disappear
Sep  8 21:17:13.312: INFO: Pod downward-api-67eb14df-627d-478e-9331-fa6005a05283 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5845" for this suite. 09/08/23 21:17:13.339
------------------------------
â€¢ [SLOW TEST] [6.350 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:07.005
    Sep  8 21:17:07.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:17:07.009
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:07.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:07.063
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 09/08/23 21:17:07.073
    Sep  8 21:17:07.108: INFO: Waiting up to 5m0s for pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283" in namespace "downward-api-5845" to be "Succeeded or Failed"
    Sep  8 21:17:07.128: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Pending", Reason="", readiness=false. Elapsed: 20.108825ms
    Sep  8 21:17:09.145: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036697888s
    Sep  8 21:17:11.140: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031957435s
    Sep  8 21:17:13.148: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040131368s
    STEP: Saw pod success 09/08/23 21:17:13.148
    Sep  8 21:17:13.149: INFO: Pod "downward-api-67eb14df-627d-478e-9331-fa6005a05283" satisfied condition "Succeeded or Failed"
    Sep  8 21:17:13.174: INFO: Trying to get logs from node node-3 pod downward-api-67eb14df-627d-478e-9331-fa6005a05283 container dapi-container: <nil>
    STEP: delete the pod 09/08/23 21:17:13.231
    Sep  8 21:17:13.292: INFO: Waiting for pod downward-api-67eb14df-627d-478e-9331-fa6005a05283 to disappear
    Sep  8 21:17:13.312: INFO: Pod downward-api-67eb14df-627d-478e-9331-fa6005a05283 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5845" for this suite. 09/08/23 21:17:13.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:13.358
Sep  8 21:17:13.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:17:13.36
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:13.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:13.449
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 09/08/23 21:17:13.462
Sep  8 21:17:13.502: INFO: Waiting up to 5m0s for pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43" in namespace "emptydir-6257" to be "Succeeded or Failed"
Sep  8 21:17:13.518: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43": Phase="Pending", Reason="", readiness=false. Elapsed: 15.741524ms
Sep  8 21:17:15.534: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031423923s
Sep  8 21:17:17.529: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026850042s
STEP: Saw pod success 09/08/23 21:17:17.529
Sep  8 21:17:17.530: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43" satisfied condition "Succeeded or Failed"
Sep  8 21:17:17.541: INFO: Trying to get logs from node node-3 pod pod-74df4da8-3e85-41d3-8e69-22e21de9fc43 container test-container: <nil>
STEP: delete the pod 09/08/23 21:17:17.56
Sep  8 21:17:17.593: INFO: Waiting for pod pod-74df4da8-3e85-41d3-8e69-22e21de9fc43 to disappear
Sep  8 21:17:17.609: INFO: Pod pod-74df4da8-3e85-41d3-8e69-22e21de9fc43 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:17.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6257" for this suite. 09/08/23 21:17:17.628
------------------------------
â€¢ [4.291 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:13.358
    Sep  8 21:17:13.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:17:13.36
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:13.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:13.449
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 09/08/23 21:17:13.462
    Sep  8 21:17:13.502: INFO: Waiting up to 5m0s for pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43" in namespace "emptydir-6257" to be "Succeeded or Failed"
    Sep  8 21:17:13.518: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43": Phase="Pending", Reason="", readiness=false. Elapsed: 15.741524ms
    Sep  8 21:17:15.534: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031423923s
    Sep  8 21:17:17.529: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026850042s
    STEP: Saw pod success 09/08/23 21:17:17.529
    Sep  8 21:17:17.530: INFO: Pod "pod-74df4da8-3e85-41d3-8e69-22e21de9fc43" satisfied condition "Succeeded or Failed"
    Sep  8 21:17:17.541: INFO: Trying to get logs from node node-3 pod pod-74df4da8-3e85-41d3-8e69-22e21de9fc43 container test-container: <nil>
    STEP: delete the pod 09/08/23 21:17:17.56
    Sep  8 21:17:17.593: INFO: Waiting for pod pod-74df4da8-3e85-41d3-8e69-22e21de9fc43 to disappear
    Sep  8 21:17:17.609: INFO: Pod pod-74df4da8-3e85-41d3-8e69-22e21de9fc43 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:17.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6257" for this suite. 09/08/23 21:17:17.628
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:17.65
Sep  8 21:17:17.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename proxy 09/08/23 21:17:17.653
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:17.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:17.709
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 09/08/23 21:17:17.762
STEP: creating replication controller proxy-service-p5zdp in namespace proxy-6444 09/08/23 21:17:17.762
I0908 21:17:17.810059      23 runners.go:193] Created replication controller with name: proxy-service-p5zdp, namespace: proxy-6444, replica count: 1
I0908 21:17:18.861503      23 runners.go:193] proxy-service-p5zdp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0908 21:17:19.862631      23 runners.go:193] proxy-service-p5zdp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0908 21:17:20.863214      23 runners.go:193] proxy-service-p5zdp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 21:17:20.872: INFO: setup took 3.156893305s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/08/23 21:17:20.872
Sep  8 21:17:20.903: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 31.187665ms)
Sep  8 21:17:20.904: INFO: (0) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 31.223662ms)
Sep  8 21:17:20.904: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 31.47429ms)
Sep  8 21:17:20.904: INFO: (0) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 31.594425ms)
Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 35.756904ms)
Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 35.970662ms)
Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 35.922634ms)
Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 35.845881ms)
Sep  8 21:17:20.913: INFO: (0) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 40.835314ms)
Sep  8 21:17:20.913: INFO: (0) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 40.839761ms)
Sep  8 21:17:20.913: INFO: (0) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.984462ms)
Sep  8 21:17:20.914: INFO: (0) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 41.838074ms)
Sep  8 21:17:20.931: INFO: (0) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 58.734033ms)
Sep  8 21:17:20.931: INFO: (0) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 58.957469ms)
Sep  8 21:17:20.931: INFO: (0) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 58.693798ms)
Sep  8 21:17:20.932: INFO: (0) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 59.709755ms)
Sep  8 21:17:20.946: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 13.552547ms)
Sep  8 21:17:20.949: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 15.060151ms)
Sep  8 21:17:20.950: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 17.835592ms)
Sep  8 21:17:20.951: INFO: (1) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 17.836623ms)
Sep  8 21:17:20.952: INFO: (1) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 19.562575ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 35.093185ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 35.801858ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 35.632591ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 35.930578ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 35.764608ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 35.347819ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 35.483964ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 36.441891ms)
Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 36.366161ms)
Sep  8 21:17:20.972: INFO: (1) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 38.76557ms)
Sep  8 21:17:20.972: INFO: (1) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 39.492696ms)
Sep  8 21:17:20.985: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 12.664339ms)
Sep  8 21:17:20.986: INFO: (2) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 13.348196ms)
Sep  8 21:17:20.986: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 13.393509ms)
Sep  8 21:17:20.987: INFO: (2) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 14.848516ms)
Sep  8 21:17:20.988: INFO: (2) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 14.88864ms)
Sep  8 21:17:20.988: INFO: (2) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.653077ms)
Sep  8 21:17:20.995: INFO: (2) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.108558ms)
Sep  8 21:17:20.996: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 22.926073ms)
Sep  8 21:17:20.997: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 24.54173ms)
Sep  8 21:17:20.997: INFO: (2) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 24.508507ms)
Sep  8 21:17:20.997: INFO: (2) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 24.398542ms)
Sep  8 21:17:20.998: INFO: (2) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 24.728438ms)
Sep  8 21:17:20.998: INFO: (2) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 24.84179ms)
Sep  8 21:17:21.002: INFO: (2) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 29.612855ms)
Sep  8 21:17:21.003: INFO: (2) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 30.301621ms)
Sep  8 21:17:21.005: INFO: (2) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 32.59427ms)
Sep  8 21:17:21.022: INFO: (3) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 16.18414ms)
Sep  8 21:17:21.022: INFO: (3) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 16.284456ms)
Sep  8 21:17:21.022: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 16.744235ms)
Sep  8 21:17:21.024: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 18.317872ms)
Sep  8 21:17:21.024: INFO: (3) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 18.219169ms)
Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 25.832119ms)
Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 25.879326ms)
Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 26.055975ms)
Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 26.027382ms)
Sep  8 21:17:21.033: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 27.173181ms)
Sep  8 21:17:21.033: INFO: (3) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 27.366953ms)
Sep  8 21:17:21.034: INFO: (3) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 28.284716ms)
Sep  8 21:17:21.038: INFO: (3) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 31.946882ms)
Sep  8 21:17:21.038: INFO: (3) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 31.88199ms)
Sep  8 21:17:21.038: INFO: (3) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 32.17104ms)
Sep  8 21:17:21.045: INFO: (3) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 38.982094ms)
Sep  8 21:17:21.056: INFO: (4) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 10.555163ms)
Sep  8 21:17:21.063: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 16.357913ms)
Sep  8 21:17:21.066: INFO: (4) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 18.586313ms)
Sep  8 21:17:21.067: INFO: (4) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 21.255806ms)
Sep  8 21:17:21.067: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.76453ms)
Sep  8 21:17:21.069: INFO: (4) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 21.840717ms)
Sep  8 21:17:21.069: INFO: (4) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 23.514241ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 32.644053ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.567951ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 34.341091ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 33.189842ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.317554ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.975352ms)
Sep  8 21:17:21.079: INFO: (4) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 32.654723ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 34.35656ms)
Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 34.978912ms)
Sep  8 21:17:21.090: INFO: (5) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 9.383326ms)
Sep  8 21:17:21.093: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 11.89834ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.501722ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 22.662943ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 22.588373ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 22.913419ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 22.770564ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 23.192151ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 23.277249ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 23.319498ms)
Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 23.328084ms)
Sep  8 21:17:21.105: INFO: (5) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 24.114412ms)
Sep  8 21:17:21.111: INFO: (5) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 29.593699ms)
Sep  8 21:17:21.111: INFO: (5) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 29.594911ms)
Sep  8 21:17:21.111: INFO: (5) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 29.740935ms)
Sep  8 21:17:21.112: INFO: (5) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 30.966232ms)
Sep  8 21:17:21.128: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 15.476078ms)
Sep  8 21:17:21.128: INFO: (6) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 15.329714ms)
Sep  8 21:17:21.140: INFO: (6) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 27.740872ms)
Sep  8 21:17:21.140: INFO: (6) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 27.667985ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 32.256539ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 32.807498ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 32.333553ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.327301ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 32.460101ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.622572ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.742176ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 32.724023ms)
Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.570576ms)
Sep  8 21:17:21.149: INFO: (6) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 36.045662ms)
Sep  8 21:17:21.154: INFO: (6) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 41.587536ms)
Sep  8 21:17:21.156: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 43.973571ms)
Sep  8 21:17:21.173: INFO: (7) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 16.294456ms)
Sep  8 21:17:21.183: INFO: (7) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 25.393889ms)
Sep  8 21:17:21.183: INFO: (7) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.453001ms)
Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 33.808728ms)
Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 34.077839ms)
Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 34.085584ms)
Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 34.046ms)
Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 34.117754ms)
Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 35.022303ms)
Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 34.909743ms)
Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 34.813362ms)
Sep  8 21:17:21.194: INFO: (7) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 37.01952ms)
Sep  8 21:17:21.195: INFO: (7) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 37.59753ms)
Sep  8 21:17:21.198: INFO: (7) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.381556ms)
Sep  8 21:17:21.198: INFO: (7) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 40.439134ms)
Sep  8 21:17:21.199: INFO: (7) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 41.638442ms)
Sep  8 21:17:21.213: INFO: (8) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 13.704139ms)
Sep  8 21:17:21.223: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 24.333491ms)
Sep  8 21:17:21.224: INFO: (8) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 24.470596ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 25.341692ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 25.484119ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 25.573706ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 25.480672ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 25.654055ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 25.601006ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 25.54378ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.608821ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 25.514796ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 25.620785ms)
Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 26.057368ms)
Sep  8 21:17:21.229: INFO: (8) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 30.164165ms)
Sep  8 21:17:21.230: INFO: (8) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 30.848182ms)
Sep  8 21:17:21.240: INFO: (9) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 9.864653ms)
Sep  8 21:17:21.247: INFO: (9) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 16.361791ms)
Sep  8 21:17:21.252: INFO: (9) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 21.965591ms)
Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 22.328318ms)
Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.165604ms)
Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 22.736069ms)
Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 22.99899ms)
Sep  8 21:17:21.256: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.333347ms)
Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 26.614679ms)
Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 26.740635ms)
Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 26.79226ms)
Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 27.060901ms)
Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 26.832696ms)
Sep  8 21:17:21.260: INFO: (9) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 29.712181ms)
Sep  8 21:17:21.260: INFO: (9) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 30.19376ms)
Sep  8 21:17:21.262: INFO: (9) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 31.555321ms)
Sep  8 21:17:21.285: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 22.098148ms)
Sep  8 21:17:21.287: INFO: (10) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.000917ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 26.246641ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 26.161703ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 26.189225ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 26.196397ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 26.341329ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 27.009015ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 26.517687ms)
Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 26.607987ms)
Sep  8 21:17:21.295: INFO: (10) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 32.298527ms)
Sep  8 21:17:21.295: INFO: (10) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.581717ms)
Sep  8 21:17:21.297: INFO: (10) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 34.343705ms)
Sep  8 21:17:21.302: INFO: (10) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.113295ms)
Sep  8 21:17:21.305: INFO: (10) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 42.04976ms)
Sep  8 21:17:21.305: INFO: (10) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 42.143945ms)
Sep  8 21:17:21.326: INFO: (11) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.178425ms)
Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 22.138122ms)
Sep  8 21:17:21.327: INFO: (11) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.487173ms)
Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 22.131801ms)
Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 22.513513ms)
Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 23.155451ms)
Sep  8 21:17:21.329: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 23.561168ms)
Sep  8 21:17:21.329: INFO: (11) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 23.258214ms)
Sep  8 21:17:21.329: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 23.257894ms)
Sep  8 21:17:21.330: INFO: (11) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 24.309686ms)
Sep  8 21:17:21.330: INFO: (11) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 24.31171ms)
Sep  8 21:17:21.330: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 25.147671ms)
Sep  8 21:17:21.332: INFO: (11) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 26.889682ms)
Sep  8 21:17:21.332: INFO: (11) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 27.153203ms)
Sep  8 21:17:21.332: INFO: (11) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 27.015167ms)
Sep  8 21:17:21.338: INFO: (11) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.229118ms)
Sep  8 21:17:21.353: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 14.7488ms)
Sep  8 21:17:21.360: INFO: (12) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 20.918205ms)
Sep  8 21:17:21.360: INFO: (12) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 21.140199ms)
Sep  8 21:17:21.360: INFO: (12) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 21.139018ms)
Sep  8 21:17:21.361: INFO: (12) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 22.553639ms)
Sep  8 21:17:21.361: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.78979ms)
Sep  8 21:17:21.363: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 24.584719ms)
Sep  8 21:17:21.363: INFO: (12) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 23.250228ms)
Sep  8 21:17:21.366: INFO: (12) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 27.274541ms)
Sep  8 21:17:21.366: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 26.67937ms)
Sep  8 21:17:21.367: INFO: (12) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 27.903604ms)
Sep  8 21:17:21.372: INFO: (12) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.671385ms)
Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 36.714891ms)
Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 36.662785ms)
Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 38.078367ms)
Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 36.825188ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 17.198203ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 18.780656ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 18.460389ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 17.272631ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 18.298295ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 18.478723ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 18.441092ms)
Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 18.338791ms)
Sep  8 21:17:21.399: INFO: (13) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 22.051132ms)
Sep  8 21:17:21.399: INFO: (13) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 22.811831ms)
Sep  8 21:17:21.400: INFO: (13) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 23.069663ms)
Sep  8 21:17:21.420: INFO: (13) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 42.480054ms)
Sep  8 21:17:21.420: INFO: (13) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 42.449777ms)
Sep  8 21:17:21.421: INFO: (13) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 43.483377ms)
Sep  8 21:17:21.421: INFO: (13) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 42.768141ms)
Sep  8 21:17:21.421: INFO: (13) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 43.650308ms)
Sep  8 21:17:21.442: INFO: (14) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 20.506828ms)
Sep  8 21:17:21.442: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 20.71718ms)
Sep  8 21:17:21.442: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.797802ms)
Sep  8 21:17:21.448: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 27.02788ms)
Sep  8 21:17:21.449: INFO: (14) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 27.278278ms)
Sep  8 21:17:21.451: INFO: (14) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 29.237584ms)
Sep  8 21:17:21.451: INFO: (14) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 29.669441ms)
Sep  8 21:17:21.451: INFO: (14) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 29.993165ms)
Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 33.229726ms)
Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 33.192938ms)
Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 33.367575ms)
Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 34.040751ms)
Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 37.051439ms)
Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 36.959388ms)
Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 36.890821ms)
Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 36.836618ms)
Sep  8 21:17:21.471: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 12.861087ms)
Sep  8 21:17:21.471: INFO: (15) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 12.646887ms)
Sep  8 21:17:21.482: INFO: (15) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 22.974193ms)
Sep  8 21:17:21.482: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 23.817258ms)
Sep  8 21:17:21.486: INFO: (15) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 27.469884ms)
Sep  8 21:17:21.486: INFO: (15) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 27.605407ms)
Sep  8 21:17:21.491: INFO: (15) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.193701ms)
Sep  8 21:17:21.491: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 32.606443ms)
Sep  8 21:17:21.491: INFO: (15) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 32.748468ms)
Sep  8 21:17:21.492: INFO: (15) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 33.326115ms)
Sep  8 21:17:21.495: INFO: (15) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 36.415944ms)
Sep  8 21:17:21.499: INFO: (15) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 40.506709ms)
Sep  8 21:17:21.499: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 40.720668ms)
Sep  8 21:17:21.503: INFO: (15) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 43.821327ms)
Sep  8 21:17:21.504: INFO: (15) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 45.46801ms)
Sep  8 21:17:21.513: INFO: (15) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 53.971463ms)
Sep  8 21:17:21.528: INFO: (16) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 14.852543ms)
Sep  8 21:17:21.529: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.312131ms)
Sep  8 21:17:21.529: INFO: (16) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.267368ms)
Sep  8 21:17:21.532: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 17.635939ms)
Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 30.161017ms)
Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 29.93701ms)
Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 29.305792ms)
Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 29.195545ms)
Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 29.544898ms)
Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 29.909149ms)
Sep  8 21:17:21.544: INFO: (16) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 30.117246ms)
Sep  8 21:17:21.546: INFO: (16) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 32.675401ms)
Sep  8 21:17:21.547: INFO: (16) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 32.932352ms)
Sep  8 21:17:21.554: INFO: (16) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 40.415318ms)
Sep  8 21:17:21.555: INFO: (16) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.932634ms)
Sep  8 21:17:21.562: INFO: (16) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 48.216521ms)
Sep  8 21:17:21.577: INFO: (17) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 14.59305ms)
Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 16.730409ms)
Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 16.447952ms)
Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 16.245794ms)
Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 16.929189ms)
Sep  8 21:17:21.584: INFO: (17) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 20.705298ms)
Sep  8 21:17:21.584: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 21.170396ms)
Sep  8 21:17:21.584: INFO: (17) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.662728ms)
Sep  8 21:17:21.588: INFO: (17) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 25.757779ms)
Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 33.367684ms)
Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 33.742845ms)
Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 34.70423ms)
Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 33.943048ms)
Sep  8 21:17:21.598: INFO: (17) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 34.14274ms)
Sep  8 21:17:21.602: INFO: (17) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 39.13548ms)
Sep  8 21:17:21.603: INFO: (17) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 40.926874ms)
Sep  8 21:17:21.615: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 12.122649ms)
Sep  8 21:17:21.616: INFO: (18) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 13.352144ms)
Sep  8 21:17:21.619: INFO: (18) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.156791ms)
Sep  8 21:17:21.619: INFO: (18) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 15.440792ms)
Sep  8 21:17:21.620: INFO: (18) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 16.742582ms)
Sep  8 21:17:21.621: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 17.591506ms)
Sep  8 21:17:21.621: INFO: (18) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 17.11719ms)
Sep  8 21:17:21.622: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 18.205091ms)
Sep  8 21:17:21.622: INFO: (18) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 19.024642ms)
Sep  8 21:17:21.625: INFO: (18) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 22.055729ms)
Sep  8 21:17:21.625: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.100713ms)
Sep  8 21:17:21.626: INFO: (18) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 22.431871ms)
Sep  8 21:17:21.628: INFO: (18) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 24.529486ms)
Sep  8 21:17:21.629: INFO: (18) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 24.973306ms)
Sep  8 21:17:21.629: INFO: (18) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 25.095373ms)
Sep  8 21:17:21.629: INFO: (18) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 26.814161ms)
Sep  8 21:17:21.642: INFO: (19) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 11.557794ms)
Sep  8 21:17:21.642: INFO: (19) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 11.823921ms)
Sep  8 21:17:21.642: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 12.157143ms)
Sep  8 21:17:21.643: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 13.621044ms)
Sep  8 21:17:21.648: INFO: (19) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 18.669969ms)
Sep  8 21:17:21.649: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 19.228753ms)
Sep  8 21:17:21.649: INFO: (19) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 19.965548ms)
Sep  8 21:17:21.652: INFO: (19) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 22.628579ms)
Sep  8 21:17:21.654: INFO: (19) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 24.273298ms)
Sep  8 21:17:21.662: INFO: (19) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.017483ms)
Sep  8 21:17:21.663: INFO: (19) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 32.60502ms)
Sep  8 21:17:21.663: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.459237ms)
Sep  8 21:17:21.663: INFO: (19) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 32.76047ms)
Sep  8 21:17:21.667: INFO: (19) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 36.527472ms)
Sep  8 21:17:21.671: INFO: (19) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 41.398825ms)
Sep  8 21:17:21.674: INFO: (19) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 44.12318ms)
STEP: deleting ReplicationController proxy-service-p5zdp in namespace proxy-6444, will wait for the garbage collector to delete the pods 09/08/23 21:17:21.674
Sep  8 21:17:21.754: INFO: Deleting ReplicationController proxy-service-p5zdp took: 18.558502ms
Sep  8 21:17:21.856: INFO: Terminating ReplicationController proxy-service-p5zdp pods took: 101.321937ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:23.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6444" for this suite. 09/08/23 21:17:23.58
------------------------------
â€¢ [SLOW TEST] [5.954 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:17.65
    Sep  8 21:17:17.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename proxy 09/08/23 21:17:17.653
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:17.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:17.709
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 09/08/23 21:17:17.762
    STEP: creating replication controller proxy-service-p5zdp in namespace proxy-6444 09/08/23 21:17:17.762
    I0908 21:17:17.810059      23 runners.go:193] Created replication controller with name: proxy-service-p5zdp, namespace: proxy-6444, replica count: 1
    I0908 21:17:18.861503      23 runners.go:193] proxy-service-p5zdp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0908 21:17:19.862631      23 runners.go:193] proxy-service-p5zdp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0908 21:17:20.863214      23 runners.go:193] proxy-service-p5zdp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 21:17:20.872: INFO: setup took 3.156893305s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 09/08/23 21:17:20.872
    Sep  8 21:17:20.903: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 31.187665ms)
    Sep  8 21:17:20.904: INFO: (0) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 31.223662ms)
    Sep  8 21:17:20.904: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 31.47429ms)
    Sep  8 21:17:20.904: INFO: (0) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 31.594425ms)
    Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 35.756904ms)
    Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 35.970662ms)
    Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 35.922634ms)
    Sep  8 21:17:20.908: INFO: (0) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 35.845881ms)
    Sep  8 21:17:20.913: INFO: (0) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 40.835314ms)
    Sep  8 21:17:20.913: INFO: (0) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 40.839761ms)
    Sep  8 21:17:20.913: INFO: (0) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.984462ms)
    Sep  8 21:17:20.914: INFO: (0) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 41.838074ms)
    Sep  8 21:17:20.931: INFO: (0) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 58.734033ms)
    Sep  8 21:17:20.931: INFO: (0) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 58.957469ms)
    Sep  8 21:17:20.931: INFO: (0) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 58.693798ms)
    Sep  8 21:17:20.932: INFO: (0) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 59.709755ms)
    Sep  8 21:17:20.946: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 13.552547ms)
    Sep  8 21:17:20.949: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 15.060151ms)
    Sep  8 21:17:20.950: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 17.835592ms)
    Sep  8 21:17:20.951: INFO: (1) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 17.836623ms)
    Sep  8 21:17:20.952: INFO: (1) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 19.562575ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 35.093185ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 35.801858ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 35.632591ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 35.930578ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 35.764608ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 35.347819ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 35.483964ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 36.441891ms)
    Sep  8 21:17:20.969: INFO: (1) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 36.366161ms)
    Sep  8 21:17:20.972: INFO: (1) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 38.76557ms)
    Sep  8 21:17:20.972: INFO: (1) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 39.492696ms)
    Sep  8 21:17:20.985: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 12.664339ms)
    Sep  8 21:17:20.986: INFO: (2) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 13.348196ms)
    Sep  8 21:17:20.986: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 13.393509ms)
    Sep  8 21:17:20.987: INFO: (2) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 14.848516ms)
    Sep  8 21:17:20.988: INFO: (2) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 14.88864ms)
    Sep  8 21:17:20.988: INFO: (2) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.653077ms)
    Sep  8 21:17:20.995: INFO: (2) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.108558ms)
    Sep  8 21:17:20.996: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 22.926073ms)
    Sep  8 21:17:20.997: INFO: (2) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 24.54173ms)
    Sep  8 21:17:20.997: INFO: (2) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 24.508507ms)
    Sep  8 21:17:20.997: INFO: (2) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 24.398542ms)
    Sep  8 21:17:20.998: INFO: (2) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 24.728438ms)
    Sep  8 21:17:20.998: INFO: (2) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 24.84179ms)
    Sep  8 21:17:21.002: INFO: (2) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 29.612855ms)
    Sep  8 21:17:21.003: INFO: (2) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 30.301621ms)
    Sep  8 21:17:21.005: INFO: (2) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 32.59427ms)
    Sep  8 21:17:21.022: INFO: (3) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 16.18414ms)
    Sep  8 21:17:21.022: INFO: (3) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 16.284456ms)
    Sep  8 21:17:21.022: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 16.744235ms)
    Sep  8 21:17:21.024: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 18.317872ms)
    Sep  8 21:17:21.024: INFO: (3) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 18.219169ms)
    Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 25.832119ms)
    Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 25.879326ms)
    Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 26.055975ms)
    Sep  8 21:17:21.032: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 26.027382ms)
    Sep  8 21:17:21.033: INFO: (3) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 27.173181ms)
    Sep  8 21:17:21.033: INFO: (3) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 27.366953ms)
    Sep  8 21:17:21.034: INFO: (3) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 28.284716ms)
    Sep  8 21:17:21.038: INFO: (3) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 31.946882ms)
    Sep  8 21:17:21.038: INFO: (3) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 31.88199ms)
    Sep  8 21:17:21.038: INFO: (3) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 32.17104ms)
    Sep  8 21:17:21.045: INFO: (3) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 38.982094ms)
    Sep  8 21:17:21.056: INFO: (4) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 10.555163ms)
    Sep  8 21:17:21.063: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 16.357913ms)
    Sep  8 21:17:21.066: INFO: (4) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 18.586313ms)
    Sep  8 21:17:21.067: INFO: (4) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 21.255806ms)
    Sep  8 21:17:21.067: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.76453ms)
    Sep  8 21:17:21.069: INFO: (4) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 21.840717ms)
    Sep  8 21:17:21.069: INFO: (4) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 23.514241ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 32.644053ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.567951ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 34.341091ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 33.189842ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.317554ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.975352ms)
    Sep  8 21:17:21.079: INFO: (4) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 32.654723ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 34.35656ms)
    Sep  8 21:17:21.080: INFO: (4) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 34.978912ms)
    Sep  8 21:17:21.090: INFO: (5) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 9.383326ms)
    Sep  8 21:17:21.093: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 11.89834ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.501722ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 22.662943ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 22.588373ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 22.913419ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 22.770564ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 23.192151ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 23.277249ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 23.319498ms)
    Sep  8 21:17:21.104: INFO: (5) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 23.328084ms)
    Sep  8 21:17:21.105: INFO: (5) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 24.114412ms)
    Sep  8 21:17:21.111: INFO: (5) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 29.593699ms)
    Sep  8 21:17:21.111: INFO: (5) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 29.594911ms)
    Sep  8 21:17:21.111: INFO: (5) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 29.740935ms)
    Sep  8 21:17:21.112: INFO: (5) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 30.966232ms)
    Sep  8 21:17:21.128: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 15.476078ms)
    Sep  8 21:17:21.128: INFO: (6) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 15.329714ms)
    Sep  8 21:17:21.140: INFO: (6) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 27.740872ms)
    Sep  8 21:17:21.140: INFO: (6) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 27.667985ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 32.256539ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 32.807498ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 32.333553ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.327301ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 32.460101ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.622572ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.742176ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 32.724023ms)
    Sep  8 21:17:21.145: INFO: (6) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.570576ms)
    Sep  8 21:17:21.149: INFO: (6) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 36.045662ms)
    Sep  8 21:17:21.154: INFO: (6) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 41.587536ms)
    Sep  8 21:17:21.156: INFO: (6) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 43.973571ms)
    Sep  8 21:17:21.173: INFO: (7) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 16.294456ms)
    Sep  8 21:17:21.183: INFO: (7) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 25.393889ms)
    Sep  8 21:17:21.183: INFO: (7) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.453001ms)
    Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 33.808728ms)
    Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 34.077839ms)
    Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 34.085584ms)
    Sep  8 21:17:21.191: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 34.046ms)
    Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 34.117754ms)
    Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 35.022303ms)
    Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 34.909743ms)
    Sep  8 21:17:21.192: INFO: (7) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 34.813362ms)
    Sep  8 21:17:21.194: INFO: (7) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 37.01952ms)
    Sep  8 21:17:21.195: INFO: (7) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 37.59753ms)
    Sep  8 21:17:21.198: INFO: (7) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.381556ms)
    Sep  8 21:17:21.198: INFO: (7) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 40.439134ms)
    Sep  8 21:17:21.199: INFO: (7) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 41.638442ms)
    Sep  8 21:17:21.213: INFO: (8) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 13.704139ms)
    Sep  8 21:17:21.223: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 24.333491ms)
    Sep  8 21:17:21.224: INFO: (8) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 24.470596ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 25.341692ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 25.484119ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 25.573706ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 25.480672ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 25.654055ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 25.601006ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 25.54378ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.608821ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 25.514796ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 25.620785ms)
    Sep  8 21:17:21.225: INFO: (8) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 26.057368ms)
    Sep  8 21:17:21.229: INFO: (8) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 30.164165ms)
    Sep  8 21:17:21.230: INFO: (8) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 30.848182ms)
    Sep  8 21:17:21.240: INFO: (9) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 9.864653ms)
    Sep  8 21:17:21.247: INFO: (9) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 16.361791ms)
    Sep  8 21:17:21.252: INFO: (9) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 21.965591ms)
    Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 22.328318ms)
    Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.165604ms)
    Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 22.736069ms)
    Sep  8 21:17:21.253: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 22.99899ms)
    Sep  8 21:17:21.256: INFO: (9) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.333347ms)
    Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 26.614679ms)
    Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 26.740635ms)
    Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 26.79226ms)
    Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 27.060901ms)
    Sep  8 21:17:21.257: INFO: (9) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 26.832696ms)
    Sep  8 21:17:21.260: INFO: (9) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 29.712181ms)
    Sep  8 21:17:21.260: INFO: (9) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 30.19376ms)
    Sep  8 21:17:21.262: INFO: (9) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 31.555321ms)
    Sep  8 21:17:21.285: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 22.098148ms)
    Sep  8 21:17:21.287: INFO: (10) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 25.000917ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 26.246641ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 26.161703ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 26.189225ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 26.196397ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 26.341329ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 27.009015ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 26.517687ms)
    Sep  8 21:17:21.289: INFO: (10) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 26.607987ms)
    Sep  8 21:17:21.295: INFO: (10) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 32.298527ms)
    Sep  8 21:17:21.295: INFO: (10) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.581717ms)
    Sep  8 21:17:21.297: INFO: (10) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 34.343705ms)
    Sep  8 21:17:21.302: INFO: (10) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.113295ms)
    Sep  8 21:17:21.305: INFO: (10) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 42.04976ms)
    Sep  8 21:17:21.305: INFO: (10) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 42.143945ms)
    Sep  8 21:17:21.326: INFO: (11) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.178425ms)
    Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 22.138122ms)
    Sep  8 21:17:21.327: INFO: (11) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.487173ms)
    Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 22.131801ms)
    Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 22.513513ms)
    Sep  8 21:17:21.328: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 23.155451ms)
    Sep  8 21:17:21.329: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 23.561168ms)
    Sep  8 21:17:21.329: INFO: (11) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 23.258214ms)
    Sep  8 21:17:21.329: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 23.257894ms)
    Sep  8 21:17:21.330: INFO: (11) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 24.309686ms)
    Sep  8 21:17:21.330: INFO: (11) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 24.31171ms)
    Sep  8 21:17:21.330: INFO: (11) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 25.147671ms)
    Sep  8 21:17:21.332: INFO: (11) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 26.889682ms)
    Sep  8 21:17:21.332: INFO: (11) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 27.153203ms)
    Sep  8 21:17:21.332: INFO: (11) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 27.015167ms)
    Sep  8 21:17:21.338: INFO: (11) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.229118ms)
    Sep  8 21:17:21.353: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 14.7488ms)
    Sep  8 21:17:21.360: INFO: (12) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 20.918205ms)
    Sep  8 21:17:21.360: INFO: (12) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 21.140199ms)
    Sep  8 21:17:21.360: INFO: (12) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 21.139018ms)
    Sep  8 21:17:21.361: INFO: (12) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 22.553639ms)
    Sep  8 21:17:21.361: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.78979ms)
    Sep  8 21:17:21.363: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 24.584719ms)
    Sep  8 21:17:21.363: INFO: (12) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 23.250228ms)
    Sep  8 21:17:21.366: INFO: (12) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 27.274541ms)
    Sep  8 21:17:21.366: INFO: (12) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 26.67937ms)
    Sep  8 21:17:21.367: INFO: (12) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 27.903604ms)
    Sep  8 21:17:21.372: INFO: (12) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.671385ms)
    Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 36.714891ms)
    Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 36.662785ms)
    Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 38.078367ms)
    Sep  8 21:17:21.376: INFO: (12) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 36.825188ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 17.198203ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 18.780656ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 18.460389ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 17.272631ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 18.298295ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 18.478723ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 18.441092ms)
    Sep  8 21:17:21.395: INFO: (13) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 18.338791ms)
    Sep  8 21:17:21.399: INFO: (13) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 22.051132ms)
    Sep  8 21:17:21.399: INFO: (13) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 22.811831ms)
    Sep  8 21:17:21.400: INFO: (13) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 23.069663ms)
    Sep  8 21:17:21.420: INFO: (13) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 42.480054ms)
    Sep  8 21:17:21.420: INFO: (13) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 42.449777ms)
    Sep  8 21:17:21.421: INFO: (13) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 43.483377ms)
    Sep  8 21:17:21.421: INFO: (13) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 42.768141ms)
    Sep  8 21:17:21.421: INFO: (13) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 43.650308ms)
    Sep  8 21:17:21.442: INFO: (14) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 20.506828ms)
    Sep  8 21:17:21.442: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 20.71718ms)
    Sep  8 21:17:21.442: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.797802ms)
    Sep  8 21:17:21.448: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 27.02788ms)
    Sep  8 21:17:21.449: INFO: (14) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 27.278278ms)
    Sep  8 21:17:21.451: INFO: (14) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 29.237584ms)
    Sep  8 21:17:21.451: INFO: (14) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 29.669441ms)
    Sep  8 21:17:21.451: INFO: (14) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 29.993165ms)
    Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 33.229726ms)
    Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 33.192938ms)
    Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 33.367575ms)
    Sep  8 21:17:21.455: INFO: (14) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 34.040751ms)
    Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 37.051439ms)
    Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 36.959388ms)
    Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 36.890821ms)
    Sep  8 21:17:21.458: INFO: (14) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 36.836618ms)
    Sep  8 21:17:21.471: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 12.861087ms)
    Sep  8 21:17:21.471: INFO: (15) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 12.646887ms)
    Sep  8 21:17:21.482: INFO: (15) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 22.974193ms)
    Sep  8 21:17:21.482: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 23.817258ms)
    Sep  8 21:17:21.486: INFO: (15) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 27.469884ms)
    Sep  8 21:17:21.486: INFO: (15) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 27.605407ms)
    Sep  8 21:17:21.491: INFO: (15) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 32.193701ms)
    Sep  8 21:17:21.491: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 32.606443ms)
    Sep  8 21:17:21.491: INFO: (15) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 32.748468ms)
    Sep  8 21:17:21.492: INFO: (15) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 33.326115ms)
    Sep  8 21:17:21.495: INFO: (15) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 36.415944ms)
    Sep  8 21:17:21.499: INFO: (15) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 40.506709ms)
    Sep  8 21:17:21.499: INFO: (15) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 40.720668ms)
    Sep  8 21:17:21.503: INFO: (15) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 43.821327ms)
    Sep  8 21:17:21.504: INFO: (15) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 45.46801ms)
    Sep  8 21:17:21.513: INFO: (15) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 53.971463ms)
    Sep  8 21:17:21.528: INFO: (16) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 14.852543ms)
    Sep  8 21:17:21.529: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.312131ms)
    Sep  8 21:17:21.529: INFO: (16) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.267368ms)
    Sep  8 21:17:21.532: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 17.635939ms)
    Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 30.161017ms)
    Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 29.93701ms)
    Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 29.305792ms)
    Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 29.195545ms)
    Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 29.544898ms)
    Sep  8 21:17:21.543: INFO: (16) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 29.909149ms)
    Sep  8 21:17:21.544: INFO: (16) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 30.117246ms)
    Sep  8 21:17:21.546: INFO: (16) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 32.675401ms)
    Sep  8 21:17:21.547: INFO: (16) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 32.932352ms)
    Sep  8 21:17:21.554: INFO: (16) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 40.415318ms)
    Sep  8 21:17:21.555: INFO: (16) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 40.932634ms)
    Sep  8 21:17:21.562: INFO: (16) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 48.216521ms)
    Sep  8 21:17:21.577: INFO: (17) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 14.59305ms)
    Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 16.730409ms)
    Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 16.447952ms)
    Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 16.245794ms)
    Sep  8 21:17:21.579: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 16.929189ms)
    Sep  8 21:17:21.584: INFO: (17) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 20.705298ms)
    Sep  8 21:17:21.584: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 21.170396ms)
    Sep  8 21:17:21.584: INFO: (17) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 20.662728ms)
    Sep  8 21:17:21.588: INFO: (17) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 25.757779ms)
    Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 33.367684ms)
    Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 33.742845ms)
    Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 34.70423ms)
    Sep  8 21:17:21.597: INFO: (17) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 33.943048ms)
    Sep  8 21:17:21.598: INFO: (17) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 34.14274ms)
    Sep  8 21:17:21.602: INFO: (17) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 39.13548ms)
    Sep  8 21:17:21.603: INFO: (17) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 40.926874ms)
    Sep  8 21:17:21.615: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 12.122649ms)
    Sep  8 21:17:21.616: INFO: (18) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 13.352144ms)
    Sep  8 21:17:21.619: INFO: (18) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 15.156791ms)
    Sep  8 21:17:21.619: INFO: (18) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 15.440792ms)
    Sep  8 21:17:21.620: INFO: (18) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 16.742582ms)
    Sep  8 21:17:21.621: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 17.591506ms)
    Sep  8 21:17:21.621: INFO: (18) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 17.11719ms)
    Sep  8 21:17:21.622: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 18.205091ms)
    Sep  8 21:17:21.622: INFO: (18) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 19.024642ms)
    Sep  8 21:17:21.625: INFO: (18) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 22.055729ms)
    Sep  8 21:17:21.625: INFO: (18) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 22.100713ms)
    Sep  8 21:17:21.626: INFO: (18) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 22.431871ms)
    Sep  8 21:17:21.628: INFO: (18) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 24.529486ms)
    Sep  8 21:17:21.629: INFO: (18) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 24.973306ms)
    Sep  8 21:17:21.629: INFO: (18) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 25.095373ms)
    Sep  8 21:17:21.629: INFO: (18) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 26.814161ms)
    Sep  8 21:17:21.642: INFO: (19) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:462/proxy/: tls qux (200; 11.557794ms)
    Sep  8 21:17:21.642: INFO: (19) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 11.823921ms)
    Sep  8 21:17:21.642: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h/proxy/rewriteme">test</a> (200; 12.157143ms)
    Sep  8 21:17:21.643: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:160/proxy/: foo (200; 13.621044ms)
    Sep  8 21:17:21.648: INFO: (19) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 18.669969ms)
    Sep  8 21:17:21.649: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">test<... (200; 19.228753ms)
    Sep  8 21:17:21.649: INFO: (19) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:443/proxy/tlsrewritem... (200; 19.965548ms)
    Sep  8 21:17:21.652: INFO: (19) /api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/: <a href="/api/v1/namespaces/proxy-6444/pods/http:proxy-service-p5zdp-7g75h:1080/proxy/rewriteme">... (200; 22.628579ms)
    Sep  8 21:17:21.654: INFO: (19) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname2/proxy/: tls qux (200; 24.273298ms)
    Sep  8 21:17:21.662: INFO: (19) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname2/proxy/: bar (200; 32.017483ms)
    Sep  8 21:17:21.663: INFO: (19) /api/v1/namespaces/proxy-6444/services/https:proxy-service-p5zdp:tlsportname1/proxy/: tls baz (200; 32.60502ms)
    Sep  8 21:17:21.663: INFO: (19) /api/v1/namespaces/proxy-6444/pods/proxy-service-p5zdp-7g75h:162/proxy/: bar (200; 32.459237ms)
    Sep  8 21:17:21.663: INFO: (19) /api/v1/namespaces/proxy-6444/pods/https:proxy-service-p5zdp-7g75h:460/proxy/: tls baz (200; 32.76047ms)
    Sep  8 21:17:21.667: INFO: (19) /api/v1/namespaces/proxy-6444/services/http:proxy-service-p5zdp:portname1/proxy/: foo (200; 36.527472ms)
    Sep  8 21:17:21.671: INFO: (19) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname2/proxy/: bar (200; 41.398825ms)
    Sep  8 21:17:21.674: INFO: (19) /api/v1/namespaces/proxy-6444/services/proxy-service-p5zdp:portname1/proxy/: foo (200; 44.12318ms)
    STEP: deleting ReplicationController proxy-service-p5zdp in namespace proxy-6444, will wait for the garbage collector to delete the pods 09/08/23 21:17:21.674
    Sep  8 21:17:21.754: INFO: Deleting ReplicationController proxy-service-p5zdp took: 18.558502ms
    Sep  8 21:17:21.856: INFO: Terminating ReplicationController proxy-service-p5zdp pods took: 101.321937ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:23.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6444" for this suite. 09/08/23 21:17:23.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:23.613
Sep  8 21:17:23.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:17:23.615
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:23.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:23.679
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 09/08/23 21:17:23.685
Sep  8 21:17:23.713: INFO: Waiting up to 5m0s for pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec" in namespace "emptydir-9161" to be "Succeeded or Failed"
Sep  8 21:17:23.725: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.633575ms
Sep  8 21:17:25.748: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034794414s
Sep  8 21:17:27.735: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021962707s
STEP: Saw pod success 09/08/23 21:17:27.735
Sep  8 21:17:27.735: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec" satisfied condition "Succeeded or Failed"
Sep  8 21:17:27.746: INFO: Trying to get logs from node node-3 pod pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec container test-container: <nil>
STEP: delete the pod 09/08/23 21:17:27.762
Sep  8 21:17:27.817: INFO: Waiting for pod pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec to disappear
Sep  8 21:17:27.841: INFO: Pod pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:27.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9161" for this suite. 09/08/23 21:17:27.857
------------------------------
â€¢ [4.274 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:23.613
    Sep  8 21:17:23.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:17:23.615
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:23.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:23.679
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/08/23 21:17:23.685
    Sep  8 21:17:23.713: INFO: Waiting up to 5m0s for pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec" in namespace "emptydir-9161" to be "Succeeded or Failed"
    Sep  8 21:17:23.725: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.633575ms
    Sep  8 21:17:25.748: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034794414s
    Sep  8 21:17:27.735: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021962707s
    STEP: Saw pod success 09/08/23 21:17:27.735
    Sep  8 21:17:27.735: INFO: Pod "pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec" satisfied condition "Succeeded or Failed"
    Sep  8 21:17:27.746: INFO: Trying to get logs from node node-3 pod pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec container test-container: <nil>
    STEP: delete the pod 09/08/23 21:17:27.762
    Sep  8 21:17:27.817: INFO: Waiting for pod pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec to disappear
    Sep  8 21:17:27.841: INFO: Pod pod-de83aec3-4251-4ab4-b1e0-3aab53db44ec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:27.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9161" for this suite. 09/08/23 21:17:27.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:27.887
Sep  8 21:17:27.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename containers 09/08/23 21:17:27.889
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:27.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:27.941
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 09/08/23 21:17:27.953
Sep  8 21:17:27.973: INFO: Waiting up to 5m0s for pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90" in namespace "containers-22" to be "Succeeded or Failed"
Sep  8 21:17:27.981: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.889435ms
Sep  8 21:17:29.990: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016810344s
Sep  8 21:17:31.993: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019892181s
STEP: Saw pod success 09/08/23 21:17:31.993
Sep  8 21:17:31.993: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90" satisfied condition "Succeeded or Failed"
Sep  8 21:17:32.009: INFO: Trying to get logs from node node-3 pod client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:17:32.035
Sep  8 21:17:32.242: INFO: Waiting for pod client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90 to disappear
Sep  8 21:17:32.269: INFO: Pod client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:32.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-22" for this suite. 09/08/23 21:17:32.286
------------------------------
â€¢ [4.447 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:27.887
    Sep  8 21:17:27.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename containers 09/08/23 21:17:27.889
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:27.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:27.941
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 09/08/23 21:17:27.953
    Sep  8 21:17:27.973: INFO: Waiting up to 5m0s for pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90" in namespace "containers-22" to be "Succeeded or Failed"
    Sep  8 21:17:27.981: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90": Phase="Pending", Reason="", readiness=false. Elapsed: 7.889435ms
    Sep  8 21:17:29.990: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016810344s
    Sep  8 21:17:31.993: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019892181s
    STEP: Saw pod success 09/08/23 21:17:31.993
    Sep  8 21:17:31.993: INFO: Pod "client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90" satisfied condition "Succeeded or Failed"
    Sep  8 21:17:32.009: INFO: Trying to get logs from node node-3 pod client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:17:32.035
    Sep  8 21:17:32.242: INFO: Waiting for pod client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90 to disappear
    Sep  8 21:17:32.269: INFO: Pod client-containers-2d6f7f55-b227-4830-9d7a-c563362c3e90 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:32.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-22" for this suite. 09/08/23 21:17:32.286
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:32.335
Sep  8 21:17:32.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:17:32.336
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:32.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:32.41
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:17:32.451
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:17:32.986
STEP: Deploying the webhook pod 09/08/23 21:17:33.003
STEP: Wait for the deployment to be ready 09/08/23 21:17:33.03
Sep  8 21:17:33.056: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/08/23 21:17:35.085
STEP: Verifying the service has paired with the endpoint 09/08/23 21:17:35.136
Sep  8 21:17:36.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/08/23 21:17:36.154
STEP: create a namespace for the webhook 09/08/23 21:17:36.185
STEP: create a configmap should be unconditionally rejected by the webhook 09/08/23 21:17:36.226
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:36.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8383" for this suite. 09/08/23 21:17:36.599
STEP: Destroying namespace "webhook-8383-markers" for this suite. 09/08/23 21:17:36.646
------------------------------
â€¢ [4.343 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:32.335
    Sep  8 21:17:32.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:17:32.336
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:32.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:32.41
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:17:32.451
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:17:32.986
    STEP: Deploying the webhook pod 09/08/23 21:17:33.003
    STEP: Wait for the deployment to be ready 09/08/23 21:17:33.03
    Sep  8 21:17:33.056: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/08/23 21:17:35.085
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:17:35.136
    Sep  8 21:17:36.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 09/08/23 21:17:36.154
    STEP: create a namespace for the webhook 09/08/23 21:17:36.185
    STEP: create a configmap should be unconditionally rejected by the webhook 09/08/23 21:17:36.226
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:36.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8383" for this suite. 09/08/23 21:17:36.599
    STEP: Destroying namespace "webhook-8383-markers" for this suite. 09/08/23 21:17:36.646
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:36.68
Sep  8 21:17:36.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 21:17:36.682
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:36.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:36.748
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-0e43bdfa-347d-4a0d-ae3f-83ce5158d9c8 09/08/23 21:17:36.755
STEP: Creating a pod to test consume secrets 09/08/23 21:17:36.789
Sep  8 21:17:36.820: INFO: Waiting up to 5m0s for pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5" in namespace "secrets-178" to be "Succeeded or Failed"
Sep  8 21:17:36.847: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.981904ms
Sep  8 21:17:38.858: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0377664s
Sep  8 21:17:40.857: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036379197s
STEP: Saw pod success 09/08/23 21:17:40.857
Sep  8 21:17:40.857: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5" satisfied condition "Succeeded or Failed"
Sep  8 21:17:40.875: INFO: Trying to get logs from node node-3 pod pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5 container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 21:17:40.899
Sep  8 21:17:40.963: INFO: Waiting for pod pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5 to disappear
Sep  8 21:17:40.980: INFO: Pod pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:40.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-178" for this suite. 09/08/23 21:17:40.991
------------------------------
â€¢ [4.327 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:36.68
    Sep  8 21:17:36.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 21:17:36.682
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:36.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:36.748
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-0e43bdfa-347d-4a0d-ae3f-83ce5158d9c8 09/08/23 21:17:36.755
    STEP: Creating a pod to test consume secrets 09/08/23 21:17:36.789
    Sep  8 21:17:36.820: INFO: Waiting up to 5m0s for pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5" in namespace "secrets-178" to be "Succeeded or Failed"
    Sep  8 21:17:36.847: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.981904ms
    Sep  8 21:17:38.858: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0377664s
    Sep  8 21:17:40.857: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036379197s
    STEP: Saw pod success 09/08/23 21:17:40.857
    Sep  8 21:17:40.857: INFO: Pod "pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5" satisfied condition "Succeeded or Failed"
    Sep  8 21:17:40.875: INFO: Trying to get logs from node node-3 pod pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5 container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 21:17:40.899
    Sep  8 21:17:40.963: INFO: Waiting for pod pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5 to disappear
    Sep  8 21:17:40.980: INFO: Pod pod-secrets-ea76e062-2c0b-412f-98ca-a824f74af0b5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:40.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-178" for this suite. 09/08/23 21:17:40.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:41.011
Sep  8 21:17:41.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:17:41.013
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:41.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:41.061
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/08/23 21:17:41.066
Sep  8 21:17:41.089: INFO: Waiting up to 5m0s for pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9" in namespace "emptydir-1803" to be "Succeeded or Failed"
Sep  8 21:17:41.100: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.992477ms
Sep  8 21:17:43.112: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.02274444s
Sep  8 21:17:45.112: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Running", Reason="", readiness=false. Elapsed: 4.023277441s
Sep  8 21:17:47.117: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028548917s
STEP: Saw pod success 09/08/23 21:17:47.117
Sep  8 21:17:47.117: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9" satisfied condition "Succeeded or Failed"
Sep  8 21:17:47.124: INFO: Trying to get logs from node node-3 pod pod-678ef0e5-33ea-413d-8903-f81c3d921ff9 container test-container: <nil>
STEP: delete the pod 09/08/23 21:17:47.142
Sep  8 21:17:47.172: INFO: Waiting for pod pod-678ef0e5-33ea-413d-8903-f81c3d921ff9 to disappear
Sep  8 21:17:47.179: INFO: Pod pod-678ef0e5-33ea-413d-8903-f81c3d921ff9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:17:47.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1803" for this suite. 09/08/23 21:17:47.191
------------------------------
â€¢ [SLOW TEST] [6.211 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:41.011
    Sep  8 21:17:41.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:17:41.013
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:41.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:41.061
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/08/23 21:17:41.066
    Sep  8 21:17:41.089: INFO: Waiting up to 5m0s for pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9" in namespace "emptydir-1803" to be "Succeeded or Failed"
    Sep  8 21:17:41.100: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.992477ms
    Sep  8 21:17:43.112: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Running", Reason="", readiness=true. Elapsed: 2.02274444s
    Sep  8 21:17:45.112: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Running", Reason="", readiness=false. Elapsed: 4.023277441s
    Sep  8 21:17:47.117: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028548917s
    STEP: Saw pod success 09/08/23 21:17:47.117
    Sep  8 21:17:47.117: INFO: Pod "pod-678ef0e5-33ea-413d-8903-f81c3d921ff9" satisfied condition "Succeeded or Failed"
    Sep  8 21:17:47.124: INFO: Trying to get logs from node node-3 pod pod-678ef0e5-33ea-413d-8903-f81c3d921ff9 container test-container: <nil>
    STEP: delete the pod 09/08/23 21:17:47.142
    Sep  8 21:17:47.172: INFO: Waiting for pod pod-678ef0e5-33ea-413d-8903-f81c3d921ff9 to disappear
    Sep  8 21:17:47.179: INFO: Pod pod-678ef0e5-33ea-413d-8903-f81c3d921ff9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:17:47.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1803" for this suite. 09/08/23 21:17:47.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:17:47.237
Sep  8 21:17:47.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 21:17:47.238
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:47.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:47.292
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44 in namespace container-probe-8030 09/08/23 21:17:47.298
Sep  8 21:17:47.319: INFO: Waiting up to 5m0s for pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44" in namespace "container-probe-8030" to be "not pending"
Sep  8 21:17:47.337: INFO: Pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44": Phase="Pending", Reason="", readiness=false. Elapsed: 18.571034ms
Sep  8 21:17:49.352: INFO: Pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44": Phase="Running", Reason="", readiness=true. Elapsed: 2.03359864s
Sep  8 21:17:49.352: INFO: Pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44" satisfied condition "not pending"
Sep  8 21:17:49.352: INFO: Started pod liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44 in namespace container-probe-8030
STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 21:17:49.352
Sep  8 21:17:49.360: INFO: Initial restart count of pod liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44 is 0
STEP: deleting the pod 09/08/23 21:21:50.932
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 21:21:50.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8030" for this suite. 09/08/23 21:21:50.99
------------------------------
â€¢ [SLOW TEST] [243.769 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:17:47.237
    Sep  8 21:17:47.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 21:17:47.238
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:17:47.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:17:47.292
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44 in namespace container-probe-8030 09/08/23 21:17:47.298
    Sep  8 21:17:47.319: INFO: Waiting up to 5m0s for pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44" in namespace "container-probe-8030" to be "not pending"
    Sep  8 21:17:47.337: INFO: Pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44": Phase="Pending", Reason="", readiness=false. Elapsed: 18.571034ms
    Sep  8 21:17:49.352: INFO: Pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44": Phase="Running", Reason="", readiness=true. Elapsed: 2.03359864s
    Sep  8 21:17:49.352: INFO: Pod "liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44" satisfied condition "not pending"
    Sep  8 21:17:49.352: INFO: Started pod liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44 in namespace container-probe-8030
    STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 21:17:49.352
    Sep  8 21:17:49.360: INFO: Initial restart count of pod liveness-b5620c4d-fcc5-4744-89c5-ee178c4daf44 is 0
    STEP: deleting the pod 09/08/23 21:21:50.932
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:21:50.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8030" for this suite. 09/08/23 21:21:50.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:21:51.017
Sep  8 21:21:51.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 21:21:51.018
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:21:51.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:21:51.087
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 09/08/23 21:21:51.09
STEP: setting up watch 09/08/23 21:21:51.091
STEP: submitting the pod to kubernetes 09/08/23 21:21:51.199
STEP: verifying the pod is in kubernetes 09/08/23 21:21:51.228
STEP: verifying pod creation was observed 09/08/23 21:21:51.275
Sep  8 21:21:51.275: INFO: Waiting up to 5m0s for pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9" in namespace "pods-3734" to be "running"
Sep  8 21:21:51.296: INFO: Pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.82412ms
Sep  8 21:21:53.305: INFO: Pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.029228403s
Sep  8 21:21:53.305: INFO: Pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9" satisfied condition "running"
STEP: deleting the pod gracefully 09/08/23 21:21:53.318
STEP: verifying pod deletion was observed 09/08/23 21:21:53.343
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 21:21:55.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3734" for this suite. 09/08/23 21:21:55.963
------------------------------
â€¢ [4.969 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:21:51.017
    Sep  8 21:21:51.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 21:21:51.018
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:21:51.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:21:51.087
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 09/08/23 21:21:51.09
    STEP: setting up watch 09/08/23 21:21:51.091
    STEP: submitting the pod to kubernetes 09/08/23 21:21:51.199
    STEP: verifying the pod is in kubernetes 09/08/23 21:21:51.228
    STEP: verifying pod creation was observed 09/08/23 21:21:51.275
    Sep  8 21:21:51.275: INFO: Waiting up to 5m0s for pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9" in namespace "pods-3734" to be "running"
    Sep  8 21:21:51.296: INFO: Pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.82412ms
    Sep  8 21:21:53.305: INFO: Pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.029228403s
    Sep  8 21:21:53.305: INFO: Pod "pod-submit-remove-44dc3c3c-90ea-4b1e-b123-ba2d50c930a9" satisfied condition "running"
    STEP: deleting the pod gracefully 09/08/23 21:21:53.318
    STEP: verifying pod deletion was observed 09/08/23 21:21:53.343
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:21:55.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3734" for this suite. 09/08/23 21:21:55.963
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:21:55.988
Sep  8 21:21:55.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 21:21:55.99
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:21:56.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:21:56.05
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7846 09/08/23 21:21:56.056
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 09/08/23 21:21:56.07
STEP: Creating stateful set ss in namespace statefulset-7846 09/08/23 21:21:56.088
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7846 09/08/23 21:21:56.104
Sep  8 21:21:56.128: INFO: Found 0 stateful pods, waiting for 1
Sep  8 21:22:06.137: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/08/23 21:22:06.137
Sep  8 21:22:06.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 21:22:06.442: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 21:22:06.442: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 21:22:06.442: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 21:22:06.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  8 21:22:16.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 21:22:16.461: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 21:22:16.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999389s
Sep  8 21:22:17.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.976410156s
Sep  8 21:22:18.552: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.967269923s
Sep  8 21:22:19.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.950194397s
Sep  8 21:22:20.572: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.941026732s
Sep  8 21:22:21.599: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.922784428s
Sep  8 21:22:22.608: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.903866011s
Sep  8 21:22:23.627: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.895371463s
Sep  8 21:22:24.635: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.876399529s
Sep  8 21:22:25.645: INFO: Verifying statefulset ss doesn't scale past 1 for another 867.47076ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7846 09/08/23 21:22:26.661
Sep  8 21:22:26.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 21:22:26.932: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  8 21:22:26.932: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 21:22:26.932: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 21:22:26.943: INFO: Found 1 stateful pods, waiting for 3
Sep  8 21:22:36.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 21:22:36.975: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 21:22:36.975: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 09/08/23 21:22:36.975
STEP: Scale down will halt with unhealthy stateful pod 09/08/23 21:22:36.975
Sep  8 21:22:37.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 21:22:37.285: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 21:22:37.285: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 21:22:37.285: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 21:22:37.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 21:22:37.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 21:22:37.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 21:22:37.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 21:22:37.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 21:22:37.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 21:22:37.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 21:22:37.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 21:22:37.833: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 21:22:37.845: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep  8 21:22:47.870: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 21:22:47.870: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 21:22:47.870: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 21:22:47.930: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999388s
Sep  8 21:22:48.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.975122981s
Sep  8 21:22:49.954: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965323247s
Sep  8 21:22:50.965: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.951244144s
Sep  8 21:22:51.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.939270593s
Sep  8 21:22:52.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.929060121s
Sep  8 21:22:54.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.908030985s
Sep  8 21:22:55.047: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.880303978s
Sep  8 21:22:56.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.857477699s
Sep  8 21:22:57.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 845.665568ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7846 09/08/23 21:22:58.067
Sep  8 21:22:58.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 21:22:58.302: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  8 21:22:58.302: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 21:22:58.302: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 21:22:58.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 21:22:58.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  8 21:22:58.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 21:22:58.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 21:22:58.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 21:22:58.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  8 21:22:58.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 21:22:58.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 21:22:58.769: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 09/08/23 21:23:08.821
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 21:23:08.821: INFO: Deleting all statefulset in ns statefulset-7846
Sep  8 21:23:08.832: INFO: Scaling statefulset ss to 0
Sep  8 21:23:08.861: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 21:23:08.866: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:23:08.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7846" for this suite. 09/08/23 21:23:08.924
------------------------------
â€¢ [SLOW TEST] [72.957 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:21:55.988
    Sep  8 21:21:55.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 21:21:55.99
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:21:56.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:21:56.05
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7846 09/08/23 21:21:56.056
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 09/08/23 21:21:56.07
    STEP: Creating stateful set ss in namespace statefulset-7846 09/08/23 21:21:56.088
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7846 09/08/23 21:21:56.104
    Sep  8 21:21:56.128: INFO: Found 0 stateful pods, waiting for 1
    Sep  8 21:22:06.137: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 09/08/23 21:22:06.137
    Sep  8 21:22:06.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 21:22:06.442: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 21:22:06.442: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 21:22:06.442: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 21:22:06.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  8 21:22:16.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 21:22:16.461: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 21:22:16.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999389s
    Sep  8 21:22:17.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.976410156s
    Sep  8 21:22:18.552: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.967269923s
    Sep  8 21:22:19.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.950194397s
    Sep  8 21:22:20.572: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.941026732s
    Sep  8 21:22:21.599: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.922784428s
    Sep  8 21:22:22.608: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.903866011s
    Sep  8 21:22:23.627: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.895371463s
    Sep  8 21:22:24.635: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.876399529s
    Sep  8 21:22:25.645: INFO: Verifying statefulset ss doesn't scale past 1 for another 867.47076ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7846 09/08/23 21:22:26.661
    Sep  8 21:22:26.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 21:22:26.932: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  8 21:22:26.932: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 21:22:26.932: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 21:22:26.943: INFO: Found 1 stateful pods, waiting for 3
    Sep  8 21:22:36.975: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 21:22:36.975: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 21:22:36.975: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 09/08/23 21:22:36.975
    STEP: Scale down will halt with unhealthy stateful pod 09/08/23 21:22:36.975
    Sep  8 21:22:37.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 21:22:37.285: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 21:22:37.285: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 21:22:37.285: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 21:22:37.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 21:22:37.533: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 21:22:37.533: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 21:22:37.533: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 21:22:37.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 21:22:37.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 21:22:37.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 21:22:37.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 21:22:37.833: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 21:22:37.845: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Sep  8 21:22:47.870: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 21:22:47.870: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 21:22:47.870: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 21:22:47.930: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999388s
    Sep  8 21:22:48.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.975122981s
    Sep  8 21:22:49.954: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965323247s
    Sep  8 21:22:50.965: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.951244144s
    Sep  8 21:22:51.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.939270593s
    Sep  8 21:22:52.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.929060121s
    Sep  8 21:22:54.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.908030985s
    Sep  8 21:22:55.047: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.880303978s
    Sep  8 21:22:56.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.857477699s
    Sep  8 21:22:57.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 845.665568ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7846 09/08/23 21:22:58.067
    Sep  8 21:22:58.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 21:22:58.302: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  8 21:22:58.302: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 21:22:58.302: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 21:22:58.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 21:22:58.549: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  8 21:22:58.549: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 21:22:58.549: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 21:22:58.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-7846 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 21:22:58.769: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  8 21:22:58.769: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 21:22:58.769: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 21:22:58.769: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 09/08/23 21:23:08.821
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 21:23:08.821: INFO: Deleting all statefulset in ns statefulset-7846
    Sep  8 21:23:08.832: INFO: Scaling statefulset ss to 0
    Sep  8 21:23:08.861: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 21:23:08.866: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:23:08.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7846" for this suite. 09/08/23 21:23:08.924
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:23:08.945
Sep  8 21:23:08.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:23:08.946
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:08.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:09.003
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:23:09.04
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:23:09.38
STEP: Deploying the webhook pod 09/08/23 21:23:09.413
STEP: Wait for the deployment to be ready 09/08/23 21:23:09.463
Sep  8 21:23:09.494: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/08/23 21:23:11.56
STEP: Verifying the service has paired with the endpoint 09/08/23 21:23:11.614
Sep  8 21:23:12.614: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 09/08/23 21:23:12.624
STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/08/23 21:23:12.667
STEP: Creating a configMap that should not be mutated 09/08/23 21:23:12.691
STEP: Patching a mutating webhook configuration's rules to include the create operation 09/08/23 21:23:12.729
STEP: Creating a configMap that should be mutated 09/08/23 21:23:12.756
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:23:12.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6099" for this suite. 09/08/23 21:23:13.05
STEP: Destroying namespace "webhook-6099-markers" for this suite. 09/08/23 21:23:13.074
------------------------------
â€¢ [4.165 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:23:08.945
    Sep  8 21:23:08.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:23:08.946
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:08.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:09.003
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:23:09.04
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:23:09.38
    STEP: Deploying the webhook pod 09/08/23 21:23:09.413
    STEP: Wait for the deployment to be ready 09/08/23 21:23:09.463
    Sep  8 21:23:09.494: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/08/23 21:23:11.56
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:23:11.614
    Sep  8 21:23:12.614: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 09/08/23 21:23:12.624
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 09/08/23 21:23:12.667
    STEP: Creating a configMap that should not be mutated 09/08/23 21:23:12.691
    STEP: Patching a mutating webhook configuration's rules to include the create operation 09/08/23 21:23:12.729
    STEP: Creating a configMap that should be mutated 09/08/23 21:23:12.756
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:23:12.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6099" for this suite. 09/08/23 21:23:13.05
    STEP: Destroying namespace "webhook-6099-markers" for this suite. 09/08/23 21:23:13.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:23:13.125
Sep  8 21:23:13.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename gc 09/08/23 21:23:13.129
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:13.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:13.19
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 09/08/23 21:23:13.224
STEP: delete the rc 09/08/23 21:23:18.267
STEP: wait for the rc to be deleted 09/08/23 21:23:18.332
Sep  8 21:23:19.400: INFO: 80 pods remaining
Sep  8 21:23:19.401: INFO: 80 pods has nil DeletionTimestamp
Sep  8 21:23:19.401: INFO: 
Sep  8 21:23:20.437: INFO: 71 pods remaining
Sep  8 21:23:20.437: INFO: 71 pods has nil DeletionTimestamp
Sep  8 21:23:20.437: INFO: 
Sep  8 21:23:21.394: INFO: 60 pods remaining
Sep  8 21:23:21.394: INFO: 60 pods has nil DeletionTimestamp
Sep  8 21:23:21.394: INFO: 
Sep  8 21:23:22.372: INFO: 40 pods remaining
Sep  8 21:23:22.372: INFO: 40 pods has nil DeletionTimestamp
Sep  8 21:23:22.372: INFO: 
Sep  8 21:23:23.430: INFO: 30 pods remaining
Sep  8 21:23:23.430: INFO: 29 pods has nil DeletionTimestamp
Sep  8 21:23:23.430: INFO: 
Sep  8 21:23:24.397: INFO: 19 pods remaining
Sep  8 21:23:24.397: INFO: 19 pods has nil DeletionTimestamp
Sep  8 21:23:24.397: INFO: 
STEP: Gathering metrics 09/08/23 21:23:25.382
Sep  8 21:23:25.528: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
Sep  8 21:23:25.558: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 29.334786ms
Sep  8 21:23:25.558: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
Sep  8 21:23:25.558: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
Sep  8 21:23:25.709: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  8 21:23:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3138" for this suite. 09/08/23 21:23:25.737
------------------------------
â€¢ [SLOW TEST] [12.683 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:23:13.125
    Sep  8 21:23:13.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename gc 09/08/23 21:23:13.129
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:13.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:13.19
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 09/08/23 21:23:13.224
    STEP: delete the rc 09/08/23 21:23:18.267
    STEP: wait for the rc to be deleted 09/08/23 21:23:18.332
    Sep  8 21:23:19.400: INFO: 80 pods remaining
    Sep  8 21:23:19.401: INFO: 80 pods has nil DeletionTimestamp
    Sep  8 21:23:19.401: INFO: 
    Sep  8 21:23:20.437: INFO: 71 pods remaining
    Sep  8 21:23:20.437: INFO: 71 pods has nil DeletionTimestamp
    Sep  8 21:23:20.437: INFO: 
    Sep  8 21:23:21.394: INFO: 60 pods remaining
    Sep  8 21:23:21.394: INFO: 60 pods has nil DeletionTimestamp
    Sep  8 21:23:21.394: INFO: 
    Sep  8 21:23:22.372: INFO: 40 pods remaining
    Sep  8 21:23:22.372: INFO: 40 pods has nil DeletionTimestamp
    Sep  8 21:23:22.372: INFO: 
    Sep  8 21:23:23.430: INFO: 30 pods remaining
    Sep  8 21:23:23.430: INFO: 29 pods has nil DeletionTimestamp
    Sep  8 21:23:23.430: INFO: 
    Sep  8 21:23:24.397: INFO: 19 pods remaining
    Sep  8 21:23:24.397: INFO: 19 pods has nil DeletionTimestamp
    Sep  8 21:23:24.397: INFO: 
    STEP: Gathering metrics 09/08/23 21:23:25.382
    Sep  8 21:23:25.528: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
    Sep  8 21:23:25.558: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 29.334786ms
    Sep  8 21:23:25.558: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
    Sep  8 21:23:25.558: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
    Sep  8 21:23:25.709: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:23:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3138" for this suite. 09/08/23 21:23:25.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:23:25.813
Sep  8 21:23:25.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-webhook 09/08/23 21:23:25.814
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:25.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:25.919
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/08/23 21:23:25.934
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/08/23 21:23:26.554
STEP: Deploying the custom resource conversion webhook pod 09/08/23 21:23:26.58
STEP: Wait for the deployment to be ready 09/08/23 21:23:26.625
Sep  8 21:23:26.672: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep  8 21:23:28.701: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:23:30.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:23:32.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:23:34.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:23:36.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:23:38.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:23:40.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 21:23:42.714
STEP: Verifying the service has paired with the endpoint 09/08/23 21:23:42.746
Sep  8 21:23:43.746: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Sep  8 21:23:43.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Creating a v1 custom resource 09/08/23 21:23:51.426
STEP: v2 custom resource should be converted 09/08/23 21:23:51.443
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:23:52.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8423" for this suite. 09/08/23 21:23:52.215
------------------------------
â€¢ [SLOW TEST] [26.432 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:23:25.813
    Sep  8 21:23:25.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-webhook 09/08/23 21:23:25.814
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:25.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:25.919
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/08/23 21:23:25.934
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/08/23 21:23:26.554
    STEP: Deploying the custom resource conversion webhook pod 09/08/23 21:23:26.58
    STEP: Wait for the deployment to be ready 09/08/23 21:23:26.625
    Sep  8 21:23:26.672: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Sep  8 21:23:28.701: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:23:30.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:23:32.712: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:23:34.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:23:36.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:23:38.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:23:40.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 23, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 21:23:42.714
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:23:42.746
    Sep  8 21:23:43.746: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Sep  8 21:23:43.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Creating a v1 custom resource 09/08/23 21:23:51.426
    STEP: v2 custom resource should be converted 09/08/23 21:23:51.443
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:23:52.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8423" for this suite. 09/08/23 21:23:52.215
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:23:52.244
Sep  8 21:23:52.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename certificates 09/08/23 21:23:52.246
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:52.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:52.339
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 09/08/23 21:23:52.724
STEP: getting /apis/certificates.k8s.io 09/08/23 21:23:52.732
STEP: getting /apis/certificates.k8s.io/v1 09/08/23 21:23:52.738
STEP: creating 09/08/23 21:23:52.74
STEP: getting 09/08/23 21:23:52.818
STEP: listing 09/08/23 21:23:52.829
STEP: watching 09/08/23 21:23:52.838
Sep  8 21:23:52.838: INFO: starting watch
STEP: patching 09/08/23 21:23:52.847
STEP: updating 09/08/23 21:23:52.863
Sep  8 21:23:52.878: INFO: waiting for watch events with expected annotations
Sep  8 21:23:52.878: INFO: saw patched and updated annotations
STEP: getting /approval 09/08/23 21:23:52.878
STEP: patching /approval 09/08/23 21:23:52.889
STEP: updating /approval 09/08/23 21:23:52.904
STEP: getting /status 09/08/23 21:23:52.929
STEP: patching /status 09/08/23 21:23:52.945
STEP: updating /status 09/08/23 21:23:52.969
STEP: deleting 09/08/23 21:23:52.996
STEP: deleting a collection 09/08/23 21:23:53.034
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:23:53.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-4486" for this suite. 09/08/23 21:23:53.088
------------------------------
â€¢ [0.870 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:23:52.244
    Sep  8 21:23:52.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename certificates 09/08/23 21:23:52.246
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:52.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:52.339
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 09/08/23 21:23:52.724
    STEP: getting /apis/certificates.k8s.io 09/08/23 21:23:52.732
    STEP: getting /apis/certificates.k8s.io/v1 09/08/23 21:23:52.738
    STEP: creating 09/08/23 21:23:52.74
    STEP: getting 09/08/23 21:23:52.818
    STEP: listing 09/08/23 21:23:52.829
    STEP: watching 09/08/23 21:23:52.838
    Sep  8 21:23:52.838: INFO: starting watch
    STEP: patching 09/08/23 21:23:52.847
    STEP: updating 09/08/23 21:23:52.863
    Sep  8 21:23:52.878: INFO: waiting for watch events with expected annotations
    Sep  8 21:23:52.878: INFO: saw patched and updated annotations
    STEP: getting /approval 09/08/23 21:23:52.878
    STEP: patching /approval 09/08/23 21:23:52.889
    STEP: updating /approval 09/08/23 21:23:52.904
    STEP: getting /status 09/08/23 21:23:52.929
    STEP: patching /status 09/08/23 21:23:52.945
    STEP: updating /status 09/08/23 21:23:52.969
    STEP: deleting 09/08/23 21:23:52.996
    STEP: deleting a collection 09/08/23 21:23:53.034
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:23:53.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-4486" for this suite. 09/08/23 21:23:53.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:23:53.117
Sep  8 21:23:53.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:23:53.118
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:53.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:53.172
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 09/08/23 21:24:10.188
STEP: Creating a ResourceQuota 09/08/23 21:24:15.198
STEP: Ensuring resource quota status is calculated 09/08/23 21:24:15.217
STEP: Creating a ConfigMap 09/08/23 21:24:17.228
STEP: Ensuring resource quota status captures configMap creation 09/08/23 21:24:17.27
STEP: Deleting a ConfigMap 09/08/23 21:24:19.282
STEP: Ensuring resource quota status released usage 09/08/23 21:24:19.298
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:24:21.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6907" for this suite. 09/08/23 21:24:21.317
------------------------------
â€¢ [SLOW TEST] [28.221 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:23:53.117
    Sep  8 21:23:53.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:23:53.118
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:23:53.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:23:53.172
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 09/08/23 21:24:10.188
    STEP: Creating a ResourceQuota 09/08/23 21:24:15.198
    STEP: Ensuring resource quota status is calculated 09/08/23 21:24:15.217
    STEP: Creating a ConfigMap 09/08/23 21:24:17.228
    STEP: Ensuring resource quota status captures configMap creation 09/08/23 21:24:17.27
    STEP: Deleting a ConfigMap 09/08/23 21:24:19.282
    STEP: Ensuring resource quota status released usage 09/08/23 21:24:19.298
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:24:21.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6907" for this suite. 09/08/23 21:24:21.317
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:24:21.339
Sep  8 21:24:21.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 21:24:21.341
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:24:21.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:24:21.393
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 09/08/23 21:24:21.403
Sep  8 21:24:21.428: INFO: created test-pod-1
Sep  8 21:24:21.478: INFO: created test-pod-2
Sep  8 21:24:21.497: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 09/08/23 21:24:21.497
Sep  8 21:24:21.497: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2305' to be running and ready
Sep  8 21:24:21.550: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  8 21:24:21.550: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  8 21:24:21.550: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  8 21:24:21.550: INFO: 0 / 3 pods in namespace 'pods-2305' are running and ready (0 seconds elapsed)
Sep  8 21:24:21.550: INFO: expected 0 pod replicas in namespace 'pods-2305', 0 are Running and Ready.
Sep  8 21:24:21.551: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
Sep  8 21:24:21.551: INFO: test-pod-1  node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
Sep  8 21:24:21.551: INFO: test-pod-2  node-4  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
Sep  8 21:24:21.551: INFO: test-pod-3  node-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
Sep  8 21:24:21.551: INFO: 
Sep  8 21:24:23.602: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  8 21:24:23.602: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Sep  8 21:24:23.602: INFO: 1 / 3 pods in namespace 'pods-2305' are running and ready (2 seconds elapsed)
Sep  8 21:24:23.602: INFO: expected 0 pod replicas in namespace 'pods-2305', 0 are Running and Ready.
Sep  8 21:24:23.602: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
Sep  8 21:24:23.602: INFO: test-pod-1  node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
Sep  8 21:24:23.602: INFO: test-pod-3  node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
Sep  8 21:24:23.602: INFO: 
Sep  8 21:24:25.584: INFO: 3 / 3 pods in namespace 'pods-2305' are running and ready (4 seconds elapsed)
Sep  8 21:24:25.585: INFO: expected 0 pod replicas in namespace 'pods-2305', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 09/08/23 21:24:25.688
Sep  8 21:24:25.703: INFO: Pod quantity 3 is different from expected quantity 0
Sep  8 21:24:26.718: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 21:24:27.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2305" for this suite. 09/08/23 21:24:27.732
------------------------------
â€¢ [SLOW TEST] [6.413 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:24:21.339
    Sep  8 21:24:21.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 21:24:21.341
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:24:21.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:24:21.393
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 09/08/23 21:24:21.403
    Sep  8 21:24:21.428: INFO: created test-pod-1
    Sep  8 21:24:21.478: INFO: created test-pod-2
    Sep  8 21:24:21.497: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 09/08/23 21:24:21.497
    Sep  8 21:24:21.497: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2305' to be running and ready
    Sep  8 21:24:21.550: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  8 21:24:21.550: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  8 21:24:21.550: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  8 21:24:21.550: INFO: 0 / 3 pods in namespace 'pods-2305' are running and ready (0 seconds elapsed)
    Sep  8 21:24:21.550: INFO: expected 0 pod replicas in namespace 'pods-2305', 0 are Running and Ready.
    Sep  8 21:24:21.551: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
    Sep  8 21:24:21.551: INFO: test-pod-1  node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
    Sep  8 21:24:21.551: INFO: test-pod-2  node-4  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
    Sep  8 21:24:21.551: INFO: test-pod-3  node-3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
    Sep  8 21:24:21.551: INFO: 
    Sep  8 21:24:23.602: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  8 21:24:23.602: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Sep  8 21:24:23.602: INFO: 1 / 3 pods in namespace 'pods-2305' are running and ready (2 seconds elapsed)
    Sep  8 21:24:23.602: INFO: expected 0 pod replicas in namespace 'pods-2305', 0 are Running and Ready.
    Sep  8 21:24:23.602: INFO: POD         NODE    PHASE    GRACE  CONDITIONS
    Sep  8 21:24:23.602: INFO: test-pod-1  node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
    Sep  8 21:24:23.602: INFO: test-pod-3  node-3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 21:24:21 +0000 UTC  }]
    Sep  8 21:24:23.602: INFO: 
    Sep  8 21:24:25.584: INFO: 3 / 3 pods in namespace 'pods-2305' are running and ready (4 seconds elapsed)
    Sep  8 21:24:25.585: INFO: expected 0 pod replicas in namespace 'pods-2305', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 09/08/23 21:24:25.688
    Sep  8 21:24:25.703: INFO: Pod quantity 3 is different from expected quantity 0
    Sep  8 21:24:26.718: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:24:27.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2305" for this suite. 09/08/23 21:24:27.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:24:27.755
Sep  8 21:24:27.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename runtimeclass 09/08/23 21:24:27.756
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:24:27.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:24:27.817
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3978-delete-me 09/08/23 21:24:27.837
STEP: Waiting for the RuntimeClass to disappear 09/08/23 21:24:27.857
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  8 21:24:27.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3978" for this suite. 09/08/23 21:24:27.942
------------------------------
â€¢ [0.204 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:24:27.755
    Sep  8 21:24:27.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename runtimeclass 09/08/23 21:24:27.756
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:24:27.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:24:27.817
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3978-delete-me 09/08/23 21:24:27.837
    STEP: Waiting for the RuntimeClass to disappear 09/08/23 21:24:27.857
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:24:27.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3978" for this suite. 09/08/23 21:24:27.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:24:27.96
Sep  8 21:24:27.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-pred 09/08/23 21:24:27.961
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:24:28.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:24:28.022
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  8 21:24:28.026: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  8 21:24:28.060: INFO: Waiting for terminating namespaces to be deleted...
Sep  8 21:24:28.069: INFO: 
Logging pods the apiserver thinks is on node node-3 before test
Sep  8 21:24:28.087: INFO: cadvisor-kbl2r from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 21:24:28.087: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-bd2m5 from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  8 21:24:28.087: INFO: kube-prometheus-node-exporter-fpqfh from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 21:24:28.087: INFO: netchecker-agent-4vxnw from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:24:28.087: INFO: netchecker-agent-hostnet-hpshq from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:24:28.087: INFO: openebs-localpv-provisioner-5d6756bcd8-xfthd from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 21:24:28.087: INFO: ingress-nginx-controller-tf5xn from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:24:28.087: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 21:24:28.087: INFO: coredns-6c86d97486-8w8fz from kube-system started at 2023-09-08 20:57:17 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container coredns ready: true, restart count 0
Sep  8 21:24:28.087: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 21:24:28.087: INFO: kubernetes-dashboard-68b4f76cdb-tkz5s from kube-system started at 2023-09-08 20:52:27 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Sep  8 21:24:28.087: INFO: kubernetes-dashboard-admin-79b7cc94c8-xw79r from kube-system started at 2023-09-08 20:52:30 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container kubernetes-dashboard-admin ready: true, restart count 0
Sep  8 21:24:28.087: INFO: kubernetes-metrics-scraper-6585f876d5-xbkgd from kube-system started at 2023-09-08 20:52:28 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
Sep  8 21:24:28.087: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 21:24:28.087: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.087: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 21:24:28.088: INFO: metallb-controller-77b687f97-969st from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.088: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:24:28.088: INFO: metallb-speaker-gfxvl from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.088: INFO: 	Container speaker ready: true, restart count 0
Sep  8 21:24:28.088: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.088: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  8 21:24:28.088: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 21:24:28.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:24:28.088: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  8 21:24:28.088: INFO: 
Logging pods the apiserver thinks is on node node-4 before test
Sep  8 21:24:28.113: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.113: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 21:24:28.113: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.113: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  8 21:24:28.113: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.113: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 21:24:28.113: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.113: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:24:28.113: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.113: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:24:28.113: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
Sep  8 21:24:28.113: INFO: 	Container etcd ready: true, restart count 0
Sep  8 21:24:28.113: INFO: 	Container netchecker-server ready: true, restart count 1
Sep  8 21:24:28.113: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 21:24:28.114: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:24:28.114: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  8 21:24:28.114: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 21:24:28.114: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 21:24:28.114: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container metrics-server ready: true, restart count 0
Sep  8 21:24:28.114: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 21:24:28.114: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 21:24:28.114: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container metallb-monitor ready: true, restart count 0
Sep  8 21:24:28.114: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container speaker ready: true, restart count 0
Sep  8 21:24:28.114: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container e2e ready: true, restart count 0
Sep  8 21:24:28.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:24:28.114: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 21:24:28.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:24:28.114: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/08/23 21:24:28.114
Sep  8 21:24:28.137: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2528" to be "running"
Sep  8 21:24:28.145: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072948ms
Sep  8 21:24:30.155: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01855669s
Sep  8 21:24:30.155: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/08/23 21:24:30.165
STEP: Trying to apply a random label on the found node. 09/08/23 21:24:30.224
STEP: verifying the node has the label kubernetes.io/e2e-c4e66c9c-55a1-43b0-bfca-5f119c01ff03 95 09/08/23 21:24:30.253
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/08/23 21:24:30.274
Sep  8 21:24:30.309: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2528" to be "not pending"
Sep  8 21:24:30.322: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.112616ms
Sep  8 21:24:32.340: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.031261626s
Sep  8 21:24:32.340: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.100.19.129 on the node which pod4 resides and expect not scheduled 09/08/23 21:24:32.34
Sep  8 21:24:32.358: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2528" to be "not pending"
Sep  8 21:24:32.372: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.054534ms
Sep  8 21:24:34.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024469829s
Sep  8 21:24:36.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026198145s
Sep  8 21:24:38.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026965038s
Sep  8 21:24:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028190688s
Sep  8 21:24:42.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0264812s
Sep  8 21:24:44.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027396931s
Sep  8 21:24:46.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023533602s
Sep  8 21:24:48.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.028195957s
Sep  8 21:24:50.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024191824s
Sep  8 21:24:52.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.024935967s
Sep  8 21:24:54.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024652702s
Sep  8 21:24:56.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030360026s
Sep  8 21:24:58.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.029004789s
Sep  8 21:25:00.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02348571s
Sep  8 21:25:02.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.034787216s
Sep  8 21:25:04.391: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.033051688s
Sep  8 21:25:06.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027053274s
Sep  8 21:25:08.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.03490268s
Sep  8 21:25:10.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026904062s
Sep  8 21:25:12.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.02561632s
Sep  8 21:25:14.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.029361013s
Sep  8 21:25:16.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.026927342s
Sep  8 21:25:18.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.028140509s
Sep  8 21:25:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.02802672s
Sep  8 21:25:22.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.034379282s
Sep  8 21:25:24.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.030329525s
Sep  8 21:25:26.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022510571s
Sep  8 21:25:28.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02553455s
Sep  8 21:25:30.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024776628s
Sep  8 21:25:32.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.030320438s
Sep  8 21:25:34.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023199702s
Sep  8 21:25:36.398: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.03951685s
Sep  8 21:25:38.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.029028596s
Sep  8 21:25:40.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024105439s
Sep  8 21:25:42.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023633001s
Sep  8 21:25:44.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024187418s
Sep  8 21:25:46.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.02683286s
Sep  8 21:25:48.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024485542s
Sep  8 21:25:50.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.024641366s
Sep  8 21:25:52.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.030189476s
Sep  8 21:25:54.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.026119462s
Sep  8 21:25:56.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.025457842s
Sep  8 21:25:58.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022795597s
Sep  8 21:26:00.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023516435s
Sep  8 21:26:02.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.028993883s
Sep  8 21:26:04.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025863382s
Sep  8 21:26:06.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.022987346s
Sep  8 21:26:08.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.035753869s
Sep  8 21:26:10.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.035253947s
Sep  8 21:26:12.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.027786781s
Sep  8 21:26:14.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022953813s
Sep  8 21:26:16.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.026641808s
Sep  8 21:26:18.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.035599401s
Sep  8 21:26:20.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.02528335s
Sep  8 21:26:22.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023012572s
Sep  8 21:26:24.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.02626152s
Sep  8 21:26:26.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026026766s
Sep  8 21:26:28.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029026335s
Sep  8 21:26:30.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.025950705s
Sep  8 21:26:32.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028449124s
Sep  8 21:26:34.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.028227754s
Sep  8 21:26:36.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02331752s
Sep  8 21:26:38.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.022219752s
Sep  8 21:26:40.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.045331102s
Sep  8 21:26:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.027500813s
Sep  8 21:26:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.029039629s
Sep  8 21:26:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.02744071s
Sep  8 21:26:48.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.025253637s
Sep  8 21:26:50.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.022823595s
Sep  8 21:26:52.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.023048637s
Sep  8 21:26:54.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.024225446s
Sep  8 21:26:56.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.028198484s
Sep  8 21:26:58.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.025705662s
Sep  8 21:27:00.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.026490449s
Sep  8 21:27:02.391: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.032126469s
Sep  8 21:27:04.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.024322946s
Sep  8 21:27:06.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.025405551s
Sep  8 21:27:08.397: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.03828785s
Sep  8 21:27:10.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.024170891s
Sep  8 21:27:12.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.025950304s
Sep  8 21:27:14.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.023353988s
Sep  8 21:27:16.397: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.038182709s
Sep  8 21:27:18.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.023027422s
Sep  8 21:27:20.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.025819286s
Sep  8 21:27:22.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.027214253s
Sep  8 21:27:24.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.023655471s
Sep  8 21:27:26.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.023732546s
Sep  8 21:27:28.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.022304531s
Sep  8 21:27:30.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.026009297s
Sep  8 21:27:32.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.023449742s
Sep  8 21:27:34.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.023722982s
Sep  8 21:27:36.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.023663713s
Sep  8 21:27:38.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.024388857s
Sep  8 21:27:40.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.029653595s
Sep  8 21:27:42.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.024214733s
Sep  8 21:27:44.395: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.03620157s
Sep  8 21:27:46.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.023479864s
Sep  8 21:27:48.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02349806s
Sep  8 21:27:50.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.02590266s
Sep  8 21:27:52.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.024395039s
Sep  8 21:27:54.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.023588625s
Sep  8 21:27:56.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.024054357s
Sep  8 21:27:58.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.028199355s
Sep  8 21:28:00.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.026231403s
Sep  8 21:28:02.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.026106602s
Sep  8 21:28:04.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.028620737s
Sep  8 21:28:06.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.023230267s
Sep  8 21:28:08.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.027286849s
Sep  8 21:28:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.027776703s
Sep  8 21:28:12.395: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.036615173s
Sep  8 21:28:14.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.022536033s
Sep  8 21:28:16.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.027366701s
Sep  8 21:28:18.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.025403328s
Sep  8 21:28:20.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.023716421s
Sep  8 21:28:22.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.029073602s
Sep  8 21:28:24.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.022224008s
Sep  8 21:28:26.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.026268469s
Sep  8 21:28:28.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.024777085s
Sep  8 21:28:30.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.0240173s
Sep  8 21:28:32.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.022374675s
Sep  8 21:28:34.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.03488585s
Sep  8 21:28:36.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.02519322s
Sep  8 21:28:38.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023745931s
Sep  8 21:28:40.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.026683948s
Sep  8 21:28:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.027372894s
Sep  8 21:28:44.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.023086223s
Sep  8 21:28:46.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.02457824s
Sep  8 21:28:48.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.026032478s
Sep  8 21:28:50.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.026775776s
Sep  8 21:28:52.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.026322841s
Sep  8 21:28:54.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.023452384s
Sep  8 21:28:56.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.029208599s
Sep  8 21:28:58.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.025310493s
Sep  8 21:29:00.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.024638932s
Sep  8 21:29:02.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.026930001s
Sep  8 21:29:04.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.024269347s
Sep  8 21:29:06.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.023645925s
Sep  8 21:29:08.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.0350207s
Sep  8 21:29:10.392: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.033950694s
Sep  8 21:29:12.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.022667386s
Sep  8 21:29:14.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.024994252s
Sep  8 21:29:16.391: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.032285723s
Sep  8 21:29:18.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024682694s
Sep  8 21:29:20.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.029607238s
Sep  8 21:29:22.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.026471356s
Sep  8 21:29:24.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.026656606s
Sep  8 21:29:26.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.022682871s
Sep  8 21:29:28.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.022835188s
Sep  8 21:29:30.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.02158155s
Sep  8 21:29:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027429026s
Sep  8 21:29:32.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.03513443s
STEP: removing the label kubernetes.io/e2e-c4e66c9c-55a1-43b0-bfca-5f119c01ff03 off the node node-3 09/08/23 21:29:32.394
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c4e66c9c-55a1-43b0-bfca-5f119c01ff03 09/08/23 21:29:32.431
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:29:32.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2528" for this suite. 09/08/23 21:29:32.463
------------------------------
â€¢ [SLOW TEST] [304.527 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:24:27.96
    Sep  8 21:24:27.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-pred 09/08/23 21:24:27.961
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:24:28.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:24:28.022
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  8 21:24:28.026: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  8 21:24:28.060: INFO: Waiting for terminating namespaces to be deleted...
    Sep  8 21:24:28.069: INFO: 
    Logging pods the apiserver thinks is on node node-3 before test
    Sep  8 21:24:28.087: INFO: cadvisor-kbl2r from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-bd2m5 from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: kube-prometheus-node-exporter-fpqfh from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: netchecker-agent-4vxnw from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: netchecker-agent-hostnet-hpshq from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: openebs-localpv-provisioner-5d6756bcd8-xfthd from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: ingress-nginx-controller-tf5xn from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: coredns-6c86d97486-8w8fz from kube-system started at 2023-09-08 20:57:17 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container coredns ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: kubernetes-dashboard-68b4f76cdb-tkz5s from kube-system started at 2023-09-08 20:52:27 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: kubernetes-dashboard-admin-79b7cc94c8-xw79r from kube-system started at 2023-09-08 20:52:30 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container kubernetes-dashboard-admin ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: kubernetes-metrics-scraper-6585f876d5-xbkgd from kube-system started at 2023-09-08 20:52:28 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container kubernetes-metrics-scraper ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 21:24:28.087: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.087: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 21:24:28.088: INFO: metallb-controller-77b687f97-969st from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.088: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:24:28.088: INFO: metallb-speaker-gfxvl from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.088: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 21:24:28.088: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.088: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  8 21:24:28.088: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 21:24:28.088: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:24:28.088: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  8 21:24:28.088: INFO: 
    Logging pods the apiserver thinks is on node node-4 before test
    Sep  8 21:24:28.113: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.113: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 21:24:28.113: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.113: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  8 21:24:28.113: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.113: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 21:24:28.113: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.113: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:24:28.113: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.113: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:24:28.113: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
    Sep  8 21:24:28.113: INFO: 	Container etcd ready: true, restart count 0
    Sep  8 21:24:28.113: INFO: 	Container netchecker-server ready: true, restart count 1
    Sep  8 21:24:28.113: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container metallb-monitor ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container e2e ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 21:24:28.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:24:28.114: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/08/23 21:24:28.114
    Sep  8 21:24:28.137: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2528" to be "running"
    Sep  8 21:24:28.145: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072948ms
    Sep  8 21:24:30.155: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01855669s
    Sep  8 21:24:30.155: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/08/23 21:24:30.165
    STEP: Trying to apply a random label on the found node. 09/08/23 21:24:30.224
    STEP: verifying the node has the label kubernetes.io/e2e-c4e66c9c-55a1-43b0-bfca-5f119c01ff03 95 09/08/23 21:24:30.253
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 09/08/23 21:24:30.274
    Sep  8 21:24:30.309: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2528" to be "not pending"
    Sep  8 21:24:30.322: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.112616ms
    Sep  8 21:24:32.340: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.031261626s
    Sep  8 21:24:32.340: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.100.19.129 on the node which pod4 resides and expect not scheduled 09/08/23 21:24:32.34
    Sep  8 21:24:32.358: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2528" to be "not pending"
    Sep  8 21:24:32.372: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.054534ms
    Sep  8 21:24:34.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024469829s
    Sep  8 21:24:36.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026198145s
    Sep  8 21:24:38.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026965038s
    Sep  8 21:24:40.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.028190688s
    Sep  8 21:24:42.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0264812s
    Sep  8 21:24:44.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027396931s
    Sep  8 21:24:46.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023533602s
    Sep  8 21:24:48.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.028195957s
    Sep  8 21:24:50.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024191824s
    Sep  8 21:24:52.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.024935967s
    Sep  8 21:24:54.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.024652702s
    Sep  8 21:24:56.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030360026s
    Sep  8 21:24:58.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.029004789s
    Sep  8 21:25:00.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02348571s
    Sep  8 21:25:02.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.034787216s
    Sep  8 21:25:04.391: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.033051688s
    Sep  8 21:25:06.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027053274s
    Sep  8 21:25:08.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.03490268s
    Sep  8 21:25:10.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026904062s
    Sep  8 21:25:12.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.02561632s
    Sep  8 21:25:14.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.029361013s
    Sep  8 21:25:16.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.026927342s
    Sep  8 21:25:18.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.028140509s
    Sep  8 21:25:20.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.02802672s
    Sep  8 21:25:22.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.034379282s
    Sep  8 21:25:24.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.030329525s
    Sep  8 21:25:26.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.022510571s
    Sep  8 21:25:28.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.02553455s
    Sep  8 21:25:30.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024776628s
    Sep  8 21:25:32.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.030320438s
    Sep  8 21:25:34.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.023199702s
    Sep  8 21:25:36.398: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.03951685s
    Sep  8 21:25:38.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.029028596s
    Sep  8 21:25:40.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024105439s
    Sep  8 21:25:42.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023633001s
    Sep  8 21:25:44.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024187418s
    Sep  8 21:25:46.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.02683286s
    Sep  8 21:25:48.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.024485542s
    Sep  8 21:25:50.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.024641366s
    Sep  8 21:25:52.389: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.030189476s
    Sep  8 21:25:54.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.026119462s
    Sep  8 21:25:56.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.025457842s
    Sep  8 21:25:58.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.022795597s
    Sep  8 21:26:00.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.023516435s
    Sep  8 21:26:02.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.028993883s
    Sep  8 21:26:04.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.025863382s
    Sep  8 21:26:06.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.022987346s
    Sep  8 21:26:08.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.035753869s
    Sep  8 21:26:10.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.035253947s
    Sep  8 21:26:12.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.027786781s
    Sep  8 21:26:14.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022953813s
    Sep  8 21:26:16.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.026641808s
    Sep  8 21:26:18.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.035599401s
    Sep  8 21:26:20.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.02528335s
    Sep  8 21:26:22.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023012572s
    Sep  8 21:26:24.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.02626152s
    Sep  8 21:26:26.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.026026766s
    Sep  8 21:26:28.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029026335s
    Sep  8 21:26:30.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.025950705s
    Sep  8 21:26:32.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.028449124s
    Sep  8 21:26:34.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.028227754s
    Sep  8 21:26:36.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.02331752s
    Sep  8 21:26:38.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.022219752s
    Sep  8 21:26:40.404: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.045331102s
    Sep  8 21:26:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.027500813s
    Sep  8 21:26:44.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.029039629s
    Sep  8 21:26:46.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.02744071s
    Sep  8 21:26:48.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.025253637s
    Sep  8 21:26:50.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.022823595s
    Sep  8 21:26:52.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.023048637s
    Sep  8 21:26:54.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.024225446s
    Sep  8 21:26:56.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.028198484s
    Sep  8 21:26:58.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.025705662s
    Sep  8 21:27:00.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.026490449s
    Sep  8 21:27:02.391: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.032126469s
    Sep  8 21:27:04.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.024322946s
    Sep  8 21:27:06.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.025405551s
    Sep  8 21:27:08.397: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.03828785s
    Sep  8 21:27:10.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.024170891s
    Sep  8 21:27:12.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.025950304s
    Sep  8 21:27:14.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.023353988s
    Sep  8 21:27:16.397: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.038182709s
    Sep  8 21:27:18.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.023027422s
    Sep  8 21:27:20.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.025819286s
    Sep  8 21:27:22.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.027214253s
    Sep  8 21:27:24.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.023655471s
    Sep  8 21:27:26.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.023732546s
    Sep  8 21:27:28.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.022304531s
    Sep  8 21:27:30.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.026009297s
    Sep  8 21:27:32.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.023449742s
    Sep  8 21:27:34.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.023722982s
    Sep  8 21:27:36.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.023663713s
    Sep  8 21:27:38.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.024388857s
    Sep  8 21:27:40.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.029653595s
    Sep  8 21:27:42.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.024214733s
    Sep  8 21:27:44.395: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.03620157s
    Sep  8 21:27:46.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.023479864s
    Sep  8 21:27:48.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.02349806s
    Sep  8 21:27:50.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.02590266s
    Sep  8 21:27:52.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.024395039s
    Sep  8 21:27:54.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.023588625s
    Sep  8 21:27:56.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.024054357s
    Sep  8 21:27:58.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.028199355s
    Sep  8 21:28:00.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.026231403s
    Sep  8 21:28:02.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.026106602s
    Sep  8 21:28:04.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.028620737s
    Sep  8 21:28:06.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.023230267s
    Sep  8 21:28:08.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.027286849s
    Sep  8 21:28:10.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.027776703s
    Sep  8 21:28:12.395: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.036615173s
    Sep  8 21:28:14.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.022536033s
    Sep  8 21:28:16.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.027366701s
    Sep  8 21:28:18.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.025403328s
    Sep  8 21:28:20.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.023716421s
    Sep  8 21:28:22.387: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.029073602s
    Sep  8 21:28:24.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.022224008s
    Sep  8 21:28:26.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.026268469s
    Sep  8 21:28:28.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.024777085s
    Sep  8 21:28:30.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.0240173s
    Sep  8 21:28:32.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.022374675s
    Sep  8 21:28:34.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.03488585s
    Sep  8 21:28:36.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.02519322s
    Sep  8 21:28:38.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.023745931s
    Sep  8 21:28:40.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.026683948s
    Sep  8 21:28:42.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.027372894s
    Sep  8 21:28:44.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.023086223s
    Sep  8 21:28:46.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.02457824s
    Sep  8 21:28:48.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.026032478s
    Sep  8 21:28:50.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.026775776s
    Sep  8 21:28:52.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.026322841s
    Sep  8 21:28:54.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.023452384s
    Sep  8 21:28:56.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.029208599s
    Sep  8 21:28:58.384: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.025310493s
    Sep  8 21:29:00.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.024638932s
    Sep  8 21:29:02.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.026930001s
    Sep  8 21:29:04.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.024269347s
    Sep  8 21:29:06.382: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.023645925s
    Sep  8 21:29:08.393: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.0350207s
    Sep  8 21:29:10.392: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.033950694s
    Sep  8 21:29:12.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.022667386s
    Sep  8 21:29:14.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.024994252s
    Sep  8 21:29:16.391: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.032285723s
    Sep  8 21:29:18.383: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.024682694s
    Sep  8 21:29:20.388: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.029607238s
    Sep  8 21:29:22.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.026471356s
    Sep  8 21:29:24.385: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.026656606s
    Sep  8 21:29:26.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.022682871s
    Sep  8 21:29:28.381: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.022835188s
    Sep  8 21:29:30.380: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.02158155s
    Sep  8 21:29:32.386: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.027429026s
    Sep  8 21:29:32.394: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.03513443s
    STEP: removing the label kubernetes.io/e2e-c4e66c9c-55a1-43b0-bfca-5f119c01ff03 off the node node-3 09/08/23 21:29:32.394
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-c4e66c9c-55a1-43b0-bfca-5f119c01ff03 09/08/23 21:29:32.431
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:29:32.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2528" for this suite. 09/08/23 21:29:32.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:29:32.491
Sep  8 21:29:32.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 21:29:32.492
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:29:32.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:29:32.55
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/08/23 21:29:32.564
Sep  8 21:29:32.599: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2852" to be "running and ready"
Sep  8 21:29:32.615: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.23797ms
Sep  8 21:29:32.615: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:29:34.639: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.039853222s
Sep  8 21:29:34.639: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  8 21:29:34.639: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 09/08/23 21:29:34.66
Sep  8 21:29:34.682: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2852" to be "running and ready"
Sep  8 21:29:34.692: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.339249ms
Sep  8 21:29:34.692: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:29:36.705: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023291147s
Sep  8 21:29:36.705: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Sep  8 21:29:36.705: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/08/23 21:29:36.716
STEP: delete the pod with lifecycle hook 09/08/23 21:29:36.752
Sep  8 21:29:36.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  8 21:29:36.780: INFO: Pod pod-with-poststart-http-hook still exists
Sep  8 21:29:38.781: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  8 21:29:38.790: INFO: Pod pod-with-poststart-http-hook still exists
Sep  8 21:29:40.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  8 21:29:40.791: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  8 21:29:40.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2852" for this suite. 09/08/23 21:29:40.806
------------------------------
â€¢ [SLOW TEST] [8.337 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:29:32.491
    Sep  8 21:29:32.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 21:29:32.492
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:29:32.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:29:32.55
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/08/23 21:29:32.564
    Sep  8 21:29:32.599: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2852" to be "running and ready"
    Sep  8 21:29:32.615: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.23797ms
    Sep  8 21:29:32.615: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:29:34.639: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.039853222s
    Sep  8 21:29:34.639: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  8 21:29:34.639: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 09/08/23 21:29:34.66
    Sep  8 21:29:34.682: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2852" to be "running and ready"
    Sep  8 21:29:34.692: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.339249ms
    Sep  8 21:29:34.692: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:29:36.705: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.023291147s
    Sep  8 21:29:36.705: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Sep  8 21:29:36.705: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/08/23 21:29:36.716
    STEP: delete the pod with lifecycle hook 09/08/23 21:29:36.752
    Sep  8 21:29:36.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  8 21:29:36.780: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  8 21:29:38.781: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  8 21:29:38.790: INFO: Pod pod-with-poststart-http-hook still exists
    Sep  8 21:29:40.782: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Sep  8 21:29:40.791: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:29:40.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2852" for this suite. 09/08/23 21:29:40.806
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:29:40.828
Sep  8 21:29:40.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename endpointslice 09/08/23 21:29:40.829
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:29:40.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:29:40.872
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 09/08/23 21:29:46.09
STEP: referencing matching pods with named port 09/08/23 21:29:51.116
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/08/23 21:29:56.148
STEP: recreating EndpointSlices after they've been deleted 09/08/23 21:30:01.171
Sep  8 21:30:01.262: INFO: EndpointSlice for Service endpointslice-4960/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  8 21:30:11.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4960" for this suite. 09/08/23 21:30:11.306
------------------------------
â€¢ [SLOW TEST] [30.499 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:29:40.828
    Sep  8 21:29:40.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename endpointslice 09/08/23 21:29:40.829
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:29:40.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:29:40.872
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 09/08/23 21:29:46.09
    STEP: referencing matching pods with named port 09/08/23 21:29:51.116
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 09/08/23 21:29:56.148
    STEP: recreating EndpointSlices after they've been deleted 09/08/23 21:30:01.171
    Sep  8 21:30:01.262: INFO: EndpointSlice for Service endpointslice-4960/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:30:11.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4960" for this suite. 09/08/23 21:30:11.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:30:11.334
Sep  8 21:30:11.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:30:11.335
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:11.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:11.395
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 09/08/23 21:30:11.401
Sep  8 21:30:11.429: INFO: Waiting up to 5m0s for pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1" in namespace "projected-3031" to be "running and ready"
Sep  8 21:30:11.448: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.723459ms
Sep  8 21:30:11.448: INFO: The phase of Pod annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:30:13.468: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039531672s
Sep  8 21:30:13.468: INFO: The phase of Pod annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:30:15.461: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.032659216s
Sep  8 21:30:15.462: INFO: The phase of Pod annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1 is Running (Ready = true)
Sep  8 21:30:15.462: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1" satisfied condition "running and ready"
Sep  8 21:30:16.019: INFO: Successfully updated pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 21:30:18.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3031" for this suite. 09/08/23 21:30:18.097
------------------------------
â€¢ [SLOW TEST] [6.796 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:30:11.334
    Sep  8 21:30:11.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:30:11.335
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:11.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:11.395
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 09/08/23 21:30:11.401
    Sep  8 21:30:11.429: INFO: Waiting up to 5m0s for pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1" in namespace "projected-3031" to be "running and ready"
    Sep  8 21:30:11.448: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.723459ms
    Sep  8 21:30:11.448: INFO: The phase of Pod annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:30:13.468: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039531672s
    Sep  8 21:30:13.468: INFO: The phase of Pod annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:30:15.461: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.032659216s
    Sep  8 21:30:15.462: INFO: The phase of Pod annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1 is Running (Ready = true)
    Sep  8 21:30:15.462: INFO: Pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1" satisfied condition "running and ready"
    Sep  8 21:30:16.019: INFO: Successfully updated pod "annotationupdate37f04238-0f47-44ba-828b-26258e0c75b1"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:30:18.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3031" for this suite. 09/08/23 21:30:18.097
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:30:18.136
Sep  8 21:30:18.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pod-network-test 09/08/23 21:30:18.137
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:18.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:18.196
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-1924 09/08/23 21:30:18.205
STEP: creating a selector 09/08/23 21:30:18.205
STEP: Creating the service pods in kubernetes 09/08/23 21:30:18.205
Sep  8 21:30:18.205: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  8 21:30:18.273: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1924" to be "running and ready"
Sep  8 21:30:18.311: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.555ms
Sep  8 21:30:18.311: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:30:20.323: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049939278s
Sep  8 21:30:20.323: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:30:22.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.049773229s
Sep  8 21:30:22.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:24.324: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.050783958s
Sep  8 21:30:24.324: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:26.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.049670503s
Sep  8 21:30:26.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:28.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.047039883s
Sep  8 21:30:28.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:30.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.04845688s
Sep  8 21:30:30.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:32.327: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.053458811s
Sep  8 21:30:32.327: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:34.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.048715247s
Sep  8 21:30:34.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:36.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0493252s
Sep  8 21:30:36.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:38.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048238895s
Sep  8 21:30:38.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 21:30:40.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.047071507s
Sep  8 21:30:40.320: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  8 21:30:40.320: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  8 21:30:40.336: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1924" to be "running and ready"
Sep  8 21:30:40.350: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.33646ms
Sep  8 21:30:40.350: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  8 21:30:40.351: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/08/23 21:30:40.362
Sep  8 21:30:40.396: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1924" to be "running"
Sep  8 21:30:40.409: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.301262ms
Sep  8 21:30:42.419: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022431121s
Sep  8 21:30:42.419: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  8 21:30:42.428: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1924" to be "running"
Sep  8 21:30:42.439: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.901188ms
Sep  8 21:30:42.439: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  8 21:30:42.445: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  8 21:30:42.446: INFO: Going to poll 10.233.75.92 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  8 21:30:42.453: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.75.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1924 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:30:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:30:42.454: INFO: ExecWithOptions: Clientset creation
Sep  8 21:30:42.454: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1924/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.75.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  8 21:30:43.585: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  8 21:30:43.585: INFO: Going to poll 10.233.103.126 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Sep  8 21:30:43.596: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.103.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1924 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:30:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:30:43.597: INFO: ExecWithOptions: Clientset creation
Sep  8 21:30:43.597: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1924/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.103.126+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  8 21:30:44.740: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  8 21:30:44.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1924" for this suite. 09/08/23 21:30:44.768
------------------------------
â€¢ [SLOW TEST] [26.650 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:30:18.136
    Sep  8 21:30:18.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pod-network-test 09/08/23 21:30:18.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:18.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:18.196
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-1924 09/08/23 21:30:18.205
    STEP: creating a selector 09/08/23 21:30:18.205
    STEP: Creating the service pods in kubernetes 09/08/23 21:30:18.205
    Sep  8 21:30:18.205: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  8 21:30:18.273: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1924" to be "running and ready"
    Sep  8 21:30:18.311: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.555ms
    Sep  8 21:30:18.311: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:30:20.323: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049939278s
    Sep  8 21:30:20.323: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:30:22.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.049773229s
    Sep  8 21:30:22.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:24.324: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.050783958s
    Sep  8 21:30:24.324: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:26.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.049670503s
    Sep  8 21:30:26.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:28.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.047039883s
    Sep  8 21:30:28.320: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:30.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.04845688s
    Sep  8 21:30:30.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:32.327: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.053458811s
    Sep  8 21:30:32.327: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:34.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.048715247s
    Sep  8 21:30:34.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:36.323: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.0493252s
    Sep  8 21:30:36.323: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:38.322: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.048238895s
    Sep  8 21:30:38.322: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 21:30:40.320: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.047071507s
    Sep  8 21:30:40.320: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  8 21:30:40.320: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  8 21:30:40.336: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1924" to be "running and ready"
    Sep  8 21:30:40.350: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 14.33646ms
    Sep  8 21:30:40.350: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  8 21:30:40.351: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/08/23 21:30:40.362
    Sep  8 21:30:40.396: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1924" to be "running"
    Sep  8 21:30:40.409: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.301262ms
    Sep  8 21:30:42.419: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.022431121s
    Sep  8 21:30:42.419: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  8 21:30:42.428: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1924" to be "running"
    Sep  8 21:30:42.439: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.901188ms
    Sep  8 21:30:42.439: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  8 21:30:42.445: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  8 21:30:42.446: INFO: Going to poll 10.233.75.92 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  8 21:30:42.453: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.75.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1924 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:30:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:30:42.454: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:30:42.454: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1924/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.75.92+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  8 21:30:43.585: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  8 21:30:43.585: INFO: Going to poll 10.233.103.126 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Sep  8 21:30:43.596: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.103.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1924 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:30:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:30:43.597: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:30:43.597: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1924/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.103.126+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  8 21:30:44.740: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:30:44.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1924" for this suite. 09/08/23 21:30:44.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:30:44.787
Sep  8 21:30:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename server-version 09/08/23 21:30:44.788
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:44.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:44.851
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 09/08/23 21:30:44.858
STEP: Confirm major version 09/08/23 21:30:44.867
Sep  8 21:30:44.867: INFO: Major version: 1
STEP: Confirm minor version 09/08/23 21:30:44.867
Sep  8 21:30:44.867: INFO: cleanMinorVersion: 26
Sep  8 21:30:44.867: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Sep  8 21:30:44.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-718" for this suite. 09/08/23 21:30:44.879
------------------------------
â€¢ [0.112 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:30:44.787
    Sep  8 21:30:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename server-version 09/08/23 21:30:44.788
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:44.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:44.851
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 09/08/23 21:30:44.858
    STEP: Confirm major version 09/08/23 21:30:44.867
    Sep  8 21:30:44.867: INFO: Major version: 1
    STEP: Confirm minor version 09/08/23 21:30:44.867
    Sep  8 21:30:44.867: INFO: cleanMinorVersion: 26
    Sep  8 21:30:44.867: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:30:44.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-718" for this suite. 09/08/23 21:30:44.879
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:30:44.899
Sep  8 21:30:44.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 21:30:44.902
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:44.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:44.963
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Sep  8 21:30:44.987: INFO: Waiting up to 5m0s for pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487" in namespace "container-probe-2680" to be "running and ready"
Sep  8 21:30:45.013: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Pending", Reason="", readiness=false. Elapsed: 25.953926ms
Sep  8 21:30:45.013: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:30:47.028: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 2.04073604s
Sep  8 21:30:47.028: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:30:49.027: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 4.03958142s
Sep  8 21:30:49.027: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:30:51.052: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 6.065351383s
Sep  8 21:30:51.053: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:30:53.031: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 8.043722884s
Sep  8 21:30:53.031: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:30:55.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 10.038488936s
Sep  8 21:30:55.026: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:30:57.032: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 12.044595206s
Sep  8 21:30:57.032: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:30:59.028: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 14.040877417s
Sep  8 21:30:59.028: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:31:01.032: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 16.044901331s
Sep  8 21:31:01.032: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:31:03.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 18.038908759s
Sep  8 21:31:03.026: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:31:05.030: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 20.04326269s
Sep  8 21:31:05.030: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
Sep  8 21:31:07.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=true. Elapsed: 22.038645805s
Sep  8 21:31:07.026: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = true)
Sep  8 21:31:07.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487" satisfied condition "running and ready"
Sep  8 21:31:07.034: INFO: Container started at 2023-09-08 21:30:46 +0000 UTC, pod became ready at 2023-09-08 21:31:05 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:07.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2680" for this suite. 09/08/23 21:31:07.047
------------------------------
â€¢ [SLOW TEST] [22.164 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:30:44.899
    Sep  8 21:30:44.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 21:30:44.902
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:30:44.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:30:44.963
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Sep  8 21:30:44.987: INFO: Waiting up to 5m0s for pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487" in namespace "container-probe-2680" to be "running and ready"
    Sep  8 21:30:45.013: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Pending", Reason="", readiness=false. Elapsed: 25.953926ms
    Sep  8 21:30:45.013: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:30:47.028: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 2.04073604s
    Sep  8 21:30:47.028: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:30:49.027: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 4.03958142s
    Sep  8 21:30:49.027: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:30:51.052: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 6.065351383s
    Sep  8 21:30:51.053: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:30:53.031: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 8.043722884s
    Sep  8 21:30:53.031: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:30:55.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 10.038488936s
    Sep  8 21:30:55.026: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:30:57.032: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 12.044595206s
    Sep  8 21:30:57.032: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:30:59.028: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 14.040877417s
    Sep  8 21:30:59.028: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:31:01.032: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 16.044901331s
    Sep  8 21:31:01.032: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:31:03.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 18.038908759s
    Sep  8 21:31:03.026: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:31:05.030: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=false. Elapsed: 20.04326269s
    Sep  8 21:31:05.030: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = false)
    Sep  8 21:31:07.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487": Phase="Running", Reason="", readiness=true. Elapsed: 22.038645805s
    Sep  8 21:31:07.026: INFO: The phase of Pod test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487 is Running (Ready = true)
    Sep  8 21:31:07.026: INFO: Pod "test-webserver-df7c5bf1-a1f8-493a-863c-9456aeb9b487" satisfied condition "running and ready"
    Sep  8 21:31:07.034: INFO: Container started at 2023-09-08 21:30:46 +0000 UTC, pod became ready at 2023-09-08 21:31:05 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:07.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2680" for this suite. 09/08/23 21:31:07.047
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:07.064
Sep  8 21:31:07.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 21:31:07.065
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:07.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:07.119
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Sep  8 21:31:07.127: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  8 21:31:07.149: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  8 21:31:12.176: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/08/23 21:31:12.176
Sep  8 21:31:12.176: INFO: Creating deployment "test-rolling-update-deployment"
Sep  8 21:31:12.191: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  8 21:31:12.205: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  8 21:31:14.231: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  8 21:31:14.238: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 21:31:14.271: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4490  7188b53c-8f7a-4d76-a546-ed397e1faf38 16835 1 2023-09-08 21:31:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-08 21:31:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003113798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-08 21:31:12 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-08 21:31:14 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  8 21:31:14.281: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4490  87376b95-4543-4006-bfd1-181f0cdb7f89 16824 1 2023-09-08 21:31:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7188b53c-8f7a-4d76-a546-ed397e1faf38 0xc0048952a7 0xc0048952a8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:31:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7188b53c-8f7a-4d76-a546-ed397e1faf38\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004895478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:31:14.281: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  8 21:31:14.281: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4490  d8c0d6f1-821b-44d3-8340-40d4f2530348 16834 2 2023-09-08 21:31:07 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7188b53c-8f7a-4d76-a546-ed397e1faf38 0xc004895177 0xc004895178}] [] [{e2e.test Update apps/v1 2023-09-08 21:31:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7188b53c-8f7a-4d76-a546-ed397e1faf38\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004895238 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:31:14.297: INFO: Pod "test-rolling-update-deployment-7549d9f46d-nqzn5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-nqzn5 test-rolling-update-deployment-7549d9f46d- deployment-4490  08cc6d31-cd23-4363-ba43-9cfc6aada0c0 16823 0 2023-09-08 21:31:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:dd012fa10a6a1018a9c54e2a19ca607e4c103577180e8f1e94ad1ad0b487f4b2 cni.projectcalico.org/podIP:10.233.75.95/32 cni.projectcalico.org/podIPs:10.233.75.95/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 87376b95-4543-4006-bfd1-181f0cdb7f89 0xc004895937 0xc004895938}] [] [{kube-controller-manager Update v1 2023-09-08 21:31:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87376b95-4543-4006-bfd1-181f0cdb7f89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skh62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skh62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.95,StartTime:2023-09-08 21:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:31:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://045469cc5959fb960fb20182b1cfce024e5f6ce503131391ae22e5179a8ee7cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:14.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4490" for this suite. 09/08/23 21:31:14.316
------------------------------
â€¢ [SLOW TEST] [7.275 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:07.064
    Sep  8 21:31:07.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 21:31:07.065
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:07.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:07.119
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Sep  8 21:31:07.127: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Sep  8 21:31:07.149: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  8 21:31:12.176: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/08/23 21:31:12.176
    Sep  8 21:31:12.176: INFO: Creating deployment "test-rolling-update-deployment"
    Sep  8 21:31:12.191: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Sep  8 21:31:12.205: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Sep  8 21:31:14.231: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Sep  8 21:31:14.238: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 21:31:14.271: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4490  7188b53c-8f7a-4d76-a546-ed397e1faf38 16835 1 2023-09-08 21:31:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-09-08 21:31:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003113798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-08 21:31:12 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-09-08 21:31:14 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  8 21:31:14.281: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4490  87376b95-4543-4006-bfd1-181f0cdb7f89 16824 1 2023-09-08 21:31:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7188b53c-8f7a-4d76-a546-ed397e1faf38 0xc0048952a7 0xc0048952a8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:31:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7188b53c-8f7a-4d76-a546-ed397e1faf38\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004895478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:31:14.281: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Sep  8 21:31:14.281: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4490  d8c0d6f1-821b-44d3-8340-40d4f2530348 16834 2 2023-09-08 21:31:07 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7188b53c-8f7a-4d76-a546-ed397e1faf38 0xc004895177 0xc004895178}] [] [{e2e.test Update apps/v1 2023-09-08 21:31:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7188b53c-8f7a-4d76-a546-ed397e1faf38\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004895238 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:31:14.297: INFO: Pod "test-rolling-update-deployment-7549d9f46d-nqzn5" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-nqzn5 test-rolling-update-deployment-7549d9f46d- deployment-4490  08cc6d31-cd23-4363-ba43-9cfc6aada0c0 16823 0 2023-09-08 21:31:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:dd012fa10a6a1018a9c54e2a19ca607e4c103577180e8f1e94ad1ad0b487f4b2 cni.projectcalico.org/podIP:10.233.75.95/32 cni.projectcalico.org/podIPs:10.233.75.95/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 87376b95-4543-4006-bfd1-181f0cdb7f89 0xc004895937 0xc004895938}] [] [{kube-controller-manager Update v1 2023-09-08 21:31:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87376b95-4543-4006-bfd1-181f0cdb7f89\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:31:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:31:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skh62,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skh62,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.95,StartTime:2023-09-08 21:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:31:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://045469cc5959fb960fb20182b1cfce024e5f6ce503131391ae22e5179a8ee7cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:14.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4490" for this suite. 09/08/23 21:31:14.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:14.34
Sep  8 21:31:14.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:31:14.341
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:14.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:14.386
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Sep  8 21:31:14.439: INFO: created pod pod-service-account-defaultsa
Sep  8 21:31:14.439: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  8 21:31:14.466: INFO: created pod pod-service-account-mountsa
Sep  8 21:31:14.466: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  8 21:31:14.499: INFO: created pod pod-service-account-nomountsa
Sep  8 21:31:14.499: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  8 21:31:14.527: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  8 21:31:14.528: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  8 21:31:14.574: INFO: created pod pod-service-account-mountsa-mountspec
Sep  8 21:31:14.574: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  8 21:31:14.600: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  8 21:31:14.600: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  8 21:31:14.621: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  8 21:31:14.621: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  8 21:31:14.651: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  8 21:31:14.651: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  8 21:31:14.674: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  8 21:31:14.674: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:14.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6274" for this suite. 09/08/23 21:31:14.712
------------------------------
â€¢ [0.394 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:14.34
    Sep  8 21:31:14.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:31:14.341
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:14.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:14.386
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Sep  8 21:31:14.439: INFO: created pod pod-service-account-defaultsa
    Sep  8 21:31:14.439: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Sep  8 21:31:14.466: INFO: created pod pod-service-account-mountsa
    Sep  8 21:31:14.466: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Sep  8 21:31:14.499: INFO: created pod pod-service-account-nomountsa
    Sep  8 21:31:14.499: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Sep  8 21:31:14.527: INFO: created pod pod-service-account-defaultsa-mountspec
    Sep  8 21:31:14.528: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Sep  8 21:31:14.574: INFO: created pod pod-service-account-mountsa-mountspec
    Sep  8 21:31:14.574: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Sep  8 21:31:14.600: INFO: created pod pod-service-account-nomountsa-mountspec
    Sep  8 21:31:14.600: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Sep  8 21:31:14.621: INFO: created pod pod-service-account-defaultsa-nomountspec
    Sep  8 21:31:14.621: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Sep  8 21:31:14.651: INFO: created pod pod-service-account-mountsa-nomountspec
    Sep  8 21:31:14.651: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Sep  8 21:31:14.674: INFO: created pod pod-service-account-nomountsa-nomountspec
    Sep  8 21:31:14.674: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:14.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6274" for this suite. 09/08/23 21:31:14.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:14.735
Sep  8 21:31:14.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:31:14.739
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:14.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:14.805
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 09/08/23 21:31:14.819
STEP: Getting a ResourceQuota 09/08/23 21:31:14.834
STEP: Listing all ResourceQuotas with LabelSelector 09/08/23 21:31:14.851
STEP: Patching the ResourceQuota 09/08/23 21:31:14.865
STEP: Deleting a Collection of ResourceQuotas 09/08/23 21:31:14.925
STEP: Verifying the deleted ResourceQuota 09/08/23 21:31:14.967
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:14.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5" for this suite. 09/08/23 21:31:14.988
------------------------------
â€¢ [0.287 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:14.735
    Sep  8 21:31:14.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:31:14.739
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:14.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:14.805
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 09/08/23 21:31:14.819
    STEP: Getting a ResourceQuota 09/08/23 21:31:14.834
    STEP: Listing all ResourceQuotas with LabelSelector 09/08/23 21:31:14.851
    STEP: Patching the ResourceQuota 09/08/23 21:31:14.865
    STEP: Deleting a Collection of ResourceQuotas 09/08/23 21:31:14.925
    STEP: Verifying the deleted ResourceQuota 09/08/23 21:31:14.967
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:14.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5" for this suite. 09/08/23 21:31:14.988
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:15.024
Sep  8 21:31:15.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:31:15.029
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:15.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:15.099
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Sep  8 21:31:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/08/23 21:31:24.58
Sep  8 21:31:24.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
Sep  8 21:31:25.768: INFO: stderr: ""
Sep  8 21:31:25.768: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  8 21:31:25.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 delete e2e-test-crd-publish-openapi-4205-crds test-foo'
Sep  8 21:31:25.954: INFO: stderr: ""
Sep  8 21:31:25.954: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  8 21:31:25.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 apply -f -'
Sep  8 21:31:27.007: INFO: stderr: ""
Sep  8 21:31:27.007: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  8 21:31:27.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 delete e2e-test-crd-publish-openapi-4205-crds test-foo'
Sep  8 21:31:27.150: INFO: stderr: ""
Sep  8 21:31:27.150: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/08/23 21:31:27.15
Sep  8 21:31:27.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
Sep  8 21:31:28.155: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/08/23 21:31:28.155
Sep  8 21:31:28.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
Sep  8 21:31:28.475: INFO: rc: 1
Sep  8 21:31:28.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 apply -f -'
Sep  8 21:31:28.819: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/08/23 21:31:28.819
Sep  8 21:31:28.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
Sep  8 21:31:29.154: INFO: rc: 1
Sep  8 21:31:29.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 apply -f -'
Sep  8 21:31:29.508: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 09/08/23 21:31:29.508
Sep  8 21:31:29.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds'
Sep  8 21:31:29.860: INFO: stderr: ""
Sep  8 21:31:29.860: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 09/08/23 21:31:29.861
Sep  8 21:31:29.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.metadata'
Sep  8 21:31:30.190: INFO: stderr: ""
Sep  8 21:31:30.190: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  8 21:31:30.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.spec'
Sep  8 21:31:30.492: INFO: stderr: ""
Sep  8 21:31:30.492: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  8 21:31:30.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.spec.bars'
Sep  8 21:31:30.805: INFO: stderr: ""
Sep  8 21:31:30.805: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/08/23 21:31:30.805
Sep  8 21:31:30.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.spec.bars2'
Sep  8 21:31:31.138: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:34.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6777" for this suite. 09/08/23 21:31:34.081
------------------------------
â€¢ [SLOW TEST] [19.077 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:15.024
    Sep  8 21:31:15.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:31:15.029
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:15.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:15.099
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Sep  8 21:31:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 09/08/23 21:31:24.58
    Sep  8 21:31:24.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
    Sep  8 21:31:25.768: INFO: stderr: ""
    Sep  8 21:31:25.768: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  8 21:31:25.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 delete e2e-test-crd-publish-openapi-4205-crds test-foo'
    Sep  8 21:31:25.954: INFO: stderr: ""
    Sep  8 21:31:25.954: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Sep  8 21:31:25.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 apply -f -'
    Sep  8 21:31:27.007: INFO: stderr: ""
    Sep  8 21:31:27.007: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Sep  8 21:31:27.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 delete e2e-test-crd-publish-openapi-4205-crds test-foo'
    Sep  8 21:31:27.150: INFO: stderr: ""
    Sep  8 21:31:27.150: INFO: stdout: "e2e-test-crd-publish-openapi-4205-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 09/08/23 21:31:27.15
    Sep  8 21:31:27.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
    Sep  8 21:31:28.155: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 09/08/23 21:31:28.155
    Sep  8 21:31:28.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
    Sep  8 21:31:28.475: INFO: rc: 1
    Sep  8 21:31:28.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 apply -f -'
    Sep  8 21:31:28.819: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 09/08/23 21:31:28.819
    Sep  8 21:31:28.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 create -f -'
    Sep  8 21:31:29.154: INFO: rc: 1
    Sep  8 21:31:29.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 --namespace=crd-publish-openapi-6777 apply -f -'
    Sep  8 21:31:29.508: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 09/08/23 21:31:29.508
    Sep  8 21:31:29.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds'
    Sep  8 21:31:29.860: INFO: stderr: ""
    Sep  8 21:31:29.860: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 09/08/23 21:31:29.861
    Sep  8 21:31:29.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.metadata'
    Sep  8 21:31:30.190: INFO: stderr: ""
    Sep  8 21:31:30.190: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Sep  8 21:31:30.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.spec'
    Sep  8 21:31:30.492: INFO: stderr: ""
    Sep  8 21:31:30.492: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Sep  8 21:31:30.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.spec.bars'
    Sep  8 21:31:30.805: INFO: stderr: ""
    Sep  8 21:31:30.805: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4205-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 09/08/23 21:31:30.805
    Sep  8 21:31:30.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-6777 explain e2e-test-crd-publish-openapi-4205-crds.spec.bars2'
    Sep  8 21:31:31.138: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:34.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6777" for this suite. 09/08/23 21:31:34.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:34.108
Sep  8 21:31:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename daemonsets 09/08/23 21:31:34.109
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:34.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:34.167
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 09/08/23 21:31:34.242
STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 21:31:34.268
Sep  8 21:31:34.287: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:34.287: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:34.287: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:34.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:31:34.296: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:31:35.334: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:35.334: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:35.334: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:35.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:31:35.353: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:31:36.307: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:36.307: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:36.307: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:31:36.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 21:31:36.331: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 09/08/23 21:31:36.341
STEP: DeleteCollection of the DaemonSets 09/08/23 21:31:36.355
STEP: Verify that ReplicaSets have been deleted 09/08/23 21:31:36.381
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Sep  8 21:31:36.435: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17167"},"items":null}

Sep  8 21:31:36.445: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17168"},"items":[{"metadata":{"name":"daemon-set-h4jdd","generateName":"daemon-set-","namespace":"daemonsets-2093","uid":"1a0f8dca-4947-4123-aa16-1655729f9b9e","resourceVersion":"17166","creationTimestamp":"2023-09-08T21:31:34Z","deletionTimestamp":"2023-09-08T21:32:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d3bdab1f419069caed0d35ae0618aaf00171b7122243d33ff0544b1ae811ef47","cni.projectcalico.org/podIP":"10.233.103.79/32","cni.projectcalico.org/podIPs":"10.233.103.79/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0f00bce6-528c-4c1b-ab11-11e323213c37","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f00bce6-528c-4c1b-ab11-11e323213c37\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dnfnp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dnfnp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"}],"hostIP":"10.100.16.26","podIP":"10.233.103.79","podIPs":[{"ip":"10.233.103.79"}],"startTime":"2023-09-08T21:31:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-08T21:31:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5a4fc951816196e7298f45ad81a95a3eadaadba74a220c834e0d1202a688cdb3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-k8zh6","generateName":"daemon-set-","namespace":"daemonsets-2093","uid":"34ed3e7a-eae0-480d-8cd7-5a0ebf33673a","resourceVersion":"17167","creationTimestamp":"2023-09-08T21:31:34Z","deletionTimestamp":"2023-09-08T21:32:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c5e3ba1294acf28c13fad82b3c571184f44316dc58eec62ddb6acd1c738edd93","cni.projectcalico.org/podIP":"10.233.75.99/32","cni.projectcalico.org/podIPs":"10.233.75.99/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0f00bce6-528c-4c1b-ab11-11e323213c37","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f00bce6-528c-4c1b-ab11-11e323213c37\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rvdc9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rvdc9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"}],"hostIP":"10.100.19.129","podIP":"10.233.75.99","podIPs":[{"ip":"10.233.75.99"}],"startTime":"2023-09-08T21:31:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-08T21:31:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://20ad48375cc951794a4b5a21c6728210e490fd37b1fc830e18ff2311bcde2df7","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:36.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2093" for this suite. 09/08/23 21:31:36.489
------------------------------
â€¢ [2.400 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:34.108
    Sep  8 21:31:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename daemonsets 09/08/23 21:31:34.109
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:34.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:34.167
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 09/08/23 21:31:34.242
    STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 21:31:34.268
    Sep  8 21:31:34.287: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:34.287: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:34.287: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:34.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:31:34.296: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:31:35.334: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:35.334: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:35.334: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:35.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:31:35.353: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:31:36.307: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:36.307: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:36.307: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:31:36.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 21:31:36.331: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 09/08/23 21:31:36.341
    STEP: DeleteCollection of the DaemonSets 09/08/23 21:31:36.355
    STEP: Verify that ReplicaSets have been deleted 09/08/23 21:31:36.381
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Sep  8 21:31:36.435: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17167"},"items":null}

    Sep  8 21:31:36.445: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17168"},"items":[{"metadata":{"name":"daemon-set-h4jdd","generateName":"daemon-set-","namespace":"daemonsets-2093","uid":"1a0f8dca-4947-4123-aa16-1655729f9b9e","resourceVersion":"17166","creationTimestamp":"2023-09-08T21:31:34Z","deletionTimestamp":"2023-09-08T21:32:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d3bdab1f419069caed0d35ae0618aaf00171b7122243d33ff0544b1ae811ef47","cni.projectcalico.org/podIP":"10.233.103.79/32","cni.projectcalico.org/podIPs":"10.233.103.79/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0f00bce6-528c-4c1b-ab11-11e323213c37","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f00bce6-528c-4c1b-ab11-11e323213c37\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dnfnp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dnfnp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-4","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-4"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"}],"hostIP":"10.100.16.26","podIP":"10.233.103.79","podIPs":[{"ip":"10.233.103.79"}],"startTime":"2023-09-08T21:31:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-08T21:31:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5a4fc951816196e7298f45ad81a95a3eadaadba74a220c834e0d1202a688cdb3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-k8zh6","generateName":"daemon-set-","namespace":"daemonsets-2093","uid":"34ed3e7a-eae0-480d-8cd7-5a0ebf33673a","resourceVersion":"17167","creationTimestamp":"2023-09-08T21:31:34Z","deletionTimestamp":"2023-09-08T21:32:06Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c5e3ba1294acf28c13fad82b3c571184f44316dc58eec62ddb6acd1c738edd93","cni.projectcalico.org/podIP":"10.233.75.99/32","cni.projectcalico.org/podIPs":"10.233.75.99/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0f00bce6-528c-4c1b-ab11-11e323213c37","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f00bce6-528c-4c1b-ab11-11e323213c37\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-09-08T21:31:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rvdc9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rvdc9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:36Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-09-08T21:31:34Z"}],"hostIP":"10.100.19.129","podIP":"10.233.75.99","podIPs":[{"ip":"10.233.75.99"}],"startTime":"2023-09-08T21:31:34Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-09-08T21:31:35Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://20ad48375cc951794a4b5a21c6728210e490fd37b1fc830e18ff2311bcde2df7","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:36.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2093" for this suite. 09/08/23 21:31:36.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:36.508
Sep  8 21:31:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:31:36.509
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:36.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:36.575
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:36.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3432" for this suite. 09/08/23 21:31:36.613
------------------------------
â€¢ [0.125 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:36.508
    Sep  8 21:31:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:31:36.509
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:36.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:36.575
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:36.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3432" for this suite. 09/08/23 21:31:36.613
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:36.633
Sep  8 21:31:36.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/08/23 21:31:36.635
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:36.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:36.696
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 09/08/23 21:31:36.701
STEP: Creating hostNetwork=false pod 09/08/23 21:31:36.701
Sep  8 21:31:36.725: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4691" to be "running and ready"
Sep  8 21:31:36.732: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.710923ms
Sep  8 21:31:36.732: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:31:38.747: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022659057s
Sep  8 21:31:38.747: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:31:40.743: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018650346s
Sep  8 21:31:40.743: INFO: The phase of Pod test-pod is Running (Ready = true)
Sep  8 21:31:40.743: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 09/08/23 21:31:40.752
Sep  8 21:31:40.767: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4691" to be "running and ready"
Sep  8 21:31:40.778: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.385061ms
Sep  8 21:31:40.778: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:31:42.802: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034828382s
Sep  8 21:31:42.802: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Sep  8 21:31:42.802: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 09/08/23 21:31:42.813
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/08/23 21:31:42.813
Sep  8 21:31:42.813: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:42.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:42.815: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:42.815: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  8 21:31:42.965: INFO: Exec stderr: ""
Sep  8 21:31:42.965: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:42.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:42.966: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:42.966: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  8 21:31:43.132: INFO: Exec stderr: ""
Sep  8 21:31:43.132: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:43.133: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:43.133: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  8 21:31:43.264: INFO: Exec stderr: ""
Sep  8 21:31:43.264: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:43.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:43.266: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:43.266: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  8 21:31:43.405: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/08/23 21:31:43.405
Sep  8 21:31:43.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:43.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:43.407: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:43.407: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  8 21:31:43.573: INFO: Exec stderr: ""
Sep  8 21:31:43.573: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:43.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:43.574: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:43.574: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Sep  8 21:31:43.723: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/08/23 21:31:43.723
Sep  8 21:31:43.723: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:43.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:43.724: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:43.724: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  8 21:31:43.841: INFO: Exec stderr: ""
Sep  8 21:31:43.841: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:43.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:43.842: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:43.842: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Sep  8 21:31:43.973: INFO: Exec stderr: ""
Sep  8 21:31:43.973: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:43.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:43.974: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:43.975: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  8 21:31:44.104: INFO: Exec stderr: ""
Sep  8 21:31:44.105: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:31:44.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:31:44.106: INFO: ExecWithOptions: Clientset creation
Sep  8 21:31:44.106: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Sep  8 21:31:44.251: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:44.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4691" for this suite. 09/08/23 21:31:44.269
------------------------------
â€¢ [SLOW TEST] [7.654 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:36.633
    Sep  8 21:31:36.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 09/08/23 21:31:36.635
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:36.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:36.696
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 09/08/23 21:31:36.701
    STEP: Creating hostNetwork=false pod 09/08/23 21:31:36.701
    Sep  8 21:31:36.725: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-4691" to be "running and ready"
    Sep  8 21:31:36.732: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.710923ms
    Sep  8 21:31:36.732: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:31:38.747: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022659057s
    Sep  8 21:31:38.747: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:31:40.743: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018650346s
    Sep  8 21:31:40.743: INFO: The phase of Pod test-pod is Running (Ready = true)
    Sep  8 21:31:40.743: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 09/08/23 21:31:40.752
    Sep  8 21:31:40.767: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-4691" to be "running and ready"
    Sep  8 21:31:40.778: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.385061ms
    Sep  8 21:31:40.778: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:31:42.802: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.034828382s
    Sep  8 21:31:42.802: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Sep  8 21:31:42.802: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 09/08/23 21:31:42.813
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 09/08/23 21:31:42.813
    Sep  8 21:31:42.813: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:42.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:42.815: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:42.815: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  8 21:31:42.965: INFO: Exec stderr: ""
    Sep  8 21:31:42.965: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:42.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:42.966: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:42.966: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  8 21:31:43.132: INFO: Exec stderr: ""
    Sep  8 21:31:43.132: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:43.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:43.133: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:43.133: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  8 21:31:43.264: INFO: Exec stderr: ""
    Sep  8 21:31:43.264: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:43.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:43.266: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:43.266: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  8 21:31:43.405: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 09/08/23 21:31:43.405
    Sep  8 21:31:43.406: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:43.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:43.407: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:43.407: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  8 21:31:43.573: INFO: Exec stderr: ""
    Sep  8 21:31:43.573: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:43.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:43.574: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:43.574: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Sep  8 21:31:43.723: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 09/08/23 21:31:43.723
    Sep  8 21:31:43.723: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:43.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:43.724: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:43.724: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  8 21:31:43.841: INFO: Exec stderr: ""
    Sep  8 21:31:43.841: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:43.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:43.842: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:43.842: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Sep  8 21:31:43.973: INFO: Exec stderr: ""
    Sep  8 21:31:43.973: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:43.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:43.974: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:43.975: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  8 21:31:44.104: INFO: Exec stderr: ""
    Sep  8 21:31:44.105: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4691 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:31:44.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:31:44.106: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:31:44.106: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4691/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Sep  8 21:31:44.251: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:44.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-4691" for this suite. 09/08/23 21:31:44.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:44.292
Sep  8 21:31:44.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 21:31:44.294
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:44.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:44.35
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-e07863c8-31ef-4982-80f9-225c036269dc 09/08/23 21:31:44.356
STEP: Creating a pod to test consume secrets 09/08/23 21:31:44.383
Sep  8 21:31:44.409: INFO: Waiting up to 5m0s for pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6" in namespace "secrets-3611" to be "Succeeded or Failed"
Sep  8 21:31:44.433: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.561085ms
Sep  8 21:31:46.442: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033576526s
Sep  8 21:31:48.444: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035457399s
STEP: Saw pod success 09/08/23 21:31:48.444
Sep  8 21:31:48.445: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6" satisfied condition "Succeeded or Failed"
Sep  8 21:31:48.454: INFO: Trying to get logs from node node-3 pod pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6 container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 21:31:48.485
Sep  8 21:31:48.526: INFO: Waiting for pod pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6 to disappear
Sep  8 21:31:48.545: INFO: Pod pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:48.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3611" for this suite. 09/08/23 21:31:48.559
------------------------------
â€¢ [4.282 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:44.292
    Sep  8 21:31:44.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 21:31:44.294
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:44.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:44.35
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-e07863c8-31ef-4982-80f9-225c036269dc 09/08/23 21:31:44.356
    STEP: Creating a pod to test consume secrets 09/08/23 21:31:44.383
    Sep  8 21:31:44.409: INFO: Waiting up to 5m0s for pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6" in namespace "secrets-3611" to be "Succeeded or Failed"
    Sep  8 21:31:44.433: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.561085ms
    Sep  8 21:31:46.442: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033576526s
    Sep  8 21:31:48.444: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035457399s
    STEP: Saw pod success 09/08/23 21:31:48.444
    Sep  8 21:31:48.445: INFO: Pod "pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6" satisfied condition "Succeeded or Failed"
    Sep  8 21:31:48.454: INFO: Trying to get logs from node node-3 pod pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6 container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 21:31:48.485
    Sep  8 21:31:48.526: INFO: Waiting for pod pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6 to disappear
    Sep  8 21:31:48.545: INFO: Pod pod-secrets-90500208-d0f0-44f9-b825-8a673dcb74b6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:48.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3611" for this suite. 09/08/23 21:31:48.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:48.576
Sep  8 21:31:48.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename gc 09/08/23 21:31:48.578
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:48.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:48.626
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 09/08/23 21:31:48.636
STEP: Wait for the Deployment to create new ReplicaSet 09/08/23 21:31:48.649
STEP: delete the deployment 09/08/23 21:31:49.179
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/08/23 21:31:49.207
STEP: Gathering metrics 09/08/23 21:31:49.811
Sep  8 21:31:49.879: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
Sep  8 21:31:49.893: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.758472ms
Sep  8 21:31:49.893: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
Sep  8 21:31:49.893: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
Sep  8 21:31:50.035: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:50.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8082" for this suite. 09/08/23 21:31:50.049
------------------------------
â€¢ [1.497 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:48.576
    Sep  8 21:31:48.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename gc 09/08/23 21:31:48.578
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:48.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:48.626
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 09/08/23 21:31:48.636
    STEP: Wait for the Deployment to create new ReplicaSet 09/08/23 21:31:48.649
    STEP: delete the deployment 09/08/23 21:31:49.179
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 09/08/23 21:31:49.207
    STEP: Gathering metrics 09/08/23 21:31:49.811
    Sep  8 21:31:49.879: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
    Sep  8 21:31:49.893: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 13.758472ms
    Sep  8 21:31:49.893: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
    Sep  8 21:31:49.893: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
    Sep  8 21:31:50.035: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:50.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8082" for this suite. 09/08/23 21:31:50.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:50.074
Sep  8 21:31:50.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:31:50.075
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:50.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:50.142
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-4281 09/08/23 21:31:50.153
STEP: creating replication controller nodeport-test in namespace services-4281 09/08/23 21:31:50.207
I0908 21:31:50.227906      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4281, replica count: 2
I0908 21:31:53.279680      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 21:31:53.279: INFO: Creating new exec pod
Sep  8 21:31:53.327: INFO: Waiting up to 5m0s for pod "execpodz45qd" in namespace "services-4281" to be "running"
Sep  8 21:31:53.347: INFO: Pod "execpodz45qd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.150075ms
Sep  8 21:31:55.362: INFO: Pod "execpodz45qd": Phase="Running", Reason="", readiness=true. Elapsed: 2.034530576s
Sep  8 21:31:55.362: INFO: Pod "execpodz45qd" satisfied condition "running"
Sep  8 21:31:56.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Sep  8 21:31:56.668: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep  8 21:31:56.668: INFO: stdout: ""
Sep  8 21:31:56.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 10.233.63.196 80'
Sep  8 21:31:56.934: INFO: stderr: "+ nc -v -z -w 2 10.233.63.196 80\nConnection to 10.233.63.196 80 port [tcp/http] succeeded!\n"
Sep  8 21:31:56.934: INFO: stdout: ""
Sep  8 21:31:56.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 30762'
Sep  8 21:31:57.185: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 30762\nConnection to 10.100.19.129 30762 port [tcp/*] succeeded!\n"
Sep  8 21:31:57.185: INFO: stdout: ""
Sep  8 21:31:57.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 30762'
Sep  8 21:31:57.440: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 30762\nConnection to 10.100.16.26 30762 port [tcp/*] succeeded!\n"
Sep  8 21:31:57.440: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:57.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4281" for this suite. 09/08/23 21:31:57.455
------------------------------
â€¢ [SLOW TEST] [7.404 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:50.074
    Sep  8 21:31:50.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:31:50.075
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:50.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:50.142
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-4281 09/08/23 21:31:50.153
    STEP: creating replication controller nodeport-test in namespace services-4281 09/08/23 21:31:50.207
    I0908 21:31:50.227906      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4281, replica count: 2
    I0908 21:31:53.279680      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 21:31:53.279: INFO: Creating new exec pod
    Sep  8 21:31:53.327: INFO: Waiting up to 5m0s for pod "execpodz45qd" in namespace "services-4281" to be "running"
    Sep  8 21:31:53.347: INFO: Pod "execpodz45qd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.150075ms
    Sep  8 21:31:55.362: INFO: Pod "execpodz45qd": Phase="Running", Reason="", readiness=true. Elapsed: 2.034530576s
    Sep  8 21:31:55.362: INFO: Pod "execpodz45qd" satisfied condition "running"
    Sep  8 21:31:56.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Sep  8 21:31:56.668: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Sep  8 21:31:56.668: INFO: stdout: ""
    Sep  8 21:31:56.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 10.233.63.196 80'
    Sep  8 21:31:56.934: INFO: stderr: "+ nc -v -z -w 2 10.233.63.196 80\nConnection to 10.233.63.196 80 port [tcp/http] succeeded!\n"
    Sep  8 21:31:56.934: INFO: stdout: ""
    Sep  8 21:31:56.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 30762'
    Sep  8 21:31:57.185: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 30762\nConnection to 10.100.19.129 30762 port [tcp/*] succeeded!\n"
    Sep  8 21:31:57.185: INFO: stdout: ""
    Sep  8 21:31:57.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-4281 exec execpodz45qd -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 30762'
    Sep  8 21:31:57.440: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 30762\nConnection to 10.100.16.26 30762 port [tcp/*] succeeded!\n"
    Sep  8 21:31:57.440: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:57.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4281" for this suite. 09/08/23 21:31:57.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:57.479
Sep  8 21:31:57.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename events 09/08/23 21:31:57.48
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:57.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:57.523
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 09/08/23 21:31:57.529
Sep  8 21:31:57.553: INFO: created test-event-1
Sep  8 21:31:57.561: INFO: created test-event-2
Sep  8 21:31:57.570: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 09/08/23 21:31:57.571
STEP: delete collection of events 09/08/23 21:31:57.581
Sep  8 21:31:57.581: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/08/23 21:31:57.645
Sep  8 21:31:57.646: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  8 21:31:57.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9535" for this suite. 09/08/23 21:31:57.68
------------------------------
â€¢ [0.219 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:57.479
    Sep  8 21:31:57.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename events 09/08/23 21:31:57.48
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:57.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:57.523
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 09/08/23 21:31:57.529
    Sep  8 21:31:57.553: INFO: created test-event-1
    Sep  8 21:31:57.561: INFO: created test-event-2
    Sep  8 21:31:57.570: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 09/08/23 21:31:57.571
    STEP: delete collection of events 09/08/23 21:31:57.581
    Sep  8 21:31:57.581: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/08/23 21:31:57.645
    Sep  8 21:31:57.646: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:31:57.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9535" for this suite. 09/08/23 21:31:57.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:31:57.7
Sep  8 21:31:57.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:31:57.702
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:57.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:57.746
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/08/23 21:31:57.75
Sep  8 21:31:57.767: INFO: Waiting up to 5m0s for pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91" in namespace "emptydir-5162" to be "Succeeded or Failed"
Sep  8 21:31:57.777: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91": Phase="Pending", Reason="", readiness=false. Elapsed: 9.233625ms
Sep  8 21:31:59.788: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020485551s
Sep  8 21:32:01.797: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029519486s
STEP: Saw pod success 09/08/23 21:32:01.797
Sep  8 21:32:01.797: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91" satisfied condition "Succeeded or Failed"
Sep  8 21:32:01.807: INFO: Trying to get logs from node node-3 pod pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91 container test-container: <nil>
STEP: delete the pod 09/08/23 21:32:01.828
Sep  8 21:32:01.875: INFO: Waiting for pod pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91 to disappear
Sep  8 21:32:01.885: INFO: Pod pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:01.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5162" for this suite. 09/08/23 21:32:01.905
------------------------------
â€¢ [4.235 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:31:57.7
    Sep  8 21:31:57.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:31:57.702
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:31:57.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:31:57.746
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/08/23 21:31:57.75
    Sep  8 21:31:57.767: INFO: Waiting up to 5m0s for pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91" in namespace "emptydir-5162" to be "Succeeded or Failed"
    Sep  8 21:31:57.777: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91": Phase="Pending", Reason="", readiness=false. Elapsed: 9.233625ms
    Sep  8 21:31:59.788: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020485551s
    Sep  8 21:32:01.797: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029519486s
    STEP: Saw pod success 09/08/23 21:32:01.797
    Sep  8 21:32:01.797: INFO: Pod "pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91" satisfied condition "Succeeded or Failed"
    Sep  8 21:32:01.807: INFO: Trying to get logs from node node-3 pod pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91 container test-container: <nil>
    STEP: delete the pod 09/08/23 21:32:01.828
    Sep  8 21:32:01.875: INFO: Waiting for pod pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91 to disappear
    Sep  8 21:32:01.885: INFO: Pod pod-20429f61-fa02-468b-b8ed-eb4e6cf14c91 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:01.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5162" for this suite. 09/08/23 21:32:01.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:01.938
Sep  8 21:32:01.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:32:01.939
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:02.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:02.071
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 09/08/23 21:32:02.081
Sep  8 21:32:02.105: INFO: Waiting up to 5m0s for pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844" in namespace "emptydir-5014" to be "Succeeded or Failed"
Sep  8 21:32:02.129: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Pending", Reason="", readiness=false. Elapsed: 23.232846ms
Sep  8 21:32:04.143: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037442213s
Sep  8 21:32:06.141: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035561325s
Sep  8 21:32:08.145: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039437211s
STEP: Saw pod success 09/08/23 21:32:08.145
Sep  8 21:32:08.145: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844" satisfied condition "Succeeded or Failed"
Sep  8 21:32:08.169: INFO: Trying to get logs from node node-3 pod pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844 container test-container: <nil>
STEP: delete the pod 09/08/23 21:32:08.194
Sep  8 21:32:08.240: INFO: Waiting for pod pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844 to disappear
Sep  8 21:32:08.256: INFO: Pod pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:08.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5014" for this suite. 09/08/23 21:32:08.272
------------------------------
â€¢ [SLOW TEST] [6.350 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:01.938
    Sep  8 21:32:01.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:32:01.939
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:02.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:02.071
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 09/08/23 21:32:02.081
    Sep  8 21:32:02.105: INFO: Waiting up to 5m0s for pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844" in namespace "emptydir-5014" to be "Succeeded or Failed"
    Sep  8 21:32:02.129: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Pending", Reason="", readiness=false. Elapsed: 23.232846ms
    Sep  8 21:32:04.143: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037442213s
    Sep  8 21:32:06.141: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035561325s
    Sep  8 21:32:08.145: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039437211s
    STEP: Saw pod success 09/08/23 21:32:08.145
    Sep  8 21:32:08.145: INFO: Pod "pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844" satisfied condition "Succeeded or Failed"
    Sep  8 21:32:08.169: INFO: Trying to get logs from node node-3 pod pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844 container test-container: <nil>
    STEP: delete the pod 09/08/23 21:32:08.194
    Sep  8 21:32:08.240: INFO: Waiting for pod pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844 to disappear
    Sep  8 21:32:08.256: INFO: Pod pod-b12fc7ef-6211-41d1-93ae-9c1cf1db0844 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:08.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5014" for this suite. 09/08/23 21:32:08.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:08.293
Sep  8 21:32:08.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:32:08.294
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:08.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:08.339
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:32:08.373
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:32:08.904
STEP: Deploying the webhook pod 09/08/23 21:32:08.941
STEP: Wait for the deployment to be ready 09/08/23 21:32:08.978
Sep  8 21:32:09.004: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/08/23 21:32:11.035
STEP: Verifying the service has paired with the endpoint 09/08/23 21:32:11.081
Sep  8 21:32:12.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 09/08/23 21:32:12.094
STEP: create a pod 09/08/23 21:32:12.158
Sep  8 21:32:12.179: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5552" to be "running"
Sep  8 21:32:12.192: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.02326ms
Sep  8 21:32:14.204: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024956076s
Sep  8 21:32:14.204: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 09/08/23 21:32:14.204
Sep  8 21:32:14.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=webhook-5552 attach --namespace=webhook-5552 to-be-attached-pod -i -c=container1'
Sep  8 21:32:14.342: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:14.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5552" for this suite. 09/08/23 21:32:14.52
STEP: Destroying namespace "webhook-5552-markers" for this suite. 09/08/23 21:32:14.557
------------------------------
â€¢ [SLOW TEST] [6.291 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:08.293
    Sep  8 21:32:08.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:32:08.294
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:08.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:08.339
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:32:08.373
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:32:08.904
    STEP: Deploying the webhook pod 09/08/23 21:32:08.941
    STEP: Wait for the deployment to be ready 09/08/23 21:32:08.978
    Sep  8 21:32:09.004: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/08/23 21:32:11.035
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:32:11.081
    Sep  8 21:32:12.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 09/08/23 21:32:12.094
    STEP: create a pod 09/08/23 21:32:12.158
    Sep  8 21:32:12.179: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5552" to be "running"
    Sep  8 21:32:12.192: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.02326ms
    Sep  8 21:32:14.204: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024956076s
    Sep  8 21:32:14.204: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 09/08/23 21:32:14.204
    Sep  8 21:32:14.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=webhook-5552 attach --namespace=webhook-5552 to-be-attached-pod -i -c=container1'
    Sep  8 21:32:14.342: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:14.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5552" for this suite. 09/08/23 21:32:14.52
    STEP: Destroying namespace "webhook-5552-markers" for this suite. 09/08/23 21:32:14.557
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:14.584
Sep  8 21:32:14.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-runtime 09/08/23 21:32:14.585
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:14.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:14.645
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 09/08/23 21:32:14.661
STEP: wait for the container to reach Succeeded 09/08/23 21:32:14.688
STEP: get the container status 09/08/23 21:32:20.819
STEP: the container should be terminated 09/08/23 21:32:20.831
STEP: the termination message should be set 09/08/23 21:32:20.831
Sep  8 21:32:20.831: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/08/23 21:32:20.832
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:20.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9651" for this suite. 09/08/23 21:32:20.908
------------------------------
â€¢ [SLOW TEST] [6.350 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:14.584
    Sep  8 21:32:14.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-runtime 09/08/23 21:32:14.585
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:14.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:14.645
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 09/08/23 21:32:14.661
    STEP: wait for the container to reach Succeeded 09/08/23 21:32:14.688
    STEP: get the container status 09/08/23 21:32:20.819
    STEP: the container should be terminated 09/08/23 21:32:20.831
    STEP: the termination message should be set 09/08/23 21:32:20.831
    Sep  8 21:32:20.831: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/08/23 21:32:20.832
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:20.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9651" for this suite. 09/08/23 21:32:20.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:20.935
Sep  8 21:32:20.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 21:32:20.939
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:20.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:20.993
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Sep  8 21:32:21.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: creating the pod 09/08/23 21:32:21.001
STEP: submitting the pod to kubernetes 09/08/23 21:32:21.002
Sep  8 21:32:21.034: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a" in namespace "pods-542" to be "running and ready"
Sep  8 21:32:21.060: INFO: Pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.532896ms
Sep  8 21:32:21.061: INFO: The phase of Pod pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:32:23.074: INFO: Pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a": Phase="Running", Reason="", readiness=true. Elapsed: 2.04053278s
Sep  8 21:32:23.075: INFO: The phase of Pod pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a is Running (Ready = true)
Sep  8 21:32:23.075: INFO: Pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:23.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-542" for this suite. 09/08/23 21:32:23.256
------------------------------
â€¢ [2.343 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:20.935
    Sep  8 21:32:20.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 21:32:20.939
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:20.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:20.993
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Sep  8 21:32:21.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: creating the pod 09/08/23 21:32:21.001
    STEP: submitting the pod to kubernetes 09/08/23 21:32:21.002
    Sep  8 21:32:21.034: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a" in namespace "pods-542" to be "running and ready"
    Sep  8 21:32:21.060: INFO: Pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.532896ms
    Sep  8 21:32:21.061: INFO: The phase of Pod pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:32:23.074: INFO: Pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a": Phase="Running", Reason="", readiness=true. Elapsed: 2.04053278s
    Sep  8 21:32:23.075: INFO: The phase of Pod pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a is Running (Ready = true)
    Sep  8 21:32:23.075: INFO: Pod "pod-exec-websocket-e8533855-0cec-481c-9c0a-ac482176128a" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:23.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-542" for this suite. 09/08/23 21:32:23.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:23.29
Sep  8 21:32:23.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replication-controller 09/08/23 21:32:23.291
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:23.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:23.355
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8 09/08/23 21:32:23.366
Sep  8 21:32:23.397: INFO: Pod name my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8: Found 0 pods out of 1
Sep  8 21:32:28.409: INFO: Pod name my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8: Found 1 pods out of 1
Sep  8 21:32:28.409: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8" are running
Sep  8 21:32:28.409: INFO: Waiting up to 5m0s for pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4" in namespace "replication-controller-6537" to be "running"
Sep  8 21:32:28.420: INFO: Pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4": Phase="Running", Reason="", readiness=true. Elapsed: 10.453482ms
Sep  8 21:32:28.420: INFO: Pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4" satisfied condition "running"
Sep  8 21:32:28.420: INFO: Pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:23 +0000 UTC Reason: Message:}])
Sep  8 21:32:28.420: INFO: Trying to dial the pod
Sep  8 21:32:33.445: INFO: Controller my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8: Got expected result from replica 1 [my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4]: "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:33.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6537" for this suite. 09/08/23 21:32:33.462
------------------------------
â€¢ [SLOW TEST] [10.188 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:23.29
    Sep  8 21:32:23.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replication-controller 09/08/23 21:32:23.291
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:23.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:23.355
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8 09/08/23 21:32:23.366
    Sep  8 21:32:23.397: INFO: Pod name my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8: Found 0 pods out of 1
    Sep  8 21:32:28.409: INFO: Pod name my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8: Found 1 pods out of 1
    Sep  8 21:32:28.409: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8" are running
    Sep  8 21:32:28.409: INFO: Waiting up to 5m0s for pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4" in namespace "replication-controller-6537" to be "running"
    Sep  8 21:32:28.420: INFO: Pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4": Phase="Running", Reason="", readiness=true. Elapsed: 10.453482ms
    Sep  8 21:32:28.420: INFO: Pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4" satisfied condition "running"
    Sep  8 21:32:28.420: INFO: Pod "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:32:23 +0000 UTC Reason: Message:}])
    Sep  8 21:32:28.420: INFO: Trying to dial the pod
    Sep  8 21:32:33.445: INFO: Controller my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8: Got expected result from replica 1 [my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4]: "my-hostname-basic-94950fe8-6a08-46bd-86cd-58ed6f6043f8-xz8g4", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:33.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6537" for this suite. 09/08/23 21:32:33.462
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:33.478
Sep  8 21:32:33.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename namespaces 09/08/23 21:32:33.487
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:33.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:33.558
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 09/08/23 21:32:33.562
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:33.612
STEP: Creating a service in the namespace 09/08/23 21:32:33.621
STEP: Deleting the namespace 09/08/23 21:32:33.667
STEP: Waiting for the namespace to be removed. 09/08/23 21:32:33.716
STEP: Recreating the namespace 09/08/23 21:32:40.727
STEP: Verifying there is no service in the namespace 09/08/23 21:32:40.778
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:40.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3482" for this suite. 09/08/23 21:32:40.809
STEP: Destroying namespace "nsdeletetest-2555" for this suite. 09/08/23 21:32:40.829
Sep  8 21:32:40.842: INFO: Namespace nsdeletetest-2555 was already deleted
STEP: Destroying namespace "nsdeletetest-2297" for this suite. 09/08/23 21:32:40.842
------------------------------
â€¢ [SLOW TEST] [7.398 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:33.478
    Sep  8 21:32:33.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename namespaces 09/08/23 21:32:33.487
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:33.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:33.558
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 09/08/23 21:32:33.562
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:33.612
    STEP: Creating a service in the namespace 09/08/23 21:32:33.621
    STEP: Deleting the namespace 09/08/23 21:32:33.667
    STEP: Waiting for the namespace to be removed. 09/08/23 21:32:33.716
    STEP: Recreating the namespace 09/08/23 21:32:40.727
    STEP: Verifying there is no service in the namespace 09/08/23 21:32:40.778
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:40.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3482" for this suite. 09/08/23 21:32:40.809
    STEP: Destroying namespace "nsdeletetest-2555" for this suite. 09/08/23 21:32:40.829
    Sep  8 21:32:40.842: INFO: Namespace nsdeletetest-2555 was already deleted
    STEP: Destroying namespace "nsdeletetest-2297" for this suite. 09/08/23 21:32:40.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:40.879
Sep  8 21:32:40.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:32:40.88
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:40.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:40.94
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 09/08/23 21:32:40.955
Sep  8 21:32:40.993: INFO: Waiting up to 5m0s for pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24" in namespace "emptydir-9375" to be "Succeeded or Failed"
Sep  8 21:32:41.013: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24": Phase="Pending", Reason="", readiness=false. Elapsed: 19.831688ms
Sep  8 21:32:43.031: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037773142s
Sep  8 21:32:45.029: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035647732s
STEP: Saw pod success 09/08/23 21:32:45.029
Sep  8 21:32:45.030: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24" satisfied condition "Succeeded or Failed"
Sep  8 21:32:45.041: INFO: Trying to get logs from node node-3 pod pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24 container test-container: <nil>
STEP: delete the pod 09/08/23 21:32:45.061
Sep  8 21:32:45.090: INFO: Waiting for pod pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24 to disappear
Sep  8 21:32:45.101: INFO: Pod pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:32:45.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9375" for this suite. 09/08/23 21:32:45.115
------------------------------
â€¢ [4.280 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:40.879
    Sep  8 21:32:40.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:32:40.88
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:40.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:40.94
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/08/23 21:32:40.955
    Sep  8 21:32:40.993: INFO: Waiting up to 5m0s for pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24" in namespace "emptydir-9375" to be "Succeeded or Failed"
    Sep  8 21:32:41.013: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24": Phase="Pending", Reason="", readiness=false. Elapsed: 19.831688ms
    Sep  8 21:32:43.031: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037773142s
    Sep  8 21:32:45.029: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035647732s
    STEP: Saw pod success 09/08/23 21:32:45.029
    Sep  8 21:32:45.030: INFO: Pod "pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24" satisfied condition "Succeeded or Failed"
    Sep  8 21:32:45.041: INFO: Trying to get logs from node node-3 pod pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24 container test-container: <nil>
    STEP: delete the pod 09/08/23 21:32:45.061
    Sep  8 21:32:45.090: INFO: Waiting for pod pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24 to disappear
    Sep  8 21:32:45.101: INFO: Pod pod-f5c8ef45-d5a4-4a1a-a7a5-2c5eb9c7da24 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:32:45.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9375" for this suite. 09/08/23 21:32:45.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:32:45.179
Sep  8 21:32:45.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 21:32:45.18
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:45.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:45.247
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-e036352d-eea2-4151-a11b-07360f1e4ee8 in namespace container-probe-6849 09/08/23 21:32:45.252
Sep  8 21:32:45.278: INFO: Waiting up to 5m0s for pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8" in namespace "container-probe-6849" to be "not pending"
Sep  8 21:32:45.294: INFO: Pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.721896ms
Sep  8 21:32:47.304: INFO: Pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8": Phase="Running", Reason="", readiness=true. Elapsed: 2.025960845s
Sep  8 21:32:47.304: INFO: Pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8" satisfied condition "not pending"
Sep  8 21:32:47.304: INFO: Started pod busybox-e036352d-eea2-4151-a11b-07360f1e4ee8 in namespace container-probe-6849
STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 21:32:47.304
Sep  8 21:32:47.311: INFO: Initial restart count of pod busybox-e036352d-eea2-4151-a11b-07360f1e4ee8 is 0
STEP: deleting the pod 09/08/23 21:36:48.94
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 21:36:48.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6849" for this suite. 09/08/23 21:36:48.985
------------------------------
â€¢ [SLOW TEST] [243.821 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:32:45.179
    Sep  8 21:32:45.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 21:32:45.18
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:32:45.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:32:45.247
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-e036352d-eea2-4151-a11b-07360f1e4ee8 in namespace container-probe-6849 09/08/23 21:32:45.252
    Sep  8 21:32:45.278: INFO: Waiting up to 5m0s for pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8" in namespace "container-probe-6849" to be "not pending"
    Sep  8 21:32:45.294: INFO: Pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.721896ms
    Sep  8 21:32:47.304: INFO: Pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8": Phase="Running", Reason="", readiness=true. Elapsed: 2.025960845s
    Sep  8 21:32:47.304: INFO: Pod "busybox-e036352d-eea2-4151-a11b-07360f1e4ee8" satisfied condition "not pending"
    Sep  8 21:32:47.304: INFO: Started pod busybox-e036352d-eea2-4151-a11b-07360f1e4ee8 in namespace container-probe-6849
    STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 21:32:47.304
    Sep  8 21:32:47.311: INFO: Initial restart count of pod busybox-e036352d-eea2-4151-a11b-07360f1e4ee8 is 0
    STEP: deleting the pod 09/08/23 21:36:48.94
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:36:48.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6849" for this suite. 09/08/23 21:36:48.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:36:49.006
Sep  8 21:36:49.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 21:36:49.008
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:49.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:49.061
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-e4e3985a-e697-42e6-a963-2f45c745c5d0 09/08/23 21:36:49.068
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 21:36:49.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5759" for this suite. 09/08/23 21:36:49.09
------------------------------
â€¢ [0.102 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:36:49.006
    Sep  8 21:36:49.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 21:36:49.008
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:49.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:49.061
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-e4e3985a-e697-42e6-a963-2f45c745c5d0 09/08/23 21:36:49.068
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:36:49.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5759" for this suite. 09/08/23 21:36:49.09
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:36:49.109
Sep  8 21:36:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 21:36:49.111
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:49.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:49.156
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 21:36:49.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9943" for this suite. 09/08/23 21:36:49.271
------------------------------
â€¢ [0.188 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:36:49.109
    Sep  8 21:36:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 21:36:49.111
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:49.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:49.156
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:36:49.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9943" for this suite. 09/08/23 21:36:49.271
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:36:49.297
Sep  8 21:36:49.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:36:49.299
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:49.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:49.345
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:36:49.349
Sep  8 21:36:49.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819" in namespace "projected-9997" to be "Succeeded or Failed"
Sep  8 21:36:49.378: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819": Phase="Pending", Reason="", readiness=false. Elapsed: 12.183671ms
Sep  8 21:36:51.394: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027861128s
Sep  8 21:36:53.387: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020957917s
STEP: Saw pod success 09/08/23 21:36:53.387
Sep  8 21:36:53.388: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819" satisfied condition "Succeeded or Failed"
Sep  8 21:36:53.398: INFO: Trying to get logs from node node-3 pod downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819 container client-container: <nil>
STEP: delete the pod 09/08/23 21:36:53.45
Sep  8 21:36:53.492: INFO: Waiting for pod downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819 to disappear
Sep  8 21:36:53.503: INFO: Pod downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 21:36:53.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9997" for this suite. 09/08/23 21:36:53.528
------------------------------
â€¢ [4.248 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:36:49.297
    Sep  8 21:36:49.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:36:49.299
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:49.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:49.345
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:36:49.349
    Sep  8 21:36:49.366: INFO: Waiting up to 5m0s for pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819" in namespace "projected-9997" to be "Succeeded or Failed"
    Sep  8 21:36:49.378: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819": Phase="Pending", Reason="", readiness=false. Elapsed: 12.183671ms
    Sep  8 21:36:51.394: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027861128s
    Sep  8 21:36:53.387: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020957917s
    STEP: Saw pod success 09/08/23 21:36:53.387
    Sep  8 21:36:53.388: INFO: Pod "downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819" satisfied condition "Succeeded or Failed"
    Sep  8 21:36:53.398: INFO: Trying to get logs from node node-3 pod downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819 container client-container: <nil>
    STEP: delete the pod 09/08/23 21:36:53.45
    Sep  8 21:36:53.492: INFO: Waiting for pod downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819 to disappear
    Sep  8 21:36:53.503: INFO: Pod downwardapi-volume-943a649d-a8ba-4e2a-91eb-0aab4483b819 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:36:53.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9997" for this suite. 09/08/23 21:36:53.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:36:53.555
Sep  8 21:36:53.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:36:53.557
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:53.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:53.611
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 09/08/23 21:36:53.63
STEP: watching for the Service to be added 09/08/23 21:36:53.672
Sep  8 21:36:53.676: INFO: Found Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep  8 21:36:53.676: INFO: Service test-service-dkdb5 created
STEP: Getting /status 09/08/23 21:36:53.676
Sep  8 21:36:53.691: INFO: Service test-service-dkdb5 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 09/08/23 21:36:53.691
STEP: watching for the Service to be patched 09/08/23 21:36:53.718
Sep  8 21:36:53.725: INFO: observed Service test-service-dkdb5 in namespace services-7001 with annotations: map[] & LoadBalancer: {[]}
Sep  8 21:36:53.725: INFO: Found Service test-service-dkdb5 in namespace services-7001 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep  8 21:36:53.725: INFO: Service test-service-dkdb5 has service status patched
STEP: updating the ServiceStatus 09/08/23 21:36:53.725
Sep  8 21:36:53.800: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 09/08/23 21:36:53.8
Sep  8 21:36:53.804: INFO: Observed Service test-service-dkdb5 in namespace services-7001 with annotations: map[] & Conditions: {[]}
Sep  8 21:36:53.804: INFO: Observed event: &Service{ObjectMeta:{test-service-dkdb5  services-7001  8d2af934-2df5-49af-95d5-20bacab3a53b 19303 0 2023-09-08 21:36:53 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.62.159,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.62.159],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep  8 21:36:53.804: INFO: Observed event: &Service{ObjectMeta:{test-service-dkdb5  services-7001  8d2af934-2df5-49af-95d5-20bacab3a53b 19304 0 2023-09-08 21:36:53 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.62.159,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.62.159],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
Sep  8 21:36:53.805: INFO: Found Service test-service-dkdb5 in namespace services-7001 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  8 21:36:53.805: INFO: Service test-service-dkdb5 has service status updated
STEP: patching the service 09/08/23 21:36:53.805
STEP: watching for the Service to be patched 09/08/23 21:36:53.824
Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
Sep  8 21:36:53.829: INFO: Found Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service:patched test-service-static:true]
Sep  8 21:36:53.829: INFO: Service test-service-dkdb5 patched
STEP: deleting the service 09/08/23 21:36:53.829
STEP: watching for the Service to be deleted 09/08/23 21:36:53.909
Sep  8 21:36:53.914: INFO: Observed event: ADDED
Sep  8 21:36:53.914: INFO: Observed event: MODIFIED
Sep  8 21:36:53.914: INFO: Observed event: MODIFIED
Sep  8 21:36:53.914: INFO: Observed event: MODIFIED
Sep  8 21:36:53.915: INFO: Observed event: MODIFIED
Sep  8 21:36:53.916: INFO: Found Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep  8 21:36:53.916: INFO: Service test-service-dkdb5 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:36:53.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7001" for this suite. 09/08/23 21:36:53.94
------------------------------
â€¢ [0.407 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:36:53.555
    Sep  8 21:36:53.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:36:53.557
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:53.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:53.611
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 09/08/23 21:36:53.63
    STEP: watching for the Service to be added 09/08/23 21:36:53.672
    Sep  8 21:36:53.676: INFO: Found Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Sep  8 21:36:53.676: INFO: Service test-service-dkdb5 created
    STEP: Getting /status 09/08/23 21:36:53.676
    Sep  8 21:36:53.691: INFO: Service test-service-dkdb5 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 09/08/23 21:36:53.691
    STEP: watching for the Service to be patched 09/08/23 21:36:53.718
    Sep  8 21:36:53.725: INFO: observed Service test-service-dkdb5 in namespace services-7001 with annotations: map[] & LoadBalancer: {[]}
    Sep  8 21:36:53.725: INFO: Found Service test-service-dkdb5 in namespace services-7001 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Sep  8 21:36:53.725: INFO: Service test-service-dkdb5 has service status patched
    STEP: updating the ServiceStatus 09/08/23 21:36:53.725
    Sep  8 21:36:53.800: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 09/08/23 21:36:53.8
    Sep  8 21:36:53.804: INFO: Observed Service test-service-dkdb5 in namespace services-7001 with annotations: map[] & Conditions: {[]}
    Sep  8 21:36:53.804: INFO: Observed event: &Service{ObjectMeta:{test-service-dkdb5  services-7001  8d2af934-2df5-49af-95d5-20bacab3a53b 19303 0 2023-09-08 21:36:53 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.62.159,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.62.159],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Sep  8 21:36:53.804: INFO: Observed event: &Service{ObjectMeta:{test-service-dkdb5  services-7001  8d2af934-2df5-49af-95d5-20bacab3a53b 19304 0 2023-09-08 21:36:53 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-09-08 21:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.62.159,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.62.159],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
    Sep  8 21:36:53.805: INFO: Found Service test-service-dkdb5 in namespace services-7001 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  8 21:36:53.805: INFO: Service test-service-dkdb5 has service status updated
    STEP: patching the service 09/08/23 21:36:53.805
    STEP: watching for the Service to be patched 09/08/23 21:36:53.824
    Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
    Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
    Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
    Sep  8 21:36:53.829: INFO: observed Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service-static:true]
    Sep  8 21:36:53.829: INFO: Found Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service:patched test-service-static:true]
    Sep  8 21:36:53.829: INFO: Service test-service-dkdb5 patched
    STEP: deleting the service 09/08/23 21:36:53.829
    STEP: watching for the Service to be deleted 09/08/23 21:36:53.909
    Sep  8 21:36:53.914: INFO: Observed event: ADDED
    Sep  8 21:36:53.914: INFO: Observed event: MODIFIED
    Sep  8 21:36:53.914: INFO: Observed event: MODIFIED
    Sep  8 21:36:53.914: INFO: Observed event: MODIFIED
    Sep  8 21:36:53.915: INFO: Observed event: MODIFIED
    Sep  8 21:36:53.916: INFO: Found Service test-service-dkdb5 in namespace services-7001 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Sep  8 21:36:53.916: INFO: Service test-service-dkdb5 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:36:53.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7001" for this suite. 09/08/23 21:36:53.94
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:36:53.962
Sep  8 21:36:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename subpath 09/08/23 21:36:53.963
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:54.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:54.012
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/08/23 21:36:54.021
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-ktbd 09/08/23 21:36:54.048
STEP: Creating a pod to test atomic-volume-subpath 09/08/23 21:36:54.048
Sep  8 21:36:54.075: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ktbd" in namespace "subpath-5939" to be "Succeeded or Failed"
Sep  8 21:36:54.095: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.849103ms
Sep  8 21:36:56.105: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.028576345s
Sep  8 21:36:58.112: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 4.03605428s
Sep  8 21:37:00.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 6.029766819s
Sep  8 21:37:02.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 8.030479766s
Sep  8 21:37:04.115: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 10.039357684s
Sep  8 21:37:06.105: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 12.029001329s
Sep  8 21:37:08.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 14.030440211s
Sep  8 21:37:10.112: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 16.036180191s
Sep  8 21:37:12.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 18.029576812s
Sep  8 21:37:14.103: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 20.027455027s
Sep  8 21:37:16.104: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=false. Elapsed: 22.028437999s
Sep  8 21:37:18.110: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.034443825s
STEP: Saw pod success 09/08/23 21:37:18.111
Sep  8 21:37:18.111: INFO: Pod "pod-subpath-test-configmap-ktbd" satisfied condition "Succeeded or Failed"
Sep  8 21:37:18.119: INFO: Trying to get logs from node node-3 pod pod-subpath-test-configmap-ktbd container test-container-subpath-configmap-ktbd: <nil>
STEP: delete the pod 09/08/23 21:37:18.155
Sep  8 21:37:18.194: INFO: Waiting for pod pod-subpath-test-configmap-ktbd to disappear
Sep  8 21:37:18.204: INFO: Pod pod-subpath-test-configmap-ktbd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ktbd 09/08/23 21:37:18.204
Sep  8 21:37:18.204: INFO: Deleting pod "pod-subpath-test-configmap-ktbd" in namespace "subpath-5939"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  8 21:37:18.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5939" for this suite. 09/08/23 21:37:18.221
------------------------------
â€¢ [SLOW TEST] [24.278 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:36:53.962
    Sep  8 21:36:53.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename subpath 09/08/23 21:36:53.963
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:36:54.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:36:54.012
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/08/23 21:36:54.021
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-ktbd 09/08/23 21:36:54.048
    STEP: Creating a pod to test atomic-volume-subpath 09/08/23 21:36:54.048
    Sep  8 21:36:54.075: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ktbd" in namespace "subpath-5939" to be "Succeeded or Failed"
    Sep  8 21:36:54.095: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.849103ms
    Sep  8 21:36:56.105: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.028576345s
    Sep  8 21:36:58.112: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 4.03605428s
    Sep  8 21:37:00.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 6.029766819s
    Sep  8 21:37:02.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 8.030479766s
    Sep  8 21:37:04.115: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 10.039357684s
    Sep  8 21:37:06.105: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 12.029001329s
    Sep  8 21:37:08.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 14.030440211s
    Sep  8 21:37:10.112: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 16.036180191s
    Sep  8 21:37:12.106: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 18.029576812s
    Sep  8 21:37:14.103: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=true. Elapsed: 20.027455027s
    Sep  8 21:37:16.104: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Running", Reason="", readiness=false. Elapsed: 22.028437999s
    Sep  8 21:37:18.110: INFO: Pod "pod-subpath-test-configmap-ktbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.034443825s
    STEP: Saw pod success 09/08/23 21:37:18.111
    Sep  8 21:37:18.111: INFO: Pod "pod-subpath-test-configmap-ktbd" satisfied condition "Succeeded or Failed"
    Sep  8 21:37:18.119: INFO: Trying to get logs from node node-3 pod pod-subpath-test-configmap-ktbd container test-container-subpath-configmap-ktbd: <nil>
    STEP: delete the pod 09/08/23 21:37:18.155
    Sep  8 21:37:18.194: INFO: Waiting for pod pod-subpath-test-configmap-ktbd to disappear
    Sep  8 21:37:18.204: INFO: Pod pod-subpath-test-configmap-ktbd no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-ktbd 09/08/23 21:37:18.204
    Sep  8 21:37:18.204: INFO: Deleting pod "pod-subpath-test-configmap-ktbd" in namespace "subpath-5939"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:37:18.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5939" for this suite. 09/08/23 21:37:18.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:37:18.242
Sep  8 21:37:18.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename job 09/08/23 21:37:18.243
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:18.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:18.303
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 09/08/23 21:37:18.31
STEP: Ensure pods equal to parallelism count is attached to the job 09/08/23 21:37:18.325
STEP: patching /status 09/08/23 21:37:20.335
STEP: updating /status 09/08/23 21:37:20.359
STEP: get /status 09/08/23 21:37:20.45
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  8 21:37:20.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1319" for this suite. 09/08/23 21:37:20.487
------------------------------
â€¢ [2.267 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:37:18.242
    Sep  8 21:37:18.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename job 09/08/23 21:37:18.243
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:18.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:18.303
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 09/08/23 21:37:18.31
    STEP: Ensure pods equal to parallelism count is attached to the job 09/08/23 21:37:18.325
    STEP: patching /status 09/08/23 21:37:20.335
    STEP: updating /status 09/08/23 21:37:20.359
    STEP: get /status 09/08/23 21:37:20.45
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:37:20.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1319" for this suite. 09/08/23 21:37:20.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:37:20.511
Sep  8 21:37:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:37:20.512
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:20.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:20.591
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:37:20.63
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:37:20.839
STEP: Deploying the webhook pod 09/08/23 21:37:20.869
STEP: Wait for the deployment to be ready 09/08/23 21:37:20.908
Sep  8 21:37:20.993: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 21:37:23.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 37, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 37, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 37, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 37, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 21:37:25.055
STEP: Verifying the service has paired with the endpoint 09/08/23 21:37:25.086
Sep  8 21:37:26.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/08/23 21:37:26.096
STEP: create a configmap that should be updated by the webhook 09/08/23 21:37:26.126
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:37:26.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1291" for this suite. 09/08/23 21:37:26.291
STEP: Destroying namespace "webhook-1291-markers" for this suite. 09/08/23 21:37:26.322
------------------------------
â€¢ [SLOW TEST] [5.845 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:37:20.511
    Sep  8 21:37:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:37:20.512
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:20.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:20.591
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:37:20.63
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:37:20.839
    STEP: Deploying the webhook pod 09/08/23 21:37:20.869
    STEP: Wait for the deployment to be ready 09/08/23 21:37:20.908
    Sep  8 21:37:20.993: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 21:37:23.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 37, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 37, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 37, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 37, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 21:37:25.055
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:37:25.086
    Sep  8 21:37:26.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 09/08/23 21:37:26.096
    STEP: create a configmap that should be updated by the webhook 09/08/23 21:37:26.126
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:37:26.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1291" for this suite. 09/08/23 21:37:26.291
    STEP: Destroying namespace "webhook-1291-markers" for this suite. 09/08/23 21:37:26.322
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:37:26.358
Sep  8 21:37:26.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replicaset 09/08/23 21:37:26.37
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:26.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:26.444
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 09/08/23 21:37:26.462
STEP: Verify that the required pods have come up. 09/08/23 21:37:26.487
Sep  8 21:37:26.499: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  8 21:37:31.515: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/08/23 21:37:31.515
STEP: Getting /status 09/08/23 21:37:31.515
Sep  8 21:37:31.527: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 09/08/23 21:37:31.527
Sep  8 21:37:31.552: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 09/08/23 21:37:31.552
Sep  8 21:37:31.556: INFO: Observed &ReplicaSet event: ADDED
Sep  8 21:37:31.556: INFO: Observed &ReplicaSet event: MODIFIED
Sep  8 21:37:31.556: INFO: Observed &ReplicaSet event: MODIFIED
Sep  8 21:37:31.557: INFO: Observed &ReplicaSet event: MODIFIED
Sep  8 21:37:31.557: INFO: Found replicaset test-rs in namespace replicaset-2970 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  8 21:37:31.557: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 09/08/23 21:37:31.557
Sep  8 21:37:31.557: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  8 21:37:31.571: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 09/08/23 21:37:31.571
Sep  8 21:37:31.575: INFO: Observed &ReplicaSet event: ADDED
Sep  8 21:37:31.575: INFO: Observed &ReplicaSet event: MODIFIED
Sep  8 21:37:31.576: INFO: Observed &ReplicaSet event: MODIFIED
Sep  8 21:37:31.576: INFO: Observed &ReplicaSet event: MODIFIED
Sep  8 21:37:31.576: INFO: Observed replicaset test-rs in namespace replicaset-2970 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  8 21:37:31.576: INFO: Observed &ReplicaSet event: MODIFIED
Sep  8 21:37:31.576: INFO: Found replicaset test-rs in namespace replicaset-2970 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Sep  8 21:37:31.576: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:37:31.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2970" for this suite. 09/08/23 21:37:31.588
------------------------------
â€¢ [SLOW TEST] [5.254 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:37:26.358
    Sep  8 21:37:26.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replicaset 09/08/23 21:37:26.37
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:26.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:26.444
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 09/08/23 21:37:26.462
    STEP: Verify that the required pods have come up. 09/08/23 21:37:26.487
    Sep  8 21:37:26.499: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  8 21:37:31.515: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/08/23 21:37:31.515
    STEP: Getting /status 09/08/23 21:37:31.515
    Sep  8 21:37:31.527: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 09/08/23 21:37:31.527
    Sep  8 21:37:31.552: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 09/08/23 21:37:31.552
    Sep  8 21:37:31.556: INFO: Observed &ReplicaSet event: ADDED
    Sep  8 21:37:31.556: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  8 21:37:31.556: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  8 21:37:31.557: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  8 21:37:31.557: INFO: Found replicaset test-rs in namespace replicaset-2970 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  8 21:37:31.557: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 09/08/23 21:37:31.557
    Sep  8 21:37:31.557: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  8 21:37:31.571: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 09/08/23 21:37:31.571
    Sep  8 21:37:31.575: INFO: Observed &ReplicaSet event: ADDED
    Sep  8 21:37:31.575: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  8 21:37:31.576: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  8 21:37:31.576: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  8 21:37:31.576: INFO: Observed replicaset test-rs in namespace replicaset-2970 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  8 21:37:31.576: INFO: Observed &ReplicaSet event: MODIFIED
    Sep  8 21:37:31.576: INFO: Found replicaset test-rs in namespace replicaset-2970 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Sep  8 21:37:31.576: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:37:31.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2970" for this suite. 09/08/23 21:37:31.588
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:37:31.612
Sep  8 21:37:31.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename init-container 09/08/23 21:37:31.614
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:31.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:31.704
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 09/08/23 21:37:31.711
Sep  8 21:37:31.712: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:37:37.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1194" for this suite. 09/08/23 21:37:37.094
------------------------------
â€¢ [SLOW TEST] [5.506 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:37:31.612
    Sep  8 21:37:31.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename init-container 09/08/23 21:37:31.614
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:31.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:31.704
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 09/08/23 21:37:31.711
    Sep  8 21:37:31.712: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:37:37.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1194" for this suite. 09/08/23 21:37:37.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:37:37.131
Sep  8 21:37:37.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:37:37.133
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:37.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:37.185
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:37:37.191
Sep  8 21:37:37.216: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c" in namespace "downward-api-9681" to be "Succeeded or Failed"
Sep  8 21:37:37.233: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.798697ms
Sep  8 21:37:39.247: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031693294s
Sep  8 21:37:41.244: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027696238s
STEP: Saw pod success 09/08/23 21:37:41.244
Sep  8 21:37:41.244: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c" satisfied condition "Succeeded or Failed"
Sep  8 21:37:41.253: INFO: Trying to get logs from node node-4 pod downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c container client-container: <nil>
STEP: delete the pod 09/08/23 21:37:41.285
Sep  8 21:37:41.321: INFO: Waiting for pod downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c to disappear
Sep  8 21:37:41.330: INFO: Pod downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 21:37:41.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9681" for this suite. 09/08/23 21:37:41.345
------------------------------
â€¢ [4.236 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:37:37.131
    Sep  8 21:37:37.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:37:37.133
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:37.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:37.185
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:37:37.191
    Sep  8 21:37:37.216: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c" in namespace "downward-api-9681" to be "Succeeded or Failed"
    Sep  8 21:37:37.233: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.798697ms
    Sep  8 21:37:39.247: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031693294s
    Sep  8 21:37:41.244: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027696238s
    STEP: Saw pod success 09/08/23 21:37:41.244
    Sep  8 21:37:41.244: INFO: Pod "downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c" satisfied condition "Succeeded or Failed"
    Sep  8 21:37:41.253: INFO: Trying to get logs from node node-4 pod downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c container client-container: <nil>
    STEP: delete the pod 09/08/23 21:37:41.285
    Sep  8 21:37:41.321: INFO: Waiting for pod downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c to disappear
    Sep  8 21:37:41.330: INFO: Pod downwardapi-volume-fe6bb94d-4061-4892-a3c6-4e02ea01322c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:37:41.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9681" for this suite. 09/08/23 21:37:41.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:37:41.37
Sep  8 21:37:41.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename disruption 09/08/23 21:37:41.372
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:41.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:41.429
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 09/08/23 21:37:41.444
STEP: Waiting for all pods to be running 09/08/23 21:37:43.542
Sep  8 21:37:43.579: INFO: running pods: 0 < 3
Sep  8 21:37:45.591: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:37:47.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-163" for this suite. 09/08/23 21:37:47.604
------------------------------
â€¢ [SLOW TEST] [6.249 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:37:41.37
    Sep  8 21:37:41.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename disruption 09/08/23 21:37:41.372
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:41.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:41.429
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 09/08/23 21:37:41.444
    STEP: Waiting for all pods to be running 09/08/23 21:37:43.542
    Sep  8 21:37:43.579: INFO: running pods: 0 < 3
    Sep  8 21:37:45.591: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:37:47.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-163" for this suite. 09/08/23 21:37:47.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:37:47.62
Sep  8 21:37:47.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename daemonsets 09/08/23 21:37:47.621
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:47.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:47.702
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Sep  8 21:37:47.762: INFO: Create a RollingUpdate DaemonSet
Sep  8 21:37:47.772: INFO: Check that daemon pods launch on every node of the cluster
Sep  8 21:37:47.782: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:47.782: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:47.782: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:47.804: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:37:47.804: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:37:48.815: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:48.815: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:48.815: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:48.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:37:48.833: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:37:49.825: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:49.825: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:49.825: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:49.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 21:37:49.832: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Sep  8 21:37:49.832: INFO: Update the DaemonSet to trigger a rollout
Sep  8 21:37:49.860: INFO: Updating DaemonSet daemon-set
Sep  8 21:37:52.906: INFO: Roll back the DaemonSet before rollout is complete
Sep  8 21:37:52.939: INFO: Updating DaemonSet daemon-set
Sep  8 21:37:52.939: INFO: Make sure DaemonSet rollback is complete
Sep  8 21:37:52.951: INFO: Wrong image for pod: daemon-set-nf9b7. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Sep  8 21:37:52.951: INFO: Pod daemon-set-nf9b7 is not available
Sep  8 21:37:52.993: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:52.993: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:52.993: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:54.044: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:54.044: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:54.044: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:55.015: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:55.015: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:55.016: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:56.025: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:56.025: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:56.025: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:57.013: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:57.014: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:57.014: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:58.003: INFO: Pod daemon-set-4n925 is not available
Sep  8 21:37:58.020: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:58.020: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:37:58.020: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/08/23 21:37:58.043
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5595, will wait for the garbage collector to delete the pods 09/08/23 21:37:58.043
Sep  8 21:37:58.121: INFO: Deleting DaemonSet.extensions daemon-set took: 18.733927ms
Sep  8 21:37:58.222: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.622514ms
Sep  8 21:38:00.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:38:00.337: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  8 21:38:00.343: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20111"},"items":null}

Sep  8 21:38:00.351: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20111"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:00.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5595" for this suite. 09/08/23 21:38:00.392
------------------------------
â€¢ [SLOW TEST] [12.791 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:37:47.62
    Sep  8 21:37:47.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename daemonsets 09/08/23 21:37:47.621
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:37:47.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:37:47.702
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Sep  8 21:37:47.762: INFO: Create a RollingUpdate DaemonSet
    Sep  8 21:37:47.772: INFO: Check that daemon pods launch on every node of the cluster
    Sep  8 21:37:47.782: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:47.782: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:47.782: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:47.804: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:37:47.804: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:37:48.815: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:48.815: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:48.815: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:48.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:37:48.833: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:37:49.825: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:49.825: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:49.825: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:49.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 21:37:49.832: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Sep  8 21:37:49.832: INFO: Update the DaemonSet to trigger a rollout
    Sep  8 21:37:49.860: INFO: Updating DaemonSet daemon-set
    Sep  8 21:37:52.906: INFO: Roll back the DaemonSet before rollout is complete
    Sep  8 21:37:52.939: INFO: Updating DaemonSet daemon-set
    Sep  8 21:37:52.939: INFO: Make sure DaemonSet rollback is complete
    Sep  8 21:37:52.951: INFO: Wrong image for pod: daemon-set-nf9b7. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Sep  8 21:37:52.951: INFO: Pod daemon-set-nf9b7 is not available
    Sep  8 21:37:52.993: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:52.993: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:52.993: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:54.044: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:54.044: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:54.044: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:55.015: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:55.015: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:55.016: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:56.025: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:56.025: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:56.025: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:57.013: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:57.014: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:57.014: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:58.003: INFO: Pod daemon-set-4n925 is not available
    Sep  8 21:37:58.020: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:58.020: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:37:58.020: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/08/23 21:37:58.043
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5595, will wait for the garbage collector to delete the pods 09/08/23 21:37:58.043
    Sep  8 21:37:58.121: INFO: Deleting DaemonSet.extensions daemon-set took: 18.733927ms
    Sep  8 21:37:58.222: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.622514ms
    Sep  8 21:38:00.337: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:38:00.337: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  8 21:38:00.343: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20111"},"items":null}

    Sep  8 21:38:00.351: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20111"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:00.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5595" for this suite. 09/08/23 21:38:00.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:00.413
Sep  8 21:38:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename endpointslicemirroring 09/08/23 21:38:00.414
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:00.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:00.466
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 09/08/23 21:38:00.497
Sep  8 21:38:00.526: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 09/08/23 21:38:02.54
STEP: mirroring deletion of a custom Endpoint 09/08/23 21:38:02.567
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:02.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-3852" for this suite. 09/08/23 21:38:02.622
------------------------------
â€¢ [2.229 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:00.413
    Sep  8 21:38:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename endpointslicemirroring 09/08/23 21:38:00.414
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:00.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:00.466
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 09/08/23 21:38:00.497
    Sep  8 21:38:00.526: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 09/08/23 21:38:02.54
    STEP: mirroring deletion of a custom Endpoint 09/08/23 21:38:02.567
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:02.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-3852" for this suite. 09/08/23 21:38:02.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:02.644
Sep  8 21:38:02.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename events 09/08/23 21:38:02.646
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:02.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:02.694
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 09/08/23 21:38:02.701
STEP: get a list of Events with a label in the current namespace 09/08/23 21:38:02.747
STEP: delete a list of events 09/08/23 21:38:02.757
Sep  8 21:38:02.757: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 09/08/23 21:38:02.876
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:02.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2705" for this suite. 09/08/23 21:38:02.893
------------------------------
â€¢ [0.268 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:02.644
    Sep  8 21:38:02.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename events 09/08/23 21:38:02.646
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:02.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:02.694
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 09/08/23 21:38:02.701
    STEP: get a list of Events with a label in the current namespace 09/08/23 21:38:02.747
    STEP: delete a list of events 09/08/23 21:38:02.757
    Sep  8 21:38:02.757: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 09/08/23 21:38:02.876
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:02.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2705" for this suite. 09/08/23 21:38:02.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:02.92
Sep  8 21:38:02.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:38:02.922
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:02.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:02.969
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:02.973
Sep  8 21:38:02.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f" in namespace "downward-api-5508" to be "Succeeded or Failed"
Sep  8 21:38:03.020: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Pending", Reason="", readiness=false. Elapsed: 25.476595ms
Sep  8 21:38:05.033: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037716249s
Sep  8 21:38:07.030: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035074564s
Sep  8 21:38:09.035: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040078861s
STEP: Saw pod success 09/08/23 21:38:09.035
Sep  8 21:38:09.035: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f" satisfied condition "Succeeded or Failed"
Sep  8 21:38:09.042: INFO: Trying to get logs from node node-3 pod downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f container client-container: <nil>
STEP: delete the pod 09/08/23 21:38:09.055
Sep  8 21:38:09.083: INFO: Waiting for pod downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f to disappear
Sep  8 21:38:09.091: INFO: Pod downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:09.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5508" for this suite. 09/08/23 21:38:09.107
------------------------------
â€¢ [SLOW TEST] [6.206 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:02.92
    Sep  8 21:38:02.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:38:02.922
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:02.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:02.969
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:02.973
    Sep  8 21:38:02.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f" in namespace "downward-api-5508" to be "Succeeded or Failed"
    Sep  8 21:38:03.020: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Pending", Reason="", readiness=false. Elapsed: 25.476595ms
    Sep  8 21:38:05.033: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037716249s
    Sep  8 21:38:07.030: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035074564s
    Sep  8 21:38:09.035: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040078861s
    STEP: Saw pod success 09/08/23 21:38:09.035
    Sep  8 21:38:09.035: INFO: Pod "downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f" satisfied condition "Succeeded or Failed"
    Sep  8 21:38:09.042: INFO: Trying to get logs from node node-3 pod downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f container client-container: <nil>
    STEP: delete the pod 09/08/23 21:38:09.055
    Sep  8 21:38:09.083: INFO: Waiting for pod downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f to disappear
    Sep  8 21:38:09.091: INFO: Pod downwardapi-volume-14ea2c10-84cf-483a-a654-97f8b0eb852f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:09.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5508" for this suite. 09/08/23 21:38:09.107
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:09.127
Sep  8 21:38:09.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 21:38:09.128
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:09.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:09.167
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Sep  8 21:38:09.196: INFO: Waiting up to 2m0s for pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" in namespace "var-expansion-2182" to be "container 0 failed with reason CreateContainerConfigError"
Sep  8 21:38:09.204: INFO: Pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.357965ms
Sep  8 21:38:11.218: INFO: Pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022051527s
Sep  8 21:38:11.219: INFO: Pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  8 21:38:11.219: INFO: Deleting pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" in namespace "var-expansion-2182"
Sep  8 21:38:11.260: INFO: Wait up to 5m0s for pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:13.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2182" for this suite. 09/08/23 21:38:13.322
------------------------------
â€¢ [4.222 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:09.127
    Sep  8 21:38:09.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 21:38:09.128
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:09.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:09.167
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Sep  8 21:38:09.196: INFO: Waiting up to 2m0s for pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" in namespace "var-expansion-2182" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  8 21:38:09.204: INFO: Pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.357965ms
    Sep  8 21:38:11.218: INFO: Pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022051527s
    Sep  8 21:38:11.219: INFO: Pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  8 21:38:11.219: INFO: Deleting pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" in namespace "var-expansion-2182"
    Sep  8 21:38:11.260: INFO: Wait up to 5m0s for pod "var-expansion-c5886d61-ec38-434a-a8b3-7021585f28f4" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:13.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2182" for this suite. 09/08/23 21:38:13.322
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:13.349
Sep  8 21:38:13.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:38:13.351
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:13.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:13.416
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Sep  8 21:38:13.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/08/23 21:38:21.563
Sep  8 21:38:21.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 create -f -'
Sep  8 21:38:23.002: INFO: stderr: ""
Sep  8 21:38:23.002: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  8 21:38:23.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 delete e2e-test-crd-publish-openapi-5133-crds test-cr'
Sep  8 21:38:23.140: INFO: stderr: ""
Sep  8 21:38:23.140: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  8 21:38:23.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 apply -f -'
Sep  8 21:38:24.177: INFO: stderr: ""
Sep  8 21:38:24.177: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  8 21:38:24.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 delete e2e-test-crd-publish-openapi-5133-crds test-cr'
Sep  8 21:38:24.296: INFO: stderr: ""
Sep  8 21:38:24.296: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 09/08/23 21:38:24.296
Sep  8 21:38:24.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 explain e2e-test-crd-publish-openapi-5133-crds'
Sep  8 21:38:25.302: INFO: stderr: ""
Sep  8 21:38:25.302: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5133-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:28.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9358" for this suite. 09/08/23 21:38:28.66
------------------------------
â€¢ [SLOW TEST] [15.332 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:13.349
    Sep  8 21:38:13.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:38:13.351
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:13.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:13.416
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Sep  8 21:38:13.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/08/23 21:38:21.563
    Sep  8 21:38:21.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 create -f -'
    Sep  8 21:38:23.002: INFO: stderr: ""
    Sep  8 21:38:23.002: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  8 21:38:23.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 delete e2e-test-crd-publish-openapi-5133-crds test-cr'
    Sep  8 21:38:23.140: INFO: stderr: ""
    Sep  8 21:38:23.140: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Sep  8 21:38:23.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 apply -f -'
    Sep  8 21:38:24.177: INFO: stderr: ""
    Sep  8 21:38:24.177: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Sep  8 21:38:24.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 --namespace=crd-publish-openapi-9358 delete e2e-test-crd-publish-openapi-5133-crds test-cr'
    Sep  8 21:38:24.296: INFO: stderr: ""
    Sep  8 21:38:24.296: INFO: stdout: "e2e-test-crd-publish-openapi-5133-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 09/08/23 21:38:24.296
    Sep  8 21:38:24.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-9358 explain e2e-test-crd-publish-openapi-5133-crds'
    Sep  8 21:38:25.302: INFO: stderr: ""
    Sep  8 21:38:25.302: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5133-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:28.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9358" for this suite. 09/08/23 21:38:28.66
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:28.682
Sep  8 21:38:28.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename containers 09/08/23 21:38:28.683
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:28.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:28.751
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Sep  8 21:38:28.770: INFO: Waiting up to 5m0s for pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b" in namespace "containers-2382" to be "running"
Sep  8 21:38:28.785: INFO: Pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.769489ms
Sep  8 21:38:30.797: INFO: Pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b": Phase="Running", Reason="", readiness=true. Elapsed: 2.026151601s
Sep  8 21:38:30.797: INFO: Pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:30.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2382" for this suite. 09/08/23 21:38:30.85
------------------------------
â€¢ [2.186 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:28.682
    Sep  8 21:38:28.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename containers 09/08/23 21:38:28.683
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:28.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:28.751
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Sep  8 21:38:28.770: INFO: Waiting up to 5m0s for pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b" in namespace "containers-2382" to be "running"
    Sep  8 21:38:28.785: INFO: Pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.769489ms
    Sep  8 21:38:30.797: INFO: Pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b": Phase="Running", Reason="", readiness=true. Elapsed: 2.026151601s
    Sep  8 21:38:30.797: INFO: Pod "client-containers-e0fdb6b8-4233-440a-9cf5-b787851f275b" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:30.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2382" for this suite. 09/08/23 21:38:30.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:30.869
Sep  8 21:38:30.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:38:30.871
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:30.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:30.931
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:30.939
Sep  8 21:38:30.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee" in namespace "projected-4012" to be "Succeeded or Failed"
Sep  8 21:38:30.982: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Pending", Reason="", readiness=false. Elapsed: 20.590735ms
Sep  8 21:38:32.994: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Running", Reason="", readiness=true. Elapsed: 2.033295557s
Sep  8 21:38:34.991: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Running", Reason="", readiness=false. Elapsed: 4.030185689s
Sep  8 21:38:36.991: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03038169s
STEP: Saw pod success 09/08/23 21:38:36.992
Sep  8 21:38:36.992: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee" satisfied condition "Succeeded or Failed"
Sep  8 21:38:37.015: INFO: Trying to get logs from node node-3 pod downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee container client-container: <nil>
STEP: delete the pod 09/08/23 21:38:37.034
Sep  8 21:38:37.088: INFO: Waiting for pod downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee to disappear
Sep  8 21:38:37.098: INFO: Pod downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:37.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4012" for this suite. 09/08/23 21:38:37.117
------------------------------
â€¢ [SLOW TEST] [6.277 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:30.869
    Sep  8 21:38:30.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:38:30.871
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:30.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:30.931
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:30.939
    Sep  8 21:38:30.961: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee" in namespace "projected-4012" to be "Succeeded or Failed"
    Sep  8 21:38:30.982: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Pending", Reason="", readiness=false. Elapsed: 20.590735ms
    Sep  8 21:38:32.994: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Running", Reason="", readiness=true. Elapsed: 2.033295557s
    Sep  8 21:38:34.991: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Running", Reason="", readiness=false. Elapsed: 4.030185689s
    Sep  8 21:38:36.991: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03038169s
    STEP: Saw pod success 09/08/23 21:38:36.992
    Sep  8 21:38:36.992: INFO: Pod "downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee" satisfied condition "Succeeded or Failed"
    Sep  8 21:38:37.015: INFO: Trying to get logs from node node-3 pod downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee container client-container: <nil>
    STEP: delete the pod 09/08/23 21:38:37.034
    Sep  8 21:38:37.088: INFO: Waiting for pod downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee to disappear
    Sep  8 21:38:37.098: INFO: Pod downwardapi-volume-3e121db4-3c91-40e7-bb45-a796cb116bee no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:37.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4012" for this suite. 09/08/23 21:38:37.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:37.149
Sep  8 21:38:37.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:38:37.15
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:37.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:37.195
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-d4838890-11b0-4ffd-b76d-138ea4c2d2a3 09/08/23 21:38:37.204
STEP: Creating a pod to test consume configMaps 09/08/23 21:38:37.215
Sep  8 21:38:37.236: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302" in namespace "projected-9975" to be "Succeeded or Failed"
Sep  8 21:38:37.268: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Pending", Reason="", readiness=false. Elapsed: 32.292287ms
Sep  8 21:38:39.279: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04284463s
Sep  8 21:38:41.279: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042999746s
Sep  8 21:38:43.280: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043993266s
STEP: Saw pod success 09/08/23 21:38:43.28
Sep  8 21:38:43.280: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302" satisfied condition "Succeeded or Failed"
Sep  8 21:38:43.291: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:38:43.325
Sep  8 21:38:43.359: INFO: Waiting for pod pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302 to disappear
Sep  8 21:38:43.373: INFO: Pod pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:43.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9975" for this suite. 09/08/23 21:38:43.39
------------------------------
â€¢ [SLOW TEST] [6.256 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:37.149
    Sep  8 21:38:37.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:38:37.15
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:37.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:37.195
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-d4838890-11b0-4ffd-b76d-138ea4c2d2a3 09/08/23 21:38:37.204
    STEP: Creating a pod to test consume configMaps 09/08/23 21:38:37.215
    Sep  8 21:38:37.236: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302" in namespace "projected-9975" to be "Succeeded or Failed"
    Sep  8 21:38:37.268: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Pending", Reason="", readiness=false. Elapsed: 32.292287ms
    Sep  8 21:38:39.279: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04284463s
    Sep  8 21:38:41.279: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042999746s
    Sep  8 21:38:43.280: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043993266s
    STEP: Saw pod success 09/08/23 21:38:43.28
    Sep  8 21:38:43.280: INFO: Pod "pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302" satisfied condition "Succeeded or Failed"
    Sep  8 21:38:43.291: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:38:43.325
    Sep  8 21:38:43.359: INFO: Waiting for pod pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302 to disappear
    Sep  8 21:38:43.373: INFO: Pod pod-projected-configmaps-c8000f3c-5f15-4018-8fee-15cba2348302 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:43.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9975" for this suite. 09/08/23 21:38:43.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:43.407
Sep  8 21:38:43.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 21:38:43.409
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:43.438
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:43.443
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-8d53ec07-0f29-4b67-b9a2-6d4207304688 09/08/23 21:38:43.451
STEP: Creating a pod to test consume secrets 09/08/23 21:38:43.462
Sep  8 21:38:43.484: INFO: Waiting up to 5m0s for pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2" in namespace "secrets-2948" to be "Succeeded or Failed"
Sep  8 21:38:43.496: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459501ms
Sep  8 21:38:45.506: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021587792s
Sep  8 21:38:47.506: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021374109s
STEP: Saw pod success 09/08/23 21:38:47.506
Sep  8 21:38:47.506: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2" satisfied condition "Succeeded or Failed"
Sep  8 21:38:47.515: INFO: Trying to get logs from node node-3 pod pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2 container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 21:38:47.535
Sep  8 21:38:47.575: INFO: Waiting for pod pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2 to disappear
Sep  8 21:38:47.582: INFO: Pod pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:47.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2948" for this suite. 09/08/23 21:38:47.593
------------------------------
â€¢ [4.206 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:43.407
    Sep  8 21:38:43.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 21:38:43.409
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:43.438
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:43.443
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-8d53ec07-0f29-4b67-b9a2-6d4207304688 09/08/23 21:38:43.451
    STEP: Creating a pod to test consume secrets 09/08/23 21:38:43.462
    Sep  8 21:38:43.484: INFO: Waiting up to 5m0s for pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2" in namespace "secrets-2948" to be "Succeeded or Failed"
    Sep  8 21:38:43.496: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459501ms
    Sep  8 21:38:45.506: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021587792s
    Sep  8 21:38:47.506: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021374109s
    STEP: Saw pod success 09/08/23 21:38:47.506
    Sep  8 21:38:47.506: INFO: Pod "pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2" satisfied condition "Succeeded or Failed"
    Sep  8 21:38:47.515: INFO: Trying to get logs from node node-3 pod pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2 container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 21:38:47.535
    Sep  8 21:38:47.575: INFO: Waiting for pod pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2 to disappear
    Sep  8 21:38:47.582: INFO: Pod pod-secrets-174df75b-3ef9-44cf-818c-e7dc705515b2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:47.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2948" for this suite. 09/08/23 21:38:47.593
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:47.613
Sep  8 21:38:47.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:38:47.615
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:47.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:47.667
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:47.672
Sep  8 21:38:47.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a" in namespace "downward-api-1330" to be "Succeeded or Failed"
Sep  8 21:38:47.704: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.971292ms
Sep  8 21:38:49.719: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02854583s
Sep  8 21:38:51.727: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036340645s
STEP: Saw pod success 09/08/23 21:38:51.73
Sep  8 21:38:51.731: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a" satisfied condition "Succeeded or Failed"
Sep  8 21:38:51.740: INFO: Trying to get logs from node node-3 pod downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a container client-container: <nil>
STEP: delete the pod 09/08/23 21:38:51.77
Sep  8 21:38:51.802: INFO: Waiting for pod downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a to disappear
Sep  8 21:38:51.815: INFO: Pod downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1330" for this suite. 09/08/23 21:38:51.826
------------------------------
â€¢ [4.230 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:47.613
    Sep  8 21:38:47.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:38:47.615
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:47.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:47.667
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:47.672
    Sep  8 21:38:47.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a" in namespace "downward-api-1330" to be "Succeeded or Failed"
    Sep  8 21:38:47.704: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.971292ms
    Sep  8 21:38:49.719: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02854583s
    Sep  8 21:38:51.727: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036340645s
    STEP: Saw pod success 09/08/23 21:38:51.73
    Sep  8 21:38:51.731: INFO: Pod "downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a" satisfied condition "Succeeded or Failed"
    Sep  8 21:38:51.740: INFO: Trying to get logs from node node-3 pod downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a container client-container: <nil>
    STEP: delete the pod 09/08/23 21:38:51.77
    Sep  8 21:38:51.802: INFO: Waiting for pod downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a to disappear
    Sep  8 21:38:51.815: INFO: Pod downwardapi-volume-6a3bf165-3fb1-4f23-99ea-3c8e1aca1c2a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:51.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1330" for this suite. 09/08/23 21:38:51.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:51.844
Sep  8 21:38:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:38:51.845
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:51.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:51.934
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:51.944
Sep  8 21:38:51.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8" in namespace "projected-3367" to be "Succeeded or Failed"
Sep  8 21:38:51.989: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.732593ms
Sep  8 21:38:54.006: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036667922s
Sep  8 21:38:55.999: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030107825s
STEP: Saw pod success 09/08/23 21:38:55.999
Sep  8 21:38:55.999: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8" satisfied condition "Succeeded or Failed"
Sep  8 21:38:56.008: INFO: Trying to get logs from node node-3 pod downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8 container client-container: <nil>
STEP: delete the pod 09/08/23 21:38:56.027
Sep  8 21:38:56.059: INFO: Waiting for pod downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8 to disappear
Sep  8 21:38:56.073: INFO: Pod downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 21:38:56.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3367" for this suite. 09/08/23 21:38:56.087
------------------------------
â€¢ [4.265 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:51.844
    Sep  8 21:38:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:38:51.845
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:51.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:51.934
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:38:51.944
    Sep  8 21:38:51.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8" in namespace "projected-3367" to be "Succeeded or Failed"
    Sep  8 21:38:51.989: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.732593ms
    Sep  8 21:38:54.006: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036667922s
    Sep  8 21:38:55.999: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030107825s
    STEP: Saw pod success 09/08/23 21:38:55.999
    Sep  8 21:38:55.999: INFO: Pod "downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8" satisfied condition "Succeeded or Failed"
    Sep  8 21:38:56.008: INFO: Trying to get logs from node node-3 pod downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8 container client-container: <nil>
    STEP: delete the pod 09/08/23 21:38:56.027
    Sep  8 21:38:56.059: INFO: Waiting for pod downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8 to disappear
    Sep  8 21:38:56.073: INFO: Pod downwardapi-volume-cdbfd823-c769-402e-b42c-2a689470b7e8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:38:56.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3367" for this suite. 09/08/23 21:38:56.087
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:38:56.112
Sep  8 21:38:56.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:38:56.114
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:56.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:56.164
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 09/08/23 21:38:56.169
Sep  8 21:38:56.194: INFO: Waiting up to 5m0s for pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd" in namespace "downward-api-7133" to be "running and ready"
Sep  8 21:38:56.205: INFO: Pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.692676ms
Sep  8 21:38:56.214: INFO: The phase of Pod annotationupdate7521484a-4b67-4f76-b60f-f00716712acd is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:38:58.223: INFO: Pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd": Phase="Running", Reason="", readiness=true. Elapsed: 2.029307423s
Sep  8 21:38:58.223: INFO: The phase of Pod annotationupdate7521484a-4b67-4f76-b60f-f00716712acd is Running (Ready = true)
Sep  8 21:38:58.223: INFO: Pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd" satisfied condition "running and ready"
Sep  8 21:38:58.778: INFO: Successfully updated pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:02.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7133" for this suite. 09/08/23 21:39:02.867
------------------------------
â€¢ [SLOW TEST] [6.793 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:38:56.112
    Sep  8 21:38:56.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:38:56.114
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:38:56.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:38:56.164
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 09/08/23 21:38:56.169
    Sep  8 21:38:56.194: INFO: Waiting up to 5m0s for pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd" in namespace "downward-api-7133" to be "running and ready"
    Sep  8 21:38:56.205: INFO: Pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.692676ms
    Sep  8 21:38:56.214: INFO: The phase of Pod annotationupdate7521484a-4b67-4f76-b60f-f00716712acd is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:38:58.223: INFO: Pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd": Phase="Running", Reason="", readiness=true. Elapsed: 2.029307423s
    Sep  8 21:38:58.223: INFO: The phase of Pod annotationupdate7521484a-4b67-4f76-b60f-f00716712acd is Running (Ready = true)
    Sep  8 21:38:58.223: INFO: Pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd" satisfied condition "running and ready"
    Sep  8 21:38:58.778: INFO: Successfully updated pod "annotationupdate7521484a-4b67-4f76-b60f-f00716712acd"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:02.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7133" for this suite. 09/08/23 21:39:02.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:02.906
Sep  8 21:39:02.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename security-context-test 09/08/23 21:39:02.908
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:02.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:02.956
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Sep  8 21:39:02.985: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6" in namespace "security-context-test-4313" to be "Succeeded or Failed"
Sep  8 21:39:03.002: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.102654ms
Sep  8 21:39:05.021: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035798368s
Sep  8 21:39:07.011: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026181603s
Sep  8 21:39:07.011: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6" satisfied condition "Succeeded or Failed"
Sep  8 21:39:07.066: INFO: Got logs for pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:07.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4313" for this suite. 09/08/23 21:39:07.077
------------------------------
â€¢ [4.192 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:02.906
    Sep  8 21:39:02.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename security-context-test 09/08/23 21:39:02.908
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:02.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:02.956
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Sep  8 21:39:02.985: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6" in namespace "security-context-test-4313" to be "Succeeded or Failed"
    Sep  8 21:39:03.002: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.102654ms
    Sep  8 21:39:05.021: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035798368s
    Sep  8 21:39:07.011: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026181603s
    Sep  8 21:39:07.011: INFO: Pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6" satisfied condition "Succeeded or Failed"
    Sep  8 21:39:07.066: INFO: Got logs for pod "busybox-privileged-false-f615e6c9-5e46-4abf-83eb-e7f668bc87a6": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:07.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4313" for this suite. 09/08/23 21:39:07.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:07.103
Sep  8 21:39:07.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 21:39:07.105
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:07.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:07.143
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 09/08/23 21:39:07.152
Sep  8 21:39:07.153: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1191 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 09/08/23 21:39:07.226
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:07.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1191" for this suite. 09/08/23 21:39:07.256
------------------------------
â€¢ [0.175 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:07.103
    Sep  8 21:39:07.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 21:39:07.105
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:07.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:07.143
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 09/08/23 21:39:07.152
    Sep  8 21:39:07.153: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1191 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 09/08/23 21:39:07.226
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:07.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1191" for this suite. 09/08/23 21:39:07.256
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:07.278
Sep  8 21:39:07.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:39:07.28
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:07.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:07.329
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:39:07.363
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:39:07.734
STEP: Deploying the webhook pod 09/08/23 21:39:07.754
STEP: Wait for the deployment to be ready 09/08/23 21:39:07.782
Sep  8 21:39:07.803: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/08/23 21:39:09.84
STEP: Verifying the service has paired with the endpoint 09/08/23 21:39:09.887
Sep  8 21:39:10.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 09/08/23 21:39:11.073
STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 21:39:11.154
STEP: Deleting the collection of validation webhooks 09/08/23 21:39:11.208
STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 21:39:11.36
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:11.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3045" for this suite. 09/08/23 21:39:11.614
STEP: Destroying namespace "webhook-3045-markers" for this suite. 09/08/23 21:39:11.679
------------------------------
â€¢ [4.429 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:07.278
    Sep  8 21:39:07.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:39:07.28
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:07.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:07.329
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:39:07.363
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:39:07.734
    STEP: Deploying the webhook pod 09/08/23 21:39:07.754
    STEP: Wait for the deployment to be ready 09/08/23 21:39:07.782
    Sep  8 21:39:07.803: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/08/23 21:39:09.84
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:39:09.887
    Sep  8 21:39:10.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 09/08/23 21:39:11.073
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 21:39:11.154
    STEP: Deleting the collection of validation webhooks 09/08/23 21:39:11.208
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 21:39:11.36
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:11.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3045" for this suite. 09/08/23 21:39:11.614
    STEP: Destroying namespace "webhook-3045-markers" for this suite. 09/08/23 21:39:11.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:11.719
Sep  8 21:39:11.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename containers 09/08/23 21:39:11.721
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:11.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:11.787
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 09/08/23 21:39:11.792
Sep  8 21:39:11.823: INFO: Waiting up to 5m0s for pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151" in namespace "containers-7326" to be "Succeeded or Failed"
Sep  8 21:39:11.847: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0439ms
Sep  8 21:39:13.855: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032298425s
Sep  8 21:39:15.863: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039627702s
STEP: Saw pod success 09/08/23 21:39:15.863
Sep  8 21:39:15.864: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151" satisfied condition "Succeeded or Failed"
Sep  8 21:39:15.889: INFO: Trying to get logs from node node-3 pod client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:39:15.904
Sep  8 21:39:15.943: INFO: Waiting for pod client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151 to disappear
Sep  8 21:39:15.949: INFO: Pod client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:15.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7326" for this suite. 09/08/23 21:39:15.964
------------------------------
â€¢ [4.263 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:11.719
    Sep  8 21:39:11.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename containers 09/08/23 21:39:11.721
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:11.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:11.787
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 09/08/23 21:39:11.792
    Sep  8 21:39:11.823: INFO: Waiting up to 5m0s for pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151" in namespace "containers-7326" to be "Succeeded or Failed"
    Sep  8 21:39:11.847: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0439ms
    Sep  8 21:39:13.855: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032298425s
    Sep  8 21:39:15.863: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039627702s
    STEP: Saw pod success 09/08/23 21:39:15.863
    Sep  8 21:39:15.864: INFO: Pod "client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151" satisfied condition "Succeeded or Failed"
    Sep  8 21:39:15.889: INFO: Trying to get logs from node node-3 pod client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:39:15.904
    Sep  8 21:39:15.943: INFO: Waiting for pod client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151 to disappear
    Sep  8 21:39:15.949: INFO: Pod client-containers-e9d3b3c1-7074-4f25-8ef0-56002f558151 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:15.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7326" for this suite. 09/08/23 21:39:15.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:15.987
Sep  8 21:39:15.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-runtime 09/08/23 21:39:15.988
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:16.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:16.03
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 09/08/23 21:39:16.036
STEP: wait for the container to reach Failed 09/08/23 21:39:16.058
STEP: get the container status 09/08/23 21:39:20.126
STEP: the container should be terminated 09/08/23 21:39:20.14
STEP: the termination message should be set 09/08/23 21:39:20.14
Sep  8 21:39:20.140: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 09/08/23 21:39:20.14
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:20.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5261" for this suite. 09/08/23 21:39:20.207
------------------------------
â€¢ [4.242 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:15.987
    Sep  8 21:39:15.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-runtime 09/08/23 21:39:15.988
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:16.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:16.03
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 09/08/23 21:39:16.036
    STEP: wait for the container to reach Failed 09/08/23 21:39:16.058
    STEP: get the container status 09/08/23 21:39:20.126
    STEP: the container should be terminated 09/08/23 21:39:20.14
    STEP: the termination message should be set 09/08/23 21:39:20.14
    Sep  8 21:39:20.140: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 09/08/23 21:39:20.14
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:20.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5261" for this suite. 09/08/23 21:39:20.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:20.23
Sep  8 21:39:20.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:39:20.231
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:20.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:20.284
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:39:20.368
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:39:21.101
STEP: Deploying the webhook pod 09/08/23 21:39:21.12
STEP: Wait for the deployment to be ready 09/08/23 21:39:21.153
Sep  8 21:39:21.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 21:39:23.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 21:39:25.221
STEP: Verifying the service has paired with the endpoint 09/08/23 21:39:25.287
Sep  8 21:39:26.288: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Sep  8 21:39:26.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4865-crds.webhook.example.com via the AdmissionRegistration API 09/08/23 21:39:31.818
STEP: Creating a custom resource that should be mutated by the webhook 09/08/23 21:39:31.858
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:34.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3077" for this suite. 09/08/23 21:39:34.625
STEP: Destroying namespace "webhook-3077-markers" for this suite. 09/08/23 21:39:34.667
------------------------------
â€¢ [SLOW TEST] [14.495 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:20.23
    Sep  8 21:39:20.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:39:20.231
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:20.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:20.284
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:39:20.368
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:39:21.101
    STEP: Deploying the webhook pod 09/08/23 21:39:21.12
    STEP: Wait for the deployment to be ready 09/08/23 21:39:21.153
    Sep  8 21:39:21.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 21:39:23.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 21:39:25.221
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:39:25.287
    Sep  8 21:39:26.288: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Sep  8 21:39:26.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4865-crds.webhook.example.com via the AdmissionRegistration API 09/08/23 21:39:31.818
    STEP: Creating a custom resource that should be mutated by the webhook 09/08/23 21:39:31.858
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:34.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3077" for this suite. 09/08/23 21:39:34.625
    STEP: Destroying namespace "webhook-3077-markers" for this suite. 09/08/23 21:39:34.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:34.736
Sep  8 21:39:34.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubelet-test 09/08/23 21:39:34.738
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:34.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:34.811
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Sep  8 21:39:34.838: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042" in namespace "kubelet-test-2033" to be "running and ready"
Sep  8 21:39:34.863: INFO: Pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042": Phase="Pending", Reason="", readiness=false. Elapsed: 24.364258ms
Sep  8 21:39:34.863: INFO: The phase of Pod busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:39:36.873: INFO: Pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042": Phase="Running", Reason="", readiness=true. Elapsed: 2.034395731s
Sep  8 21:39:36.873: INFO: The phase of Pod busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042 is Running (Ready = true)
Sep  8 21:39:36.873: INFO: Pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:36.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2033" for this suite. 09/08/23 21:39:36.929
------------------------------
â€¢ [2.210 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:34.736
    Sep  8 21:39:34.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubelet-test 09/08/23 21:39:34.738
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:34.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:34.811
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Sep  8 21:39:34.838: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042" in namespace "kubelet-test-2033" to be "running and ready"
    Sep  8 21:39:34.863: INFO: Pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042": Phase="Pending", Reason="", readiness=false. Elapsed: 24.364258ms
    Sep  8 21:39:34.863: INFO: The phase of Pod busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:39:36.873: INFO: Pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042": Phase="Running", Reason="", readiness=true. Elapsed: 2.034395731s
    Sep  8 21:39:36.873: INFO: The phase of Pod busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042 is Running (Ready = true)
    Sep  8 21:39:36.873: INFO: Pod "busybox-readonly-fsbae61da8-8a98-469c-8c7b-071f92d67042" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:36.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2033" for this suite. 09/08/23 21:39:36.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:36.949
Sep  8 21:39:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:39:36.953
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:36.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:36.99
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 09/08/23 21:39:36.997
Sep  8 21:39:37.014: INFO: Waiting up to 5m0s for pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852" in namespace "emptydir-2194" to be "Succeeded or Failed"
Sep  8 21:39:37.031: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852": Phase="Pending", Reason="", readiness=false. Elapsed: 17.061216ms
Sep  8 21:39:39.039: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025373137s
Sep  8 21:39:41.045: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030769126s
STEP: Saw pod success 09/08/23 21:39:41.045
Sep  8 21:39:41.045: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852" satisfied condition "Succeeded or Failed"
Sep  8 21:39:41.053: INFO: Trying to get logs from node node-4 pod pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852 container test-container: <nil>
STEP: delete the pod 09/08/23 21:39:41.082
Sep  8 21:39:41.125: INFO: Waiting for pod pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852 to disappear
Sep  8 21:39:41.131: INFO: Pod pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:41.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2194" for this suite. 09/08/23 21:39:41.15
------------------------------
â€¢ [4.228 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:36.949
    Sep  8 21:39:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:39:36.953
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:36.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:36.99
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 09/08/23 21:39:36.997
    Sep  8 21:39:37.014: INFO: Waiting up to 5m0s for pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852" in namespace "emptydir-2194" to be "Succeeded or Failed"
    Sep  8 21:39:37.031: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852": Phase="Pending", Reason="", readiness=false. Elapsed: 17.061216ms
    Sep  8 21:39:39.039: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025373137s
    Sep  8 21:39:41.045: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030769126s
    STEP: Saw pod success 09/08/23 21:39:41.045
    Sep  8 21:39:41.045: INFO: Pod "pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852" satisfied condition "Succeeded or Failed"
    Sep  8 21:39:41.053: INFO: Trying to get logs from node node-4 pod pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852 container test-container: <nil>
    STEP: delete the pod 09/08/23 21:39:41.082
    Sep  8 21:39:41.125: INFO: Waiting for pod pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852 to disappear
    Sep  8 21:39:41.131: INFO: Pod pod-e0e15fe8-dcb7-466e-8e04-9d58f95fc852 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:41.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2194" for this suite. 09/08/23 21:39:41.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:41.181
Sep  8 21:39:41.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename daemonsets 09/08/23 21:39:41.182
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:41.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:41.25
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Sep  8 21:39:41.321: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 09/08/23 21:39:41.336
Sep  8 21:39:41.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:41.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 09/08/23 21:39:41.358
Sep  8 21:39:41.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:41.420: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:39:42.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:42.431: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:39:43.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 21:39:43.436: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 09/08/23 21:39:43.448
Sep  8 21:39:43.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 21:39:43.519: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Sep  8 21:39:44.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:44.532: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/08/23 21:39:44.532
Sep  8 21:39:44.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:44.568: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:39:45.579: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:45.579: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:39:46.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:46.591: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:39:47.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:47.578: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:39:48.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 21:39:48.585: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/08/23 21:39:48.604
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7737, will wait for the garbage collector to delete the pods 09/08/23 21:39:48.604
Sep  8 21:39:48.681: INFO: Deleting DaemonSet.extensions daemon-set took: 17.254026ms
Sep  8 21:39:48.781: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.375421ms
Sep  8 21:39:50.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:39:50.993: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  8 21:39:51.007: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21342"},"items":null}

Sep  8 21:39:51.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21342"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7737" for this suite. 09/08/23 21:39:51.097
------------------------------
â€¢ [SLOW TEST] [9.945 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:41.181
    Sep  8 21:39:41.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename daemonsets 09/08/23 21:39:41.182
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:41.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:41.25
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Sep  8 21:39:41.321: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 09/08/23 21:39:41.336
    Sep  8 21:39:41.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:41.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 09/08/23 21:39:41.358
    Sep  8 21:39:41.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:41.420: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:39:42.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:42.431: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:39:43.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 21:39:43.436: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 09/08/23 21:39:43.448
    Sep  8 21:39:43.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 21:39:43.519: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Sep  8 21:39:44.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:44.532: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 09/08/23 21:39:44.532
    Sep  8 21:39:44.568: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:44.568: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:39:45.579: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:45.579: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:39:46.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:46.591: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:39:47.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:47.578: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:39:48.585: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 21:39:48.585: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/08/23 21:39:48.604
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7737, will wait for the garbage collector to delete the pods 09/08/23 21:39:48.604
    Sep  8 21:39:48.681: INFO: Deleting DaemonSet.extensions daemon-set took: 17.254026ms
    Sep  8 21:39:48.781: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.375421ms
    Sep  8 21:39:50.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:39:50.993: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  8 21:39:51.007: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21342"},"items":null}

    Sep  8 21:39:51.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21342"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7737" for this suite. 09/08/23 21:39:51.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:51.13
Sep  8 21:39:51.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:39:51.131
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:51.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:51.178
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  09/08/23 21:39:51.184
Sep  8 21:39:51.203: INFO: Waiting up to 5m0s for pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1" in namespace "svcaccounts-7371" to be "Succeeded or Failed"
Sep  8 21:39:51.218: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.557208ms
Sep  8 21:39:53.228: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025626491s
Sep  8 21:39:55.233: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030040406s
STEP: Saw pod success 09/08/23 21:39:55.233
Sep  8 21:39:55.233: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1" satisfied condition "Succeeded or Failed"
Sep  8 21:39:55.244: INFO: Trying to get logs from node node-4 pod test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:39:55.269
Sep  8 21:39:55.306: INFO: Waiting for pod test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1 to disappear
Sep  8 21:39:55.313: INFO: Pod test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  8 21:39:55.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7371" for this suite. 09/08/23 21:39:55.324
------------------------------
â€¢ [4.214 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:51.13
    Sep  8 21:39:51.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:39:51.131
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:51.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:51.178
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  09/08/23 21:39:51.184
    Sep  8 21:39:51.203: INFO: Waiting up to 5m0s for pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1" in namespace "svcaccounts-7371" to be "Succeeded or Failed"
    Sep  8 21:39:51.218: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.557208ms
    Sep  8 21:39:53.228: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025626491s
    Sep  8 21:39:55.233: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030040406s
    STEP: Saw pod success 09/08/23 21:39:55.233
    Sep  8 21:39:55.233: INFO: Pod "test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1" satisfied condition "Succeeded or Failed"
    Sep  8 21:39:55.244: INFO: Trying to get logs from node node-4 pod test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:39:55.269
    Sep  8 21:39:55.306: INFO: Waiting for pod test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1 to disappear
    Sep  8 21:39:55.313: INFO: Pod test-pod-2998b4b0-7d16-4e18-b82c-5be55f2ce4c1 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:39:55.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7371" for this suite. 09/08/23 21:39:55.324
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:39:55.345
Sep  8 21:39:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename aggregator 09/08/23 21:39:55.348
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:55.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:55.396
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Sep  8 21:39:55.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 09/08/23 21:39:55.401
Sep  8 21:39:55.802: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  8 21:39:57.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:39:59.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:01.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:03.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:05.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:07.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:09.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:11.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:13.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:15.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:17.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:19.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:21.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:23.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:25.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:27.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:29.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:31.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:33.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:35.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:37.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:39.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:41.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:43.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:45.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:47.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:49.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:51.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:53.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:40:56.240: INFO: Waited 244.089618ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 09/08/23 21:40:56.338
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/08/23 21:40:56.348
STEP: List APIServices 09/08/23 21:40:56.367
Sep  8 21:40:56.382: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Sep  8 21:40:56.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2927" for this suite. 09/08/23 21:40:56.839
------------------------------
â€¢ [SLOW TEST] [61.549 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:39:55.345
    Sep  8 21:39:55.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename aggregator 09/08/23 21:39:55.348
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:39:55.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:39:55.396
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Sep  8 21:39:55.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 09/08/23 21:39:55.401
    Sep  8 21:39:55.802: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Sep  8 21:39:57.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:39:59.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:01.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:03.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:05.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:07.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:09.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:11.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:13.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:15.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:17.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:19.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:21.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:23.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:25.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:27.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:29.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:31.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:33.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:35.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:37.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:39.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:41.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:43.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:45.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:47.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:49.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:51.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:53.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 39, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:40:56.240: INFO: Waited 244.089618ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 09/08/23 21:40:56.338
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 09/08/23 21:40:56.348
    STEP: List APIServices 09/08/23 21:40:56.367
    Sep  8 21:40:56.382: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:40:56.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2927" for this suite. 09/08/23 21:40:56.839
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:40:56.896
Sep  8 21:40:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-webhook 09/08/23 21:40:56.899
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:40:56.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:40:56.973
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 09/08/23 21:40:56.979
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/08/23 21:40:57.405
STEP: Deploying the custom resource conversion webhook pod 09/08/23 21:40:57.423
STEP: Wait for the deployment to be ready 09/08/23 21:40:57.452
Sep  8 21:40:57.484: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep  8 21:40:59.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 21:41:01.531
STEP: Verifying the service has paired with the endpoint 09/08/23 21:41:01.595
Sep  8 21:41:02.597: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Sep  8 21:41:02.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Creating a v1 custom resource 09/08/23 21:41:10.29
STEP: Create a v2 custom resource 09/08/23 21:41:10.348
STEP: List CRs in v1 09/08/23 21:41:10.382
STEP: List CRs in v2 09/08/23 21:41:10.463
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:41:11.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-2321" for this suite. 09/08/23 21:41:11.19
------------------------------
â€¢ [SLOW TEST] [14.362 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:40:56.896
    Sep  8 21:40:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-webhook 09/08/23 21:40:56.899
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:40:56.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:40:56.973
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 09/08/23 21:40:56.979
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 09/08/23 21:40:57.405
    STEP: Deploying the custom resource conversion webhook pod 09/08/23 21:40:57.423
    STEP: Wait for the deployment to be ready 09/08/23 21:40:57.452
    Sep  8 21:40:57.484: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Sep  8 21:40:59.512: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 40, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-74ff66dd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 21:41:01.531
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:41:01.595
    Sep  8 21:41:02.597: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Sep  8 21:41:02.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Creating a v1 custom resource 09/08/23 21:41:10.29
    STEP: Create a v2 custom resource 09/08/23 21:41:10.348
    STEP: List CRs in v1 09/08/23 21:41:10.382
    STEP: List CRs in v2 09/08/23 21:41:10.463
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:41:11.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-2321" for this suite. 09/08/23 21:41:11.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:41:11.28
Sep  8 21:41:11.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename ingressclass 09/08/23 21:41:11.283
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:11.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:11.374
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 09/08/23 21:41:11.381
STEP: getting /apis/networking.k8s.io 09/08/23 21:41:11.389
STEP: getting /apis/networking.k8s.iov1 09/08/23 21:41:11.393
STEP: creating 09/08/23 21:41:11.396
STEP: getting 09/08/23 21:41:11.463
STEP: listing 09/08/23 21:41:11.481
STEP: watching 09/08/23 21:41:11.491
Sep  8 21:41:11.492: INFO: starting watch
STEP: patching 09/08/23 21:41:11.494
STEP: updating 09/08/23 21:41:11.509
Sep  8 21:41:11.559: INFO: waiting for watch events with expected annotations
Sep  8 21:41:11.559: INFO: saw patched and updated annotations
STEP: deleting 09/08/23 21:41:11.56
STEP: deleting a collection 09/08/23 21:41:11.638
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Sep  8 21:41:11.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-1684" for this suite. 09/08/23 21:41:11.749
------------------------------
â€¢ [0.499 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:41:11.28
    Sep  8 21:41:11.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename ingressclass 09/08/23 21:41:11.283
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:11.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:11.374
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 09/08/23 21:41:11.381
    STEP: getting /apis/networking.k8s.io 09/08/23 21:41:11.389
    STEP: getting /apis/networking.k8s.iov1 09/08/23 21:41:11.393
    STEP: creating 09/08/23 21:41:11.396
    STEP: getting 09/08/23 21:41:11.463
    STEP: listing 09/08/23 21:41:11.481
    STEP: watching 09/08/23 21:41:11.491
    Sep  8 21:41:11.492: INFO: starting watch
    STEP: patching 09/08/23 21:41:11.494
    STEP: updating 09/08/23 21:41:11.509
    Sep  8 21:41:11.559: INFO: waiting for watch events with expected annotations
    Sep  8 21:41:11.559: INFO: saw patched and updated annotations
    STEP: deleting 09/08/23 21:41:11.56
    STEP: deleting a collection 09/08/23 21:41:11.638
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:41:11.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-1684" for this suite. 09/08/23 21:41:11.749
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:41:11.781
Sep  8 21:41:11.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:41:11.786
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:11.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:11.828
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 09/08/23 21:41:11.832
STEP: fetching the ConfigMap 09/08/23 21:41:11.84
STEP: patching the ConfigMap 09/08/23 21:41:11.85
STEP: listing all ConfigMaps in all namespaces with a label selector 09/08/23 21:41:11.875
STEP: deleting the ConfigMap by collection with a label selector 09/08/23 21:41:11.883
STEP: listing all ConfigMaps in test namespace 09/08/23 21:41:11.925
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:41:11.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4627" for this suite. 09/08/23 21:41:11.976
------------------------------
â€¢ [0.213 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:41:11.781
    Sep  8 21:41:11.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:41:11.786
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:11.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:11.828
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 09/08/23 21:41:11.832
    STEP: fetching the ConfigMap 09/08/23 21:41:11.84
    STEP: patching the ConfigMap 09/08/23 21:41:11.85
    STEP: listing all ConfigMaps in all namespaces with a label selector 09/08/23 21:41:11.875
    STEP: deleting the ConfigMap by collection with a label selector 09/08/23 21:41:11.883
    STEP: listing all ConfigMaps in test namespace 09/08/23 21:41:11.925
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:41:11.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4627" for this suite. 09/08/23 21:41:11.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:41:12
Sep  8 21:41:12.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 21:41:12.002
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:12.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:12.058
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Sep  8 21:41:12.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: creating the pod 09/08/23 21:41:12.065
STEP: submitting the pod to kubernetes 09/08/23 21:41:12.065
Sep  8 21:41:12.092: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4" in namespace "pods-8560" to be "running and ready"
Sep  8 21:41:12.102: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.272003ms
Sep  8 21:41:12.102: INFO: The phase of Pod pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:41:14.119: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027028067s
Sep  8 21:41:14.119: INFO: The phase of Pod pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:41:16.113: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4": Phase="Running", Reason="", readiness=true. Elapsed: 4.021436036s
Sep  8 21:41:16.113: INFO: The phase of Pod pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4 is Running (Ready = true)
Sep  8 21:41:16.113: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 21:41:16.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8560" for this suite. 09/08/23 21:41:16.193
------------------------------
â€¢ [4.208 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:41:12
    Sep  8 21:41:12.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 21:41:12.002
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:12.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:12.058
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Sep  8 21:41:12.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: creating the pod 09/08/23 21:41:12.065
    STEP: submitting the pod to kubernetes 09/08/23 21:41:12.065
    Sep  8 21:41:12.092: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4" in namespace "pods-8560" to be "running and ready"
    Sep  8 21:41:12.102: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.272003ms
    Sep  8 21:41:12.102: INFO: The phase of Pod pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:41:14.119: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027028067s
    Sep  8 21:41:14.119: INFO: The phase of Pod pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:41:16.113: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4": Phase="Running", Reason="", readiness=true. Elapsed: 4.021436036s
    Sep  8 21:41:16.113: INFO: The phase of Pod pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4 is Running (Ready = true)
    Sep  8 21:41:16.113: INFO: Pod "pod-logs-websocket-0e6aa8e1-4ca3-4c03-980c-ddb962dd45a4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:41:16.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8560" for this suite. 09/08/23 21:41:16.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:41:16.212
Sep  8 21:41:16.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename runtimeclass 09/08/23 21:41:16.213
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:16.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:16.25
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Sep  8 21:41:16.302: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1422 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  8 21:41:16.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1422" for this suite. 09/08/23 21:41:16.348
------------------------------
â€¢ [0.155 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:41:16.212
    Sep  8 21:41:16.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename runtimeclass 09/08/23 21:41:16.213
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:16.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:16.25
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Sep  8 21:41:16.302: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1422 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:41:16.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1422" for this suite. 09/08/23 21:41:16.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:41:16.369
Sep  8 21:41:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:41:16.371
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:16.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:16.419
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 09/08/23 21:41:16.431
STEP: Getting a ResourceQuota 09/08/23 21:41:16.444
STEP: Updating a ResourceQuota 09/08/23 21:41:16.456
STEP: Verifying a ResourceQuota was modified 09/08/23 21:41:16.465
STEP: Deleting a ResourceQuota 09/08/23 21:41:16.476
STEP: Verifying the deleted ResourceQuota 09/08/23 21:41:16.492
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:41:16.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9211" for this suite. 09/08/23 21:41:16.51
------------------------------
â€¢ [0.161 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:41:16.369
    Sep  8 21:41:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:41:16.371
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:16.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:16.419
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 09/08/23 21:41:16.431
    STEP: Getting a ResourceQuota 09/08/23 21:41:16.444
    STEP: Updating a ResourceQuota 09/08/23 21:41:16.456
    STEP: Verifying a ResourceQuota was modified 09/08/23 21:41:16.465
    STEP: Deleting a ResourceQuota 09/08/23 21:41:16.476
    STEP: Verifying the deleted ResourceQuota 09/08/23 21:41:16.492
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:41:16.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9211" for this suite. 09/08/23 21:41:16.51
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:41:16.531
Sep  8 21:41:16.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-watch 09/08/23 21:41:16.532
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:16.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:16.575
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Sep  8 21:41:16.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Creating first CR  09/08/23 21:41:24.213
Sep  8 21:41:24.246: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:24Z]] name:name1 resourceVersion:22065 uid:aa10e1fa-792e-4284-8093-5686a395bb5c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 09/08/23 21:41:34.246
Sep  8 21:41:34.264: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:34Z]] name:name2 resourceVersion:22115 uid:f0ee947f-f7ea-49f7-bd18-e3ae2dfca8c3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 09/08/23 21:41:44.265
Sep  8 21:41:44.280: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:44Z]] name:name1 resourceVersion:22160 uid:aa10e1fa-792e-4284-8093-5686a395bb5c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 09/08/23 21:41:54.28
Sep  8 21:41:54.296: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:54Z]] name:name2 resourceVersion:22207 uid:f0ee947f-f7ea-49f7-bd18-e3ae2dfca8c3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 09/08/23 21:42:04.297
Sep  8 21:42:04.331: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:44Z]] name:name1 resourceVersion:22258 uid:aa10e1fa-792e-4284-8093-5686a395bb5c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 09/08/23 21:42:14.331
Sep  8 21:42:14.350: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:54Z]] name:name2 resourceVersion:22302 uid:f0ee947f-f7ea-49f7-bd18-e3ae2dfca8c3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:42:24.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-7336" for this suite. 09/08/23 21:42:24.928
------------------------------
â€¢ [SLOW TEST] [68.414 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:41:16.531
    Sep  8 21:41:16.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-watch 09/08/23 21:41:16.532
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:41:16.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:41:16.575
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Sep  8 21:41:16.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Creating first CR  09/08/23 21:41:24.213
    Sep  8 21:41:24.246: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:24Z]] name:name1 resourceVersion:22065 uid:aa10e1fa-792e-4284-8093-5686a395bb5c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 09/08/23 21:41:34.246
    Sep  8 21:41:34.264: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:34Z]] name:name2 resourceVersion:22115 uid:f0ee947f-f7ea-49f7-bd18-e3ae2dfca8c3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 09/08/23 21:41:44.265
    Sep  8 21:41:44.280: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:44Z]] name:name1 resourceVersion:22160 uid:aa10e1fa-792e-4284-8093-5686a395bb5c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 09/08/23 21:41:54.28
    Sep  8 21:41:54.296: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:54Z]] name:name2 resourceVersion:22207 uid:f0ee947f-f7ea-49f7-bd18-e3ae2dfca8c3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 09/08/23 21:42:04.297
    Sep  8 21:42:04.331: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:44Z]] name:name1 resourceVersion:22258 uid:aa10e1fa-792e-4284-8093-5686a395bb5c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 09/08/23 21:42:14.331
    Sep  8 21:42:14.350: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-09-08T21:41:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-09-08T21:41:54Z]] name:name2 resourceVersion:22302 uid:f0ee947f-f7ea-49f7-bd18-e3ae2dfca8c3] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:42:24.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-7336" for this suite. 09/08/23 21:42:24.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:42:24.948
Sep  8 21:42:24.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename taint-single-pod 09/08/23 21:42:24.95
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:42:25.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:42:25.014
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Sep  8 21:42:25.030: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  8 21:43:25.108: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Sep  8 21:43:25.117: INFO: Starting informer...
STEP: Starting pod... 09/08/23 21:43:25.117
Sep  8 21:43:25.351: INFO: Pod is running on node-3. Tainting Node
STEP: Trying to apply a taint on the Node 09/08/23 21:43:25.351
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 21:43:25.378
STEP: Waiting short time to make sure Pod is queued for deletion 09/08/23 21:43:25.407
Sep  8 21:43:25.407: INFO: Pod wasn't evicted. Proceeding
Sep  8 21:43:25.408: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 21:43:25.433
STEP: Waiting some time to make sure that toleration time passed. 09/08/23 21:43:25.459
Sep  8 21:44:40.460: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:44:40.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3826" for this suite. 09/08/23 21:44:40.477
------------------------------
â€¢ [SLOW TEST] [135.552 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:42:24.948
    Sep  8 21:42:24.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename taint-single-pod 09/08/23 21:42:24.95
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:42:25.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:42:25.014
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Sep  8 21:42:25.030: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  8 21:43:25.108: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Sep  8 21:43:25.117: INFO: Starting informer...
    STEP: Starting pod... 09/08/23 21:43:25.117
    Sep  8 21:43:25.351: INFO: Pod is running on node-3. Tainting Node
    STEP: Trying to apply a taint on the Node 09/08/23 21:43:25.351
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 21:43:25.378
    STEP: Waiting short time to make sure Pod is queued for deletion 09/08/23 21:43:25.407
    Sep  8 21:43:25.407: INFO: Pod wasn't evicted. Proceeding
    Sep  8 21:43:25.408: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 21:43:25.433
    STEP: Waiting some time to make sure that toleration time passed. 09/08/23 21:43:25.459
    Sep  8 21:44:40.460: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:44:40.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3826" for this suite. 09/08/23 21:44:40.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:44:40.504
Sep  8 21:44:40.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:44:40.506
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:40.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:40.582
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 09/08/23 21:44:40.588
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:44:40.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6962" for this suite. 09/08/23 21:44:40.636
------------------------------
â€¢ [0.152 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:44:40.504
    Sep  8 21:44:40.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:44:40.506
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:40.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:40.582
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 09/08/23 21:44:40.588
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:44:40.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6962" for this suite. 09/08/23 21:44:40.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:44:40.664
Sep  8 21:44:40.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:44:40.664
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:40.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:40.719
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:44:40.765
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:44:41.115
STEP: Deploying the webhook pod 09/08/23 21:44:41.133
STEP: Wait for the deployment to be ready 09/08/23 21:44:41.158
Sep  8 21:44:41.188: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/08/23 21:44:43.217
STEP: Verifying the service has paired with the endpoint 09/08/23 21:44:43.248
Sep  8 21:44:44.249: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 09/08/23 21:44:44.258
STEP: create a pod that should be denied by the webhook 09/08/23 21:44:44.289
STEP: create a pod that causes the webhook to hang 09/08/23 21:44:44.308
STEP: create a configmap that should be denied by the webhook 09/08/23 21:44:54.329
STEP: create a configmap that should be admitted by the webhook 09/08/23 21:44:54.348
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/08/23 21:44:54.378
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/08/23 21:44:54.401
STEP: create a namespace that bypass the webhook 09/08/23 21:44:54.416
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/08/23 21:44:54.438
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:44:54.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8096" for this suite. 09/08/23 21:44:54.6
STEP: Destroying namespace "webhook-8096-markers" for this suite. 09/08/23 21:44:54.622
------------------------------
â€¢ [SLOW TEST] [13.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:44:40.664
    Sep  8 21:44:40.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:44:40.664
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:40.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:40.719
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:44:40.765
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:44:41.115
    STEP: Deploying the webhook pod 09/08/23 21:44:41.133
    STEP: Wait for the deployment to be ready 09/08/23 21:44:41.158
    Sep  8 21:44:41.188: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/08/23 21:44:43.217
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:44:43.248
    Sep  8 21:44:44.249: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 09/08/23 21:44:44.258
    STEP: create a pod that should be denied by the webhook 09/08/23 21:44:44.289
    STEP: create a pod that causes the webhook to hang 09/08/23 21:44:44.308
    STEP: create a configmap that should be denied by the webhook 09/08/23 21:44:54.329
    STEP: create a configmap that should be admitted by the webhook 09/08/23 21:44:54.348
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 09/08/23 21:44:54.378
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 09/08/23 21:44:54.401
    STEP: create a namespace that bypass the webhook 09/08/23 21:44:54.416
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 09/08/23 21:44:54.438
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:44:54.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8096" for this suite. 09/08/23 21:44:54.6
    STEP: Destroying namespace "webhook-8096-markers" for this suite. 09/08/23 21:44:54.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:44:54.654
Sep  8 21:44:54.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:44:54.656
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:54.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:54.702
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:44:54.708
Sep  8 21:44:54.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3" in namespace "downward-api-6937" to be "Succeeded or Failed"
Sep  8 21:44:54.745: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.774501ms
Sep  8 21:44:56.762: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033404074s
Sep  8 21:44:58.761: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033066226s
STEP: Saw pod success 09/08/23 21:44:58.761
Sep  8 21:44:58.762: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3" satisfied condition "Succeeded or Failed"
Sep  8 21:44:58.788: INFO: Trying to get logs from node node-3 pod downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3 container client-container: <nil>
STEP: delete the pod 09/08/23 21:44:58.833
Sep  8 21:44:58.866: INFO: Waiting for pod downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3 to disappear
Sep  8 21:44:58.882: INFO: Pod downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 21:44:58.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6937" for this suite. 09/08/23 21:44:58.897
------------------------------
â€¢ [4.260 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:44:54.654
    Sep  8 21:44:54.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:44:54.656
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:54.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:54.702
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:44:54.708
    Sep  8 21:44:54.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3" in namespace "downward-api-6937" to be "Succeeded or Failed"
    Sep  8 21:44:54.745: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.774501ms
    Sep  8 21:44:56.762: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033404074s
    Sep  8 21:44:58.761: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033066226s
    STEP: Saw pod success 09/08/23 21:44:58.761
    Sep  8 21:44:58.762: INFO: Pod "downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3" satisfied condition "Succeeded or Failed"
    Sep  8 21:44:58.788: INFO: Trying to get logs from node node-3 pod downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3 container client-container: <nil>
    STEP: delete the pod 09/08/23 21:44:58.833
    Sep  8 21:44:58.866: INFO: Waiting for pod downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3 to disappear
    Sep  8 21:44:58.882: INFO: Pod downwardapi-volume-3b98df19-2050-46d7-b66f-88d1d701c7c3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:44:58.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6937" for this suite. 09/08/23 21:44:58.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:44:58.917
Sep  8 21:44:58.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 21:44:58.918
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:58.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:58.96
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-c64b4da1-3e0d-4ed9-b543-318fd6682a71 09/08/23 21:44:58.968
STEP: Creating a pod to test consume secrets 09/08/23 21:44:58.987
Sep  8 21:44:59.006: INFO: Waiting up to 5m0s for pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a" in namespace "secrets-8464" to be "Succeeded or Failed"
Sep  8 21:44:59.013: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.862099ms
Sep  8 21:45:01.036: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029079844s
Sep  8 21:45:03.033: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026555736s
STEP: Saw pod success 09/08/23 21:45:03.033
Sep  8 21:45:03.034: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a" satisfied condition "Succeeded or Failed"
Sep  8 21:45:03.052: INFO: Trying to get logs from node node-3 pod pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 21:45:03.07
Sep  8 21:45:03.120: INFO: Waiting for pod pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a to disappear
Sep  8 21:45:03.131: INFO: Pod pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 21:45:03.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8464" for this suite. 09/08/23 21:45:03.143
------------------------------
â€¢ [4.241 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:44:58.917
    Sep  8 21:44:58.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 21:44:58.918
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:44:58.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:44:58.96
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-c64b4da1-3e0d-4ed9-b543-318fd6682a71 09/08/23 21:44:58.968
    STEP: Creating a pod to test consume secrets 09/08/23 21:44:58.987
    Sep  8 21:44:59.006: INFO: Waiting up to 5m0s for pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a" in namespace "secrets-8464" to be "Succeeded or Failed"
    Sep  8 21:44:59.013: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.862099ms
    Sep  8 21:45:01.036: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029079844s
    Sep  8 21:45:03.033: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026555736s
    STEP: Saw pod success 09/08/23 21:45:03.033
    Sep  8 21:45:03.034: INFO: Pod "pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a" satisfied condition "Succeeded or Failed"
    Sep  8 21:45:03.052: INFO: Trying to get logs from node node-3 pod pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 21:45:03.07
    Sep  8 21:45:03.120: INFO: Waiting for pod pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a to disappear
    Sep  8 21:45:03.131: INFO: Pod pod-secrets-7e0ba195-0bb1-468b-aa52-83af8edde24a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:45:03.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8464" for this suite. 09/08/23 21:45:03.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:45:03.17
Sep  8 21:45:03.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 21:45:03.171
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:03.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:03.211
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 09/08/23 21:45:03.217
STEP: waiting for pod running 09/08/23 21:45:03.25
Sep  8 21:45:03.251: INFO: Waiting up to 2m0s for pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" in namespace "var-expansion-3906" to be "running"
Sep  8 21:45:03.261: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02345ms
Sep  8 21:45:05.283: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec": Phase="Running", Reason="", readiness=true. Elapsed: 2.03233507s
Sep  8 21:45:05.283: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" satisfied condition "running"
STEP: creating a file in subpath 09/08/23 21:45:05.283
Sep  8 21:45:05.294: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3906 PodName:var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:45:05.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:45:05.296: INFO: ExecWithOptions: Clientset creation
Sep  8 21:45:05.303: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3906/pods/var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 09/08/23 21:45:05.448
Sep  8 21:45:05.458: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3906 PodName:var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:45:05.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:45:05.459: INFO: ExecWithOptions: Clientset creation
Sep  8 21:45:05.459: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3906/pods/var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 09/08/23 21:45:05.585
Sep  8 21:45:06.115: INFO: Successfully updated pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec"
STEP: waiting for annotated pod running 09/08/23 21:45:06.115
Sep  8 21:45:06.115: INFO: Waiting up to 2m0s for pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" in namespace "var-expansion-3906" to be "running"
Sep  8 21:45:06.127: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec": Phase="Running", Reason="", readiness=true. Elapsed: 11.59289ms
Sep  8 21:45:06.127: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" satisfied condition "running"
STEP: deleting the pod gracefully 09/08/23 21:45:06.127
Sep  8 21:45:06.127: INFO: Deleting pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" in namespace "var-expansion-3906"
Sep  8 21:45:06.154: INFO: Wait up to 5m0s for pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 21:45:40.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3906" for this suite. 09/08/23 21:45:40.188
------------------------------
â€¢ [SLOW TEST] [37.057 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:45:03.17
    Sep  8 21:45:03.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 21:45:03.171
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:03.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:03.211
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 09/08/23 21:45:03.217
    STEP: waiting for pod running 09/08/23 21:45:03.25
    Sep  8 21:45:03.251: INFO: Waiting up to 2m0s for pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" in namespace "var-expansion-3906" to be "running"
    Sep  8 21:45:03.261: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02345ms
    Sep  8 21:45:05.283: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec": Phase="Running", Reason="", readiness=true. Elapsed: 2.03233507s
    Sep  8 21:45:05.283: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" satisfied condition "running"
    STEP: creating a file in subpath 09/08/23 21:45:05.283
    Sep  8 21:45:05.294: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3906 PodName:var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:45:05.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:45:05.296: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:45:05.303: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3906/pods/var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 09/08/23 21:45:05.448
    Sep  8 21:45:05.458: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3906 PodName:var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:45:05.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:45:05.459: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:45:05.459: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3906/pods/var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 09/08/23 21:45:05.585
    Sep  8 21:45:06.115: INFO: Successfully updated pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec"
    STEP: waiting for annotated pod running 09/08/23 21:45:06.115
    Sep  8 21:45:06.115: INFO: Waiting up to 2m0s for pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" in namespace "var-expansion-3906" to be "running"
    Sep  8 21:45:06.127: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec": Phase="Running", Reason="", readiness=true. Elapsed: 11.59289ms
    Sep  8 21:45:06.127: INFO: Pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" satisfied condition "running"
    STEP: deleting the pod gracefully 09/08/23 21:45:06.127
    Sep  8 21:45:06.127: INFO: Deleting pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" in namespace "var-expansion-3906"
    Sep  8 21:45:06.154: INFO: Wait up to 5m0s for pod "var-expansion-8ac0efb3-7fbc-4110-bb95-bcc698448aec" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:45:40.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3906" for this suite. 09/08/23 21:45:40.188
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:45:40.228
Sep  8 21:45:40.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:45:40.229
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:40.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:40.264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:45:40.326
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:45:40.655
STEP: Deploying the webhook pod 09/08/23 21:45:40.678
STEP: Wait for the deployment to be ready 09/08/23 21:45:40.713
Sep  8 21:45:40.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 21:45:42.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 21:45:44.809
STEP: Verifying the service has paired with the endpoint 09/08/23 21:45:44.847
Sep  8 21:45:45.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 09/08/23 21:45:45.855
STEP: Creating a custom resource definition that should be denied by the webhook 09/08/23 21:45:45.891
Sep  8 21:45:45.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:45:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2227" for this suite. 09/08/23 21:45:46.079
STEP: Destroying namespace "webhook-2227-markers" for this suite. 09/08/23 21:45:46.101
------------------------------
â€¢ [SLOW TEST] [5.898 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:45:40.228
    Sep  8 21:45:40.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:45:40.229
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:40.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:40.264
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:45:40.326
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:45:40.655
    STEP: Deploying the webhook pod 09/08/23 21:45:40.678
    STEP: Wait for the deployment to be ready 09/08/23 21:45:40.713
    Sep  8 21:45:40.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 21:45:42.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 45, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 21:45:44.809
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:45:44.847
    Sep  8 21:45:45.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 09/08/23 21:45:45.855
    STEP: Creating a custom resource definition that should be denied by the webhook 09/08/23 21:45:45.891
    Sep  8 21:45:45.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:45:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2227" for this suite. 09/08/23 21:45:46.079
    STEP: Destroying namespace "webhook-2227-markers" for this suite. 09/08/23 21:45:46.101
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:45:46.128
Sep  8 21:45:46.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:45:46.13
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:46.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:46.224
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-cba2571a-40a9-4c5b-aae0-d9db5c592189 09/08/23 21:45:46.235
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:45:46.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-271" for this suite. 09/08/23 21:45:46.268
------------------------------
â€¢ [0.158 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:45:46.128
    Sep  8 21:45:46.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:45:46.13
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:46.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:46.224
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-cba2571a-40a9-4c5b-aae0-d9db5c592189 09/08/23 21:45:46.235
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:45:46.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-271" for this suite. 09/08/23 21:45:46.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:45:46.289
Sep  8 21:45:46.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename hostport 09/08/23 21:45:46.29
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:46.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:46.326
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/08/23 21:45:46.34
Sep  8 21:45:46.363: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-717" to be "running and ready"
Sep  8 21:45:46.373: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.716376ms
Sep  8 21:45:46.373: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:45:48.387: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023739405s
Sep  8 21:45:48.387: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:45:50.393: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.029906369s
Sep  8 21:45:50.393: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  8 21:45:50.393: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.100.16.26 on the node which pod1 resides and expect scheduled 09/08/23 21:45:50.393
Sep  8 21:45:50.418: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-717" to be "running and ready"
Sep  8 21:45:50.439: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.794295ms
Sep  8 21:45:50.439: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:45:52.453: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035066367s
Sep  8 21:45:52.453: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:45:54.448: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.030003249s
Sep  8 21:45:54.448: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  8 21:45:54.448: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.100.16.26 but use UDP protocol on the node which pod2 resides 09/08/23 21:45:54.448
Sep  8 21:45:54.470: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-717" to be "running and ready"
Sep  8 21:45:54.480: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.761217ms
Sep  8 21:45:54.481: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:45:56.491: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021220432s
Sep  8 21:45:56.491: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:45:58.491: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.021113154s
Sep  8 21:45:58.491: INFO: The phase of Pod pod3 is Running (Ready = true)
Sep  8 21:45:58.491: INFO: Pod "pod3" satisfied condition "running and ready"
Sep  8 21:45:58.502: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-717" to be "running and ready"
Sep  8 21:45:58.515: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.790786ms
Sep  8 21:45:58.515: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:46:00.527: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.024625187s
Sep  8 21:46:00.527: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Sep  8 21:46:00.527: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/08/23 21:46:00.536
Sep  8 21:46:00.536: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.100.16.26 http://127.0.0.1:54323/hostname] Namespace:hostport-717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:46:00.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:46:00.537: INFO: ExecWithOptions: Clientset creation
Sep  8 21:46:00.537: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.100.16.26+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.100.16.26, port: 54323 09/08/23 21:46:00.651
Sep  8 21:46:00.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.100.16.26:54323/hostname] Namespace:hostport-717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:46:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:46:00.652: INFO: ExecWithOptions: Clientset creation
Sep  8 21:46:00.653: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.100.16.26%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.100.16.26, port: 54323 UDP 09/08/23 21:46:00.758
Sep  8 21:46:00.758: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.100.16.26 54323] Namespace:hostport-717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:46:00.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:46:00.759: INFO: ExecWithOptions: Clientset creation
Sep  8 21:46:00.759: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.100.16.26+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:05.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-717" for this suite. 09/08/23 21:46:05.924
------------------------------
â€¢ [SLOW TEST] [19.649 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:45:46.289
    Sep  8 21:45:46.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename hostport 09/08/23 21:45:46.29
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:45:46.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:45:46.326
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 09/08/23 21:45:46.34
    Sep  8 21:45:46.363: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-717" to be "running and ready"
    Sep  8 21:45:46.373: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.716376ms
    Sep  8 21:45:46.373: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:45:48.387: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023739405s
    Sep  8 21:45:48.387: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:45:50.393: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.029906369s
    Sep  8 21:45:50.393: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  8 21:45:50.393: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.100.16.26 on the node which pod1 resides and expect scheduled 09/08/23 21:45:50.393
    Sep  8 21:45:50.418: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-717" to be "running and ready"
    Sep  8 21:45:50.439: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.794295ms
    Sep  8 21:45:50.439: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:45:52.453: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035066367s
    Sep  8 21:45:52.453: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:45:54.448: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.030003249s
    Sep  8 21:45:54.448: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  8 21:45:54.448: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.100.16.26 but use UDP protocol on the node which pod2 resides 09/08/23 21:45:54.448
    Sep  8 21:45:54.470: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-717" to be "running and ready"
    Sep  8 21:45:54.480: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.761217ms
    Sep  8 21:45:54.481: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:45:56.491: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021220432s
    Sep  8 21:45:56.491: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:45:58.491: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.021113154s
    Sep  8 21:45:58.491: INFO: The phase of Pod pod3 is Running (Ready = true)
    Sep  8 21:45:58.491: INFO: Pod "pod3" satisfied condition "running and ready"
    Sep  8 21:45:58.502: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-717" to be "running and ready"
    Sep  8 21:45:58.515: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.790786ms
    Sep  8 21:45:58.515: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:46:00.527: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.024625187s
    Sep  8 21:46:00.527: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Sep  8 21:46:00.527: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 09/08/23 21:46:00.536
    Sep  8 21:46:00.536: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.100.16.26 http://127.0.0.1:54323/hostname] Namespace:hostport-717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:46:00.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:46:00.537: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:46:00.537: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.100.16.26+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.100.16.26, port: 54323 09/08/23 21:46:00.651
    Sep  8 21:46:00.651: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.100.16.26:54323/hostname] Namespace:hostport-717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:46:00.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:46:00.652: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:46:00.653: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.100.16.26%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.100.16.26, port: 54323 UDP 09/08/23 21:46:00.758
    Sep  8 21:46:00.758: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.100.16.26 54323] Namespace:hostport-717 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:46:00.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:46:00.759: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:46:00.759: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-717/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.100.16.26+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:05.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-717" for this suite. 09/08/23 21:46:05.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:05.943
Sep  8 21:46:05.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:46:05.946
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:05.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:05.989
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cebeb556-dca8-4b1c-bbce-f142450d9b5d 09/08/23 21:46:06.005
STEP: Creating the pod 09/08/23 21:46:06.014
Sep  8 21:46:06.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80" in namespace "projected-4830" to be "running and ready"
Sep  8 21:46:06.066: INFO: Pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80": Phase="Pending", Reason="", readiness=false. Elapsed: 9.103201ms
Sep  8 21:46:06.066: INFO: The phase of Pod pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:46:08.081: INFO: Pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80": Phase="Running", Reason="", readiness=true. Elapsed: 2.023925852s
Sep  8 21:46:08.081: INFO: The phase of Pod pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80 is Running (Ready = true)
Sep  8 21:46:08.081: INFO: Pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-cebeb556-dca8-4b1c-bbce-f142450d9b5d 09/08/23 21:46:08.112
STEP: waiting to observe update in volume 09/08/23 21:46:08.125
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:10.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4830" for this suite. 09/08/23 21:46:10.189
------------------------------
â€¢ [4.269 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:05.943
    Sep  8 21:46:05.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:46:05.946
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:05.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:05.989
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-cebeb556-dca8-4b1c-bbce-f142450d9b5d 09/08/23 21:46:06.005
    STEP: Creating the pod 09/08/23 21:46:06.014
    Sep  8 21:46:06.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80" in namespace "projected-4830" to be "running and ready"
    Sep  8 21:46:06.066: INFO: Pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80": Phase="Pending", Reason="", readiness=false. Elapsed: 9.103201ms
    Sep  8 21:46:06.066: INFO: The phase of Pod pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:46:08.081: INFO: Pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80": Phase="Running", Reason="", readiness=true. Elapsed: 2.023925852s
    Sep  8 21:46:08.081: INFO: The phase of Pod pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80 is Running (Ready = true)
    Sep  8 21:46:08.081: INFO: Pod "pod-projected-configmaps-e756fe95-e264-4868-9086-75b3cbf38f80" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-cebeb556-dca8-4b1c-bbce-f142450d9b5d 09/08/23 21:46:08.112
    STEP: waiting to observe update in volume 09/08/23 21:46:08.125
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:10.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4830" for this suite. 09/08/23 21:46:10.189
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:10.211
Sep  8 21:46:10.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename daemonsets 09/08/23 21:46:10.215
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:10.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:10.264
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 09/08/23 21:46:10.319
STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 21:46:10.341
Sep  8 21:46:10.357: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:10.357: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:10.357: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:10.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:46:10.380: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:46:11.422: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:11.422: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:11.422: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:11.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:46:11.462: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 21:46:12.399: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:12.399: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:12.399: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:12.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 21:46:12.410: INFO: Node node-4 is running 0 daemon pod, expected 1
Sep  8 21:46:13.397: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:13.397: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:13.397: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 21:46:13.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 21:46:13.405: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 09/08/23 21:46:13.412
Sep  8 21:46:13.423: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 09/08/23 21:46:13.43
Sep  8 21:46:13.472: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 09/08/23 21:46:13.472
Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: ADDED
Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.478: INFO: Found daemon set daemon-set in namespace daemonsets-9994 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  8 21:46:13.478: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 09/08/23 21:46:13.478
STEP: watching for the daemon set status to be patched 09/08/23 21:46:13.504
Sep  8 21:46:13.514: INFO: Observed &DaemonSet event: ADDED
Sep  8 21:46:13.514: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.515: INFO: Observed daemon set daemon-set in namespace daemonsets-9994 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
Sep  8 21:46:13.515: INFO: Found daemon set daemon-set in namespace daemonsets-9994 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Sep  8 21:46:13.515: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/08/23 21:46:13.524
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9994, will wait for the garbage collector to delete the pods 09/08/23 21:46:13.524
Sep  8 21:46:13.621: INFO: Deleting DaemonSet.extensions daemon-set took: 30.812635ms
Sep  8 21:46:13.722: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.095744ms
Sep  8 21:46:15.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 21:46:15.933: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  8 21:46:15.939: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24283"},"items":null}

Sep  8 21:46:15.945: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24283"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9994" for this suite. 09/08/23 21:46:16.02
------------------------------
â€¢ [SLOW TEST] [5.825 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:10.211
    Sep  8 21:46:10.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename daemonsets 09/08/23 21:46:10.215
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:10.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:10.264
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 09/08/23 21:46:10.319
    STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 21:46:10.341
    Sep  8 21:46:10.357: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:10.357: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:10.357: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:10.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:46:10.380: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:46:11.422: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:11.422: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:11.422: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:11.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:46:11.462: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 21:46:12.399: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:12.399: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:12.399: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:12.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 21:46:12.410: INFO: Node node-4 is running 0 daemon pod, expected 1
    Sep  8 21:46:13.397: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:13.397: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:13.397: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 21:46:13.405: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 21:46:13.405: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 09/08/23 21:46:13.412
    Sep  8 21:46:13.423: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 09/08/23 21:46:13.43
    Sep  8 21:46:13.472: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 09/08/23 21:46:13.472
    Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: ADDED
    Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.477: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.478: INFO: Found daemon set daemon-set in namespace daemonsets-9994 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  8 21:46:13.478: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 09/08/23 21:46:13.478
    STEP: watching for the daemon set status to be patched 09/08/23 21:46:13.504
    Sep  8 21:46:13.514: INFO: Observed &DaemonSet event: ADDED
    Sep  8 21:46:13.514: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.515: INFO: Observed daemon set daemon-set in namespace daemonsets-9994 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Sep  8 21:46:13.515: INFO: Observed &DaemonSet event: MODIFIED
    Sep  8 21:46:13.515: INFO: Found daemon set daemon-set in namespace daemonsets-9994 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Sep  8 21:46:13.515: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/08/23 21:46:13.524
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9994, will wait for the garbage collector to delete the pods 09/08/23 21:46:13.524
    Sep  8 21:46:13.621: INFO: Deleting DaemonSet.extensions daemon-set took: 30.812635ms
    Sep  8 21:46:13.722: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.095744ms
    Sep  8 21:46:15.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 21:46:15.933: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  8 21:46:15.939: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24283"},"items":null}

    Sep  8 21:46:15.945: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24283"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:15.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9994" for this suite. 09/08/23 21:46:16.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:16.051
Sep  8 21:46:16.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename discovery 09/08/23 21:46:16.052
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:16.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:16.095
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 09/08/23 21:46:16.104
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Sep  8 21:46:16.522: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  8 21:46:16.526: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  8 21:46:16.526: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Sep  8 21:46:16.526: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  8 21:46:16.526: INFO: Checking APIGroup: apps
Sep  8 21:46:16.528: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  8 21:46:16.528: INFO: Versions found [{apps/v1 v1}]
Sep  8 21:46:16.528: INFO: apps/v1 matches apps/v1
Sep  8 21:46:16.528: INFO: Checking APIGroup: events.k8s.io
Sep  8 21:46:16.529: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  8 21:46:16.529: INFO: Versions found [{events.k8s.io/v1 v1}]
Sep  8 21:46:16.529: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  8 21:46:16.529: INFO: Checking APIGroup: authentication.k8s.io
Sep  8 21:46:16.532: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  8 21:46:16.532: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Sep  8 21:46:16.532: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  8 21:46:16.532: INFO: Checking APIGroup: authorization.k8s.io
Sep  8 21:46:16.534: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  8 21:46:16.534: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Sep  8 21:46:16.534: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  8 21:46:16.534: INFO: Checking APIGroup: autoscaling
Sep  8 21:46:16.536: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Sep  8 21:46:16.536: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Sep  8 21:46:16.536: INFO: autoscaling/v2 matches autoscaling/v2
Sep  8 21:46:16.536: INFO: Checking APIGroup: batch
Sep  8 21:46:16.537: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  8 21:46:16.537: INFO: Versions found [{batch/v1 v1}]
Sep  8 21:46:16.537: INFO: batch/v1 matches batch/v1
Sep  8 21:46:16.537: INFO: Checking APIGroup: certificates.k8s.io
Sep  8 21:46:16.544: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  8 21:46:16.544: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Sep  8 21:46:16.544: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  8 21:46:16.544: INFO: Checking APIGroup: networking.k8s.io
Sep  8 21:46:16.547: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  8 21:46:16.547: INFO: Versions found [{networking.k8s.io/v1 v1}]
Sep  8 21:46:16.547: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  8 21:46:16.547: INFO: Checking APIGroup: policy
Sep  8 21:46:16.549: INFO: PreferredVersion.GroupVersion: policy/v1
Sep  8 21:46:16.549: INFO: Versions found [{policy/v1 v1}]
Sep  8 21:46:16.549: INFO: policy/v1 matches policy/v1
Sep  8 21:46:16.549: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  8 21:46:16.552: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  8 21:46:16.552: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Sep  8 21:46:16.552: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  8 21:46:16.552: INFO: Checking APIGroup: storage.k8s.io
Sep  8 21:46:16.556: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  8 21:46:16.556: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  8 21:46:16.556: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  8 21:46:16.556: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  8 21:46:16.558: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  8 21:46:16.558: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Sep  8 21:46:16.558: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  8 21:46:16.558: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  8 21:46:16.560: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  8 21:46:16.560: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Sep  8 21:46:16.561: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  8 21:46:16.561: INFO: Checking APIGroup: scheduling.k8s.io
Sep  8 21:46:16.564: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  8 21:46:16.564: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Sep  8 21:46:16.564: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  8 21:46:16.564: INFO: Checking APIGroup: coordination.k8s.io
Sep  8 21:46:16.568: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  8 21:46:16.568: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Sep  8 21:46:16.569: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  8 21:46:16.569: INFO: Checking APIGroup: node.k8s.io
Sep  8 21:46:16.572: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  8 21:46:16.572: INFO: Versions found [{node.k8s.io/v1 v1}]
Sep  8 21:46:16.572: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  8 21:46:16.572: INFO: Checking APIGroup: discovery.k8s.io
Sep  8 21:46:16.577: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep  8 21:46:16.577: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Sep  8 21:46:16.578: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep  8 21:46:16.578: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  8 21:46:16.581: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Sep  8 21:46:16.581: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Sep  8 21:46:16.581: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Sep  8 21:46:16.581: INFO: Checking APIGroup: crd.projectcalico.org
Sep  8 21:46:16.588: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep  8 21:46:16.589: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep  8 21:46:16.589: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep  8 21:46:16.589: INFO: Checking APIGroup: monitoring.coreos.com
Sep  8 21:46:16.590: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Sep  8 21:46:16.590: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Sep  8 21:46:16.591: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Sep  8 21:46:16.591: INFO: Checking APIGroup: metallb.io
Sep  8 21:46:16.592: INFO: PreferredVersion.GroupVersion: metallb.io/v1beta2
Sep  8 21:46:16.592: INFO: Versions found [{metallb.io/v1beta2 v1beta2} {metallb.io/v1beta1 v1beta1} {metallb.io/v1alpha1 v1alpha1}]
Sep  8 21:46:16.593: INFO: metallb.io/v1beta2 matches metallb.io/v1beta2
Sep  8 21:46:16.593: INFO: Checking APIGroup: openebs.io
Sep  8 21:46:16.595: INFO: PreferredVersion.GroupVersion: openebs.io/v1alpha1
Sep  8 21:46:16.595: INFO: Versions found [{openebs.io/v1alpha1 v1alpha1}]
Sep  8 21:46:16.596: INFO: openebs.io/v1alpha1 matches openebs.io/v1alpha1
Sep  8 21:46:16.596: INFO: Checking APIGroup: metrics.k8s.io
Sep  8 21:46:16.602: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Sep  8 21:46:16.603: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Sep  8 21:46:16.603: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:16.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3440" for this suite. 09/08/23 21:46:16.624
------------------------------
â€¢ [0.590 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:16.051
    Sep  8 21:46:16.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename discovery 09/08/23 21:46:16.052
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:16.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:16.095
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 09/08/23 21:46:16.104
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Sep  8 21:46:16.522: INFO: Checking APIGroup: apiregistration.k8s.io
    Sep  8 21:46:16.526: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Sep  8 21:46:16.526: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Sep  8 21:46:16.526: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Sep  8 21:46:16.526: INFO: Checking APIGroup: apps
    Sep  8 21:46:16.528: INFO: PreferredVersion.GroupVersion: apps/v1
    Sep  8 21:46:16.528: INFO: Versions found [{apps/v1 v1}]
    Sep  8 21:46:16.528: INFO: apps/v1 matches apps/v1
    Sep  8 21:46:16.528: INFO: Checking APIGroup: events.k8s.io
    Sep  8 21:46:16.529: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Sep  8 21:46:16.529: INFO: Versions found [{events.k8s.io/v1 v1}]
    Sep  8 21:46:16.529: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Sep  8 21:46:16.529: INFO: Checking APIGroup: authentication.k8s.io
    Sep  8 21:46:16.532: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Sep  8 21:46:16.532: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Sep  8 21:46:16.532: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Sep  8 21:46:16.532: INFO: Checking APIGroup: authorization.k8s.io
    Sep  8 21:46:16.534: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Sep  8 21:46:16.534: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Sep  8 21:46:16.534: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Sep  8 21:46:16.534: INFO: Checking APIGroup: autoscaling
    Sep  8 21:46:16.536: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Sep  8 21:46:16.536: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Sep  8 21:46:16.536: INFO: autoscaling/v2 matches autoscaling/v2
    Sep  8 21:46:16.536: INFO: Checking APIGroup: batch
    Sep  8 21:46:16.537: INFO: PreferredVersion.GroupVersion: batch/v1
    Sep  8 21:46:16.537: INFO: Versions found [{batch/v1 v1}]
    Sep  8 21:46:16.537: INFO: batch/v1 matches batch/v1
    Sep  8 21:46:16.537: INFO: Checking APIGroup: certificates.k8s.io
    Sep  8 21:46:16.544: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Sep  8 21:46:16.544: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Sep  8 21:46:16.544: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Sep  8 21:46:16.544: INFO: Checking APIGroup: networking.k8s.io
    Sep  8 21:46:16.547: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Sep  8 21:46:16.547: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Sep  8 21:46:16.547: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Sep  8 21:46:16.547: INFO: Checking APIGroup: policy
    Sep  8 21:46:16.549: INFO: PreferredVersion.GroupVersion: policy/v1
    Sep  8 21:46:16.549: INFO: Versions found [{policy/v1 v1}]
    Sep  8 21:46:16.549: INFO: policy/v1 matches policy/v1
    Sep  8 21:46:16.549: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Sep  8 21:46:16.552: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Sep  8 21:46:16.552: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Sep  8 21:46:16.552: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Sep  8 21:46:16.552: INFO: Checking APIGroup: storage.k8s.io
    Sep  8 21:46:16.556: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Sep  8 21:46:16.556: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Sep  8 21:46:16.556: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Sep  8 21:46:16.556: INFO: Checking APIGroup: admissionregistration.k8s.io
    Sep  8 21:46:16.558: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Sep  8 21:46:16.558: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Sep  8 21:46:16.558: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Sep  8 21:46:16.558: INFO: Checking APIGroup: apiextensions.k8s.io
    Sep  8 21:46:16.560: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Sep  8 21:46:16.560: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Sep  8 21:46:16.561: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Sep  8 21:46:16.561: INFO: Checking APIGroup: scheduling.k8s.io
    Sep  8 21:46:16.564: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Sep  8 21:46:16.564: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Sep  8 21:46:16.564: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Sep  8 21:46:16.564: INFO: Checking APIGroup: coordination.k8s.io
    Sep  8 21:46:16.568: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Sep  8 21:46:16.568: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Sep  8 21:46:16.569: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Sep  8 21:46:16.569: INFO: Checking APIGroup: node.k8s.io
    Sep  8 21:46:16.572: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Sep  8 21:46:16.572: INFO: Versions found [{node.k8s.io/v1 v1}]
    Sep  8 21:46:16.572: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Sep  8 21:46:16.572: INFO: Checking APIGroup: discovery.k8s.io
    Sep  8 21:46:16.577: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Sep  8 21:46:16.577: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Sep  8 21:46:16.578: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Sep  8 21:46:16.578: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Sep  8 21:46:16.581: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Sep  8 21:46:16.581: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Sep  8 21:46:16.581: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Sep  8 21:46:16.581: INFO: Checking APIGroup: crd.projectcalico.org
    Sep  8 21:46:16.588: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Sep  8 21:46:16.589: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Sep  8 21:46:16.589: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Sep  8 21:46:16.589: INFO: Checking APIGroup: monitoring.coreos.com
    Sep  8 21:46:16.590: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Sep  8 21:46:16.590: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Sep  8 21:46:16.591: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Sep  8 21:46:16.591: INFO: Checking APIGroup: metallb.io
    Sep  8 21:46:16.592: INFO: PreferredVersion.GroupVersion: metallb.io/v1beta2
    Sep  8 21:46:16.592: INFO: Versions found [{metallb.io/v1beta2 v1beta2} {metallb.io/v1beta1 v1beta1} {metallb.io/v1alpha1 v1alpha1}]
    Sep  8 21:46:16.593: INFO: metallb.io/v1beta2 matches metallb.io/v1beta2
    Sep  8 21:46:16.593: INFO: Checking APIGroup: openebs.io
    Sep  8 21:46:16.595: INFO: PreferredVersion.GroupVersion: openebs.io/v1alpha1
    Sep  8 21:46:16.595: INFO: Versions found [{openebs.io/v1alpha1 v1alpha1}]
    Sep  8 21:46:16.596: INFO: openebs.io/v1alpha1 matches openebs.io/v1alpha1
    Sep  8 21:46:16.596: INFO: Checking APIGroup: metrics.k8s.io
    Sep  8 21:46:16.602: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Sep  8 21:46:16.603: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Sep  8 21:46:16.603: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:16.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3440" for this suite. 09/08/23 21:46:16.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:16.641
Sep  8 21:46:16.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 21:46:16.643
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:16.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:16.704
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 09/08/23 21:46:16.71
Sep  8 21:46:16.741: INFO: Waiting up to 5m0s for pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f" in namespace "var-expansion-4257" to be "Succeeded or Failed"
Sep  8 21:46:16.761: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.857478ms
Sep  8 21:46:18.771: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Running", Reason="", readiness=true. Elapsed: 2.029691333s
Sep  8 21:46:20.770: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Running", Reason="", readiness=false. Elapsed: 4.028309376s
Sep  8 21:46:22.775: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033140917s
STEP: Saw pod success 09/08/23 21:46:22.775
Sep  8 21:46:22.775: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f" satisfied condition "Succeeded or Failed"
Sep  8 21:46:22.784: INFO: Trying to get logs from node node-3 pod var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f container dapi-container: <nil>
STEP: delete the pod 09/08/23 21:46:22.803
Sep  8 21:46:22.841: INFO: Waiting for pod var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f to disappear
Sep  8 21:46:22.852: INFO: Pod var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:22.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4257" for this suite. 09/08/23 21:46:22.868
------------------------------
â€¢ [SLOW TEST] [6.248 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:16.641
    Sep  8 21:46:16.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 21:46:16.643
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:16.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:16.704
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 09/08/23 21:46:16.71
    Sep  8 21:46:16.741: INFO: Waiting up to 5m0s for pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f" in namespace "var-expansion-4257" to be "Succeeded or Failed"
    Sep  8 21:46:16.761: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.857478ms
    Sep  8 21:46:18.771: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Running", Reason="", readiness=true. Elapsed: 2.029691333s
    Sep  8 21:46:20.770: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Running", Reason="", readiness=false. Elapsed: 4.028309376s
    Sep  8 21:46:22.775: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033140917s
    STEP: Saw pod success 09/08/23 21:46:22.775
    Sep  8 21:46:22.775: INFO: Pod "var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f" satisfied condition "Succeeded or Failed"
    Sep  8 21:46:22.784: INFO: Trying to get logs from node node-3 pod var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f container dapi-container: <nil>
    STEP: delete the pod 09/08/23 21:46:22.803
    Sep  8 21:46:22.841: INFO: Waiting for pod var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f to disappear
    Sep  8 21:46:22.852: INFO: Pod var-expansion-0319aaab-7f37-4d2b-b496-d70748e4230f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:22.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4257" for this suite. 09/08/23 21:46:22.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:22.89
Sep  8 21:46:22.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:46:22.891
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:22.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:22.937
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-6117 09/08/23 21:46:22.946
STEP: creating service affinity-nodeport in namespace services-6117 09/08/23 21:46:22.946
STEP: creating replication controller affinity-nodeport in namespace services-6117 09/08/23 21:46:22.991
I0908 21:46:23.024353      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6117, replica count: 3
I0908 21:46:26.077554      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 21:46:26.110: INFO: Creating new exec pod
Sep  8 21:46:26.129: INFO: Waiting up to 5m0s for pod "execpod-affinitymtt4q" in namespace "services-6117" to be "running"
Sep  8 21:46:26.138: INFO: Pod "execpod-affinitymtt4q": Phase="Pending", Reason="", readiness=false. Elapsed: 9.893989ms
Sep  8 21:46:28.148: INFO: Pod "execpod-affinitymtt4q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019837943s
Sep  8 21:46:30.161: INFO: Pod "execpod-affinitymtt4q": Phase="Running", Reason="", readiness=true. Elapsed: 4.032696728s
Sep  8 21:46:30.161: INFO: Pod "execpod-affinitymtt4q" satisfied condition "running"
Sep  8 21:46:31.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Sep  8 21:46:31.497: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep  8 21:46:31.497: INFO: stdout: ""
Sep  8 21:46:31.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 10.233.37.218 80'
Sep  8 21:46:31.741: INFO: stderr: "+ nc -v -z -w 2 10.233.37.218 80\nConnection to 10.233.37.218 80 port [tcp/http] succeeded!\n"
Sep  8 21:46:31.741: INFO: stdout: ""
Sep  8 21:46:31.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 30863'
Sep  8 21:46:32.015: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 30863\nConnection to 10.100.19.129 30863 port [tcp/*] succeeded!\n"
Sep  8 21:46:32.015: INFO: stdout: ""
Sep  8 21:46:32.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 30863'
Sep  8 21:46:32.275: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 30863\nConnection to 10.100.16.26 30863 port [tcp/*] succeeded!\n"
Sep  8 21:46:32.275: INFO: stdout: ""
Sep  8 21:46:32.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.19.129:30863/ ; done'
Sep  8 21:46:32.628: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n"
Sep  8 21:46:32.628: INFO: stdout: "\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt"
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
Sep  8 21:46:32.628: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6117, will wait for the garbage collector to delete the pods 09/08/23 21:46:32.657
Sep  8 21:46:32.746: INFO: Deleting ReplicationController affinity-nodeport took: 27.244464ms
Sep  8 21:46:32.847: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.734937ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:35.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6117" for this suite. 09/08/23 21:46:35.695
------------------------------
â€¢ [SLOW TEST] [12.833 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:22.89
    Sep  8 21:46:22.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:46:22.891
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:22.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:22.937
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-6117 09/08/23 21:46:22.946
    STEP: creating service affinity-nodeport in namespace services-6117 09/08/23 21:46:22.946
    STEP: creating replication controller affinity-nodeport in namespace services-6117 09/08/23 21:46:22.991
    I0908 21:46:23.024353      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6117, replica count: 3
    I0908 21:46:26.077554      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 21:46:26.110: INFO: Creating new exec pod
    Sep  8 21:46:26.129: INFO: Waiting up to 5m0s for pod "execpod-affinitymtt4q" in namespace "services-6117" to be "running"
    Sep  8 21:46:26.138: INFO: Pod "execpod-affinitymtt4q": Phase="Pending", Reason="", readiness=false. Elapsed: 9.893989ms
    Sep  8 21:46:28.148: INFO: Pod "execpod-affinitymtt4q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019837943s
    Sep  8 21:46:30.161: INFO: Pod "execpod-affinitymtt4q": Phase="Running", Reason="", readiness=true. Elapsed: 4.032696728s
    Sep  8 21:46:30.161: INFO: Pod "execpod-affinitymtt4q" satisfied condition "running"
    Sep  8 21:46:31.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Sep  8 21:46:31.497: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Sep  8 21:46:31.497: INFO: stdout: ""
    Sep  8 21:46:31.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 10.233.37.218 80'
    Sep  8 21:46:31.741: INFO: stderr: "+ nc -v -z -w 2 10.233.37.218 80\nConnection to 10.233.37.218 80 port [tcp/http] succeeded!\n"
    Sep  8 21:46:31.741: INFO: stdout: ""
    Sep  8 21:46:31.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 30863'
    Sep  8 21:46:32.015: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 30863\nConnection to 10.100.19.129 30863 port [tcp/*] succeeded!\n"
    Sep  8 21:46:32.015: INFO: stdout: ""
    Sep  8 21:46:32.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 30863'
    Sep  8 21:46:32.275: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 30863\nConnection to 10.100.16.26 30863 port [tcp/*] succeeded!\n"
    Sep  8 21:46:32.275: INFO: stdout: ""
    Sep  8 21:46:32.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6117 exec execpod-affinitymtt4q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.19.129:30863/ ; done'
    Sep  8 21:46:32.628: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:30863/\n"
    Sep  8 21:46:32.628: INFO: stdout: "\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt\naffinity-nodeport-lqvzt"
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Received response from host: affinity-nodeport-lqvzt
    Sep  8 21:46:32.628: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6117, will wait for the garbage collector to delete the pods 09/08/23 21:46:32.657
    Sep  8 21:46:32.746: INFO: Deleting ReplicationController affinity-nodeport took: 27.244464ms
    Sep  8 21:46:32.847: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.734937ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:35.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6117" for this suite. 09/08/23 21:46:35.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:35.726
Sep  8 21:46:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:46:35.728
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:35.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:35.792
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:46:35.841
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:46:36.245
STEP: Deploying the webhook pod 09/08/23 21:46:36.263
STEP: Wait for the deployment to be ready 09/08/23 21:46:36.297
Sep  8 21:46:36.326: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 21:46:38.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 21:46:40.377
STEP: Verifying the service has paired with the endpoint 09/08/23 21:46:40.405
Sep  8 21:46:41.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/08/23 21:46:41.413
STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:41.413
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/08/23 21:46:41.453
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/08/23 21:46:42.483
STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:42.483
STEP: Having no error when timeout is longer than webhook latency 09/08/23 21:46:43.567
STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:43.567
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/08/23 21:46:48.65
STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:48.65
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:53.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8" for this suite. 09/08/23 21:46:53.925
STEP: Destroying namespace "webhook-8-markers" for this suite. 09/08/23 21:46:53.947
------------------------------
â€¢ [SLOW TEST] [18.265 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:35.726
    Sep  8 21:46:35.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:46:35.728
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:35.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:35.792
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:46:35.841
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:46:36.245
    STEP: Deploying the webhook pod 09/08/23 21:46:36.263
    STEP: Wait for the deployment to be ready 09/08/23 21:46:36.297
    Sep  8 21:46:36.326: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 21:46:38.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 46, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 21:46:40.377
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:46:40.405
    Sep  8 21:46:41.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 09/08/23 21:46:41.413
    STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:41.413
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 09/08/23 21:46:41.453
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 09/08/23 21:46:42.483
    STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:42.483
    STEP: Having no error when timeout is longer than webhook latency 09/08/23 21:46:43.567
    STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:43.567
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 09/08/23 21:46:48.65
    STEP: Registering slow webhook via the AdmissionRegistration API 09/08/23 21:46:48.65
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:53.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8" for this suite. 09/08/23 21:46:53.925
    STEP: Destroying namespace "webhook-8-markers" for this suite. 09/08/23 21:46:53.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:53.992
Sep  8 21:46:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename init-container 09/08/23 21:46:53.994
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:54.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:54.096
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 09/08/23 21:46:54.104
Sep  8 21:46:54.105: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:46:58.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6998" for this suite. 09/08/23 21:46:58.489
------------------------------
â€¢ [4.517 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:53.992
    Sep  8 21:46:53.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename init-container 09/08/23 21:46:53.994
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:54.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:54.096
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 09/08/23 21:46:54.104
    Sep  8 21:46:54.105: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:46:58.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6998" for this suite. 09/08/23 21:46:58.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:46:58.51
Sep  8 21:46:58.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:46:58.512
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:58.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:58.557
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1401/configmap-test-841fa292-6e25-41d5-88dd-f5e8abc86604 09/08/23 21:46:58.563
STEP: Creating a pod to test consume configMaps 09/08/23 21:46:58.577
Sep  8 21:46:58.597: INFO: Waiting up to 5m0s for pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd" in namespace "configmap-1401" to be "Succeeded or Failed"
Sep  8 21:46:58.609: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.810956ms
Sep  8 21:47:00.623: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026495339s
Sep  8 21:47:02.620: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02288338s
STEP: Saw pod success 09/08/23 21:47:02.62
Sep  8 21:47:02.620: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd" satisfied condition "Succeeded or Failed"
Sep  8 21:47:02.635: INFO: Trying to get logs from node node-3 pod pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd container env-test: <nil>
STEP: delete the pod 09/08/23 21:47:02.653
Sep  8 21:47:02.701: INFO: Waiting for pod pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd to disappear
Sep  8 21:47:02.717: INFO: Pod pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:02.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1401" for this suite. 09/08/23 21:47:02.732
------------------------------
â€¢ [4.254 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:46:58.51
    Sep  8 21:46:58.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:46:58.512
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:46:58.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:46:58.557
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1401/configmap-test-841fa292-6e25-41d5-88dd-f5e8abc86604 09/08/23 21:46:58.563
    STEP: Creating a pod to test consume configMaps 09/08/23 21:46:58.577
    Sep  8 21:46:58.597: INFO: Waiting up to 5m0s for pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd" in namespace "configmap-1401" to be "Succeeded or Failed"
    Sep  8 21:46:58.609: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.810956ms
    Sep  8 21:47:00.623: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026495339s
    Sep  8 21:47:02.620: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02288338s
    STEP: Saw pod success 09/08/23 21:47:02.62
    Sep  8 21:47:02.620: INFO: Pod "pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd" satisfied condition "Succeeded or Failed"
    Sep  8 21:47:02.635: INFO: Trying to get logs from node node-3 pod pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd container env-test: <nil>
    STEP: delete the pod 09/08/23 21:47:02.653
    Sep  8 21:47:02.701: INFO: Waiting for pod pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd to disappear
    Sep  8 21:47:02.717: INFO: Pod pod-configmaps-a081524b-82b3-4aa3-80fa-f4e02c0accbd no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:02.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1401" for this suite. 09/08/23 21:47:02.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:02.768
Sep  8 21:47:02.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:47:02.769
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:02.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:02.81
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-15f16695-7320-4a69-9fbf-b41c6c71be4f 09/08/23 21:47:02.821
STEP: Creating a pod to test consume configMaps 09/08/23 21:47:02.835
Sep  8 21:47:02.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0" in namespace "configmap-4580" to be "Succeeded or Failed"
Sep  8 21:47:02.890: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.532058ms
Sep  8 21:47:04.902: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034099451s
Sep  8 21:47:06.901: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033776641s
STEP: Saw pod success 09/08/23 21:47:06.901
Sep  8 21:47:06.901: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0" satisfied condition "Succeeded or Failed"
Sep  8 21:47:06.910: INFO: Trying to get logs from node node-3 pod pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0 container configmap-volume-test: <nil>
STEP: delete the pod 09/08/23 21:47:06.935
Sep  8 21:47:06.962: INFO: Waiting for pod pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0 to disappear
Sep  8 21:47:06.969: INFO: Pod pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:06.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4580" for this suite. 09/08/23 21:47:06.982
------------------------------
â€¢ [4.236 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:02.768
    Sep  8 21:47:02.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:47:02.769
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:02.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:02.81
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-15f16695-7320-4a69-9fbf-b41c6c71be4f 09/08/23 21:47:02.821
    STEP: Creating a pod to test consume configMaps 09/08/23 21:47:02.835
    Sep  8 21:47:02.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0" in namespace "configmap-4580" to be "Succeeded or Failed"
    Sep  8 21:47:02.890: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.532058ms
    Sep  8 21:47:04.902: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034099451s
    Sep  8 21:47:06.901: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033776641s
    STEP: Saw pod success 09/08/23 21:47:06.901
    Sep  8 21:47:06.901: INFO: Pod "pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0" satisfied condition "Succeeded or Failed"
    Sep  8 21:47:06.910: INFO: Trying to get logs from node node-3 pod pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0 container configmap-volume-test: <nil>
    STEP: delete the pod 09/08/23 21:47:06.935
    Sep  8 21:47:06.962: INFO: Waiting for pod pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0 to disappear
    Sep  8 21:47:06.969: INFO: Pod pod-configmaps-0772ff1b-8c16-4473-84fb-6e6b77357fe0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:06.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4580" for this suite. 09/08/23 21:47:06.982
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:07.007
Sep  8 21:47:07.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 21:47:07.014
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:07.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:07.076
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Sep  8 21:47:07.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3087 version'
Sep  8 21:47:07.191: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Sep  8 21:47:07.191: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:08:49Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:07.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3087" for this suite. 09/08/23 21:47:07.202
------------------------------
â€¢ [0.225 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:07.007
    Sep  8 21:47:07.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 21:47:07.014
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:07.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:07.076
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Sep  8 21:47:07.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3087 version'
    Sep  8 21:47:07.191: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Sep  8 21:47:07.191: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:08:49Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:07.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3087" for this suite. 09/08/23 21:47:07.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:07.233
Sep  8 21:47:07.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 21:47:07.234
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:07.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:07.279
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8235 09/08/23 21:47:07.286
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-8235 09/08/23 21:47:07.327
Sep  8 21:47:07.352: INFO: Found 0 stateful pods, waiting for 1
Sep  8 21:47:17.365: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 09/08/23 21:47:17.383
STEP: Getting /status 09/08/23 21:47:17.406
Sep  8 21:47:17.416: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 09/08/23 21:47:17.416
Sep  8 21:47:17.444: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 09/08/23 21:47:17.444
Sep  8 21:47:17.448: INFO: Observed &StatefulSet event: ADDED
Sep  8 21:47:17.448: INFO: Found Statefulset ss in namespace statefulset-8235 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  8 21:47:17.448: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 09/08/23 21:47:17.448
Sep  8 21:47:17.448: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  8 21:47:17.464: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 09/08/23 21:47:17.464
Sep  8 21:47:17.470: INFO: Observed &StatefulSet event: ADDED
Sep  8 21:47:17.470: INFO: Observed Statefulset ss in namespace statefulset-8235 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  8 21:47:17.470: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 21:47:17.470: INFO: Deleting all statefulset in ns statefulset-8235
Sep  8 21:47:17.481: INFO: Scaling statefulset ss to 0
Sep  8 21:47:27.534: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 21:47:27.542: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:27.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8235" for this suite. 09/08/23 21:47:27.615
------------------------------
â€¢ [SLOW TEST] [20.403 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:07.233
    Sep  8 21:47:07.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 21:47:07.234
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:07.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:07.279
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8235 09/08/23 21:47:07.286
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-8235 09/08/23 21:47:07.327
    Sep  8 21:47:07.352: INFO: Found 0 stateful pods, waiting for 1
    Sep  8 21:47:17.365: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 09/08/23 21:47:17.383
    STEP: Getting /status 09/08/23 21:47:17.406
    Sep  8 21:47:17.416: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 09/08/23 21:47:17.416
    Sep  8 21:47:17.444: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 09/08/23 21:47:17.444
    Sep  8 21:47:17.448: INFO: Observed &StatefulSet event: ADDED
    Sep  8 21:47:17.448: INFO: Found Statefulset ss in namespace statefulset-8235 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  8 21:47:17.448: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 09/08/23 21:47:17.448
    Sep  8 21:47:17.448: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  8 21:47:17.464: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 09/08/23 21:47:17.464
    Sep  8 21:47:17.470: INFO: Observed &StatefulSet event: ADDED
    Sep  8 21:47:17.470: INFO: Observed Statefulset ss in namespace statefulset-8235 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  8 21:47:17.470: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 21:47:17.470: INFO: Deleting all statefulset in ns statefulset-8235
    Sep  8 21:47:17.481: INFO: Scaling statefulset ss to 0
    Sep  8 21:47:27.534: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 21:47:27.542: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:27.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8235" for this suite. 09/08/23 21:47:27.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:27.637
Sep  8 21:47:27.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 21:47:27.639
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:27.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:27.686
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 09/08/23 21:47:27.701
Sep  8 21:47:27.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 create -f -'
Sep  8 21:47:29.320: INFO: stderr: ""
Sep  8 21:47:29.320: INFO: stdout: "pod/pause created\n"
Sep  8 21:47:29.320: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  8 21:47:29.320: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9356" to be "running and ready"
Sep  8 21:47:29.327: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.375958ms
Sep  8 21:47:29.327: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'node-3' to be 'Running' but was 'Pending'
Sep  8 21:47:31.346: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.026552305s
Sep  8 21:47:31.346: INFO: Pod "pause" satisfied condition "running and ready"
Sep  8 21:47:31.346: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 09/08/23 21:47:31.346
Sep  8 21:47:31.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 label pods pause testing-label=testing-label-value'
Sep  8 21:47:31.490: INFO: stderr: ""
Sep  8 21:47:31.491: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 09/08/23 21:47:31.491
Sep  8 21:47:31.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get pod pause -L testing-label'
Sep  8 21:47:31.589: INFO: stderr: ""
Sep  8 21:47:31.589: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 09/08/23 21:47:31.589
Sep  8 21:47:31.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 label pods pause testing-label-'
Sep  8 21:47:31.729: INFO: stderr: ""
Sep  8 21:47:31.729: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 09/08/23 21:47:31.729
Sep  8 21:47:31.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get pod pause -L testing-label'
Sep  8 21:47:31.843: INFO: stderr: ""
Sep  8 21:47:31.843: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 09/08/23 21:47:31.843
Sep  8 21:47:31.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 delete --grace-period=0 --force -f -'
Sep  8 21:47:32.012: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 21:47:32.012: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  8 21:47:32.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get rc,svc -l name=pause --no-headers'
Sep  8 21:47:32.183: INFO: stderr: "No resources found in kubectl-9356 namespace.\n"
Sep  8 21:47:32.184: INFO: stdout: ""
Sep  8 21:47:32.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  8 21:47:32.335: INFO: stderr: ""
Sep  8 21:47:32.335: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:32.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9356" for this suite. 09/08/23 21:47:32.346
------------------------------
â€¢ [4.727 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:27.637
    Sep  8 21:47:27.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 21:47:27.639
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:27.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:27.686
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 09/08/23 21:47:27.701
    Sep  8 21:47:27.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 create -f -'
    Sep  8 21:47:29.320: INFO: stderr: ""
    Sep  8 21:47:29.320: INFO: stdout: "pod/pause created\n"
    Sep  8 21:47:29.320: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Sep  8 21:47:29.320: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9356" to be "running and ready"
    Sep  8 21:47:29.327: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.375958ms
    Sep  8 21:47:29.327: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'node-3' to be 'Running' but was 'Pending'
    Sep  8 21:47:31.346: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.026552305s
    Sep  8 21:47:31.346: INFO: Pod "pause" satisfied condition "running and ready"
    Sep  8 21:47:31.346: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 09/08/23 21:47:31.346
    Sep  8 21:47:31.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 label pods pause testing-label=testing-label-value'
    Sep  8 21:47:31.490: INFO: stderr: ""
    Sep  8 21:47:31.491: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 09/08/23 21:47:31.491
    Sep  8 21:47:31.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get pod pause -L testing-label'
    Sep  8 21:47:31.589: INFO: stderr: ""
    Sep  8 21:47:31.589: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 09/08/23 21:47:31.589
    Sep  8 21:47:31.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 label pods pause testing-label-'
    Sep  8 21:47:31.729: INFO: stderr: ""
    Sep  8 21:47:31.729: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 09/08/23 21:47:31.729
    Sep  8 21:47:31.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get pod pause -L testing-label'
    Sep  8 21:47:31.843: INFO: stderr: ""
    Sep  8 21:47:31.843: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 09/08/23 21:47:31.843
    Sep  8 21:47:31.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 delete --grace-period=0 --force -f -'
    Sep  8 21:47:32.012: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 21:47:32.012: INFO: stdout: "pod \"pause\" force deleted\n"
    Sep  8 21:47:32.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get rc,svc -l name=pause --no-headers'
    Sep  8 21:47:32.183: INFO: stderr: "No resources found in kubectl-9356 namespace.\n"
    Sep  8 21:47:32.184: INFO: stdout: ""
    Sep  8 21:47:32.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-9356 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  8 21:47:32.335: INFO: stderr: ""
    Sep  8 21:47:32.335: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:32.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9356" for this suite. 09/08/23 21:47:32.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:32.369
Sep  8 21:47:32.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:47:32.371
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:32.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:32.423
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-a137fa6b-5f3e-4ba9-8449-7dd09150a26f 09/08/23 21:47:32.436
STEP: Creating a pod to test consume configMaps 09/08/23 21:47:32.447
Sep  8 21:47:32.474: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac" in namespace "configmap-634" to be "Succeeded or Failed"
Sep  8 21:47:32.498: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Pending", Reason="", readiness=false. Elapsed: 24.096649ms
Sep  8 21:47:34.512: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.037780033s
Sep  8 21:47:36.511: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Running", Reason="", readiness=false. Elapsed: 4.037034575s
Sep  8 21:47:38.507: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032303127s
STEP: Saw pod success 09/08/23 21:47:38.507
Sep  8 21:47:38.507: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac" satisfied condition "Succeeded or Failed"
Sep  8 21:47:38.513: INFO: Trying to get logs from node node-3 pod pod-configmaps-e5c57845-f479-472c-805c-1646078236ac container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:47:38.532
Sep  8 21:47:38.572: INFO: Waiting for pod pod-configmaps-e5c57845-f479-472c-805c-1646078236ac to disappear
Sep  8 21:47:38.583: INFO: Pod pod-configmaps-e5c57845-f479-472c-805c-1646078236ac no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:38.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-634" for this suite. 09/08/23 21:47:38.596
------------------------------
â€¢ [SLOW TEST] [6.249 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:32.369
    Sep  8 21:47:32.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:47:32.371
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:32.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:32.423
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-a137fa6b-5f3e-4ba9-8449-7dd09150a26f 09/08/23 21:47:32.436
    STEP: Creating a pod to test consume configMaps 09/08/23 21:47:32.447
    Sep  8 21:47:32.474: INFO: Waiting up to 5m0s for pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac" in namespace "configmap-634" to be "Succeeded or Failed"
    Sep  8 21:47:32.498: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Pending", Reason="", readiness=false. Elapsed: 24.096649ms
    Sep  8 21:47:34.512: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Running", Reason="", readiness=true. Elapsed: 2.037780033s
    Sep  8 21:47:36.511: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Running", Reason="", readiness=false. Elapsed: 4.037034575s
    Sep  8 21:47:38.507: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032303127s
    STEP: Saw pod success 09/08/23 21:47:38.507
    Sep  8 21:47:38.507: INFO: Pod "pod-configmaps-e5c57845-f479-472c-805c-1646078236ac" satisfied condition "Succeeded or Failed"
    Sep  8 21:47:38.513: INFO: Trying to get logs from node node-3 pod pod-configmaps-e5c57845-f479-472c-805c-1646078236ac container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:47:38.532
    Sep  8 21:47:38.572: INFO: Waiting for pod pod-configmaps-e5c57845-f479-472c-805c-1646078236ac to disappear
    Sep  8 21:47:38.583: INFO: Pod pod-configmaps-e5c57845-f479-472c-805c-1646078236ac no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:38.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-634" for this suite. 09/08/23 21:47:38.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:38.625
Sep  8 21:47:38.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:47:38.626
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:38.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:38.665
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7061 09/08/23 21:47:38.669
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/08/23 21:47:38.73
STEP: creating service externalsvc in namespace services-7061 09/08/23 21:47:38.73
STEP: creating replication controller externalsvc in namespace services-7061 09/08/23 21:47:38.784
I0908 21:47:38.805266      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7061, replica count: 2
I0908 21:47:41.856548      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 09/08/23 21:47:41.865
Sep  8 21:47:41.935: INFO: Creating new exec pod
Sep  8 21:47:41.962: INFO: Waiting up to 5m0s for pod "execpodbvv4x" in namespace "services-7061" to be "running"
Sep  8 21:47:41.997: INFO: Pod "execpodbvv4x": Phase="Pending", Reason="", readiness=false. Elapsed: 34.793866ms
Sep  8 21:47:44.013: INFO: Pod "execpodbvv4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.051011741s
Sep  8 21:47:44.013: INFO: Pod "execpodbvv4x" satisfied condition "running"
Sep  8 21:47:44.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7061 exec execpodbvv4x -- /bin/sh -x -c nslookup nodeport-service.services-7061.svc.cluster.local'
Sep  8 21:47:44.298: INFO: stderr: "+ nslookup nodeport-service.services-7061.svc.cluster.local\n"
Sep  8 21:47:44.298: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-7061.svc.cluster.local\tcanonical name = externalsvc.services-7061.svc.cluster.local.\nName:\texternalsvc.services-7061.svc.cluster.local\nAddress: 10.233.44.208\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7061, will wait for the garbage collector to delete the pods 09/08/23 21:47:44.298
Sep  8 21:47:44.381: INFO: Deleting ReplicationController externalsvc took: 20.156113ms
Sep  8 21:47:44.481: INFO: Terminating ReplicationController externalsvc pods took: 100.2316ms
Sep  8 21:47:47.019: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:47.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7061" for this suite. 09/08/23 21:47:47.137
------------------------------
â€¢ [SLOW TEST] [8.532 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:38.625
    Sep  8 21:47:38.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:47:38.626
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:38.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:38.665
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-7061 09/08/23 21:47:38.669
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/08/23 21:47:38.73
    STEP: creating service externalsvc in namespace services-7061 09/08/23 21:47:38.73
    STEP: creating replication controller externalsvc in namespace services-7061 09/08/23 21:47:38.784
    I0908 21:47:38.805266      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7061, replica count: 2
    I0908 21:47:41.856548      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 09/08/23 21:47:41.865
    Sep  8 21:47:41.935: INFO: Creating new exec pod
    Sep  8 21:47:41.962: INFO: Waiting up to 5m0s for pod "execpodbvv4x" in namespace "services-7061" to be "running"
    Sep  8 21:47:41.997: INFO: Pod "execpodbvv4x": Phase="Pending", Reason="", readiness=false. Elapsed: 34.793866ms
    Sep  8 21:47:44.013: INFO: Pod "execpodbvv4x": Phase="Running", Reason="", readiness=true. Elapsed: 2.051011741s
    Sep  8 21:47:44.013: INFO: Pod "execpodbvv4x" satisfied condition "running"
    Sep  8 21:47:44.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7061 exec execpodbvv4x -- /bin/sh -x -c nslookup nodeport-service.services-7061.svc.cluster.local'
    Sep  8 21:47:44.298: INFO: stderr: "+ nslookup nodeport-service.services-7061.svc.cluster.local\n"
    Sep  8 21:47:44.298: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-7061.svc.cluster.local\tcanonical name = externalsvc.services-7061.svc.cluster.local.\nName:\texternalsvc.services-7061.svc.cluster.local\nAddress: 10.233.44.208\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7061, will wait for the garbage collector to delete the pods 09/08/23 21:47:44.298
    Sep  8 21:47:44.381: INFO: Deleting ReplicationController externalsvc took: 20.156113ms
    Sep  8 21:47:44.481: INFO: Terminating ReplicationController externalsvc pods took: 100.2316ms
    Sep  8 21:47:47.019: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:47.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7061" for this suite. 09/08/23 21:47:47.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:47.165
Sep  8 21:47:47.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename podtemplate 09/08/23 21:47:47.167
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:47.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:47.259
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 09/08/23 21:47:47.269
Sep  8 21:47:47.283: INFO: created test-podtemplate-1
Sep  8 21:47:47.301: INFO: created test-podtemplate-2
Sep  8 21:47:47.315: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 09/08/23 21:47:47.315
STEP: delete collection of pod templates 09/08/23 21:47:47.329
Sep  8 21:47:47.329: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 09/08/23 21:47:47.385
Sep  8 21:47:47.386: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:47.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5321" for this suite. 09/08/23 21:47:47.404
------------------------------
â€¢ [0.261 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:47.165
    Sep  8 21:47:47.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename podtemplate 09/08/23 21:47:47.167
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:47.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:47.259
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 09/08/23 21:47:47.269
    Sep  8 21:47:47.283: INFO: created test-podtemplate-1
    Sep  8 21:47:47.301: INFO: created test-podtemplate-2
    Sep  8 21:47:47.315: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 09/08/23 21:47:47.315
    STEP: delete collection of pod templates 09/08/23 21:47:47.329
    Sep  8 21:47:47.329: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 09/08/23 21:47:47.385
    Sep  8 21:47:47.386: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:47.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5321" for this suite. 09/08/23 21:47:47.404
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:47.43
Sep  8 21:47:47.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename job 09/08/23 21:47:47.431
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:47.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:47.467
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 09/08/23 21:47:47.484
STEP: Patching the Job 09/08/23 21:47:47.505
STEP: Watching for Job to be patched 09/08/23 21:47:47.551
Sep  8 21:47:47.555: INFO: Event ADDED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  8 21:47:47.555: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking:]
Sep  8 21:47:47.555: INFO: Event MODIFIED found for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 09/08/23 21:47:47.555
STEP: Watching for Job to be updated 09/08/23 21:47:47.579
Sep  8 21:47:47.586: INFO: Event MODIFIED found for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:47.586: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 09/08/23 21:47:47.586
Sep  8 21:47:47.618: INFO: Job: e2e-58kdz as labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz]
STEP: Waiting for job to complete 09/08/23 21:47:47.618
STEP: Delete a job collection with a labelselector 09/08/23 21:47:57.63
STEP: Watching for Job to be deleted 09/08/23 21:47:57.649
Sep  8 21:47:57.652: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:57.652: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:57.652: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Sep  8 21:47:57.654: INFO: Event DELETED found for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 09/08/23 21:47:57.654
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  8 21:47:57.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9354" for this suite. 09/08/23 21:47:57.676
------------------------------
â€¢ [SLOW TEST] [10.281 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:47.43
    Sep  8 21:47:47.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename job 09/08/23 21:47:47.431
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:47.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:47.467
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 09/08/23 21:47:47.484
    STEP: Patching the Job 09/08/23 21:47:47.505
    STEP: Watching for Job to be patched 09/08/23 21:47:47.551
    Sep  8 21:47:47.555: INFO: Event ADDED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  8 21:47:47.555: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking:]
    Sep  8 21:47:47.555: INFO: Event MODIFIED found for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 09/08/23 21:47:47.555
    STEP: Watching for Job to be updated 09/08/23 21:47:47.579
    Sep  8 21:47:47.586: INFO: Event MODIFIED found for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:47.586: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 09/08/23 21:47:47.586
    Sep  8 21:47:47.618: INFO: Job: e2e-58kdz as labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz]
    STEP: Waiting for job to complete 09/08/23 21:47:47.618
    STEP: Delete a job collection with a labelselector 09/08/23 21:47:57.63
    STEP: Watching for Job to be deleted 09/08/23 21:47:57.649
    Sep  8 21:47:57.652: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:57.652: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:57.652: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:57.653: INFO: Event MODIFIED observed for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Sep  8 21:47:57.654: INFO: Event DELETED found for Job e2e-58kdz in namespace job-9354 with labels: map[e2e-58kdz:patched e2e-job-label:e2e-58kdz] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 09/08/23 21:47:57.654
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:47:57.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9354" for this suite. 09/08/23 21:47:57.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:47:57.714
Sep  8 21:47:57.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:47:57.715
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:57.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:57.765
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 09/08/23 21:47:57.771
STEP: Counting existing ResourceQuota 09/08/23 21:48:02.796
STEP: Creating a ResourceQuota 09/08/23 21:48:07.806
STEP: Ensuring resource quota status is calculated 09/08/23 21:48:07.818
STEP: Creating a Secret 09/08/23 21:48:09.833
STEP: Ensuring resource quota status captures secret creation 09/08/23 21:48:09.871
STEP: Deleting a secret 09/08/23 21:48:11.889
STEP: Ensuring resource quota status released usage 09/08/23 21:48:11.916
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:13.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9626" for this suite. 09/08/23 21:48:13.95
------------------------------
â€¢ [SLOW TEST] [16.250 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:47:57.714
    Sep  8 21:47:57.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:47:57.715
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:47:57.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:47:57.765
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 09/08/23 21:47:57.771
    STEP: Counting existing ResourceQuota 09/08/23 21:48:02.796
    STEP: Creating a ResourceQuota 09/08/23 21:48:07.806
    STEP: Ensuring resource quota status is calculated 09/08/23 21:48:07.818
    STEP: Creating a Secret 09/08/23 21:48:09.833
    STEP: Ensuring resource quota status captures secret creation 09/08/23 21:48:09.871
    STEP: Deleting a secret 09/08/23 21:48:11.889
    STEP: Ensuring resource quota status released usage 09/08/23 21:48:11.916
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:13.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9626" for this suite. 09/08/23 21:48:13.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:13.973
Sep  8 21:48:13.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:48:13.974
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:14.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:14.021
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 09/08/23 21:48:14.027
Sep  8 21:48:14.055: INFO: Waiting up to 5m0s for pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3" in namespace "downward-api-2314" to be "running and ready"
Sep  8 21:48:14.069: INFO: Pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.326191ms
Sep  8 21:48:14.070: INFO: The phase of Pod labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:48:16.078: INFO: Pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.023305775s
Sep  8 21:48:16.078: INFO: The phase of Pod labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3 is Running (Ready = true)
Sep  8 21:48:16.078: INFO: Pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3" satisfied condition "running and ready"
Sep  8 21:48:16.654: INFO: Successfully updated pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:18.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2314" for this suite. 09/08/23 21:48:18.735
------------------------------
â€¢ [4.789 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:13.973
    Sep  8 21:48:13.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:48:13.974
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:14.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:14.021
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 09/08/23 21:48:14.027
    Sep  8 21:48:14.055: INFO: Waiting up to 5m0s for pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3" in namespace "downward-api-2314" to be "running and ready"
    Sep  8 21:48:14.069: INFO: Pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.326191ms
    Sep  8 21:48:14.070: INFO: The phase of Pod labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:48:16.078: INFO: Pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.023305775s
    Sep  8 21:48:16.078: INFO: The phase of Pod labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3 is Running (Ready = true)
    Sep  8 21:48:16.078: INFO: Pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3" satisfied condition "running and ready"
    Sep  8 21:48:16.654: INFO: Successfully updated pod "labelsupdate263697bc-79fc-4749-8c8a-41db88e497a3"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:18.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2314" for this suite. 09/08/23 21:48:18.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:18.765
Sep  8 21:48:18.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:48:18.767
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:18.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:18.805
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:48:18.811
Sep  8 21:48:18.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf" in namespace "projected-6221" to be "Succeeded or Failed"
Sep  8 21:48:18.863: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Pending", Reason="", readiness=false. Elapsed: 23.897616ms
Sep  8 21:48:20.876: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036973828s
Sep  8 21:48:22.875: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036215907s
Sep  8 21:48:24.880: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041566046s
STEP: Saw pod success 09/08/23 21:48:24.88
Sep  8 21:48:24.881: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf" satisfied condition "Succeeded or Failed"
Sep  8 21:48:24.890: INFO: Trying to get logs from node node-3 pod downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf container client-container: <nil>
STEP: delete the pod 09/08/23 21:48:24.908
Sep  8 21:48:24.941: INFO: Waiting for pod downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf to disappear
Sep  8 21:48:24.952: INFO: Pod downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:24.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6221" for this suite. 09/08/23 21:48:24.965
------------------------------
â€¢ [SLOW TEST] [6.218 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:18.765
    Sep  8 21:48:18.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:48:18.767
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:18.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:18.805
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:48:18.811
    Sep  8 21:48:18.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf" in namespace "projected-6221" to be "Succeeded or Failed"
    Sep  8 21:48:18.863: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Pending", Reason="", readiness=false. Elapsed: 23.897616ms
    Sep  8 21:48:20.876: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036973828s
    Sep  8 21:48:22.875: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036215907s
    Sep  8 21:48:24.880: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041566046s
    STEP: Saw pod success 09/08/23 21:48:24.88
    Sep  8 21:48:24.881: INFO: Pod "downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf" satisfied condition "Succeeded or Failed"
    Sep  8 21:48:24.890: INFO: Trying to get logs from node node-3 pod downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf container client-container: <nil>
    STEP: delete the pod 09/08/23 21:48:24.908
    Sep  8 21:48:24.941: INFO: Waiting for pod downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf to disappear
    Sep  8 21:48:24.952: INFO: Pod downwardapi-volume-bacf338a-51e8-406b-a2b5-ce8014a728cf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:24.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6221" for this suite. 09/08/23 21:48:24.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:24.987
Sep  8 21:48:24.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename disruption 09/08/23 21:48:24.989
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:25.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:25.037
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 09/08/23 21:48:25.043
STEP: Waiting for the pdb to be processed 09/08/23 21:48:25.052
STEP: First trying to evict a pod which shouldn't be evictable 09/08/23 21:48:27.092
STEP: Waiting for all pods to be running 09/08/23 21:48:27.092
Sep  8 21:48:27.103: INFO: pods: 0 < 3
STEP: locating a running pod 09/08/23 21:48:29.12
STEP: Updating the pdb to allow a pod to be evicted 09/08/23 21:48:29.143
STEP: Waiting for the pdb to be processed 09/08/23 21:48:29.163
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/08/23 21:48:31.189
STEP: Waiting for all pods to be running 09/08/23 21:48:31.189
STEP: Waiting for the pdb to observed all healthy pods 09/08/23 21:48:31.207
STEP: Patching the pdb to disallow a pod to be evicted 09/08/23 21:48:31.292
STEP: Waiting for the pdb to be processed 09/08/23 21:48:31.367
STEP: Waiting for all pods to be running 09/08/23 21:48:33.388
STEP: locating a running pod 09/08/23 21:48:33.401
STEP: Deleting the pdb to allow a pod to be evicted 09/08/23 21:48:33.425
STEP: Waiting for the pdb to be deleted 09/08/23 21:48:33.44
STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/08/23 21:48:33.447
STEP: Waiting for all pods to be running 09/08/23 21:48:33.447
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2683" for this suite. 09/08/23 21:48:33.543
------------------------------
â€¢ [SLOW TEST] [8.616 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:24.987
    Sep  8 21:48:24.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename disruption 09/08/23 21:48:24.989
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:25.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:25.037
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 09/08/23 21:48:25.043
    STEP: Waiting for the pdb to be processed 09/08/23 21:48:25.052
    STEP: First trying to evict a pod which shouldn't be evictable 09/08/23 21:48:27.092
    STEP: Waiting for all pods to be running 09/08/23 21:48:27.092
    Sep  8 21:48:27.103: INFO: pods: 0 < 3
    STEP: locating a running pod 09/08/23 21:48:29.12
    STEP: Updating the pdb to allow a pod to be evicted 09/08/23 21:48:29.143
    STEP: Waiting for the pdb to be processed 09/08/23 21:48:29.163
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/08/23 21:48:31.189
    STEP: Waiting for all pods to be running 09/08/23 21:48:31.189
    STEP: Waiting for the pdb to observed all healthy pods 09/08/23 21:48:31.207
    STEP: Patching the pdb to disallow a pod to be evicted 09/08/23 21:48:31.292
    STEP: Waiting for the pdb to be processed 09/08/23 21:48:31.367
    STEP: Waiting for all pods to be running 09/08/23 21:48:33.388
    STEP: locating a running pod 09/08/23 21:48:33.401
    STEP: Deleting the pdb to allow a pod to be evicted 09/08/23 21:48:33.425
    STEP: Waiting for the pdb to be deleted 09/08/23 21:48:33.44
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 09/08/23 21:48:33.447
    STEP: Waiting for all pods to be running 09/08/23 21:48:33.447
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2683" for this suite. 09/08/23 21:48:33.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:33.608
Sep  8 21:48:33.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename tables 09/08/23 21:48:33.61
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:33.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:33.645
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:33.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3269" for this suite. 09/08/23 21:48:33.667
------------------------------
â€¢ [0.074 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:33.608
    Sep  8 21:48:33.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename tables 09/08/23 21:48:33.61
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:33.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:33.645
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:33.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3269" for this suite. 09/08/23 21:48:33.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:33.685
Sep  8 21:48:33.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:48:33.687
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:33.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:33.724
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 09/08/23 21:48:33.734
Sep  8 21:48:33.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4" in namespace "projected-7033" to be "Succeeded or Failed"
Sep  8 21:48:33.775: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.478134ms
Sep  8 21:48:35.793: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036030457s
Sep  8 21:48:37.788: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03094077s
STEP: Saw pod success 09/08/23 21:48:37.788
Sep  8 21:48:37.788: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4" satisfied condition "Succeeded or Failed"
Sep  8 21:48:37.798: INFO: Trying to get logs from node node-3 pod downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4 container client-container: <nil>
STEP: delete the pod 09/08/23 21:48:37.814
Sep  8 21:48:37.852: INFO: Waiting for pod downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4 to disappear
Sep  8 21:48:37.867: INFO: Pod downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:37.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7033" for this suite. 09/08/23 21:48:37.884
------------------------------
â€¢ [4.224 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:33.685
    Sep  8 21:48:33.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:48:33.687
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:33.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:33.724
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 09/08/23 21:48:33.734
    Sep  8 21:48:33.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4" in namespace "projected-7033" to be "Succeeded or Failed"
    Sep  8 21:48:33.775: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.478134ms
    Sep  8 21:48:35.793: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036030457s
    Sep  8 21:48:37.788: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03094077s
    STEP: Saw pod success 09/08/23 21:48:37.788
    Sep  8 21:48:37.788: INFO: Pod "downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4" satisfied condition "Succeeded or Failed"
    Sep  8 21:48:37.798: INFO: Trying to get logs from node node-3 pod downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4 container client-container: <nil>
    STEP: delete the pod 09/08/23 21:48:37.814
    Sep  8 21:48:37.852: INFO: Waiting for pod downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4 to disappear
    Sep  8 21:48:37.867: INFO: Pod downwardapi-volume-4621a80b-9541-4744-8ca7-9f2525a9e2d4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:37.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7033" for this suite. 09/08/23 21:48:37.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:37.909
Sep  8 21:48:37.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:48:37.91
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:37.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:37.973
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 09/08/23 21:48:37.989
Sep  8 21:48:37.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: mark a version not serverd 09/08/23 21:48:48.989
STEP: check the unserved version gets removed 09/08/23 21:48:49.035
STEP: check the other version is not changed 09/08/23 21:48:52.488
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:57.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5641" for this suite. 09/08/23 21:48:57.439
------------------------------
â€¢ [SLOW TEST] [19.579 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:37.909
    Sep  8 21:48:37.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:48:37.91
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:37.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:37.973
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 09/08/23 21:48:37.989
    Sep  8 21:48:37.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: mark a version not serverd 09/08/23 21:48:48.989
    STEP: check the unserved version gets removed 09/08/23 21:48:49.035
    STEP: check the other version is not changed 09/08/23 21:48:52.488
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:57.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5641" for this suite. 09/08/23 21:48:57.439
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:57.488
Sep  8 21:48:57.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replication-controller 09/08/23 21:48:57.49
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:57.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:57.558
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-5cc8x" 09/08/23 21:48:57.563
Sep  8 21:48:57.580: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
Sep  8 21:48:58.592: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
Sep  8 21:48:58.604: INFO: Found 1 replicas for "e2e-rc-5cc8x" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-5cc8x" 09/08/23 21:48:58.605
STEP: Updating a scale subresource 09/08/23 21:48:58.623
STEP: Verifying replicas where modified for replication controller "e2e-rc-5cc8x" 09/08/23 21:48:58.638
Sep  8 21:48:58.638: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
Sep  8 21:48:59.652: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
Sep  8 21:48:59.668: INFO: Found 2 replicas for "e2e-rc-5cc8x" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:59.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5981" for this suite. 09/08/23 21:48:59.706
------------------------------
â€¢ [2.248 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:57.488
    Sep  8 21:48:57.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replication-controller 09/08/23 21:48:57.49
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:57.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:57.558
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-5cc8x" 09/08/23 21:48:57.563
    Sep  8 21:48:57.580: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
    Sep  8 21:48:58.592: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
    Sep  8 21:48:58.604: INFO: Found 1 replicas for "e2e-rc-5cc8x" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-5cc8x" 09/08/23 21:48:58.605
    STEP: Updating a scale subresource 09/08/23 21:48:58.623
    STEP: Verifying replicas where modified for replication controller "e2e-rc-5cc8x" 09/08/23 21:48:58.638
    Sep  8 21:48:58.638: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
    Sep  8 21:48:59.652: INFO: Get Replication Controller "e2e-rc-5cc8x" to confirm replicas
    Sep  8 21:48:59.668: INFO: Found 2 replicas for "e2e-rc-5cc8x" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:59.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5981" for this suite. 09/08/23 21:48:59.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:59.742
Sep  8 21:48:59.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename runtimeclass 09/08/23 21:48:59.743
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:59.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:59.844
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  8 21:48:59.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6093" for this suite. 09/08/23 21:48:59.887
------------------------------
â€¢ [0.167 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:59.742
    Sep  8 21:48:59.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename runtimeclass 09/08/23 21:48:59.743
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:59.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:59.844
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:48:59.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6093" for this suite. 09/08/23 21:48:59.887
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:48:59.911
Sep  8 21:48:59.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 21:48:59.93
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:59.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:59.993
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 09/08/23 21:49:00
Sep  8 21:49:00.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 create -f -'
Sep  8 21:49:00.978: INFO: stderr: ""
Sep  8 21:49:00.978: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 21:49:00.978
Sep  8 21:49:00.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:01.133: INFO: stderr: ""
Sep  8 21:49:01.133: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
Sep  8 21:49:01.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:01.253: INFO: stderr: ""
Sep  8 21:49:01.253: INFO: stdout: ""
Sep  8 21:49:01.253: INFO: update-demo-nautilus-chfbq is created but not running
Sep  8 21:49:06.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:06.389: INFO: stderr: ""
Sep  8 21:49:06.389: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
Sep  8 21:49:06.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:06.513: INFO: stderr: ""
Sep  8 21:49:06.513: INFO: stdout: ""
Sep  8 21:49:06.513: INFO: update-demo-nautilus-chfbq is created but not running
Sep  8 21:49:11.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:11.640: INFO: stderr: ""
Sep  8 21:49:11.640: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
Sep  8 21:49:11.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:11.769: INFO: stderr: ""
Sep  8 21:49:11.769: INFO: stdout: "true"
Sep  8 21:49:11.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 21:49:11.866: INFO: stderr: ""
Sep  8 21:49:11.866: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 21:49:11.866: INFO: validating pod update-demo-nautilus-chfbq
Sep  8 21:49:11.903: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 21:49:11.903: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 21:49:11.903: INFO: update-demo-nautilus-chfbq is verified up and running
Sep  8 21:49:11.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-sqbfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:12.015: INFO: stderr: ""
Sep  8 21:49:12.015: INFO: stdout: ""
Sep  8 21:49:12.015: INFO: update-demo-nautilus-sqbfj is created but not running
Sep  8 21:49:17.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:17.133: INFO: stderr: ""
Sep  8 21:49:17.133: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
Sep  8 21:49:17.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:17.252: INFO: stderr: ""
Sep  8 21:49:17.252: INFO: stdout: "true"
Sep  8 21:49:17.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 21:49:17.366: INFO: stderr: ""
Sep  8 21:49:17.366: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 21:49:17.366: INFO: validating pod update-demo-nautilus-chfbq
Sep  8 21:49:17.376: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 21:49:17.376: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 21:49:17.376: INFO: update-demo-nautilus-chfbq is verified up and running
Sep  8 21:49:17.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-sqbfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:17.482: INFO: stderr: ""
Sep  8 21:49:17.482: INFO: stdout: "true"
Sep  8 21:49:17.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-sqbfj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 21:49:17.598: INFO: stderr: ""
Sep  8 21:49:17.598: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 21:49:17.598: INFO: validating pod update-demo-nautilus-sqbfj
Sep  8 21:49:17.613: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 21:49:17.613: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 21:49:17.613: INFO: update-demo-nautilus-sqbfj is verified up and running
STEP: scaling down the replication controller 09/08/23 21:49:17.613
Sep  8 21:49:17.617: INFO: scanned /root for discovery docs: <nil>
Sep  8 21:49:17.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  8 21:49:18.818: INFO: stderr: ""
Sep  8 21:49:18.818: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 21:49:18.818
Sep  8 21:49:18.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:18.951: INFO: stderr: ""
Sep  8 21:49:18.951: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
STEP: Replicas for name=update-demo: expected=1 actual=2 09/08/23 21:49:18.951
Sep  8 21:49:23.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:24.064: INFO: stderr: ""
Sep  8 21:49:24.064: INFO: stdout: "update-demo-nautilus-chfbq "
Sep  8 21:49:24.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:24.193: INFO: stderr: ""
Sep  8 21:49:24.193: INFO: stdout: "true"
Sep  8 21:49:24.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 21:49:24.330: INFO: stderr: ""
Sep  8 21:49:24.330: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 21:49:24.330: INFO: validating pod update-demo-nautilus-chfbq
Sep  8 21:49:24.342: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 21:49:24.342: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 21:49:24.342: INFO: update-demo-nautilus-chfbq is verified up and running
STEP: scaling up the replication controller 09/08/23 21:49:24.342
Sep  8 21:49:24.345: INFO: scanned /root for discovery docs: <nil>
Sep  8 21:49:24.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  8 21:49:25.511: INFO: stderr: ""
Sep  8 21:49:25.511: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 21:49:25.511
Sep  8 21:49:25.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:25.630: INFO: stderr: ""
Sep  8 21:49:25.630: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-h2mf4 "
Sep  8 21:49:25.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:25.738: INFO: stderr: ""
Sep  8 21:49:25.738: INFO: stdout: "true"
Sep  8 21:49:25.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 21:49:25.848: INFO: stderr: ""
Sep  8 21:49:25.848: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 21:49:25.848: INFO: validating pod update-demo-nautilus-chfbq
Sep  8 21:49:25.857: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 21:49:25.857: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 21:49:25.857: INFO: update-demo-nautilus-chfbq is verified up and running
Sep  8 21:49:25.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-h2mf4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:25.975: INFO: stderr: ""
Sep  8 21:49:25.975: INFO: stdout: ""
Sep  8 21:49:25.975: INFO: update-demo-nautilus-h2mf4 is created but not running
Sep  8 21:49:30.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 21:49:31.115: INFO: stderr: ""
Sep  8 21:49:31.115: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-h2mf4 "
Sep  8 21:49:31.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:31.241: INFO: stderr: ""
Sep  8 21:49:31.241: INFO: stdout: "true"
Sep  8 21:49:31.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 21:49:31.376: INFO: stderr: ""
Sep  8 21:49:31.376: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 21:49:31.376: INFO: validating pod update-demo-nautilus-chfbq
Sep  8 21:49:31.396: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 21:49:31.396: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 21:49:31.396: INFO: update-demo-nautilus-chfbq is verified up and running
Sep  8 21:49:31.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-h2mf4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 21:49:31.511: INFO: stderr: ""
Sep  8 21:49:31.511: INFO: stdout: "true"
Sep  8 21:49:31.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-h2mf4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 21:49:31.622: INFO: stderr: ""
Sep  8 21:49:31.622: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 21:49:31.622: INFO: validating pod update-demo-nautilus-h2mf4
Sep  8 21:49:31.636: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 21:49:31.636: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 21:49:31.636: INFO: update-demo-nautilus-h2mf4 is verified up and running
STEP: using delete to clean up resources 09/08/23 21:49:31.636
Sep  8 21:49:31.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 delete --grace-period=0 --force -f -'
Sep  8 21:49:31.806: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 21:49:31.806: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  8 21:49:31.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get rc,svc -l name=update-demo --no-headers'
Sep  8 21:49:32.010: INFO: stderr: "No resources found in kubectl-194 namespace.\n"
Sep  8 21:49:32.010: INFO: stdout: ""
Sep  8 21:49:32.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  8 21:49:32.147: INFO: stderr: ""
Sep  8 21:49:32.147: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 21:49:32.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-194" for this suite. 09/08/23 21:49:32.164
------------------------------
â€¢ [SLOW TEST] [32.265 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:48:59.911
    Sep  8 21:48:59.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 21:48:59.93
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:48:59.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:48:59.993
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 09/08/23 21:49:00
    Sep  8 21:49:00.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 create -f -'
    Sep  8 21:49:00.978: INFO: stderr: ""
    Sep  8 21:49:00.978: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 21:49:00.978
    Sep  8 21:49:00.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:01.133: INFO: stderr: ""
    Sep  8 21:49:01.133: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
    Sep  8 21:49:01.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:01.253: INFO: stderr: ""
    Sep  8 21:49:01.253: INFO: stdout: ""
    Sep  8 21:49:01.253: INFO: update-demo-nautilus-chfbq is created but not running
    Sep  8 21:49:06.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:06.389: INFO: stderr: ""
    Sep  8 21:49:06.389: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
    Sep  8 21:49:06.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:06.513: INFO: stderr: ""
    Sep  8 21:49:06.513: INFO: stdout: ""
    Sep  8 21:49:06.513: INFO: update-demo-nautilus-chfbq is created but not running
    Sep  8 21:49:11.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:11.640: INFO: stderr: ""
    Sep  8 21:49:11.640: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
    Sep  8 21:49:11.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:11.769: INFO: stderr: ""
    Sep  8 21:49:11.769: INFO: stdout: "true"
    Sep  8 21:49:11.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 21:49:11.866: INFO: stderr: ""
    Sep  8 21:49:11.866: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 21:49:11.866: INFO: validating pod update-demo-nautilus-chfbq
    Sep  8 21:49:11.903: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 21:49:11.903: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 21:49:11.903: INFO: update-demo-nautilus-chfbq is verified up and running
    Sep  8 21:49:11.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-sqbfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:12.015: INFO: stderr: ""
    Sep  8 21:49:12.015: INFO: stdout: ""
    Sep  8 21:49:12.015: INFO: update-demo-nautilus-sqbfj is created but not running
    Sep  8 21:49:17.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:17.133: INFO: stderr: ""
    Sep  8 21:49:17.133: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
    Sep  8 21:49:17.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:17.252: INFO: stderr: ""
    Sep  8 21:49:17.252: INFO: stdout: "true"
    Sep  8 21:49:17.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 21:49:17.366: INFO: stderr: ""
    Sep  8 21:49:17.366: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 21:49:17.366: INFO: validating pod update-demo-nautilus-chfbq
    Sep  8 21:49:17.376: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 21:49:17.376: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 21:49:17.376: INFO: update-demo-nautilus-chfbq is verified up and running
    Sep  8 21:49:17.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-sqbfj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:17.482: INFO: stderr: ""
    Sep  8 21:49:17.482: INFO: stdout: "true"
    Sep  8 21:49:17.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-sqbfj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 21:49:17.598: INFO: stderr: ""
    Sep  8 21:49:17.598: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 21:49:17.598: INFO: validating pod update-demo-nautilus-sqbfj
    Sep  8 21:49:17.613: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 21:49:17.613: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 21:49:17.613: INFO: update-demo-nautilus-sqbfj is verified up and running
    STEP: scaling down the replication controller 09/08/23 21:49:17.613
    Sep  8 21:49:17.617: INFO: scanned /root for discovery docs: <nil>
    Sep  8 21:49:17.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Sep  8 21:49:18.818: INFO: stderr: ""
    Sep  8 21:49:18.818: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 21:49:18.818
    Sep  8 21:49:18.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:18.951: INFO: stderr: ""
    Sep  8 21:49:18.951: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-sqbfj "
    STEP: Replicas for name=update-demo: expected=1 actual=2 09/08/23 21:49:18.951
    Sep  8 21:49:23.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:24.064: INFO: stderr: ""
    Sep  8 21:49:24.064: INFO: stdout: "update-demo-nautilus-chfbq "
    Sep  8 21:49:24.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:24.193: INFO: stderr: ""
    Sep  8 21:49:24.193: INFO: stdout: "true"
    Sep  8 21:49:24.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 21:49:24.330: INFO: stderr: ""
    Sep  8 21:49:24.330: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 21:49:24.330: INFO: validating pod update-demo-nautilus-chfbq
    Sep  8 21:49:24.342: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 21:49:24.342: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 21:49:24.342: INFO: update-demo-nautilus-chfbq is verified up and running
    STEP: scaling up the replication controller 09/08/23 21:49:24.342
    Sep  8 21:49:24.345: INFO: scanned /root for discovery docs: <nil>
    Sep  8 21:49:24.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Sep  8 21:49:25.511: INFO: stderr: ""
    Sep  8 21:49:25.511: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 21:49:25.511
    Sep  8 21:49:25.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:25.630: INFO: stderr: ""
    Sep  8 21:49:25.630: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-h2mf4 "
    Sep  8 21:49:25.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:25.738: INFO: stderr: ""
    Sep  8 21:49:25.738: INFO: stdout: "true"
    Sep  8 21:49:25.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 21:49:25.848: INFO: stderr: ""
    Sep  8 21:49:25.848: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 21:49:25.848: INFO: validating pod update-demo-nautilus-chfbq
    Sep  8 21:49:25.857: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 21:49:25.857: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 21:49:25.857: INFO: update-demo-nautilus-chfbq is verified up and running
    Sep  8 21:49:25.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-h2mf4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:25.975: INFO: stderr: ""
    Sep  8 21:49:25.975: INFO: stdout: ""
    Sep  8 21:49:25.975: INFO: update-demo-nautilus-h2mf4 is created but not running
    Sep  8 21:49:30.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 21:49:31.115: INFO: stderr: ""
    Sep  8 21:49:31.115: INFO: stdout: "update-demo-nautilus-chfbq update-demo-nautilus-h2mf4 "
    Sep  8 21:49:31.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:31.241: INFO: stderr: ""
    Sep  8 21:49:31.241: INFO: stdout: "true"
    Sep  8 21:49:31.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-chfbq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 21:49:31.376: INFO: stderr: ""
    Sep  8 21:49:31.376: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 21:49:31.376: INFO: validating pod update-demo-nautilus-chfbq
    Sep  8 21:49:31.396: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 21:49:31.396: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 21:49:31.396: INFO: update-demo-nautilus-chfbq is verified up and running
    Sep  8 21:49:31.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-h2mf4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 21:49:31.511: INFO: stderr: ""
    Sep  8 21:49:31.511: INFO: stdout: "true"
    Sep  8 21:49:31.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods update-demo-nautilus-h2mf4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 21:49:31.622: INFO: stderr: ""
    Sep  8 21:49:31.622: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 21:49:31.622: INFO: validating pod update-demo-nautilus-h2mf4
    Sep  8 21:49:31.636: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 21:49:31.636: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 21:49:31.636: INFO: update-demo-nautilus-h2mf4 is verified up and running
    STEP: using delete to clean up resources 09/08/23 21:49:31.636
    Sep  8 21:49:31.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 delete --grace-period=0 --force -f -'
    Sep  8 21:49:31.806: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 21:49:31.806: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  8 21:49:31.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get rc,svc -l name=update-demo --no-headers'
    Sep  8 21:49:32.010: INFO: stderr: "No resources found in kubectl-194 namespace.\n"
    Sep  8 21:49:32.010: INFO: stdout: ""
    Sep  8 21:49:32.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-194 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  8 21:49:32.147: INFO: stderr: ""
    Sep  8 21:49:32.147: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:49:32.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-194" for this suite. 09/08/23 21:49:32.164
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:49:32.177
Sep  8 21:49:32.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 21:49:32.181
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:49:32.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:49:32.235
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 09/08/23 21:49:32.24
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 09/08/23 21:49:32.265
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 09/08/23 21:49:32.266
STEP: creating a pod to probe DNS 09/08/23 21:49:32.267
STEP: submitting the pod to kubernetes 09/08/23 21:49:32.267
Sep  8 21:49:32.304: INFO: Waiting up to 15m0s for pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0" in namespace "dns-9720" to be "running"
Sep  8 21:49:32.332: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.377098ms
Sep  8 21:49:34.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039498689s
Sep  8 21:49:36.348: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043688152s
Sep  8 21:49:38.362: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057489195s
Sep  8 21:49:40.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039633238s
Sep  8 21:49:42.343: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039416616s
Sep  8 21:49:44.343: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038695291s
Sep  8 21:49:46.347: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.043050423s
Sep  8 21:49:48.343: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03883148s
Sep  8 21:49:50.352: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.048117957s
Sep  8 21:49:52.347: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.04290549s
Sep  8 21:49:54.348: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.044418797s
Sep  8 21:49:56.342: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.037946438s
Sep  8 21:49:58.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Running", Reason="", readiness=true. Elapsed: 26.040080844s
Sep  8 21:49:58.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0" satisfied condition "running"
STEP: retrieving the pod 09/08/23 21:49:58.344
STEP: looking for the results for each expected name from probers 09/08/23 21:49:58.354
Sep  8 21:49:58.398: INFO: DNS probes using dns-9720/dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0 succeeded

STEP: deleting the pod 09/08/23 21:49:58.398
STEP: deleting the test headless service 09/08/23 21:49:58.433
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 21:49:58.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9720" for this suite. 09/08/23 21:49:58.496
------------------------------
â€¢ [SLOW TEST] [26.345 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:49:32.177
    Sep  8 21:49:32.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 21:49:32.181
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:49:32.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:49:32.235
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 09/08/23 21:49:32.24
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     09/08/23 21:49:32.265
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9720.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     09/08/23 21:49:32.266
    STEP: creating a pod to probe DNS 09/08/23 21:49:32.267
    STEP: submitting the pod to kubernetes 09/08/23 21:49:32.267
    Sep  8 21:49:32.304: INFO: Waiting up to 15m0s for pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0" in namespace "dns-9720" to be "running"
    Sep  8 21:49:32.332: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.377098ms
    Sep  8 21:49:34.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039498689s
    Sep  8 21:49:36.348: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043688152s
    Sep  8 21:49:38.362: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057489195s
    Sep  8 21:49:40.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039633238s
    Sep  8 21:49:42.343: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039416616s
    Sep  8 21:49:44.343: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.038695291s
    Sep  8 21:49:46.347: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.043050423s
    Sep  8 21:49:48.343: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03883148s
    Sep  8 21:49:50.352: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.048117957s
    Sep  8 21:49:52.347: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.04290549s
    Sep  8 21:49:54.348: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.044418797s
    Sep  8 21:49:56.342: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.037946438s
    Sep  8 21:49:58.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0": Phase="Running", Reason="", readiness=true. Elapsed: 26.040080844s
    Sep  8 21:49:58.344: INFO: Pod "dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 21:49:58.344
    STEP: looking for the results for each expected name from probers 09/08/23 21:49:58.354
    Sep  8 21:49:58.398: INFO: DNS probes using dns-9720/dns-test-3878bc00-dfa5-4cce-ae74-ebf0dbc87ac0 succeeded

    STEP: deleting the pod 09/08/23 21:49:58.398
    STEP: deleting the test headless service 09/08/23 21:49:58.433
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:49:58.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9720" for this suite. 09/08/23 21:49:58.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:49:58.524
Sep  8 21:49:58.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:49:58.526
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:49:58.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:49:58.579
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 09/08/23 21:49:58.585
Sep  8 21:49:58.585: INFO: Creating e2e-svc-a-whd8q
Sep  8 21:49:58.617: INFO: Creating e2e-svc-b-6nk5z
Sep  8 21:49:58.646: INFO: Creating e2e-svc-c-nmn62
STEP: deleting service collection 09/08/23 21:49:58.702
Sep  8 21:49:58.809: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:49:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4317" for this suite. 09/08/23 21:49:58.819
------------------------------
â€¢ [0.322 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:49:58.524
    Sep  8 21:49:58.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:49:58.526
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:49:58.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:49:58.579
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 09/08/23 21:49:58.585
    Sep  8 21:49:58.585: INFO: Creating e2e-svc-a-whd8q
    Sep  8 21:49:58.617: INFO: Creating e2e-svc-b-6nk5z
    Sep  8 21:49:58.646: INFO: Creating e2e-svc-c-nmn62
    STEP: deleting service collection 09/08/23 21:49:58.702
    Sep  8 21:49:58.809: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:49:58.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4317" for this suite. 09/08/23 21:49:58.819
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:49:58.848
Sep  8 21:49:58.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename proxy 09/08/23 21:49:58.849
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:49:58.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:49:58.918
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Sep  8 21:49:58.936: INFO: Creating pod...
Sep  8 21:49:58.976: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3381" to be "running"
Sep  8 21:49:58.990: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.968253ms
Sep  8 21:50:01.000: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024338713s
Sep  8 21:50:03.007: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.030693887s
Sep  8 21:50:03.007: INFO: Pod "agnhost" satisfied condition "running"
Sep  8 21:50:03.007: INFO: Creating service...
Sep  8 21:50:03.051: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=DELETE
Sep  8 21:50:03.084: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  8 21:50:03.085: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=OPTIONS
Sep  8 21:50:03.103: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  8 21:50:03.103: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=PATCH
Sep  8 21:50:03.111: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  8 21:50:03.111: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=POST
Sep  8 21:50:03.120: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  8 21:50:03.120: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=PUT
Sep  8 21:50:03.133: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  8 21:50:03.133: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=DELETE
Sep  8 21:50:03.164: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  8 21:50:03.164: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=OPTIONS
Sep  8 21:50:03.208: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  8 21:50:03.208: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=PATCH
Sep  8 21:50:03.233: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  8 21:50:03.233: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=POST
Sep  8 21:50:03.257: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  8 21:50:03.257: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=PUT
Sep  8 21:50:03.279: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  8 21:50:03.279: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=GET
Sep  8 21:50:03.293: INFO: http.Client request:GET StatusCode:301
Sep  8 21:50:03.293: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=GET
Sep  8 21:50:03.320: INFO: http.Client request:GET StatusCode:301
Sep  8 21:50:03.320: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=HEAD
Sep  8 21:50:03.326: INFO: http.Client request:HEAD StatusCode:301
Sep  8 21:50:03.326: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=HEAD
Sep  8 21:50:03.347: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:03.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3381" for this suite. 09/08/23 21:50:03.365
------------------------------
â€¢ [4.534 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:49:58.848
    Sep  8 21:49:58.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename proxy 09/08/23 21:49:58.849
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:49:58.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:49:58.918
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Sep  8 21:49:58.936: INFO: Creating pod...
    Sep  8 21:49:58.976: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3381" to be "running"
    Sep  8 21:49:58.990: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.968253ms
    Sep  8 21:50:01.000: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024338713s
    Sep  8 21:50:03.007: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.030693887s
    Sep  8 21:50:03.007: INFO: Pod "agnhost" satisfied condition "running"
    Sep  8 21:50:03.007: INFO: Creating service...
    Sep  8 21:50:03.051: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=DELETE
    Sep  8 21:50:03.084: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  8 21:50:03.085: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=OPTIONS
    Sep  8 21:50:03.103: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  8 21:50:03.103: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=PATCH
    Sep  8 21:50:03.111: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  8 21:50:03.111: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=POST
    Sep  8 21:50:03.120: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  8 21:50:03.120: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=PUT
    Sep  8 21:50:03.133: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  8 21:50:03.133: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=DELETE
    Sep  8 21:50:03.164: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  8 21:50:03.164: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Sep  8 21:50:03.208: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  8 21:50:03.208: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=PATCH
    Sep  8 21:50:03.233: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  8 21:50:03.233: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=POST
    Sep  8 21:50:03.257: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  8 21:50:03.257: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=PUT
    Sep  8 21:50:03.279: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  8 21:50:03.279: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=GET
    Sep  8 21:50:03.293: INFO: http.Client request:GET StatusCode:301
    Sep  8 21:50:03.293: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=GET
    Sep  8 21:50:03.320: INFO: http.Client request:GET StatusCode:301
    Sep  8 21:50:03.320: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/pods/agnhost/proxy?method=HEAD
    Sep  8 21:50:03.326: INFO: http.Client request:HEAD StatusCode:301
    Sep  8 21:50:03.326: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3381/services/e2e-proxy-test-service/proxy?method=HEAD
    Sep  8 21:50:03.347: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:03.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3381" for this suite. 09/08/23 21:50:03.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:03.39
Sep  8 21:50:03.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename proxy 09/08/23 21:50:03.391
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:03.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:03.439
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Sep  8 21:50:03.446: INFO: Creating pod...
Sep  8 21:50:03.483: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2246" to be "running"
Sep  8 21:50:03.493: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.618303ms
Sep  8 21:50:05.504: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02098892s
Sep  8 21:50:07.503: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.020255193s
Sep  8 21:50:07.503: INFO: Pod "agnhost" satisfied condition "running"
Sep  8 21:50:07.504: INFO: Creating service...
Sep  8 21:50:07.529: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/DELETE
Sep  8 21:50:07.559: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  8 21:50:07.559: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/GET
Sep  8 21:50:07.581: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  8 21:50:07.581: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/HEAD
Sep  8 21:50:07.602: INFO: http.Client request:HEAD | StatusCode:200
Sep  8 21:50:07.602: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/OPTIONS
Sep  8 21:50:07.644: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  8 21:50:07.644: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/PATCH
Sep  8 21:50:07.656: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  8 21:50:07.656: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/POST
Sep  8 21:50:07.671: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  8 21:50:07.671: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/PUT
Sep  8 21:50:07.681: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep  8 21:50:07.682: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/DELETE
Sep  8 21:50:07.699: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep  8 21:50:07.699: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/GET
Sep  8 21:50:07.719: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep  8 21:50:07.719: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/HEAD
Sep  8 21:50:07.735: INFO: http.Client request:HEAD | StatusCode:200
Sep  8 21:50:07.735: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/OPTIONS
Sep  8 21:50:07.748: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep  8 21:50:07.748: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/PATCH
Sep  8 21:50:07.764: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep  8 21:50:07.764: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/POST
Sep  8 21:50:07.781: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep  8 21:50:07.781: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/PUT
Sep  8 21:50:07.803: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:07.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2246" for this suite. 09/08/23 21:50:07.815
------------------------------
â€¢ [4.446 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:03.39
    Sep  8 21:50:03.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename proxy 09/08/23 21:50:03.391
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:03.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:03.439
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Sep  8 21:50:03.446: INFO: Creating pod...
    Sep  8 21:50:03.483: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2246" to be "running"
    Sep  8 21:50:03.493: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.618303ms
    Sep  8 21:50:05.504: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02098892s
    Sep  8 21:50:07.503: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.020255193s
    Sep  8 21:50:07.503: INFO: Pod "agnhost" satisfied condition "running"
    Sep  8 21:50:07.504: INFO: Creating service...
    Sep  8 21:50:07.529: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/DELETE
    Sep  8 21:50:07.559: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  8 21:50:07.559: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/GET
    Sep  8 21:50:07.581: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  8 21:50:07.581: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/HEAD
    Sep  8 21:50:07.602: INFO: http.Client request:HEAD | StatusCode:200
    Sep  8 21:50:07.602: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/OPTIONS
    Sep  8 21:50:07.644: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  8 21:50:07.644: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/PATCH
    Sep  8 21:50:07.656: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  8 21:50:07.656: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/POST
    Sep  8 21:50:07.671: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  8 21:50:07.671: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/pods/agnhost/proxy/some/path/with/PUT
    Sep  8 21:50:07.681: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Sep  8 21:50:07.682: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/DELETE
    Sep  8 21:50:07.699: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Sep  8 21:50:07.699: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/GET
    Sep  8 21:50:07.719: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Sep  8 21:50:07.719: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/HEAD
    Sep  8 21:50:07.735: INFO: http.Client request:HEAD | StatusCode:200
    Sep  8 21:50:07.735: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/OPTIONS
    Sep  8 21:50:07.748: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Sep  8 21:50:07.748: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/PATCH
    Sep  8 21:50:07.764: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Sep  8 21:50:07.764: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/POST
    Sep  8 21:50:07.781: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Sep  8 21:50:07.781: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2246/services/test-service/proxy/some/path/with/PUT
    Sep  8 21:50:07.803: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:07.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2246" for this suite. 09/08/23 21:50:07.815
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:07.84
Sep  8 21:50:07.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename endpointslice 09/08/23 21:50:07.841
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:07.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:07.892
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Sep  8 21:50:07.941: INFO: Endpoints addresses: [10.100.16.120 10.100.16.142 10.100.20.57] , ports: [6443]
Sep  8 21:50:07.941: INFO: EndpointSlices addresses: [10.100.16.120 10.100.16.142 10.100.20.57] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:07.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1045" for this suite. 09/08/23 21:50:07.968
------------------------------
â€¢ [0.153 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:07.84
    Sep  8 21:50:07.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename endpointslice 09/08/23 21:50:07.841
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:07.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:07.892
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Sep  8 21:50:07.941: INFO: Endpoints addresses: [10.100.16.120 10.100.16.142 10.100.20.57] , ports: [6443]
    Sep  8 21:50:07.941: INFO: EndpointSlices addresses: [10.100.16.120 10.100.16.142 10.100.20.57] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:07.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1045" for this suite. 09/08/23 21:50:07.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:07.994
Sep  8 21:50:07.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:50:07.995
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:08.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:08.073
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:08.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9623" for this suite. 09/08/23 21:50:08.245
------------------------------
â€¢ [0.267 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:07.994
    Sep  8 21:50:07.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:50:07.995
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:08.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:08.073
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:08.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9623" for this suite. 09/08/23 21:50:08.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:08.263
Sep  8 21:50:08.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:50:08.267
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:08.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:08.328
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5381 09/08/23 21:50:08.341
STEP: changing the ExternalName service to type=NodePort 09/08/23 21:50:08.362
STEP: creating replication controller externalname-service in namespace services-5381 09/08/23 21:50:08.462
I0908 21:50:08.490788      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5381, replica count: 2
I0908 21:50:11.542295      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 21:50:11.542: INFO: Creating new exec pod
Sep  8 21:50:11.567: INFO: Waiting up to 5m0s for pod "execpodhcf4h" in namespace "services-5381" to be "running"
Sep  8 21:50:11.578: INFO: Pod "execpodhcf4h": Phase="Pending", Reason="", readiness=false. Elapsed: 11.789435ms
Sep  8 21:50:13.599: INFO: Pod "execpodhcf4h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032697935s
Sep  8 21:50:15.598: INFO: Pod "execpodhcf4h": Phase="Running", Reason="", readiness=true. Elapsed: 4.031562328s
Sep  8 21:50:15.609: INFO: Pod "execpodhcf4h" satisfied condition "running"
Sep  8 21:50:16.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  8 21:50:16.919: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  8 21:50:16.919: INFO: stdout: ""
Sep  8 21:50:16.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 10.233.8.144 80'
Sep  8 21:50:17.165: INFO: stderr: "+ nc -v -z -w 2 10.233.8.144 80\nConnection to 10.233.8.144 80 port [tcp/http] succeeded!\n"
Sep  8 21:50:17.165: INFO: stdout: ""
Sep  8 21:50:17.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 30170'
Sep  8 21:50:17.409: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 30170\nConnection to 10.100.19.129 30170 port [tcp/*] succeeded!\n"
Sep  8 21:50:17.409: INFO: stdout: ""
Sep  8 21:50:17.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 30170'
Sep  8 21:50:17.667: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 30170\nConnection to 10.100.16.26 30170 port [tcp/*] succeeded!\n"
Sep  8 21:50:17.667: INFO: stdout: ""
Sep  8 21:50:17.667: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:17.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5381" for this suite. 09/08/23 21:50:17.791
------------------------------
â€¢ [SLOW TEST] [9.566 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:08.263
    Sep  8 21:50:08.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:50:08.267
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:08.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:08.328
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5381 09/08/23 21:50:08.341
    STEP: changing the ExternalName service to type=NodePort 09/08/23 21:50:08.362
    STEP: creating replication controller externalname-service in namespace services-5381 09/08/23 21:50:08.462
    I0908 21:50:08.490788      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5381, replica count: 2
    I0908 21:50:11.542295      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 21:50:11.542: INFO: Creating new exec pod
    Sep  8 21:50:11.567: INFO: Waiting up to 5m0s for pod "execpodhcf4h" in namespace "services-5381" to be "running"
    Sep  8 21:50:11.578: INFO: Pod "execpodhcf4h": Phase="Pending", Reason="", readiness=false. Elapsed: 11.789435ms
    Sep  8 21:50:13.599: INFO: Pod "execpodhcf4h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032697935s
    Sep  8 21:50:15.598: INFO: Pod "execpodhcf4h": Phase="Running", Reason="", readiness=true. Elapsed: 4.031562328s
    Sep  8 21:50:15.609: INFO: Pod "execpodhcf4h" satisfied condition "running"
    Sep  8 21:50:16.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  8 21:50:16.919: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  8 21:50:16.919: INFO: stdout: ""
    Sep  8 21:50:16.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 10.233.8.144 80'
    Sep  8 21:50:17.165: INFO: stderr: "+ nc -v -z -w 2 10.233.8.144 80\nConnection to 10.233.8.144 80 port [tcp/http] succeeded!\n"
    Sep  8 21:50:17.165: INFO: stdout: ""
    Sep  8 21:50:17.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 30170'
    Sep  8 21:50:17.409: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 30170\nConnection to 10.100.19.129 30170 port [tcp/*] succeeded!\n"
    Sep  8 21:50:17.409: INFO: stdout: ""
    Sep  8 21:50:17.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5381 exec execpodhcf4h -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 30170'
    Sep  8 21:50:17.667: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 30170\nConnection to 10.100.16.26 30170 port [tcp/*] succeeded!\n"
    Sep  8 21:50:17.667: INFO: stdout: ""
    Sep  8 21:50:17.667: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:17.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5381" for this suite. 09/08/23 21:50:17.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:17.836
Sep  8 21:50:17.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 21:50:17.838
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:17.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:17.905
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-7760/secret-test-894c9df1-aefe-4233-954a-7749cf8a24b9 09/08/23 21:50:17.912
STEP: Creating a pod to test consume secrets 09/08/23 21:50:17.941
Sep  8 21:50:17.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f" in namespace "secrets-7760" to be "Succeeded or Failed"
Sep  8 21:50:18.011: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.742547ms
Sep  8 21:50:20.022: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03811066s
Sep  8 21:50:22.023: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038881202s
STEP: Saw pod success 09/08/23 21:50:22.023
Sep  8 21:50:22.023: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f" satisfied condition "Succeeded or Failed"
Sep  8 21:50:22.036: INFO: Trying to get logs from node node-3 pod pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f container env-test: <nil>
STEP: delete the pod 09/08/23 21:50:22.075
Sep  8 21:50:22.140: INFO: Waiting for pod pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f to disappear
Sep  8 21:50:22.153: INFO: Pod pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:22.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7760" for this suite. 09/08/23 21:50:22.172
------------------------------
â€¢ [4.354 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:17.836
    Sep  8 21:50:17.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 21:50:17.838
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:17.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:17.905
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-7760/secret-test-894c9df1-aefe-4233-954a-7749cf8a24b9 09/08/23 21:50:17.912
    STEP: Creating a pod to test consume secrets 09/08/23 21:50:17.941
    Sep  8 21:50:17.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f" in namespace "secrets-7760" to be "Succeeded or Failed"
    Sep  8 21:50:18.011: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.742547ms
    Sep  8 21:50:20.022: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03811066s
    Sep  8 21:50:22.023: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038881202s
    STEP: Saw pod success 09/08/23 21:50:22.023
    Sep  8 21:50:22.023: INFO: Pod "pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f" satisfied condition "Succeeded or Failed"
    Sep  8 21:50:22.036: INFO: Trying to get logs from node node-3 pod pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f container env-test: <nil>
    STEP: delete the pod 09/08/23 21:50:22.075
    Sep  8 21:50:22.140: INFO: Waiting for pod pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f to disappear
    Sep  8 21:50:22.153: INFO: Pod pod-configmaps-34d56c28-03cd-45bb-a73f-7baa1aaf7b3f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:22.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7760" for this suite. 09/08/23 21:50:22.172
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:22.19
Sep  8 21:50:22.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replicaset 09/08/23 21:50:22.191
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:22.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:22.357
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Sep  8 21:50:22.371: INFO: Creating ReplicaSet my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301
Sep  8 21:50:22.390: INFO: Pod name my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301: Found 0 pods out of 1
Sep  8 21:50:27.406: INFO: Pod name my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301: Found 1 pods out of 1
Sep  8 21:50:27.406: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301" is running
Sep  8 21:50:27.406: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq" in namespace "replicaset-5995" to be "running"
Sep  8 21:50:27.415: INFO: Pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq": Phase="Running", Reason="", readiness=true. Elapsed: 8.827759ms
Sep  8 21:50:27.415: INFO: Pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq" satisfied condition "running"
Sep  8 21:50:27.415: INFO: Pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:22 +0000 UTC Reason: Message:}])
Sep  8 21:50:27.415: INFO: Trying to dial the pod
Sep  8 21:50:32.452: INFO: Controller my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301: Got expected result from replica 1 [my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq]: "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:32.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5995" for this suite. 09/08/23 21:50:32.468
------------------------------
â€¢ [SLOW TEST] [10.295 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:22.19
    Sep  8 21:50:22.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replicaset 09/08/23 21:50:22.191
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:22.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:22.357
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Sep  8 21:50:22.371: INFO: Creating ReplicaSet my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301
    Sep  8 21:50:22.390: INFO: Pod name my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301: Found 0 pods out of 1
    Sep  8 21:50:27.406: INFO: Pod name my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301: Found 1 pods out of 1
    Sep  8 21:50:27.406: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301" is running
    Sep  8 21:50:27.406: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq" in namespace "replicaset-5995" to be "running"
    Sep  8 21:50:27.415: INFO: Pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq": Phase="Running", Reason="", readiness=true. Elapsed: 8.827759ms
    Sep  8 21:50:27.415: INFO: Pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq" satisfied condition "running"
    Sep  8 21:50:27.415: INFO: Pod "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:22 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-09-08 21:50:22 +0000 UTC Reason: Message:}])
    Sep  8 21:50:27.415: INFO: Trying to dial the pod
    Sep  8 21:50:32.452: INFO: Controller my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301: Got expected result from replica 1 [my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq]: "my-hostname-basic-e2acc1ec-5c60-43dd-ba9a-5e01be9d6301-jnsxq", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:32.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5995" for this suite. 09/08/23 21:50:32.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:32.487
Sep  8 21:50:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 21:50:32.488
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:32.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:32.533
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 09/08/23 21:50:32.538
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_tcp@PTR;sleep 1; done
 09/08/23 21:50:32.6
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_tcp@PTR;sleep 1; done
 09/08/23 21:50:32.6
STEP: creating a pod to probe DNS 09/08/23 21:50:32.6
STEP: submitting the pod to kubernetes 09/08/23 21:50:32.6
Sep  8 21:50:32.641: INFO: Waiting up to 15m0s for pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66" in namespace "dns-9078" to be "running"
Sep  8 21:50:32.667: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66": Phase="Pending", Reason="", readiness=false. Elapsed: 25.756155ms
Sep  8 21:50:34.675: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033644833s
Sep  8 21:50:36.677: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66": Phase="Running", Reason="", readiness=true. Elapsed: 4.035778507s
Sep  8 21:50:36.677: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66" satisfied condition "running"
STEP: retrieving the pod 09/08/23 21:50:36.677
STEP: looking for the results for each expected name from probers 09/08/23 21:50:36.731
Sep  8 21:50:36.741: INFO: Unable to read wheezy_udp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.767: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.775: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.829: INFO: Unable to read jessie_udp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.838: INFO: Unable to read jessie_tcp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.851: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.879: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
Sep  8 21:50:36.915: INFO: Lookups using dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66 failed for: [wheezy_udp@dns-test-service.dns-9078.svc.cluster.local wheezy_tcp@dns-test-service.dns-9078.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local jessie_udp@dns-test-service.dns-9078.svc.cluster.local jessie_tcp@dns-test-service.dns-9078.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local]

Sep  8 21:50:42.158: INFO: DNS probes using dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66 succeeded

STEP: deleting the pod 09/08/23 21:50:42.158
STEP: deleting the test service 09/08/23 21:50:42.236
STEP: deleting the test headless service 09/08/23 21:50:42.319
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:42.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9078" for this suite. 09/08/23 21:50:42.393
------------------------------
â€¢ [SLOW TEST] [9.922 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:32.487
    Sep  8 21:50:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 21:50:32.488
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:32.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:32.533
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 09/08/23 21:50:32.538
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_tcp@PTR;sleep 1; done
     09/08/23 21:50:32.6
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9078.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9078.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9078.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_udp@PTR;check="$$(dig +tcp +noall +answer +search 51.16.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.16.51_tcp@PTR;sleep 1; done
     09/08/23 21:50:32.6
    STEP: creating a pod to probe DNS 09/08/23 21:50:32.6
    STEP: submitting the pod to kubernetes 09/08/23 21:50:32.6
    Sep  8 21:50:32.641: INFO: Waiting up to 15m0s for pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66" in namespace "dns-9078" to be "running"
    Sep  8 21:50:32.667: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66": Phase="Pending", Reason="", readiness=false. Elapsed: 25.756155ms
    Sep  8 21:50:34.675: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033644833s
    Sep  8 21:50:36.677: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66": Phase="Running", Reason="", readiness=true. Elapsed: 4.035778507s
    Sep  8 21:50:36.677: INFO: Pod "dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 21:50:36.677
    STEP: looking for the results for each expected name from probers 09/08/23 21:50:36.731
    Sep  8 21:50:36.741: INFO: Unable to read wheezy_udp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.767: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.775: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.829: INFO: Unable to read jessie_udp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.838: INFO: Unable to read jessie_tcp@dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.851: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.879: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local from pod dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66: the server could not find the requested resource (get pods dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66)
    Sep  8 21:50:36.915: INFO: Lookups using dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66 failed for: [wheezy_udp@dns-test-service.dns-9078.svc.cluster.local wheezy_tcp@dns-test-service.dns-9078.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local jessie_udp@dns-test-service.dns-9078.svc.cluster.local jessie_tcp@dns-test-service.dns-9078.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9078.svc.cluster.local]

    Sep  8 21:50:42.158: INFO: DNS probes using dns-9078/dns-test-dff4f61d-1821-4ace-9e90-0da6c6a00f66 succeeded

    STEP: deleting the pod 09/08/23 21:50:42.158
    STEP: deleting the test service 09/08/23 21:50:42.236
    STEP: deleting the test headless service 09/08/23 21:50:42.319
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:42.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9078" for this suite. 09/08/23 21:50:42.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:42.416
Sep  8 21:50:42.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename init-container 09/08/23 21:50:42.418
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:42.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:42.471
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 09/08/23 21:50:42.476
Sep  8 21:50:42.476: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:47.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4675" for this suite. 09/08/23 21:50:48.007
------------------------------
â€¢ [SLOW TEST] [5.627 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:42.416
    Sep  8 21:50:42.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename init-container 09/08/23 21:50:42.418
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:42.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:42.471
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 09/08/23 21:50:42.476
    Sep  8 21:50:42.476: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:47.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4675" for this suite. 09/08/23 21:50:48.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:48.046
Sep  8 21:50:48.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename watch 09/08/23 21:50:48.048
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:48.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:48.102
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 09/08/23 21:50:48.114
STEP: modifying the configmap once 09/08/23 21:50:48.131
STEP: modifying the configmap a second time 09/08/23 21:50:48.154
STEP: deleting the configmap 09/08/23 21:50:48.179
STEP: creating a watch on configmaps from the resource version returned by the first update 09/08/23 21:50:48.207
STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/08/23 21:50:48.209
Sep  8 21:50:48.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2212  efc55103-7489-4096-aac6-82bf3e3edb1c 27232 0 2023-09-08 21:50:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-08 21:50:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 21:50:48.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2212  efc55103-7489-4096-aac6-82bf3e3edb1c 27233 0 2023-09-08 21:50:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-08 21:50:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:48.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2212" for this suite. 09/08/23 21:50:48.226
------------------------------
â€¢ [0.205 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:48.046
    Sep  8 21:50:48.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename watch 09/08/23 21:50:48.048
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:48.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:48.102
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 09/08/23 21:50:48.114
    STEP: modifying the configmap once 09/08/23 21:50:48.131
    STEP: modifying the configmap a second time 09/08/23 21:50:48.154
    STEP: deleting the configmap 09/08/23 21:50:48.179
    STEP: creating a watch on configmaps from the resource version returned by the first update 09/08/23 21:50:48.207
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 09/08/23 21:50:48.209
    Sep  8 21:50:48.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2212  efc55103-7489-4096-aac6-82bf3e3edb1c 27232 0 2023-09-08 21:50:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-08 21:50:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 21:50:48.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2212  efc55103-7489-4096-aac6-82bf3e3edb1c 27233 0 2023-09-08 21:50:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-09-08 21:50:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:48.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2212" for this suite. 09/08/23 21:50:48.226
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:48.251
Sep  8 21:50:48.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 21:50:48.253
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:48.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:48.363
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 21:50:48.414
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:50:48.858
STEP: Deploying the webhook pod 09/08/23 21:50:48.88
STEP: Wait for the deployment to be ready 09/08/23 21:50:48.908
Sep  8 21:50:48.940: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 21:50:50.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 50, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 50, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 50, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 50, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 21:50:53.006
STEP: Verifying the service has paired with the endpoint 09/08/23 21:50:53.045
Sep  8 21:50:54.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 09/08/23 21:50:54.057
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/08/23 21:50:54.063
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/08/23 21:50:54.063
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/08/23 21:50:54.064
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/08/23 21:50:54.067
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/08/23 21:50:54.067
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/08/23 21:50:54.069
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:54.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5073" for this suite. 09/08/23 21:50:54.244
STEP: Destroying namespace "webhook-5073-markers" for this suite. 09/08/23 21:50:54.273
------------------------------
â€¢ [SLOW TEST] [6.042 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:48.251
    Sep  8 21:50:48.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 21:50:48.253
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:48.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:48.363
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 21:50:48.414
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 21:50:48.858
    STEP: Deploying the webhook pod 09/08/23 21:50:48.88
    STEP: Wait for the deployment to be ready 09/08/23 21:50:48.908
    Sep  8 21:50:48.940: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 21:50:50.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 21, 50, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 50, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 50, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 50, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 21:50:53.006
    STEP: Verifying the service has paired with the endpoint 09/08/23 21:50:53.045
    Sep  8 21:50:54.045: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 09/08/23 21:50:54.057
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 09/08/23 21:50:54.063
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 09/08/23 21:50:54.063
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 09/08/23 21:50:54.064
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 09/08/23 21:50:54.067
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 09/08/23 21:50:54.067
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 09/08/23 21:50:54.069
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:54.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5073" for this suite. 09/08/23 21:50:54.244
    STEP: Destroying namespace "webhook-5073-markers" for this suite. 09/08/23 21:50:54.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:54.296
Sep  8 21:50:54.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-pred 09/08/23 21:50:54.297
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:54.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:54.353
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  8 21:50:54.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  8 21:50:54.398: INFO: Waiting for terminating namespaces to be deleted...
Sep  8 21:50:54.410: INFO: 
Logging pods the apiserver thinks is on node node-3 before test
Sep  8 21:50:54.432: INFO: cadvisor-lp4nn from default started at 2023-09-08 21:43:38 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.432: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 21:50:54.432: INFO: kube-prometheus-node-exporter-j7tkw from default started at 2023-09-08 21:43:35 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.432: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 21:50:54.432: INFO: netchecker-agent-hostnet-pthvh from default started at 2023-09-08 21:43:36 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.432: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:50:54.432: INFO: netchecker-agent-jkrf7 from default started at 2023-09-08 21:43:33 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:50:54.433: INFO: openebs-localpv-provisioner-5d6756bcd8-hwnnv from default started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 21:50:54.433: INFO: ingress-nginx-controller-rhxd4 from ingress-nginx started at 2023-09-08 21:43:41 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:50:54.433: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 21:50:54.433: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 21:50:54.433: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 21:50:54.433: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 21:50:54.433: INFO: metallb-controller-77b687f97-8fnfx from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:50:54.433: INFO: metallb-speaker-csr5m from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.433: INFO: 	Container speaker ready: true, restart count 0
Sep  8 21:50:54.433: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.434: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  8 21:50:54.434: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 21:50:54.434: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:50:54.434: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  8 21:50:54.434: INFO: 
Logging pods the apiserver thinks is on node node-4 before test
Sep  8 21:50:54.466: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 21:50:54.466: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  8 21:50:54.466: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-hffhp from default started at 2023-09-08 21:43:25 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  8 21:50:54.466: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 21:50:54.466: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:50:54.466: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 21:50:54.466: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container etcd ready: true, restart count 0
Sep  8 21:50:54.466: INFO: 	Container netchecker-server ready: true, restart count 1
Sep  8 21:50:54.466: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 21:50:54.466: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container controller ready: true, restart count 0
Sep  8 21:50:54.466: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  8 21:50:54.466: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 21:50:54.466: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 21:50:54.466: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container metrics-server ready: true, restart count 0
Sep  8 21:50:54.466: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 21:50:54.466: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 21:50:54.466: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container metallb-monitor ready: true, restart count 0
Sep  8 21:50:54.466: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container speaker ready: true, restart count 0
Sep  8 21:50:54.466: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container e2e ready: true, restart count 0
Sep  8 21:50:54.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:50:54.466: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 21:50:54.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 21:50:54.466: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node node-3 09/08/23 21:50:54.527
STEP: verifying the node has the label node node-4 09/08/23 21:50:54.587
Sep  8 21:50:54.642: INFO: Pod cadvisor-2bq6z requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod cadvisor-lp4nn requesting resource cpu=0m on Node node-3
Sep  8 21:50:54.642: INFO: Pod kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod kube-prometheus-kube-state-metrics-7787c6cfbc-hffhp requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod kube-prometheus-node-exporter-j7tkw requesting resource cpu=0m on Node node-3
Sep  8 21:50:54.642: INFO: Pod kube-prometheus-node-exporter-vzk9h requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod netchecker-agent-hostnet-47k69 requesting resource cpu=1m on Node node-4
Sep  8 21:50:54.642: INFO: Pod netchecker-agent-hostnet-pthvh requesting resource cpu=1m on Node node-3
Sep  8 21:50:54.642: INFO: Pod netchecker-agent-jkrf7 requesting resource cpu=1m on Node node-3
Sep  8 21:50:54.642: INFO: Pod netchecker-agent-t6llz requesting resource cpu=1m on Node node-4
Sep  8 21:50:54.642: INFO: Pod netchecker-server-57d55b464c-qrshp requesting resource cpu=150m on Node node-4
Sep  8 21:50:54.642: INFO: Pod openebs-localpv-provisioner-5d6756bcd8-9d9sf requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod openebs-localpv-provisioner-5d6756bcd8-hwnnv requesting resource cpu=0m on Node node-3
Sep  8 21:50:54.642: INFO: Pod ingress-nginx-controller-l2p2l requesting resource cpu=100m on Node node-4
Sep  8 21:50:54.642: INFO: Pod ingress-nginx-controller-rhxd4 requesting resource cpu=100m on Node node-3
Sep  8 21:50:54.642: INFO: Pod calico-kube-controllers-68fd66c797-c5t5m requesting resource cpu=30m on Node node-4
Sep  8 21:50:54.642: INFO: Pod calico-node-46wp7 requesting resource cpu=150m on Node node-3
Sep  8 21:50:54.642: INFO: Pod calico-node-bvxmx requesting resource cpu=150m on Node node-4
Sep  8 21:50:54.642: INFO: Pod kube-proxy-4nz2v requesting resource cpu=0m on Node node-3
Sep  8 21:50:54.642: INFO: Pod kube-proxy-wjfp8 requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod metrics-server-8468bb47f8-wjspw requesting resource cpu=100m on Node node-4
Sep  8 21:50:54.642: INFO: Pod nginx-proxy-node-3 requesting resource cpu=25m on Node node-3
Sep  8 21:50:54.642: INFO: Pod nginx-proxy-node-4 requesting resource cpu=25m on Node node-4
Sep  8 21:50:54.642: INFO: Pod nodelocaldns-j7fmg requesting resource cpu=100m on Node node-3
Sep  8 21:50:54.642: INFO: Pod nodelocaldns-mqlmh requesting resource cpu=100m on Node node-4
Sep  8 21:50:54.642: INFO: Pod metallb-controller-77b687f97-8fnfx requesting resource cpu=0m on Node node-3
Sep  8 21:50:54.642: INFO: Pod metallb-monitor-deployment-6947cfdbd8-kvjhq requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod metallb-speaker-csr5m requesting resource cpu=0m on Node node-3
Sep  8 21:50:54.642: INFO: Pod metallb-speaker-srxnf requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod sonobuoy requesting resource cpu=0m on Node node-3
Sep  8 21:50:54.642: INFO: Pod sonobuoy-e2e-job-5595c445240f482c requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v requesting resource cpu=0m on Node node-4
Sep  8 21:50:54.642: INFO: Pod sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p requesting resource cpu=0m on Node node-3
STEP: Starting Pods to consume most of the cluster CPU. 09/08/23 21:50:54.642
Sep  8 21:50:54.642: INFO: Creating a pod which consumes cpu=2536m on Node node-3
Sep  8 21:50:54.676: INFO: Creating a pod which consumes cpu=2340m on Node node-4
Sep  8 21:50:54.703: INFO: Waiting up to 5m0s for pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c" in namespace "sched-pred-8062" to be "running"
Sep  8 21:50:54.727: INFO: Pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.512489ms
Sep  8 21:50:56.744: INFO: Pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c": Phase="Running", Reason="", readiness=true. Elapsed: 2.040688892s
Sep  8 21:50:56.744: INFO: Pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c" satisfied condition "running"
Sep  8 21:50:56.744: INFO: Waiting up to 5m0s for pod "filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e" in namespace "sched-pred-8062" to be "running"
Sep  8 21:50:56.755: INFO: Pod "filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e": Phase="Running", Reason="", readiness=true. Elapsed: 10.935111ms
Sep  8 21:50:56.755: INFO: Pod "filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 09/08/23 21:50:56.755
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae2f3cd9d90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8062/filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c to node-3] 09/08/23 21:50:56.763
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae3319cea67], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/08/23 21:50:56.763
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae33347461e], Reason = [Created], Message = [Created container filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c] 09/08/23 21:50:56.763
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae33b73b5b5], Reason = [Started], Message = [Started container filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c] 09/08/23 21:50:56.764
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae2f4fe9f79], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8062/filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e to node-4] 09/08/23 21:50:56.764
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae333b8ca38], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/08/23 21:50:56.764
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae33546179a], Reason = [Created], Message = [Created container filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e] 09/08/23 21:50:56.764
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae33f78e6d0], Reason = [Started], Message = [Started container filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e] 09/08/23 21:50:56.764
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17830ae3709a7b95], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 09/08/23 21:50:56.809
STEP: removing the label node off the node node-3 09/08/23 21:50:57.793
STEP: verifying the node doesn't have the label node 09/08/23 21:50:57.837
STEP: removing the label node off the node node-4 09/08/23 21:50:57.861
STEP: verifying the node doesn't have the label node 09/08/23 21:50:57.894
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:50:57.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8062" for this suite. 09/08/23 21:50:57.925
------------------------------
â€¢ [3.663 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:54.296
    Sep  8 21:50:54.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-pred 09/08/23 21:50:54.297
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:54.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:54.353
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  8 21:50:54.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  8 21:50:54.398: INFO: Waiting for terminating namespaces to be deleted...
    Sep  8 21:50:54.410: INFO: 
    Logging pods the apiserver thinks is on node node-3 before test
    Sep  8 21:50:54.432: INFO: cadvisor-lp4nn from default started at 2023-09-08 21:43:38 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.432: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 21:50:54.432: INFO: kube-prometheus-node-exporter-j7tkw from default started at 2023-09-08 21:43:35 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.432: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 21:50:54.432: INFO: netchecker-agent-hostnet-pthvh from default started at 2023-09-08 21:43:36 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.432: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:50:54.432: INFO: netchecker-agent-jkrf7 from default started at 2023-09-08 21:43:33 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: openebs-localpv-provisioner-5d6756bcd8-hwnnv from default started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: ingress-nginx-controller-rhxd4 from ingress-nginx started at 2023-09-08 21:43:41 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: metallb-controller-77b687f97-8fnfx from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: metallb-speaker-csr5m from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.433: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 21:50:54.433: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.434: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  8 21:50:54.434: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 21:50:54.434: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:50:54.434: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  8 21:50:54.434: INFO: 
    Logging pods the apiserver thinks is on node node-4 before test
    Sep  8 21:50:54.466: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-hffhp from default started at 2023-09-08 21:43:25 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container etcd ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: 	Container netchecker-server ready: true, restart count 1
    Sep  8 21:50:54.466: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container controller ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container metallb-monitor ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container e2e ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 21:50:54.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 21:50:54.466: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node node-3 09/08/23 21:50:54.527
    STEP: verifying the node has the label node node-4 09/08/23 21:50:54.587
    Sep  8 21:50:54.642: INFO: Pod cadvisor-2bq6z requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod cadvisor-lp4nn requesting resource cpu=0m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod kube-prometheus-kube-state-metrics-7787c6cfbc-hffhp requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod kube-prometheus-node-exporter-j7tkw requesting resource cpu=0m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod kube-prometheus-node-exporter-vzk9h requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod netchecker-agent-hostnet-47k69 requesting resource cpu=1m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod netchecker-agent-hostnet-pthvh requesting resource cpu=1m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod netchecker-agent-jkrf7 requesting resource cpu=1m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod netchecker-agent-t6llz requesting resource cpu=1m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod netchecker-server-57d55b464c-qrshp requesting resource cpu=150m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod openebs-localpv-provisioner-5d6756bcd8-9d9sf requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod openebs-localpv-provisioner-5d6756bcd8-hwnnv requesting resource cpu=0m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod ingress-nginx-controller-l2p2l requesting resource cpu=100m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod ingress-nginx-controller-rhxd4 requesting resource cpu=100m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod calico-kube-controllers-68fd66c797-c5t5m requesting resource cpu=30m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod calico-node-46wp7 requesting resource cpu=150m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod calico-node-bvxmx requesting resource cpu=150m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod kube-proxy-4nz2v requesting resource cpu=0m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod kube-proxy-wjfp8 requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod metrics-server-8468bb47f8-wjspw requesting resource cpu=100m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod nginx-proxy-node-3 requesting resource cpu=25m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod nginx-proxy-node-4 requesting resource cpu=25m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod nodelocaldns-j7fmg requesting resource cpu=100m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod nodelocaldns-mqlmh requesting resource cpu=100m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod metallb-controller-77b687f97-8fnfx requesting resource cpu=0m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod metallb-monitor-deployment-6947cfdbd8-kvjhq requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod metallb-speaker-csr5m requesting resource cpu=0m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod metallb-speaker-srxnf requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod sonobuoy requesting resource cpu=0m on Node node-3
    Sep  8 21:50:54.642: INFO: Pod sonobuoy-e2e-job-5595c445240f482c requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v requesting resource cpu=0m on Node node-4
    Sep  8 21:50:54.642: INFO: Pod sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p requesting resource cpu=0m on Node node-3
    STEP: Starting Pods to consume most of the cluster CPU. 09/08/23 21:50:54.642
    Sep  8 21:50:54.642: INFO: Creating a pod which consumes cpu=2536m on Node node-3
    Sep  8 21:50:54.676: INFO: Creating a pod which consumes cpu=2340m on Node node-4
    Sep  8 21:50:54.703: INFO: Waiting up to 5m0s for pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c" in namespace "sched-pred-8062" to be "running"
    Sep  8 21:50:54.727: INFO: Pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.512489ms
    Sep  8 21:50:56.744: INFO: Pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c": Phase="Running", Reason="", readiness=true. Elapsed: 2.040688892s
    Sep  8 21:50:56.744: INFO: Pod "filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c" satisfied condition "running"
    Sep  8 21:50:56.744: INFO: Waiting up to 5m0s for pod "filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e" in namespace "sched-pred-8062" to be "running"
    Sep  8 21:50:56.755: INFO: Pod "filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e": Phase="Running", Reason="", readiness=true. Elapsed: 10.935111ms
    Sep  8 21:50:56.755: INFO: Pod "filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 09/08/23 21:50:56.755
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae2f3cd9d90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8062/filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c to node-3] 09/08/23 21:50:56.763
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae3319cea67], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/08/23 21:50:56.763
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae33347461e], Reason = [Created], Message = [Created container filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c] 09/08/23 21:50:56.763
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c.17830ae33b73b5b5], Reason = [Started], Message = [Started container filler-pod-54df4700-2b88-46ca-bb23-e7acaf62864c] 09/08/23 21:50:56.764
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae2f4fe9f79], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8062/filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e to node-4] 09/08/23 21:50:56.764
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae333b8ca38], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 09/08/23 21:50:56.764
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae33546179a], Reason = [Created], Message = [Created container filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e] 09/08/23 21:50:56.764
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e.17830ae33f78e6d0], Reason = [Started], Message = [Started container filler-pod-93bf7a3c-044c-46dc-9fb3-7ec3dffcd54e] 09/08/23 21:50:56.764
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17830ae3709a7b95], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 09/08/23 21:50:56.809
    STEP: removing the label node off the node node-3 09/08/23 21:50:57.793
    STEP: verifying the node doesn't have the label node 09/08/23 21:50:57.837
    STEP: removing the label node off the node node-4 09/08/23 21:50:57.861
    STEP: verifying the node doesn't have the label node 09/08/23 21:50:57.894
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:50:57.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8062" for this suite. 09/08/23 21:50:57.925
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:50:57.959
Sep  8 21:50:57.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:50:57.961
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:57.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:58.008
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 09/08/23 21:50:58.014
STEP: Creating a ResourceQuota 09/08/23 21:51:03.041
STEP: Ensuring resource quota status is calculated 09/08/23 21:51:03.058
STEP: Creating a ReplicaSet 09/08/23 21:51:05.07
STEP: Ensuring resource quota status captures replicaset creation 09/08/23 21:51:05.136
STEP: Deleting a ReplicaSet 09/08/23 21:51:07.152
STEP: Ensuring resource quota status released usage 09/08/23 21:51:07.171
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:51:09.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5232" for this suite. 09/08/23 21:51:09.194
------------------------------
â€¢ [SLOW TEST] [11.255 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:50:57.959
    Sep  8 21:50:57.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:50:57.961
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:50:57.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:50:58.008
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 09/08/23 21:50:58.014
    STEP: Creating a ResourceQuota 09/08/23 21:51:03.041
    STEP: Ensuring resource quota status is calculated 09/08/23 21:51:03.058
    STEP: Creating a ReplicaSet 09/08/23 21:51:05.07
    STEP: Ensuring resource quota status captures replicaset creation 09/08/23 21:51:05.136
    STEP: Deleting a ReplicaSet 09/08/23 21:51:07.152
    STEP: Ensuring resource quota status released usage 09/08/23 21:51:07.171
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:51:09.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5232" for this suite. 09/08/23 21:51:09.194
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:51:09.215
Sep  8 21:51:09.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:51:09.216
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:09.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:09.332
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 09/08/23 21:51:09.349
STEP: waiting for available Endpoint 09/08/23 21:51:09.363
STEP: listing all Endpoints 09/08/23 21:51:09.368
STEP: updating the Endpoint 09/08/23 21:51:09.383
STEP: fetching the Endpoint 09/08/23 21:51:09.401
STEP: patching the Endpoint 09/08/23 21:51:09.408
STEP: fetching the Endpoint 09/08/23 21:51:09.433
STEP: deleting the Endpoint by Collection 09/08/23 21:51:09.441
STEP: waiting for Endpoint deletion 09/08/23 21:51:09.467
STEP: fetching the Endpoint 09/08/23 21:51:09.47
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:51:09.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3159" for this suite. 09/08/23 21:51:09.499
------------------------------
â€¢ [0.323 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:51:09.215
    Sep  8 21:51:09.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:51:09.216
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:09.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:09.332
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 09/08/23 21:51:09.349
    STEP: waiting for available Endpoint 09/08/23 21:51:09.363
    STEP: listing all Endpoints 09/08/23 21:51:09.368
    STEP: updating the Endpoint 09/08/23 21:51:09.383
    STEP: fetching the Endpoint 09/08/23 21:51:09.401
    STEP: patching the Endpoint 09/08/23 21:51:09.408
    STEP: fetching the Endpoint 09/08/23 21:51:09.433
    STEP: deleting the Endpoint by Collection 09/08/23 21:51:09.441
    STEP: waiting for Endpoint deletion 09/08/23 21:51:09.467
    STEP: fetching the Endpoint 09/08/23 21:51:09.47
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:51:09.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3159" for this suite. 09/08/23 21:51:09.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:51:09.539
Sep  8 21:51:09.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 21:51:09.54
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:09.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:09.603
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Sep  8 21:51:09.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:51:15.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6348" for this suite. 09/08/23 21:51:15.726
------------------------------
â€¢ [SLOW TEST] [6.208 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:51:09.539
    Sep  8 21:51:09.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 21:51:09.54
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:09.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:09.603
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Sep  8 21:51:09.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:51:15.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6348" for this suite. 09/08/23 21:51:15.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:51:15.748
Sep  8 21:51:15.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename security-context-test 09/08/23 21:51:15.753
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:15.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:15.835
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Sep  8 21:51:15.861: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23" in namespace "security-context-test-4703" to be "Succeeded or Failed"
Sep  8 21:51:15.926: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 64.98541ms
Sep  8 21:51:17.934: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073578362s
Sep  8 21:51:19.955: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09416474s
Sep  8 21:51:21.936: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075236221s
Sep  8 21:51:23.947: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.086134946s
Sep  8 21:51:23.947: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  8 21:51:23.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4703" for this suite. 09/08/23 21:51:23.991
------------------------------
â€¢ [SLOW TEST] [8.275 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:51:15.748
    Sep  8 21:51:15.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename security-context-test 09/08/23 21:51:15.753
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:15.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:15.835
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Sep  8 21:51:15.861: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23" in namespace "security-context-test-4703" to be "Succeeded or Failed"
    Sep  8 21:51:15.926: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 64.98541ms
    Sep  8 21:51:17.934: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073578362s
    Sep  8 21:51:19.955: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.09416474s
    Sep  8 21:51:21.936: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075236221s
    Sep  8 21:51:23.947: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.086134946s
    Sep  8 21:51:23.947: INFO: Pod "alpine-nnp-false-fae84e8f-8f48-4e41-9b9c-ed61b5ae3a23" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:51:23.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4703" for this suite. 09/08/23 21:51:23.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:51:24.025
Sep  8 21:51:24.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 21:51:24.027
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:24.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:24.083
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Sep  8 21:51:24.111: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  8 21:51:29.121: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/08/23 21:51:29.121
Sep  8 21:51:29.121: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  8 21:51:31.133: INFO: Creating deployment "test-rollover-deployment"
Sep  8 21:51:31.182: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  8 21:51:33.196: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  8 21:51:33.222: INFO: Ensure that both replica sets have 1 created replica
Sep  8 21:51:33.237: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  8 21:51:33.263: INFO: Updating deployment test-rollover-deployment
Sep  8 21:51:33.263: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  8 21:51:35.325: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  8 21:51:35.352: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  8 21:51:35.371: INFO: all replica sets need to contain the pod-template-hash label
Sep  8 21:51:35.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:51:37.393: INFO: all replica sets need to contain the pod-template-hash label
Sep  8 21:51:37.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:51:39.406: INFO: all replica sets need to contain the pod-template-hash label
Sep  8 21:51:39.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:51:41.402: INFO: all replica sets need to contain the pod-template-hash label
Sep  8 21:51:41.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:51:43.390: INFO: all replica sets need to contain the pod-template-hash label
Sep  8 21:51:43.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:51:45.410: INFO: 
Sep  8 21:51:45.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  8 21:51:47.392: INFO: 
Sep  8 21:51:47.392: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 21:51:47.416: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5286  643679a4-3a68-4d3e-b692-9038ff708064 27826 2 2023-09-08 21:51:31 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ec5268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-08 21:51:31 +0000 UTC,LastTransitionTime:2023-09-08 21:51:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-08 21:51:45 +0000 UTC,LastTransitionTime:2023-09-08 21:51:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  8 21:51:47.425: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5286  2f491d87-0145-4465-9fec-99a09427c8c4 27816 2 2023-09-08 21:51:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 643679a4-3a68-4d3e-b692-9038ff708064 0xc003112e27 0xc003112e28}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"643679a4-3a68-4d3e-b692-9038ff708064\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003112ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:51:47.425: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  8 21:51:47.425: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5286  827a4da5-5873-4757-a50c-619776fc7819 27825 2 2023-09-08 21:51:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 643679a4-3a68-4d3e-b692-9038ff708064 0xc003112cf7 0xc003112cf8}] [] [{e2e.test Update apps/v1 2023-09-08 21:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"643679a4-3a68-4d3e-b692-9038ff708064\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003112db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:51:47.426: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5286  3a9ffdbb-8056-491e-9b72-0b37659f2127 27751 2 2023-09-08 21:51:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 643679a4-3a68-4d3e-b692-9038ff708064 0xc003112f47 0xc003112f48}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"643679a4-3a68-4d3e-b692-9038ff708064\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003112ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:51:47.435: INFO: Pod "test-rollover-deployment-6c6df9974f-t6kgb" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-t6kgb test-rollover-deployment-6c6df9974f- deployment-5286  e21098f1-59c3-458c-a83e-63f7726bd2b8 27767 0 2023-09-08 21:51:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:13b19ed4bc1a04953cf61443610f9c4810f566ae82ddf9b191d1056ea27eecd8 cni.projectcalico.org/podIP:10.233.75.66/32 cni.projectcalico.org/podIPs:10.233.75.66/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2f491d87-0145-4465-9fec-99a09427c8c4 0xc003113567 0xc003113568}] [] [{kube-controller-manager Update v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f491d87-0145-4465-9fec-99a09427c8c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:51:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqxkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqxkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.66,StartTime:2023-09-08 21:51:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:51:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://b251ba7639532f02b54ef2ef3199b698ffecad007f46858de0ea97bbf71a80fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 21:51:47.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5286" for this suite. 09/08/23 21:51:47.452
------------------------------
â€¢ [SLOW TEST] [23.449 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:51:24.025
    Sep  8 21:51:24.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 21:51:24.027
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:24.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:24.083
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Sep  8 21:51:24.111: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Sep  8 21:51:29.121: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/08/23 21:51:29.121
    Sep  8 21:51:29.121: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Sep  8 21:51:31.133: INFO: Creating deployment "test-rollover-deployment"
    Sep  8 21:51:31.182: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Sep  8 21:51:33.196: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Sep  8 21:51:33.222: INFO: Ensure that both replica sets have 1 created replica
    Sep  8 21:51:33.237: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Sep  8 21:51:33.263: INFO: Updating deployment test-rollover-deployment
    Sep  8 21:51:33.263: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Sep  8 21:51:35.325: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Sep  8 21:51:35.352: INFO: Make sure deployment "test-rollover-deployment" is complete
    Sep  8 21:51:35.371: INFO: all replica sets need to contain the pod-template-hash label
    Sep  8 21:51:35.371: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:51:37.393: INFO: all replica sets need to contain the pod-template-hash label
    Sep  8 21:51:37.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:51:39.406: INFO: all replica sets need to contain the pod-template-hash label
    Sep  8 21:51:39.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:51:41.402: INFO: all replica sets need to contain the pod-template-hash label
    Sep  8 21:51:41.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:51:43.390: INFO: all replica sets need to contain the pod-template-hash label
    Sep  8 21:51:43.390: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:51:45.410: INFO: 
    Sep  8 21:51:45.410: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 51, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 51, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Sep  8 21:51:47.392: INFO: 
    Sep  8 21:51:47.392: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 21:51:47.416: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5286  643679a4-3a68-4d3e-b692-9038ff708064 27826 2 2023-09-08 21:51:31 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ec5268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-09-08 21:51:31 +0000 UTC,LastTransitionTime:2023-09-08 21:51:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-09-08 21:51:45 +0000 UTC,LastTransitionTime:2023-09-08 21:51:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  8 21:51:47.425: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-5286  2f491d87-0145-4465-9fec-99a09427c8c4 27816 2 2023-09-08 21:51:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 643679a4-3a68-4d3e-b692-9038ff708064 0xc003112e27 0xc003112e28}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"643679a4-3a68-4d3e-b692-9038ff708064\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003112ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:51:47.425: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Sep  8 21:51:47.425: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5286  827a4da5-5873-4757-a50c-619776fc7819 27825 2 2023-09-08 21:51:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 643679a4-3a68-4d3e-b692-9038ff708064 0xc003112cf7 0xc003112cf8}] [] [{e2e.test Update apps/v1 2023-09-08 21:51:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"643679a4-3a68-4d3e-b692-9038ff708064\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003112db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:51:47.426: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-5286  3a9ffdbb-8056-491e-9b72-0b37659f2127 27751 2 2023-09-08 21:51:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 643679a4-3a68-4d3e-b692-9038ff708064 0xc003112f47 0xc003112f48}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"643679a4-3a68-4d3e-b692-9038ff708064\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003112ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:51:47.435: INFO: Pod "test-rollover-deployment-6c6df9974f-t6kgb" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-t6kgb test-rollover-deployment-6c6df9974f- deployment-5286  e21098f1-59c3-458c-a83e-63f7726bd2b8 27767 0 2023-09-08 21:51:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:13b19ed4bc1a04953cf61443610f9c4810f566ae82ddf9b191d1056ea27eecd8 cni.projectcalico.org/podIP:10.233.75.66/32 cni.projectcalico.org/podIPs:10.233.75.66/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2f491d87-0145-4465-9fec-99a09427c8c4 0xc003113567 0xc003113568}] [] [{kube-controller-manager Update v1 2023-09-08 21:51:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f491d87-0145-4465-9fec-99a09427c8c4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:51:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.66\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqxkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqxkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:51:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.66,StartTime:2023-09-08 21:51:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:51:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://b251ba7639532f02b54ef2ef3199b698ffecad007f46858de0ea97bbf71a80fb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:51:47.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5286" for this suite. 09/08/23 21:51:47.452
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:51:47.477
Sep  8 21:51:47.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:51:47.478
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:47.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:47.532
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-7207 09/08/23 21:51:47.539
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[] 09/08/23 21:51:47.585
Sep  8 21:51:47.623: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7207 09/08/23 21:51:47.623
Sep  8 21:51:47.647: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7207" to be "running and ready"
Sep  8 21:51:47.664: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.72044ms
Sep  8 21:51:47.664: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:51:49.678: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030554525s
Sep  8 21:51:49.678: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  8 21:51:49.678: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[pod1:[100]] 09/08/23 21:51:49.687
Sep  8 21:51:49.727: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7207 09/08/23 21:51:49.728
Sep  8 21:51:49.747: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7207" to be "running and ready"
Sep  8 21:51:49.760: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.236646ms
Sep  8 21:51:49.760: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:51:51.775: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.02789977s
Sep  8 21:51:51.775: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  8 21:51:51.775: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[pod1:[100] pod2:[101]] 09/08/23 21:51:51.787
Sep  8 21:51:51.821: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 09/08/23 21:51:51.821
Sep  8 21:51:51.821: INFO: Creating new exec pod
Sep  8 21:51:51.833: INFO: Waiting up to 5m0s for pod "execpodfs28r" in namespace "services-7207" to be "running"
Sep  8 21:51:51.864: INFO: Pod "execpodfs28r": Phase="Pending", Reason="", readiness=false. Elapsed: 30.109932ms
Sep  8 21:51:53.884: INFO: Pod "execpodfs28r": Phase="Running", Reason="", readiness=true. Elapsed: 2.050330497s
Sep  8 21:51:53.884: INFO: Pod "execpodfs28r" satisfied condition "running"
Sep  8 21:51:54.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Sep  8 21:51:55.138: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Sep  8 21:51:55.138: INFO: stdout: ""
Sep  8 21:51:55.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 10.233.9.131 80'
Sep  8 21:51:55.387: INFO: stderr: "+ nc -v -z -w 2 10.233.9.131 80\nConnection to 10.233.9.131 80 port [tcp/http] succeeded!\n"
Sep  8 21:51:55.387: INFO: stdout: ""
Sep  8 21:51:55.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Sep  8 21:51:55.621: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Sep  8 21:51:55.621: INFO: stdout: ""
Sep  8 21:51:55.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 10.233.9.131 81'
Sep  8 21:51:55.834: INFO: stderr: "+ nc -v -z -w 2 10.233.9.131 81\nConnection to 10.233.9.131 81 port [tcp/*] succeeded!\n"
Sep  8 21:51:55.834: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7207 09/08/23 21:51:55.834
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[pod2:[101]] 09/08/23 21:51:55.884
Sep  8 21:51:55.942: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7207 09/08/23 21:51:55.942
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[] 09/08/23 21:51:55.999
Sep  8 21:51:56.034: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:51:56.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7207" for this suite. 09/08/23 21:51:56.121
------------------------------
â€¢ [SLOW TEST] [8.659 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:51:47.477
    Sep  8 21:51:47.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:51:47.478
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:47.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:47.532
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-7207 09/08/23 21:51:47.539
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[] 09/08/23 21:51:47.585
    Sep  8 21:51:47.623: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7207 09/08/23 21:51:47.623
    Sep  8 21:51:47.647: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7207" to be "running and ready"
    Sep  8 21:51:47.664: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.72044ms
    Sep  8 21:51:47.664: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:51:49.678: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.030554525s
    Sep  8 21:51:49.678: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  8 21:51:49.678: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[pod1:[100]] 09/08/23 21:51:49.687
    Sep  8 21:51:49.727: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7207 09/08/23 21:51:49.728
    Sep  8 21:51:49.747: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7207" to be "running and ready"
    Sep  8 21:51:49.760: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.236646ms
    Sep  8 21:51:49.760: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:51:51.775: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.02789977s
    Sep  8 21:51:51.775: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  8 21:51:51.775: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[pod1:[100] pod2:[101]] 09/08/23 21:51:51.787
    Sep  8 21:51:51.821: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 09/08/23 21:51:51.821
    Sep  8 21:51:51.821: INFO: Creating new exec pod
    Sep  8 21:51:51.833: INFO: Waiting up to 5m0s for pod "execpodfs28r" in namespace "services-7207" to be "running"
    Sep  8 21:51:51.864: INFO: Pod "execpodfs28r": Phase="Pending", Reason="", readiness=false. Elapsed: 30.109932ms
    Sep  8 21:51:53.884: INFO: Pod "execpodfs28r": Phase="Running", Reason="", readiness=true. Elapsed: 2.050330497s
    Sep  8 21:51:53.884: INFO: Pod "execpodfs28r" satisfied condition "running"
    Sep  8 21:51:54.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Sep  8 21:51:55.138: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Sep  8 21:51:55.138: INFO: stdout: ""
    Sep  8 21:51:55.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 10.233.9.131 80'
    Sep  8 21:51:55.387: INFO: stderr: "+ nc -v -z -w 2 10.233.9.131 80\nConnection to 10.233.9.131 80 port [tcp/http] succeeded!\n"
    Sep  8 21:51:55.387: INFO: stdout: ""
    Sep  8 21:51:55.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Sep  8 21:51:55.621: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Sep  8 21:51:55.621: INFO: stdout: ""
    Sep  8 21:51:55.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7207 exec execpodfs28r -- /bin/sh -x -c nc -v -z -w 2 10.233.9.131 81'
    Sep  8 21:51:55.834: INFO: stderr: "+ nc -v -z -w 2 10.233.9.131 81\nConnection to 10.233.9.131 81 port [tcp/*] succeeded!\n"
    Sep  8 21:51:55.834: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7207 09/08/23 21:51:55.834
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[pod2:[101]] 09/08/23 21:51:55.884
    Sep  8 21:51:55.942: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7207 09/08/23 21:51:55.942
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7207 to expose endpoints map[] 09/08/23 21:51:55.999
    Sep  8 21:51:56.034: INFO: successfully validated that service multi-endpoint-test in namespace services-7207 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:51:56.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7207" for this suite. 09/08/23 21:51:56.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:51:56.136
Sep  8 21:51:56.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 21:51:56.139
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:56.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:56.207
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/08/23 21:51:56.215
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 09/08/23 21:51:56.215
STEP: creating a pod to probe DNS 09/08/23 21:51:56.215
STEP: submitting the pod to kubernetes 09/08/23 21:51:56.215
Sep  8 21:51:56.238: INFO: Waiting up to 15m0s for pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9" in namespace "dns-5403" to be "running"
Sep  8 21:51:56.252: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.571321ms
Sep  8 21:51:58.295: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056807358s
Sep  8 21:52:00.261: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9": Phase="Running", Reason="", readiness=true. Elapsed: 4.02270792s
Sep  8 21:52:00.261: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9" satisfied condition "running"
STEP: retrieving the pod 09/08/23 21:52:00.261
STEP: looking for the results for each expected name from probers 09/08/23 21:52:00.271
Sep  8 21:52:00.324: INFO: DNS probes using dns-5403/dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9 succeeded

STEP: deleting the pod 09/08/23 21:52:00.324
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 21:52:00.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5403" for this suite. 09/08/23 21:52:00.436
------------------------------
â€¢ [4.326 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:51:56.136
    Sep  8 21:51:56.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 21:51:56.139
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:51:56.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:51:56.207
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/08/23 21:51:56.215
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     09/08/23 21:51:56.215
    STEP: creating a pod to probe DNS 09/08/23 21:51:56.215
    STEP: submitting the pod to kubernetes 09/08/23 21:51:56.215
    Sep  8 21:51:56.238: INFO: Waiting up to 15m0s for pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9" in namespace "dns-5403" to be "running"
    Sep  8 21:51:56.252: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.571321ms
    Sep  8 21:51:58.295: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056807358s
    Sep  8 21:52:00.261: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9": Phase="Running", Reason="", readiness=true. Elapsed: 4.02270792s
    Sep  8 21:52:00.261: INFO: Pod "dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 21:52:00.261
    STEP: looking for the results for each expected name from probers 09/08/23 21:52:00.271
    Sep  8 21:52:00.324: INFO: DNS probes using dns-5403/dns-test-b0a944a7-c774-492d-94b1-7013c69b0ea9 succeeded

    STEP: deleting the pod 09/08/23 21:52:00.324
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:52:00.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5403" for this suite. 09/08/23 21:52:00.436
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:52:00.467
Sep  8 21:52:00.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:52:00.47
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:00.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:00.526
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 09/08/23 21:52:00.53
STEP: Creating a ResourceQuota 09/08/23 21:52:05.539
STEP: Ensuring resource quota status is calculated 09/08/23 21:52:05.549
STEP: Creating a Pod that fits quota 09/08/23 21:52:07.559
STEP: Ensuring ResourceQuota status captures the pod usage 09/08/23 21:52:07.6
STEP: Not allowing a pod to be created that exceeds remaining quota 09/08/23 21:52:09.612
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/08/23 21:52:09.621
STEP: Ensuring a pod cannot update its resource requirements 09/08/23 21:52:09.626
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/08/23 21:52:09.637
STEP: Deleting the pod 09/08/23 21:52:11.65
STEP: Ensuring resource quota status released the pod usage 09/08/23 21:52:11.716
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:52:13.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5693" for this suite. 09/08/23 21:52:13.744
------------------------------
â€¢ [SLOW TEST] [13.296 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:52:00.467
    Sep  8 21:52:00.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:52:00.47
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:00.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:00.526
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 09/08/23 21:52:00.53
    STEP: Creating a ResourceQuota 09/08/23 21:52:05.539
    STEP: Ensuring resource quota status is calculated 09/08/23 21:52:05.549
    STEP: Creating a Pod that fits quota 09/08/23 21:52:07.559
    STEP: Ensuring ResourceQuota status captures the pod usage 09/08/23 21:52:07.6
    STEP: Not allowing a pod to be created that exceeds remaining quota 09/08/23 21:52:09.612
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 09/08/23 21:52:09.621
    STEP: Ensuring a pod cannot update its resource requirements 09/08/23 21:52:09.626
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 09/08/23 21:52:09.637
    STEP: Deleting the pod 09/08/23 21:52:11.65
    STEP: Ensuring resource quota status released the pod usage 09/08/23 21:52:11.716
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:52:13.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5693" for this suite. 09/08/23 21:52:13.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:52:13.766
Sep  8 21:52:13.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename job 09/08/23 21:52:13.767
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:13.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:13.808
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 09/08/23 21:52:13.819
STEP: Ensuring active pods == parallelism 09/08/23 21:52:13.836
STEP: Orphaning one of the Job's Pods 09/08/23 21:52:15.847
Sep  8 21:52:16.407: INFO: Successfully updated pod "adopt-release-nxgw4"
STEP: Checking that the Job readopts the Pod 09/08/23 21:52:16.407
Sep  8 21:52:16.409: INFO: Waiting up to 15m0s for pod "adopt-release-nxgw4" in namespace "job-9624" to be "adopted"
Sep  8 21:52:16.427: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 17.784696ms
Sep  8 21:52:18.437: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028311439s
Sep  8 21:52:18.437: INFO: Pod "adopt-release-nxgw4" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 09/08/23 21:52:18.437
Sep  8 21:52:18.979: INFO: Successfully updated pod "adopt-release-nxgw4"
STEP: Checking that the Job releases the Pod 09/08/23 21:52:18.979
Sep  8 21:52:18.979: INFO: Waiting up to 15m0s for pod "adopt-release-nxgw4" in namespace "job-9624" to be "released"
Sep  8 21:52:19.000: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 20.265407ms
Sep  8 21:52:21.021: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 2.041652729s
Sep  8 21:52:21.021: INFO: Pod "adopt-release-nxgw4" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  8 21:52:21.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9624" for this suite. 09/08/23 21:52:21.036
------------------------------
â€¢ [SLOW TEST] [7.296 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:52:13.766
    Sep  8 21:52:13.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename job 09/08/23 21:52:13.767
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:13.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:13.808
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 09/08/23 21:52:13.819
    STEP: Ensuring active pods == parallelism 09/08/23 21:52:13.836
    STEP: Orphaning one of the Job's Pods 09/08/23 21:52:15.847
    Sep  8 21:52:16.407: INFO: Successfully updated pod "adopt-release-nxgw4"
    STEP: Checking that the Job readopts the Pod 09/08/23 21:52:16.407
    Sep  8 21:52:16.409: INFO: Waiting up to 15m0s for pod "adopt-release-nxgw4" in namespace "job-9624" to be "adopted"
    Sep  8 21:52:16.427: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 17.784696ms
    Sep  8 21:52:18.437: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 2.028311439s
    Sep  8 21:52:18.437: INFO: Pod "adopt-release-nxgw4" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 09/08/23 21:52:18.437
    Sep  8 21:52:18.979: INFO: Successfully updated pod "adopt-release-nxgw4"
    STEP: Checking that the Job releases the Pod 09/08/23 21:52:18.979
    Sep  8 21:52:18.979: INFO: Waiting up to 15m0s for pod "adopt-release-nxgw4" in namespace "job-9624" to be "released"
    Sep  8 21:52:19.000: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 20.265407ms
    Sep  8 21:52:21.021: INFO: Pod "adopt-release-nxgw4": Phase="Running", Reason="", readiness=true. Elapsed: 2.041652729s
    Sep  8 21:52:21.021: INFO: Pod "adopt-release-nxgw4" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:52:21.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9624" for this suite. 09/08/23 21:52:21.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:52:21.065
Sep  8 21:52:21.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replicaset 09/08/23 21:52:21.067
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:21.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:21.138
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/08/23 21:52:21.145
Sep  8 21:52:21.168: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  8 21:52:26.178: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/08/23 21:52:26.178
STEP: getting scale subresource 09/08/23 21:52:26.178
STEP: updating a scale subresource 09/08/23 21:52:26.185
STEP: verifying the replicaset Spec.Replicas was modified 09/08/23 21:52:26.194
STEP: Patch a scale subresource 09/08/23 21:52:26.215
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:52:26.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-124" for this suite. 09/08/23 21:52:26.326
------------------------------
â€¢ [SLOW TEST] [5.299 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:52:21.065
    Sep  8 21:52:21.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replicaset 09/08/23 21:52:21.067
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:21.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:21.138
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 09/08/23 21:52:21.145
    Sep  8 21:52:21.168: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  8 21:52:26.178: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/08/23 21:52:26.178
    STEP: getting scale subresource 09/08/23 21:52:26.178
    STEP: updating a scale subresource 09/08/23 21:52:26.185
    STEP: verifying the replicaset Spec.Replicas was modified 09/08/23 21:52:26.194
    STEP: Patch a scale subresource 09/08/23 21:52:26.215
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:52:26.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-124" for this suite. 09/08/23 21:52:26.326
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:52:26.365
Sep  8 21:52:26.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-preemption 09/08/23 21:52:26.383
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:26.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:26.498
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  8 21:52:26.552: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  8 21:53:26.643: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 09/08/23 21:53:26.649
Sep  8 21:53:26.724: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  8 21:53:26.754: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  8 21:53:26.841: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  8 21:53:26.871: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/08/23 21:53:26.871
Sep  8 21:53:26.872: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8420" to be "running"
Sep  8 21:53:26.907: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 35.146203ms
Sep  8 21:53:28.916: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.04401438s
Sep  8 21:53:28.916: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  8 21:53:28.916: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8420" to be "running"
Sep  8 21:53:28.922: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067866ms
Sep  8 21:53:30.938: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.02188419s
Sep  8 21:53:30.938: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  8 21:53:30.938: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8420" to be "running"
Sep  8 21:53:30.947: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.214921ms
Sep  8 21:53:30.947: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  8 21:53:30.947: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8420" to be "running"
Sep  8 21:53:30.958: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.53224ms
Sep  8 21:53:30.958: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/08/23 21:53:30.958
Sep  8 21:53:30.989: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8420" to be "running"
Sep  8 21:53:31.021: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 31.354306ms
Sep  8 21:53:33.031: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041295384s
Sep  8 21:53:35.031: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041270871s
Sep  8 21:53:35.031: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:53:35.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8420" for this suite. 09/08/23 21:53:35.184
------------------------------
â€¢ [SLOW TEST] [68.858 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:52:26.365
    Sep  8 21:52:26.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-preemption 09/08/23 21:52:26.383
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:52:26.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:52:26.498
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  8 21:52:26.552: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  8 21:53:26.643: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 09/08/23 21:53:26.649
    Sep  8 21:53:26.724: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  8 21:53:26.754: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  8 21:53:26.841: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  8 21:53:26.871: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/08/23 21:53:26.871
    Sep  8 21:53:26.872: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8420" to be "running"
    Sep  8 21:53:26.907: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 35.146203ms
    Sep  8 21:53:28.916: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.04401438s
    Sep  8 21:53:28.916: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  8 21:53:28.916: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8420" to be "running"
    Sep  8 21:53:28.922: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067866ms
    Sep  8 21:53:30.938: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.02188419s
    Sep  8 21:53:30.938: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  8 21:53:30.938: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8420" to be "running"
    Sep  8 21:53:30.947: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.214921ms
    Sep  8 21:53:30.947: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  8 21:53:30.947: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8420" to be "running"
    Sep  8 21:53:30.958: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.53224ms
    Sep  8 21:53:30.958: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 09/08/23 21:53:30.958
    Sep  8 21:53:30.989: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8420" to be "running"
    Sep  8 21:53:31.021: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 31.354306ms
    Sep  8 21:53:33.031: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041295384s
    Sep  8 21:53:35.031: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041270871s
    Sep  8 21:53:35.031: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:53:35.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8420" for this suite. 09/08/23 21:53:35.184
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:53:35.223
Sep  8 21:53:35.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:53:35.224
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:35.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:35.314
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Sep  8 21:53:35.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/08/23 21:53:43.802
Sep  8 21:53:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 create -f -'
Sep  8 21:53:45.078: INFO: stderr: ""
Sep  8 21:53:45.078: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  8 21:53:45.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 delete e2e-test-crd-publish-openapi-1009-crds test-cr'
Sep  8 21:53:45.327: INFO: stderr: ""
Sep  8 21:53:45.327: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  8 21:53:45.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 apply -f -'
Sep  8 21:53:46.427: INFO: stderr: ""
Sep  8 21:53:46.427: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  8 21:53:46.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 delete e2e-test-crd-publish-openapi-1009-crds test-cr'
Sep  8 21:53:46.546: INFO: stderr: ""
Sep  8 21:53:46.546: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/08/23 21:53:46.546
Sep  8 21:53:46.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 explain e2e-test-crd-publish-openapi-1009-crds'
Sep  8 21:53:47.460: INFO: stderr: ""
Sep  8 21:53:47.460: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1009-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:53:50.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4282" for this suite. 09/08/23 21:53:50.329
------------------------------
â€¢ [SLOW TEST] [15.121 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:53:35.223
    Sep  8 21:53:35.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:53:35.224
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:35.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:35.314
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Sep  8 21:53:35.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/08/23 21:53:43.802
    Sep  8 21:53:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 create -f -'
    Sep  8 21:53:45.078: INFO: stderr: ""
    Sep  8 21:53:45.078: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  8 21:53:45.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 delete e2e-test-crd-publish-openapi-1009-crds test-cr'
    Sep  8 21:53:45.327: INFO: stderr: ""
    Sep  8 21:53:45.327: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Sep  8 21:53:45.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 apply -f -'
    Sep  8 21:53:46.427: INFO: stderr: ""
    Sep  8 21:53:46.427: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Sep  8 21:53:46.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 --namespace=crd-publish-openapi-4282 delete e2e-test-crd-publish-openapi-1009-crds test-cr'
    Sep  8 21:53:46.546: INFO: stderr: ""
    Sep  8 21:53:46.546: INFO: stdout: "e2e-test-crd-publish-openapi-1009-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/08/23 21:53:46.546
    Sep  8 21:53:46.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-4282 explain e2e-test-crd-publish-openapi-1009-crds'
    Sep  8 21:53:47.460: INFO: stderr: ""
    Sep  8 21:53:47.460: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1009-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:53:50.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4282" for this suite. 09/08/23 21:53:50.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:53:50.348
Sep  8 21:53:50.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename namespaces 09/08/23 21:53:50.355
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:50.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:50.413
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 09/08/23 21:53:50.418
Sep  8 21:53:50.442: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 09/08/23 21:53:50.442
Sep  8 21:53:50.457: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 09/08/23 21:53:50.457
Sep  8 21:53:50.496: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:53:50.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-352" for this suite. 09/08/23 21:53:50.513
------------------------------
â€¢ [0.187 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:53:50.348
    Sep  8 21:53:50.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename namespaces 09/08/23 21:53:50.355
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:50.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:50.413
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 09/08/23 21:53:50.418
    Sep  8 21:53:50.442: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 09/08/23 21:53:50.442
    Sep  8 21:53:50.457: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 09/08/23 21:53:50.457
    Sep  8 21:53:50.496: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:53:50.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-352" for this suite. 09/08/23 21:53:50.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:53:50.541
Sep  8 21:53:50.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 21:53:50.544
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:50.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:50.598
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 09/08/23 21:53:50.605
Sep  8 21:53:50.643: INFO: Waiting up to 5m0s for pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43" in namespace "emptydir-1292" to be "Succeeded or Failed"
Sep  8 21:53:50.653: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Pending", Reason="", readiness=false. Elapsed: 10.070697ms
Sep  8 21:53:52.663: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Running", Reason="", readiness=true. Elapsed: 2.019615837s
Sep  8 21:53:54.661: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Running", Reason="", readiness=false. Elapsed: 4.017900518s
Sep  8 21:53:56.669: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025522659s
STEP: Saw pod success 09/08/23 21:53:56.669
Sep  8 21:53:56.669: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43" satisfied condition "Succeeded or Failed"
Sep  8 21:53:56.677: INFO: Trying to get logs from node node-3 pod pod-8f595b59-84a9-47b7-881c-0786b9356c43 container test-container: <nil>
STEP: delete the pod 09/08/23 21:53:56.703
Sep  8 21:53:56.741: INFO: Waiting for pod pod-8f595b59-84a9-47b7-881c-0786b9356c43 to disappear
Sep  8 21:53:56.752: INFO: Pod pod-8f595b59-84a9-47b7-881c-0786b9356c43 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:53:56.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1292" for this suite. 09/08/23 21:53:56.768
------------------------------
â€¢ [SLOW TEST] [6.246 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:53:50.541
    Sep  8 21:53:50.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 21:53:50.544
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:50.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:50.598
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 09/08/23 21:53:50.605
    Sep  8 21:53:50.643: INFO: Waiting up to 5m0s for pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43" in namespace "emptydir-1292" to be "Succeeded or Failed"
    Sep  8 21:53:50.653: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Pending", Reason="", readiness=false. Elapsed: 10.070697ms
    Sep  8 21:53:52.663: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Running", Reason="", readiness=true. Elapsed: 2.019615837s
    Sep  8 21:53:54.661: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Running", Reason="", readiness=false. Elapsed: 4.017900518s
    Sep  8 21:53:56.669: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025522659s
    STEP: Saw pod success 09/08/23 21:53:56.669
    Sep  8 21:53:56.669: INFO: Pod "pod-8f595b59-84a9-47b7-881c-0786b9356c43" satisfied condition "Succeeded or Failed"
    Sep  8 21:53:56.677: INFO: Trying to get logs from node node-3 pod pod-8f595b59-84a9-47b7-881c-0786b9356c43 container test-container: <nil>
    STEP: delete the pod 09/08/23 21:53:56.703
    Sep  8 21:53:56.741: INFO: Waiting for pod pod-8f595b59-84a9-47b7-881c-0786b9356c43 to disappear
    Sep  8 21:53:56.752: INFO: Pod pod-8f595b59-84a9-47b7-881c-0786b9356c43 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:53:56.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1292" for this suite. 09/08/23 21:53:56.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:53:56.792
Sep  8 21:53:56.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename disruption 09/08/23 21:53:56.793
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:56.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:56.859
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 09/08/23 21:53:56.884
STEP: Updating PodDisruptionBudget status 09/08/23 21:53:58.933
STEP: Waiting for all pods to be running 09/08/23 21:53:58.954
Sep  8 21:53:58.968: INFO: running pods: 0 < 1
Sep  8 21:54:00.979: INFO: running pods: 0 < 1
STEP: locating a running pod 09/08/23 21:54:02.975
STEP: Waiting for the pdb to be processed 09/08/23 21:54:03.005
STEP: Patching PodDisruptionBudget status 09/08/23 21:54:03.053
STEP: Waiting for the pdb to be processed 09/08/23 21:54:03.079
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:03.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5120" for this suite. 09/08/23 21:54:03.11
------------------------------
â€¢ [SLOW TEST] [6.332 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:53:56.792
    Sep  8 21:53:56.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename disruption 09/08/23 21:53:56.793
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:53:56.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:53:56.859
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 09/08/23 21:53:56.884
    STEP: Updating PodDisruptionBudget status 09/08/23 21:53:58.933
    STEP: Waiting for all pods to be running 09/08/23 21:53:58.954
    Sep  8 21:53:58.968: INFO: running pods: 0 < 1
    Sep  8 21:54:00.979: INFO: running pods: 0 < 1
    STEP: locating a running pod 09/08/23 21:54:02.975
    STEP: Waiting for the pdb to be processed 09/08/23 21:54:03.005
    STEP: Patching PodDisruptionBudget status 09/08/23 21:54:03.053
    STEP: Waiting for the pdb to be processed 09/08/23 21:54:03.079
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:03.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5120" for this suite. 09/08/23 21:54:03.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:03.133
Sep  8 21:54:03.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:54:03.135
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:03.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:03.181
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-dce5801c-160a-477f-83ff-386247cbf03c 09/08/23 21:54:03.219
STEP: Creating the pod 09/08/23 21:54:03.236
Sep  8 21:54:03.260: INFO: Waiting up to 5m0s for pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15" in namespace "configmap-1301" to be "running and ready"
Sep  8 21:54:03.271: INFO: Pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15": Phase="Pending", Reason="", readiness=false. Elapsed: 11.518039ms
Sep  8 21:54:03.272: INFO: The phase of Pod pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:54:05.283: INFO: Pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15": Phase="Running", Reason="", readiness=true. Elapsed: 2.022870151s
Sep  8 21:54:05.283: INFO: The phase of Pod pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15 is Running (Ready = true)
Sep  8 21:54:05.284: INFO: Pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-dce5801c-160a-477f-83ff-386247cbf03c 09/08/23 21:54:05.317
STEP: waiting to observe update in volume 09/08/23 21:54:05.331
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:07.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1301" for this suite. 09/08/23 21:54:07.392
------------------------------
â€¢ [4.278 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:03.133
    Sep  8 21:54:03.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:54:03.135
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:03.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:03.181
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-dce5801c-160a-477f-83ff-386247cbf03c 09/08/23 21:54:03.219
    STEP: Creating the pod 09/08/23 21:54:03.236
    Sep  8 21:54:03.260: INFO: Waiting up to 5m0s for pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15" in namespace "configmap-1301" to be "running and ready"
    Sep  8 21:54:03.271: INFO: Pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15": Phase="Pending", Reason="", readiness=false. Elapsed: 11.518039ms
    Sep  8 21:54:03.272: INFO: The phase of Pod pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:54:05.283: INFO: Pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15": Phase="Running", Reason="", readiness=true. Elapsed: 2.022870151s
    Sep  8 21:54:05.283: INFO: The phase of Pod pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15 is Running (Ready = true)
    Sep  8 21:54:05.284: INFO: Pod "pod-configmaps-826f2d8c-8fb3-4b2e-a3f5-990a3278cb15" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-dce5801c-160a-477f-83ff-386247cbf03c 09/08/23 21:54:05.317
    STEP: waiting to observe update in volume 09/08/23 21:54:05.331
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:07.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1301" for this suite. 09/08/23 21:54:07.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:07.432
Sep  8 21:54:07.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:54:07.433
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:07.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:07.494
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 09/08/23 21:54:07.501
Sep  8 21:54:07.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: rename a version 09/08/23 21:54:18.499
STEP: check the new version name is served 09/08/23 21:54:18.534
STEP: check the old version name is removed 09/08/23 21:54:21.938
STEP: check the other version is not changed 09/08/23 21:54:22.928
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:28.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4657" for this suite. 09/08/23 21:54:28.147
------------------------------
â€¢ [SLOW TEST] [20.741 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:07.432
    Sep  8 21:54:07.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 21:54:07.433
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:07.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:07.494
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 09/08/23 21:54:07.501
    Sep  8 21:54:07.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: rename a version 09/08/23 21:54:18.499
    STEP: check the new version name is served 09/08/23 21:54:18.534
    STEP: check the old version name is removed 09/08/23 21:54:21.938
    STEP: check the other version is not changed 09/08/23 21:54:22.928
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:28.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4657" for this suite. 09/08/23 21:54:28.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:28.179
Sep  8 21:54:28.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename csiinlinevolumes 09/08/23 21:54:28.181
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:28.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:28.221
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 09/08/23 21:54:28.225
STEP: getting 09/08/23 21:54:28.294
STEP: listing in namespace 09/08/23 21:54:28.311
STEP: patching 09/08/23 21:54:28.317
STEP: deleting 09/08/23 21:54:28.334
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:28.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-7396" for this suite. 09/08/23 21:54:28.389
------------------------------
â€¢ [0.231 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:28.179
    Sep  8 21:54:28.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename csiinlinevolumes 09/08/23 21:54:28.181
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:28.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:28.221
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 09/08/23 21:54:28.225
    STEP: getting 09/08/23 21:54:28.294
    STEP: listing in namespace 09/08/23 21:54:28.311
    STEP: patching 09/08/23 21:54:28.317
    STEP: deleting 09/08/23 21:54:28.334
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:28.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-7396" for this suite. 09/08/23 21:54:28.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:28.418
Sep  8 21:54:28.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replicaset 09/08/23 21:54:28.42
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:28.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:28.498
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/08/23 21:54:28.504
Sep  8 21:54:28.548: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6903" to be "running and ready"
Sep  8 21:54:28.556: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.232678ms
Sep  8 21:54:28.556: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:54:30.577: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.029123916s
Sep  8 21:54:30.577: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Sep  8 21:54:30.577: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 09/08/23 21:54:30.586
STEP: Then the orphan pod is adopted 09/08/23 21:54:30.599
STEP: When the matched label of one of its pods change 09/08/23 21:54:31.629
Sep  8 21:54:31.641: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 09/08/23 21:54:31.677
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:32.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6903" for this suite. 09/08/23 21:54:32.729
------------------------------
â€¢ [4.330 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:28.418
    Sep  8 21:54:28.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replicaset 09/08/23 21:54:28.42
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:28.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:28.498
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 09/08/23 21:54:28.504
    Sep  8 21:54:28.548: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6903" to be "running and ready"
    Sep  8 21:54:28.556: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.232678ms
    Sep  8 21:54:28.556: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:54:30.577: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.029123916s
    Sep  8 21:54:30.577: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Sep  8 21:54:30.577: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 09/08/23 21:54:30.586
    STEP: Then the orphan pod is adopted 09/08/23 21:54:30.599
    STEP: When the matched label of one of its pods change 09/08/23 21:54:31.629
    Sep  8 21:54:31.641: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/08/23 21:54:31.677
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:32.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6903" for this suite. 09/08/23 21:54:32.729
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:32.749
Sep  8 21:54:32.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:54:32.75
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:32.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:32.793
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-c5502b09-3a1c-4171-aaff-7b557474e6d4 09/08/23 21:54:32.798
STEP: Creating a pod to test consume configMaps 09/08/23 21:54:32.809
Sep  8 21:54:32.836: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee" in namespace "projected-6592" to be "Succeeded or Failed"
Sep  8 21:54:32.872: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Pending", Reason="", readiness=false. Elapsed: 35.524289ms
Sep  8 21:54:34.882: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.045893727s
Sep  8 21:54:36.881: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Running", Reason="", readiness=false. Elapsed: 4.04455437s
Sep  8 21:54:38.886: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049099958s
STEP: Saw pod success 09/08/23 21:54:38.886
Sep  8 21:54:38.886: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee" satisfied condition "Succeeded or Failed"
Sep  8 21:54:38.897: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:54:38.936
Sep  8 21:54:38.972: INFO: Waiting for pod pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee to disappear
Sep  8 21:54:38.977: INFO: Pod pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:38.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6592" for this suite. 09/08/23 21:54:38.991
------------------------------
â€¢ [SLOW TEST] [6.258 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:32.749
    Sep  8 21:54:32.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:54:32.75
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:32.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:32.793
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-c5502b09-3a1c-4171-aaff-7b557474e6d4 09/08/23 21:54:32.798
    STEP: Creating a pod to test consume configMaps 09/08/23 21:54:32.809
    Sep  8 21:54:32.836: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee" in namespace "projected-6592" to be "Succeeded or Failed"
    Sep  8 21:54:32.872: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Pending", Reason="", readiness=false. Elapsed: 35.524289ms
    Sep  8 21:54:34.882: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.045893727s
    Sep  8 21:54:36.881: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Running", Reason="", readiness=false. Elapsed: 4.04455437s
    Sep  8 21:54:38.886: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049099958s
    STEP: Saw pod success 09/08/23 21:54:38.886
    Sep  8 21:54:38.886: INFO: Pod "pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee" satisfied condition "Succeeded or Failed"
    Sep  8 21:54:38.897: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:54:38.936
    Sep  8 21:54:38.972: INFO: Waiting for pod pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee to disappear
    Sep  8 21:54:38.977: INFO: Pod pod-projected-configmaps-d7a15eb0-1899-40c9-bc33-b57d3f50e8ee no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:38.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6592" for this suite. 09/08/23 21:54:38.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:39.007
Sep  8 21:54:39.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 21:54:39.009
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:39.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:39.067
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-89b06a0f-e052-4ddf-8db5-cf8f99d66b49 09/08/23 21:54:39.073
STEP: Creating a pod to test consume configMaps 09/08/23 21:54:39.081
Sep  8 21:54:39.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793" in namespace "configmap-8896" to be "Succeeded or Failed"
Sep  8 21:54:39.112: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Pending", Reason="", readiness=false. Elapsed: 10.46335ms
Sep  8 21:54:41.124: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Running", Reason="", readiness=true. Elapsed: 2.021610231s
Sep  8 21:54:43.127: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Running", Reason="", readiness=false. Elapsed: 4.024704851s
Sep  8 21:54:45.123: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020773535s
STEP: Saw pod success 09/08/23 21:54:45.123
Sep  8 21:54:45.123: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793" satisfied condition "Succeeded or Failed"
Sep  8 21:54:45.147: INFO: Trying to get logs from node node-3 pod pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:54:45.177
Sep  8 21:54:45.232: INFO: Waiting for pod pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793 to disappear
Sep  8 21:54:45.248: INFO: Pod pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8896" for this suite. 09/08/23 21:54:45.284
------------------------------
â€¢ [SLOW TEST] [6.301 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:39.007
    Sep  8 21:54:39.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 21:54:39.009
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:39.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:39.067
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-89b06a0f-e052-4ddf-8db5-cf8f99d66b49 09/08/23 21:54:39.073
    STEP: Creating a pod to test consume configMaps 09/08/23 21:54:39.081
    Sep  8 21:54:39.102: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793" in namespace "configmap-8896" to be "Succeeded or Failed"
    Sep  8 21:54:39.112: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Pending", Reason="", readiness=false. Elapsed: 10.46335ms
    Sep  8 21:54:41.124: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Running", Reason="", readiness=true. Elapsed: 2.021610231s
    Sep  8 21:54:43.127: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Running", Reason="", readiness=false. Elapsed: 4.024704851s
    Sep  8 21:54:45.123: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020773535s
    STEP: Saw pod success 09/08/23 21:54:45.123
    Sep  8 21:54:45.123: INFO: Pod "pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793" satisfied condition "Succeeded or Failed"
    Sep  8 21:54:45.147: INFO: Trying to get logs from node node-3 pod pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:54:45.177
    Sep  8 21:54:45.232: INFO: Waiting for pod pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793 to disappear
    Sep  8 21:54:45.248: INFO: Pod pod-configmaps-0ed3607d-8a07-4076-9f0e-458df137d793 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8896" for this suite. 09/08/23 21:54:45.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:45.31
Sep  8 21:54:45.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename containers 09/08/23 21:54:45.311
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:45.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:45.351
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 09/08/23 21:54:45.356
Sep  8 21:54:45.378: INFO: Waiting up to 5m0s for pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc" in namespace "containers-6991" to be "Succeeded or Failed"
Sep  8 21:54:45.388: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.658579ms
Sep  8 21:54:47.407: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028541086s
Sep  8 21:54:49.398: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019302275s
Sep  8 21:54:51.401: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022320941s
STEP: Saw pod success 09/08/23 21:54:51.401
Sep  8 21:54:51.401: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc" satisfied condition "Succeeded or Failed"
Sep  8 21:54:51.414: INFO: Trying to get logs from node node-3 pod client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc container agnhost-container: <nil>
STEP: delete the pod 09/08/23 21:54:51.432
Sep  8 21:54:51.463: INFO: Waiting for pod client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc to disappear
Sep  8 21:54:51.479: INFO: Pod client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Sep  8 21:54:51.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6991" for this suite. 09/08/23 21:54:51.491
------------------------------
â€¢ [SLOW TEST] [6.200 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:45.31
    Sep  8 21:54:45.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename containers 09/08/23 21:54:45.311
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:45.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:45.351
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 09/08/23 21:54:45.356
    Sep  8 21:54:45.378: INFO: Waiting up to 5m0s for pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc" in namespace "containers-6991" to be "Succeeded or Failed"
    Sep  8 21:54:45.388: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.658579ms
    Sep  8 21:54:47.407: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028541086s
    Sep  8 21:54:49.398: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019302275s
    Sep  8 21:54:51.401: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022320941s
    STEP: Saw pod success 09/08/23 21:54:51.401
    Sep  8 21:54:51.401: INFO: Pod "client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc" satisfied condition "Succeeded or Failed"
    Sep  8 21:54:51.414: INFO: Trying to get logs from node node-3 pod client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 21:54:51.432
    Sep  8 21:54:51.463: INFO: Waiting for pod client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc to disappear
    Sep  8 21:54:51.479: INFO: Pod client-containers-56b8a29d-7386-47f4-8597-560c838fcfdc no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:54:51.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6991" for this suite. 09/08/23 21:54:51.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:54:51.52
Sep  8 21:54:51.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename job 09/08/23 21:54:51.522
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:51.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:51.571
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 09/08/23 21:54:51.575
STEP: Ensuring job reaches completions 09/08/23 21:54:51.585
STEP: Ensuring pods with index for job exist 09/08/23 21:55:01.594
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:01.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9859" for this suite. 09/08/23 21:55:01.628
------------------------------
â€¢ [SLOW TEST] [10.128 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:54:51.52
    Sep  8 21:54:51.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename job 09/08/23 21:54:51.522
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:54:51.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:54:51.571
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 09/08/23 21:54:51.575
    STEP: Ensuring job reaches completions 09/08/23 21:54:51.585
    STEP: Ensuring pods with index for job exist 09/08/23 21:55:01.594
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:01.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9859" for this suite. 09/08/23 21:55:01.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:01.649
Sep  8 21:55:01.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename gc 09/08/23 21:55:01.651
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:01.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:01.744
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 09/08/23 21:55:01.752
STEP: delete the rc 09/08/23 21:55:06.783
STEP: wait for all pods to be garbage collected 09/08/23 21:55:06.831
STEP: Gathering metrics 09/08/23 21:55:11.849
Sep  8 21:55:11.919: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
Sep  8 21:55:11.927: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.459475ms
Sep  8 21:55:11.927: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
Sep  8 21:55:11.927: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
Sep  8 21:55:12.063: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:12.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7029" for this suite. 09/08/23 21:55:12.073
------------------------------
â€¢ [SLOW TEST] [10.438 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:01.649
    Sep  8 21:55:01.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename gc 09/08/23 21:55:01.651
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:01.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:01.744
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 09/08/23 21:55:01.752
    STEP: delete the rc 09/08/23 21:55:06.783
    STEP: wait for all pods to be garbage collected 09/08/23 21:55:06.831
    STEP: Gathering metrics 09/08/23 21:55:11.849
    Sep  8 21:55:11.919: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
    Sep  8 21:55:11.927: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.459475ms
    Sep  8 21:55:11.927: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
    Sep  8 21:55:11.927: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
    Sep  8 21:55:12.063: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:12.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7029" for this suite. 09/08/23 21:55:12.073
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:12.095
Sep  8 21:55:12.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 21:55:12.097
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:12.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:12.155
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 09/08/23 21:55:12.179
Sep  8 21:55:12.179: INFO: Creating simple deployment test-deployment-7csft
Sep  8 21:55:12.219: INFO: deployment "test-deployment-7csft" doesn't have the required revision set
STEP: Getting /status 09/08/23 21:55:14.257
Sep  8 21:55:14.268: INFO: Deployment test-deployment-7csft has Conditions: [{Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 09/08/23 21:55:14.268
Sep  8 21:55:14.302: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 55, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 55, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 55, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 55, 12, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7csft-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 09/08/23 21:55:14.303
Sep  8 21:55:14.307: INFO: Observed &Deployment event: ADDED
Sep  8 21:55:14.307: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
Sep  8 21:55:14.307: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.307: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
Sep  8 21:55:14.307: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  8 21:55:14.308: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7csft-54bc444df" is progressing.}
Sep  8 21:55:14.308: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
Sep  8 21:55:14.308: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.320: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  8 21:55:14.320: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
Sep  8 21:55:14.321: INFO: Found Deployment test-deployment-7csft in namespace deployment-5250 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  8 21:55:14.321: INFO: Deployment test-deployment-7csft has an updated status
STEP: patching the Statefulset Status 09/08/23 21:55:14.321
Sep  8 21:55:14.321: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Sep  8 21:55:14.338: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 09/08/23 21:55:14.338
Sep  8 21:55:14.344: INFO: Observed &Deployment event: ADDED
Sep  8 21:55:14.344: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
Sep  8 21:55:14.344: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.345: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
Sep  8 21:55:14.345: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  8 21:55:14.345: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7csft-54bc444df" is progressing.}
Sep  8 21:55:14.346: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
Sep  8 21:55:14.347: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.347: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Sep  8 21:55:14.347: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
Sep  8 21:55:14.347: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Sep  8 21:55:14.347: INFO: Observed &Deployment event: MODIFIED
Sep  8 21:55:14.347: INFO: Found deployment test-deployment-7csft in namespace deployment-5250 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Sep  8 21:55:14.347: INFO: Deployment test-deployment-7csft has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 21:55:14.359: INFO: Deployment "test-deployment-7csft":
&Deployment{ObjectMeta:{test-deployment-7csft  deployment-5250  29d8f630-82a7-486e-a138-6f8cc92e6611 29766 1 2023-09-08 21:55:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-08 21:55:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-08 21:55:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004000178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  8 21:55:14.367: INFO: New ReplicaSet "test-deployment-7csft-54bc444df" of Deployment "test-deployment-7csft":
&ReplicaSet{ObjectMeta:{test-deployment-7csft-54bc444df  deployment-5250  11a0beca-965e-460d-896f-45fdefc80fe6 29759 1 2023-09-08 21:55:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7csft 29d8f630-82a7-486e-a138-6f8cc92e6611 0xc004000580 0xc004000581}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:55:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29d8f630-82a7-486e-a138-6f8cc92e6611\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004000638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:55:14.377: INFO: Pod "test-deployment-7csft-54bc444df-dpbtd" is available:
&Pod{ObjectMeta:{test-deployment-7csft-54bc444df-dpbtd test-deployment-7csft-54bc444df- deployment-5250  7eaabdd8-1a06-40aa-912d-22299f9a7fd5 29758 0 2023-09-08 21:55:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:f055be22b1d0f78bf8d46f68a6941356d45d1f50e08f80a4c9c8685a16ff7ea6 cni.projectcalico.org/podIP:10.233.75.117/32 cni.projectcalico.org/podIPs:10.233.75.117/32] [{apps/v1 ReplicaSet test-deployment-7csft-54bc444df 11a0beca-965e-460d-896f-45fdefc80fe6 0xc004e0e8f0 0xc004e0e8f1}] [] [{kube-controller-manager Update v1 2023-09-08 21:55:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11a0beca-965e-460d-896f-45fdefc80fe6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69qns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69qns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.117,StartTime:2023-09-08 21:55:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:55:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f9213ed23c1f933cd7c7cf47daddb1adca15ca3cfec24abcfb354a29666259f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:14.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5250" for this suite. 09/08/23 21:55:14.391
------------------------------
â€¢ [2.312 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:12.095
    Sep  8 21:55:12.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 21:55:12.097
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:12.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:12.155
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 09/08/23 21:55:12.179
    Sep  8 21:55:12.179: INFO: Creating simple deployment test-deployment-7csft
    Sep  8 21:55:12.219: INFO: deployment "test-deployment-7csft" doesn't have the required revision set
    STEP: Getting /status 09/08/23 21:55:14.257
    Sep  8 21:55:14.268: INFO: Deployment test-deployment-7csft has Conditions: [{Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 09/08/23 21:55:14.268
    Sep  8 21:55:14.302: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 55, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 55, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 21, 55, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 21, 55, 12, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7csft-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 09/08/23 21:55:14.303
    Sep  8 21:55:14.307: INFO: Observed &Deployment event: ADDED
    Sep  8 21:55:14.307: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
    Sep  8 21:55:14.307: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.307: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
    Sep  8 21:55:14.307: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  8 21:55:14.308: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7csft-54bc444df" is progressing.}
    Sep  8 21:55:14.308: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  8 21:55:14.308: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
    Sep  8 21:55:14.308: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.320: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  8 21:55:14.320: INFO: Observed Deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
    Sep  8 21:55:14.321: INFO: Found Deployment test-deployment-7csft in namespace deployment-5250 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  8 21:55:14.321: INFO: Deployment test-deployment-7csft has an updated status
    STEP: patching the Statefulset Status 09/08/23 21:55:14.321
    Sep  8 21:55:14.321: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Sep  8 21:55:14.338: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 09/08/23 21:55:14.338
    Sep  8 21:55:14.344: INFO: Observed &Deployment event: ADDED
    Sep  8 21:55:14.344: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
    Sep  8 21:55:14.344: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.345: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7csft-54bc444df"}
    Sep  8 21:55:14.345: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  8 21:55:14.345: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:12 +0000 UTC 2023-09-08 21:55:12 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7csft-54bc444df" is progressing.}
    Sep  8 21:55:14.346: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  8 21:55:14.346: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
    Sep  8 21:55:14.347: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.347: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Sep  8 21:55:14.347: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-09-08 21:55:13 +0000 UTC 2023-09-08 21:55:12 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7csft-54bc444df" has successfully progressed.}
    Sep  8 21:55:14.347: INFO: Observed deployment test-deployment-7csft in namespace deployment-5250 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Sep  8 21:55:14.347: INFO: Observed &Deployment event: MODIFIED
    Sep  8 21:55:14.347: INFO: Found deployment test-deployment-7csft in namespace deployment-5250 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Sep  8 21:55:14.347: INFO: Deployment test-deployment-7csft has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 21:55:14.359: INFO: Deployment "test-deployment-7csft":
    &Deployment{ObjectMeta:{test-deployment-7csft  deployment-5250  29d8f630-82a7-486e-a138-6f8cc92e6611 29766 1 2023-09-08 21:55:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-09-08 21:55:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-09-08 21:55:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004000178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Sep  8 21:55:14.367: INFO: New ReplicaSet "test-deployment-7csft-54bc444df" of Deployment "test-deployment-7csft":
    &ReplicaSet{ObjectMeta:{test-deployment-7csft-54bc444df  deployment-5250  11a0beca-965e-460d-896f-45fdefc80fe6 29759 1 2023-09-08 21:55:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7csft 29d8f630-82a7-486e-a138-6f8cc92e6611 0xc004000580 0xc004000581}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:55:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"29d8f630-82a7-486e-a138-6f8cc92e6611\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004000638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:55:14.377: INFO: Pod "test-deployment-7csft-54bc444df-dpbtd" is available:
    &Pod{ObjectMeta:{test-deployment-7csft-54bc444df-dpbtd test-deployment-7csft-54bc444df- deployment-5250  7eaabdd8-1a06-40aa-912d-22299f9a7fd5 29758 0 2023-09-08 21:55:12 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:f055be22b1d0f78bf8d46f68a6941356d45d1f50e08f80a4c9c8685a16ff7ea6 cni.projectcalico.org/podIP:10.233.75.117/32 cni.projectcalico.org/podIPs:10.233.75.117/32] [{apps/v1 ReplicaSet test-deployment-7csft-54bc444df 11a0beca-965e-460d-896f-45fdefc80fe6 0xc004e0e8f0 0xc004e0e8f1}] [] [{kube-controller-manager Update v1 2023-09-08 21:55:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11a0beca-965e-460d-896f-45fdefc80fe6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:55:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69qns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69qns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.117,StartTime:2023-09-08 21:55:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:55:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5f9213ed23c1f933cd7c7cf47daddb1adca15ca3cfec24abcfb354a29666259f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:14.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5250" for this suite. 09/08/23 21:55:14.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:14.419
Sep  8 21:55:14.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replication-controller 09/08/23 21:55:14.42
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:14.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:14.473
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 09/08/23 21:55:14.479
Sep  8 21:55:14.505: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1542" to be "running and ready"
Sep  8 21:55:14.523: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 17.543848ms
Sep  8 21:55:14.523: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:55:16.542: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036221534s
Sep  8 21:55:16.542: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:55:18.535: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.029906931s
Sep  8 21:55:18.536: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Sep  8 21:55:18.536: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 09/08/23 21:55:18.55
STEP: Then the orphan pod is adopted 09/08/23 21:55:18.565
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:19.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1542" for this suite. 09/08/23 21:55:19.592
------------------------------
â€¢ [SLOW TEST] [5.191 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:14.419
    Sep  8 21:55:14.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replication-controller 09/08/23 21:55:14.42
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:14.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:14.473
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 09/08/23 21:55:14.479
    Sep  8 21:55:14.505: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-1542" to be "running and ready"
    Sep  8 21:55:14.523: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 17.543848ms
    Sep  8 21:55:14.523: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:55:16.542: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036221534s
    Sep  8 21:55:16.542: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:55:18.535: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.029906931s
    Sep  8 21:55:18.536: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Sep  8 21:55:18.536: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 09/08/23 21:55:18.55
    STEP: Then the orphan pod is adopted 09/08/23 21:55:18.565
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:19.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1542" for this suite. 09/08/23 21:55:19.592
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:19.612
Sep  8 21:55:19.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 21:55:19.613
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:19.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:19.658
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Sep  8 21:55:19.708: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  8 21:55:24.718: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/08/23 21:55:24.718
Sep  8 21:55:24.718: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/08/23 21:55:24.752
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 21:55:24.793: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3362  474d9ebc-cde0-40fc-b717-3a1a34e6b7b7 29889 1 2023-09-08 21:55:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-08 21:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00877ae18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  8 21:55:24.805: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep  8 21:55:24.805: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  8 21:55:24.805: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3362  36b2585d-299a-424f-80c8-739c62c113fd 29890 1 2023-09-08 21:55:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 474d9ebc-cde0-40fc-b717-3a1a34e6b7b7 0xc00877b157 0xc00877b158}] [] [{e2e.test Update apps/v1 2023-09-08 21:55:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-08 21:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"474d9ebc-cde0-40fc-b717-3a1a34e6b7b7\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00877b218 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:55:24.827: INFO: Pod "test-cleanup-controller-pqj95" is available:
&Pod{ObjectMeta:{test-cleanup-controller-pqj95 test-cleanup-controller- deployment-3362  f005f200-7957-41b7-b249-c2d60c31a908 29870 0 2023-09-08 21:55:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:97fe6c8573f0e5eb7f18bde129b37f6716be213d1de54d557f996a042bd85865 cni.projectcalico.org/podIP:10.233.75.122/32 cni.projectcalico.org/podIPs:10.233.75.122/32] [{apps/v1 ReplicaSet test-cleanup-controller 36b2585d-299a-424f-80c8-739c62c113fd 0xc00877b547 0xc00877b548}] [] [{kube-controller-manager Update v1 2023-09-08 21:55:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36b2585d-299a-424f-80c8-739c62c113fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:55:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:55:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddwx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddwx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.122,StartTime:2023-09-08 21:55:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:55:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b50539db7201fb70d380e4d9eebac0915a2c1d750d12c1c6d4bfc719d6f147ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:24.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3362" for this suite. 09/08/23 21:55:24.865
------------------------------
â€¢ [SLOW TEST] [5.295 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:19.612
    Sep  8 21:55:19.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 21:55:19.613
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:19.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:19.658
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Sep  8 21:55:19.708: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Sep  8 21:55:24.718: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/08/23 21:55:24.718
    Sep  8 21:55:24.718: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 09/08/23 21:55:24.752
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 21:55:24.793: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3362  474d9ebc-cde0-40fc-b717-3a1a34e6b7b7 29889 1 2023-09-08 21:55:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-09-08 21:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00877ae18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  8 21:55:24.805: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Sep  8 21:55:24.805: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Sep  8 21:55:24.805: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3362  36b2585d-299a-424f-80c8-739c62c113fd 29890 1 2023-09-08 21:55:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 474d9ebc-cde0-40fc-b717-3a1a34e6b7b7 0xc00877b157 0xc00877b158}] [] [{e2e.test Update apps/v1 2023-09-08 21:55:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-09-08 21:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"474d9ebc-cde0-40fc-b717-3a1a34e6b7b7\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00877b218 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:55:24.827: INFO: Pod "test-cleanup-controller-pqj95" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-pqj95 test-cleanup-controller- deployment-3362  f005f200-7957-41b7-b249-c2d60c31a908 29870 0 2023-09-08 21:55:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:97fe6c8573f0e5eb7f18bde129b37f6716be213d1de54d557f996a042bd85865 cni.projectcalico.org/podIP:10.233.75.122/32 cni.projectcalico.org/podIPs:10.233.75.122/32] [{apps/v1 ReplicaSet test-cleanup-controller 36b2585d-299a-424f-80c8-739c62c113fd 0xc00877b547 0xc00877b548}] [] [{kube-controller-manager Update v1 2023-09-08 21:55:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"36b2585d-299a-424f-80c8-739c62c113fd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 21:55:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 21:55:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddwx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddwx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.122,StartTime:2023-09-08 21:55:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 21:55:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b50539db7201fb70d380e4d9eebac0915a2c1d750d12c1c6d4bfc719d6f147ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:24.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3362" for this suite. 09/08/23 21:55:24.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:24.913
Sep  8 21:55:24.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 21:55:24.914
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:24.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:24.971
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 09/08/23 21:55:24.977
STEP: submitting the pod to kubernetes 09/08/23 21:55:24.977
Sep  8 21:55:24.998: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" in namespace "pods-9219" to be "running and ready"
Sep  8 21:55:25.010: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.136533ms
Sep  8 21:55:25.010: INFO: The phase of Pod pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:55:27.021: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022937857s
Sep  8 21:55:27.021: INFO: The phase of Pod pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f is Running (Ready = true)
Sep  8 21:55:27.021: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/08/23 21:55:27.03
STEP: updating the pod 09/08/23 21:55:27.038
Sep  8 21:55:27.582: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f"
Sep  8 21:55:27.582: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" in namespace "pods-9219" to be "terminated with reason DeadlineExceeded"
Sep  8 21:55:27.590: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=true. Elapsed: 8.186662ms
Sep  8 21:55:29.605: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.023325062s
Sep  8 21:55:31.601: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=false. Elapsed: 4.019093627s
Sep  8 21:55:33.603: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.021382972s
Sep  8 21:55:33.603: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9219" for this suite. 09/08/23 21:55:33.628
------------------------------
â€¢ [SLOW TEST] [8.742 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:24.913
    Sep  8 21:55:24.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 21:55:24.914
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:24.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:24.971
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 09/08/23 21:55:24.977
    STEP: submitting the pod to kubernetes 09/08/23 21:55:24.977
    Sep  8 21:55:24.998: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" in namespace "pods-9219" to be "running and ready"
    Sep  8 21:55:25.010: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.136533ms
    Sep  8 21:55:25.010: INFO: The phase of Pod pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:55:27.021: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.022937857s
    Sep  8 21:55:27.021: INFO: The phase of Pod pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f is Running (Ready = true)
    Sep  8 21:55:27.021: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/08/23 21:55:27.03
    STEP: updating the pod 09/08/23 21:55:27.038
    Sep  8 21:55:27.582: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f"
    Sep  8 21:55:27.582: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" in namespace "pods-9219" to be "terminated with reason DeadlineExceeded"
    Sep  8 21:55:27.590: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=true. Elapsed: 8.186662ms
    Sep  8 21:55:29.605: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.023325062s
    Sep  8 21:55:31.601: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Running", Reason="", readiness=false. Elapsed: 4.019093627s
    Sep  8 21:55:33.603: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.021382972s
    Sep  8 21:55:33.603: INFO: Pod "pod-update-activedeadlineseconds-5ee68bd0-84e7-4db0-ae82-e7ef92df1d0f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:33.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9219" for this suite. 09/08/23 21:55:33.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:33.664
Sep  8 21:55:33.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename disruption 09/08/23 21:55:33.665
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:33.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:33.727
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:33.733
Sep  8 21:55:33.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename disruption-2 09/08/23 21:55:33.735
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:33.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:33.795
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 09/08/23 21:55:33.819
STEP: Waiting for the pdb to be processed 09/08/23 21:55:35.855
STEP: Waiting for the pdb to be processed 09/08/23 21:55:37.903
STEP: listing a collection of PDBs across all namespaces 09/08/23 21:55:37.919
STEP: listing a collection of PDBs in namespace disruption-9978 09/08/23 21:55:37.928
STEP: deleting a collection of PDBs 09/08/23 21:55:37.948
STEP: Waiting for the PDB collection to be deleted 09/08/23 21:55:37.999
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:38.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-2920" for this suite. 09/08/23 21:55:38.034
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9978" for this suite. 09/08/23 21:55:38.057
------------------------------
â€¢ [4.423 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:33.664
    Sep  8 21:55:33.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename disruption 09/08/23 21:55:33.665
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:33.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:33.727
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:33.733
    Sep  8 21:55:33.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename disruption-2 09/08/23 21:55:33.735
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:33.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:33.795
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 09/08/23 21:55:33.819
    STEP: Waiting for the pdb to be processed 09/08/23 21:55:35.855
    STEP: Waiting for the pdb to be processed 09/08/23 21:55:37.903
    STEP: listing a collection of PDBs across all namespaces 09/08/23 21:55:37.919
    STEP: listing a collection of PDBs in namespace disruption-9978 09/08/23 21:55:37.928
    STEP: deleting a collection of PDBs 09/08/23 21:55:37.948
    STEP: Waiting for the PDB collection to be deleted 09/08/23 21:55:37.999
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:38.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-2920" for this suite. 09/08/23 21:55:38.034
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9978" for this suite. 09/08/23 21:55:38.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:38.088
Sep  8 21:55:38.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 21:55:38.089
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:38.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:38.147
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Sep  8 21:55:38.161: INFO: Creating deployment "test-recreate-deployment"
Sep  8 21:55:38.174: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  8 21:55:38.215: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  8 21:55:40.236: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  8 21:55:40.243: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  8 21:55:40.276: INFO: Updating deployment test-recreate-deployment
Sep  8 21:55:40.276: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 21:55:40.567: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5094  9bc697dc-92ed-44b6-bd93-8d67ef8a641e 30124 2 2023-09-08 21:55:38 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e9c4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-08 21:55:40 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-08 21:55:40 +0000 UTC,LastTransitionTime:2023-09-08 21:55:38 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  8 21:55:40.580: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5094  a55ff96d-0351-46ce-bd0e-7d58ce4bbfe4 30123 1 2023-09-08 21:55:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9bc697dc-92ed-44b6-bd93-8d67ef8a641e 0xc000e9d020 0xc000e9d021}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bc697dc-92ed-44b6-bd93-8d67ef8a641e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e9d0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:55:40.580: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  8 21:55:40.580: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5094  65c37de3-e372-401a-87f0-5758b5aa1e22 30112 2 2023-09-08 21:55:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9bc697dc-92ed-44b6-bd93-8d67ef8a641e 0xc000e9ced7 0xc000e9ced8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bc697dc-92ed-44b6-bd93-8d67ef8a641e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e9cf98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  8 21:55:40.595: INFO: Pod "test-recreate-deployment-cff6dc657-bwwm9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-bwwm9 test-recreate-deployment-cff6dc657- deployment-5094  39d959a5-829d-4aa7-9dec-6a41d087aebe 30121 0 2023-09-08 21:55:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 a55ff96d-0351-46ce-bd0e-7d58ce4bbfe4 0xc003b40e90 0xc003b40e91}] [] [{kube-controller-manager Update v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a55ff96d-0351-46ce-bd0e-7d58ce4bbfe4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxbnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxbnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 21:55:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5094" for this suite. 09/08/23 21:55:40.616
------------------------------
â€¢ [2.553 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:38.088
    Sep  8 21:55:38.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 21:55:38.089
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:38.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:38.147
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Sep  8 21:55:38.161: INFO: Creating deployment "test-recreate-deployment"
    Sep  8 21:55:38.174: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Sep  8 21:55:38.215: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Sep  8 21:55:40.236: INFO: Waiting deployment "test-recreate-deployment" to complete
    Sep  8 21:55:40.243: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Sep  8 21:55:40.276: INFO: Updating deployment test-recreate-deployment
    Sep  8 21:55:40.276: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 21:55:40.567: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5094  9bc697dc-92ed-44b6-bd93-8d67ef8a641e 30124 2 2023-09-08 21:55:38 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e9c4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-08 21:55:40 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-09-08 21:55:40 +0000 UTC,LastTransitionTime:2023-09-08 21:55:38 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Sep  8 21:55:40.580: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-5094  a55ff96d-0351-46ce-bd0e-7d58ce4bbfe4 30123 1 2023-09-08 21:55:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9bc697dc-92ed-44b6-bd93-8d67ef8a641e 0xc000e9d020 0xc000e9d021}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bc697dc-92ed-44b6-bd93-8d67ef8a641e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e9d0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:55:40.580: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Sep  8 21:55:40.580: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-5094  65c37de3-e372-401a-87f0-5758b5aa1e22 30112 2 2023-09-08 21:55:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9bc697dc-92ed-44b6-bd93-8d67ef8a641e 0xc000e9ced7 0xc000e9ced8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bc697dc-92ed-44b6-bd93-8d67ef8a641e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000e9cf98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 21:55:40.595: INFO: Pod "test-recreate-deployment-cff6dc657-bwwm9" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-bwwm9 test-recreate-deployment-cff6dc657- deployment-5094  39d959a5-829d-4aa7-9dec-6a41d087aebe 30121 0 2023-09-08 21:55:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 a55ff96d-0351-46ce-bd0e-7d58ce4bbfe4 0xc003b40e90 0xc003b40e91}] [] [{kube-controller-manager Update v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a55ff96d-0351-46ce-bd0e-7d58ce4bbfe4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 21:55:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxbnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxbnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 21:55:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 21:55:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5094" for this suite. 09/08/23 21:55:40.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:40.655
Sep  8 21:55:40.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:55:40.657
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:40.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:40.703
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 09/08/23 21:55:40.711
STEP: Creating a ResourceQuota 09/08/23 21:55:45.722
STEP: Ensuring resource quota status is calculated 09/08/23 21:55:45.736
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:55:47.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6082" for this suite. 09/08/23 21:55:47.778
------------------------------
â€¢ [SLOW TEST] [7.147 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:40.655
    Sep  8 21:55:40.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:55:40.657
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:40.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:40.703
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 09/08/23 21:55:40.711
    STEP: Creating a ResourceQuota 09/08/23 21:55:45.722
    STEP: Ensuring resource quota status is calculated 09/08/23 21:55:45.736
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:55:47.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6082" for this suite. 09/08/23 21:55:47.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:55:47.804
Sep  8 21:55:47.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-runtime 09/08/23 21:55:47.807
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:47.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:47.864
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/08/23 21:55:47.911
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/08/23 21:56:08.162
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/08/23 21:56:08.171
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/08/23 21:56:08.19
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/08/23 21:56:08.19
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/08/23 21:56:08.258
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/08/23 21:56:11.309
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/08/23 21:56:13.355
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/08/23 21:56:13.377
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/08/23 21:56:13.377
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/08/23 21:56:13.472
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/08/23 21:56:14.495
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/08/23 21:56:17.535
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/08/23 21:56:17.563
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/08/23 21:56:17.563
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  8 21:56:17.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1046" for this suite. 09/08/23 21:56:17.658
------------------------------
â€¢ [SLOW TEST] [29.877 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:55:47.804
    Sep  8 21:55:47.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-runtime 09/08/23 21:55:47.807
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:55:47.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:55:47.864
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 09/08/23 21:55:47.911
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 09/08/23 21:56:08.162
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 09/08/23 21:56:08.171
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 09/08/23 21:56:08.19
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 09/08/23 21:56:08.19
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 09/08/23 21:56:08.258
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 09/08/23 21:56:11.309
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 09/08/23 21:56:13.355
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 09/08/23 21:56:13.377
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 09/08/23 21:56:13.377
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 09/08/23 21:56:13.472
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 09/08/23 21:56:14.495
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 09/08/23 21:56:17.535
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 09/08/23 21:56:17.563
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 09/08/23 21:56:17.563
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:56:17.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1046" for this suite. 09/08/23 21:56:17.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:56:17.683
Sep  8 21:56:17.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename events 09/08/23 21:56:17.685
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:17.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:17.74
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 09/08/23 21:56:17.745
STEP: listing all events in all namespaces 09/08/23 21:56:17.756
STEP: patching the test event 09/08/23 21:56:17.783
STEP: fetching the test event 09/08/23 21:56:17.813
STEP: updating the test event 09/08/23 21:56:17.826
STEP: getting the test event 09/08/23 21:56:17.848
STEP: deleting the test event 09/08/23 21:56:17.856
STEP: listing all events in all namespaces 09/08/23 21:56:17.878
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Sep  8 21:56:17.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6550" for this suite. 09/08/23 21:56:17.927
------------------------------
â€¢ [0.264 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:56:17.683
    Sep  8 21:56:17.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename events 09/08/23 21:56:17.685
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:17.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:17.74
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 09/08/23 21:56:17.745
    STEP: listing all events in all namespaces 09/08/23 21:56:17.756
    STEP: patching the test event 09/08/23 21:56:17.783
    STEP: fetching the test event 09/08/23 21:56:17.813
    STEP: updating the test event 09/08/23 21:56:17.826
    STEP: getting the test event 09/08/23 21:56:17.848
    STEP: deleting the test event 09/08/23 21:56:17.856
    STEP: listing all events in all namespaces 09/08/23 21:56:17.878
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:56:17.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6550" for this suite. 09/08/23 21:56:17.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:56:17.947
Sep  8 21:56:17.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename watch 09/08/23 21:56:17.95
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:17.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:18.002
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 09/08/23 21:56:18.008
STEP: creating a new configmap 09/08/23 21:56:18.011
STEP: modifying the configmap once 09/08/23 21:56:18.021
STEP: changing the label value of the configmap 09/08/23 21:56:18.062
STEP: Expecting to observe a delete notification for the watched object 09/08/23 21:56:18.088
Sep  8 21:56:18.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30436 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 21:56:18.089: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30437 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 21:56:18.089: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30438 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 09/08/23 21:56:18.089
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/08/23 21:56:18.111
STEP: changing the label value of the configmap back 09/08/23 21:56:28.112
STEP: modifying the configmap a third time 09/08/23 21:56:28.14
STEP: deleting the configmap 09/08/23 21:56:28.163
STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/08/23 21:56:28.179
Sep  8 21:56:28.179: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30506 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 21:56:28.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30507 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 21:56:28.181: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30508 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  8 21:56:28.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8019" for this suite. 09/08/23 21:56:28.193
------------------------------
â€¢ [SLOW TEST] [10.263 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:56:17.947
    Sep  8 21:56:17.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename watch 09/08/23 21:56:17.95
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:17.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:18.002
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 09/08/23 21:56:18.008
    STEP: creating a new configmap 09/08/23 21:56:18.011
    STEP: modifying the configmap once 09/08/23 21:56:18.021
    STEP: changing the label value of the configmap 09/08/23 21:56:18.062
    STEP: Expecting to observe a delete notification for the watched object 09/08/23 21:56:18.088
    Sep  8 21:56:18.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30436 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 21:56:18.089: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30437 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 21:56:18.089: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30438 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 09/08/23 21:56:18.089
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 09/08/23 21:56:18.111
    STEP: changing the label value of the configmap back 09/08/23 21:56:28.112
    STEP: modifying the configmap a third time 09/08/23 21:56:28.14
    STEP: deleting the configmap 09/08/23 21:56:28.163
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 09/08/23 21:56:28.179
    Sep  8 21:56:28.179: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30506 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 21:56:28.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30507 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 21:56:28.181: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8019  511ac59d-b074-4c71-94ad-04c34ff19ae3 30508 0 2023-09-08 21:56:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-09-08 21:56:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:56:28.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8019" for this suite. 09/08/23 21:56:28.193
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:56:28.211
Sep  8 21:56:28.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 21:56:28.213
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:28.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:28.264
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 09/08/23 21:56:28.271
STEP: submitting the pod to kubernetes 09/08/23 21:56:28.271
STEP: verifying QOS class is set on the pod 09/08/23 21:56:28.293
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Sep  8 21:56:28.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3503" for this suite. 09/08/23 21:56:28.323
------------------------------
â€¢ [0.132 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:56:28.211
    Sep  8 21:56:28.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 21:56:28.213
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:28.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:28.264
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 09/08/23 21:56:28.271
    STEP: submitting the pod to kubernetes 09/08/23 21:56:28.271
    STEP: verifying QOS class is set on the pod 09/08/23 21:56:28.293
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:56:28.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3503" for this suite. 09/08/23 21:56:28.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:56:28.344
Sep  8 21:56:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 21:56:28.347
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:28.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:28.388
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9794.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 09/08/23 21:56:28.394
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9794.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 09/08/23 21:56:28.394
STEP: creating a pod to probe /etc/hosts 09/08/23 21:56:28.394
STEP: submitting the pod to kubernetes 09/08/23 21:56:28.395
Sep  8 21:56:28.416: INFO: Waiting up to 15m0s for pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35" in namespace "dns-9794" to be "running"
Sep  8 21:56:28.429: INFO: Pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35": Phase="Pending", Reason="", readiness=false. Elapsed: 13.679162ms
Sep  8 21:56:30.455: INFO: Pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35": Phase="Running", Reason="", readiness=true. Elapsed: 2.038856061s
Sep  8 21:56:30.455: INFO: Pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35" satisfied condition "running"
STEP: retrieving the pod 09/08/23 21:56:30.455
STEP: looking for the results for each expected name from probers 09/08/23 21:56:30.463
Sep  8 21:56:30.509: INFO: DNS probes using dns-9794/dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35 succeeded

STEP: deleting the pod 09/08/23 21:56:30.509
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 21:56:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9794" for this suite. 09/08/23 21:56:30.568
------------------------------
â€¢ [2.249 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:56:28.344
    Sep  8 21:56:28.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 21:56:28.347
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:28.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:28.388
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9794.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     09/08/23 21:56:28.394
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9794.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9794.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     09/08/23 21:56:28.394
    STEP: creating a pod to probe /etc/hosts 09/08/23 21:56:28.394
    STEP: submitting the pod to kubernetes 09/08/23 21:56:28.395
    Sep  8 21:56:28.416: INFO: Waiting up to 15m0s for pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35" in namespace "dns-9794" to be "running"
    Sep  8 21:56:28.429: INFO: Pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35": Phase="Pending", Reason="", readiness=false. Elapsed: 13.679162ms
    Sep  8 21:56:30.455: INFO: Pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35": Phase="Running", Reason="", readiness=true. Elapsed: 2.038856061s
    Sep  8 21:56:30.455: INFO: Pod "dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 21:56:30.455
    STEP: looking for the results for each expected name from probers 09/08/23 21:56:30.463
    Sep  8 21:56:30.509: INFO: DNS probes using dns-9794/dns-test-5e2fe444-f76c-4ebb-b9ef-dde1dad53f35 succeeded

    STEP: deleting the pod 09/08/23 21:56:30.509
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:56:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9794" for this suite. 09/08/23 21:56:30.568
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:56:30.593
Sep  8 21:56:30.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:56:30.595
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:30.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:30.654
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Sep  8 21:56:30.711: INFO: created pod
Sep  8 21:56:30.711: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9683" to be "Succeeded or Failed"
Sep  8 21:56:30.729: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.462646ms
Sep  8 21:56:32.736: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.0248085s
Sep  8 21:56:34.739: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.027153741s
Sep  8 21:56:36.738: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026103633s
STEP: Saw pod success 09/08/23 21:56:36.738
Sep  8 21:56:36.738: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep  8 21:57:06.739: INFO: polling logs
Sep  8 21:57:06.786: INFO: Pod logs: 
I0908 21:56:31.889827       1 log.go:198] OK: Got token
I0908 21:56:31.890237       1 log.go:198] validating with in-cluster discovery
I0908 21:56:31.890820       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0908 21:56:31.891008       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9683:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694210791, NotBefore:1694210191, IssuedAt:1694210191, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9683", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8b713274-cf41-4b45-a71f-7d75731bb957"}}}
I0908 21:56:31.932528       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0908 21:56:31.943836       1 log.go:198] OK: Validated signature on JWT
I0908 21:56:31.944431       1 log.go:198] OK: Got valid claims from token!
I0908 21:56:31.944596       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9683:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694210791, NotBefore:1694210191, IssuedAt:1694210191, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9683", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8b713274-cf41-4b45-a71f-7d75731bb957"}}}

Sep  8 21:57:06.786: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  8 21:57:06.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9683" for this suite. 09/08/23 21:57:06.819
------------------------------
â€¢ [SLOW TEST] [36.255 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:56:30.593
    Sep  8 21:56:30.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svcaccounts 09/08/23 21:56:30.595
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:56:30.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:56:30.654
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Sep  8 21:56:30.711: INFO: created pod
    Sep  8 21:56:30.711: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-9683" to be "Succeeded or Failed"
    Sep  8 21:56:30.729: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.462646ms
    Sep  8 21:56:32.736: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=true. Elapsed: 2.0248085s
    Sep  8 21:56:34.739: INFO: Pod "oidc-discovery-validator": Phase="Running", Reason="", readiness=false. Elapsed: 4.027153741s
    Sep  8 21:56:36.738: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026103633s
    STEP: Saw pod success 09/08/23 21:56:36.738
    Sep  8 21:56:36.738: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Sep  8 21:57:06.739: INFO: polling logs
    Sep  8 21:57:06.786: INFO: Pod logs: 
    I0908 21:56:31.889827       1 log.go:198] OK: Got token
    I0908 21:56:31.890237       1 log.go:198] validating with in-cluster discovery
    I0908 21:56:31.890820       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0908 21:56:31.891008       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9683:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694210791, NotBefore:1694210191, IssuedAt:1694210191, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9683", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8b713274-cf41-4b45-a71f-7d75731bb957"}}}
    I0908 21:56:31.932528       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0908 21:56:31.943836       1 log.go:198] OK: Validated signature on JWT
    I0908 21:56:31.944431       1 log.go:198] OK: Got valid claims from token!
    I0908 21:56:31.944596       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9683:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1694210791, NotBefore:1694210191, IssuedAt:1694210191, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9683", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"8b713274-cf41-4b45-a71f-7d75731bb957"}}}

    Sep  8 21:57:06.786: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:57:06.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9683" for this suite. 09/08/23 21:57:06.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:57:06.855
Sep  8 21:57:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 21:57:06.856
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:06.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:06.905
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3194 09/08/23 21:57:06.911
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/08/23 21:57:06.95
STEP: creating service externalsvc in namespace services-3194 09/08/23 21:57:06.959
STEP: creating replication controller externalsvc in namespace services-3194 09/08/23 21:57:07.002
I0908 21:57:07.024477      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3194, replica count: 2
I0908 21:57:10.075120      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 09/08/23 21:57:10.095
Sep  8 21:57:10.145: INFO: Creating new exec pod
Sep  8 21:57:10.176: INFO: Waiting up to 5m0s for pod "execpodvgtcj" in namespace "services-3194" to be "running"
Sep  8 21:57:10.212: INFO: Pod "execpodvgtcj": Phase="Pending", Reason="", readiness=false. Elapsed: 36.181266ms
Sep  8 21:57:12.231: INFO: Pod "execpodvgtcj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055006006s
Sep  8 21:57:14.224: INFO: Pod "execpodvgtcj": Phase="Running", Reason="", readiness=true. Elapsed: 4.047764832s
Sep  8 21:57:14.224: INFO: Pod "execpodvgtcj" satisfied condition "running"
Sep  8 21:57:14.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-3194 exec execpodvgtcj -- /bin/sh -x -c nslookup clusterip-service.services-3194.svc.cluster.local'
Sep  8 21:57:14.527: INFO: stderr: "+ nslookup clusterip-service.services-3194.svc.cluster.local\n"
Sep  8 21:57:14.527: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-3194.svc.cluster.local\tcanonical name = externalsvc.services-3194.svc.cluster.local.\nName:\texternalsvc.services-3194.svc.cluster.local\nAddress: 10.233.50.136\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3194, will wait for the garbage collector to delete the pods 09/08/23 21:57:14.527
Sep  8 21:57:14.604: INFO: Deleting ReplicationController externalsvc took: 18.483851ms
Sep  8 21:57:14.705: INFO: Terminating ReplicationController externalsvc pods took: 101.064054ms
Sep  8 21:57:17.485: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 21:57:17.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3194" for this suite. 09/08/23 21:57:17.591
------------------------------
â€¢ [SLOW TEST] [10.779 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:57:06.855
    Sep  8 21:57:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 21:57:06.856
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:06.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:06.905
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3194 09/08/23 21:57:06.911
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 09/08/23 21:57:06.95
    STEP: creating service externalsvc in namespace services-3194 09/08/23 21:57:06.959
    STEP: creating replication controller externalsvc in namespace services-3194 09/08/23 21:57:07.002
    I0908 21:57:07.024477      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3194, replica count: 2
    I0908 21:57:10.075120      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 09/08/23 21:57:10.095
    Sep  8 21:57:10.145: INFO: Creating new exec pod
    Sep  8 21:57:10.176: INFO: Waiting up to 5m0s for pod "execpodvgtcj" in namespace "services-3194" to be "running"
    Sep  8 21:57:10.212: INFO: Pod "execpodvgtcj": Phase="Pending", Reason="", readiness=false. Elapsed: 36.181266ms
    Sep  8 21:57:12.231: INFO: Pod "execpodvgtcj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055006006s
    Sep  8 21:57:14.224: INFO: Pod "execpodvgtcj": Phase="Running", Reason="", readiness=true. Elapsed: 4.047764832s
    Sep  8 21:57:14.224: INFO: Pod "execpodvgtcj" satisfied condition "running"
    Sep  8 21:57:14.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-3194 exec execpodvgtcj -- /bin/sh -x -c nslookup clusterip-service.services-3194.svc.cluster.local'
    Sep  8 21:57:14.527: INFO: stderr: "+ nslookup clusterip-service.services-3194.svc.cluster.local\n"
    Sep  8 21:57:14.527: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-3194.svc.cluster.local\tcanonical name = externalsvc.services-3194.svc.cluster.local.\nName:\texternalsvc.services-3194.svc.cluster.local\nAddress: 10.233.50.136\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3194, will wait for the garbage collector to delete the pods 09/08/23 21:57:14.527
    Sep  8 21:57:14.604: INFO: Deleting ReplicationController externalsvc took: 18.483851ms
    Sep  8 21:57:14.705: INFO: Terminating ReplicationController externalsvc pods took: 101.064054ms
    Sep  8 21:57:17.485: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:57:17.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3194" for this suite. 09/08/23 21:57:17.591
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:57:17.635
Sep  8 21:57:17.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename ephemeral-containers-test 09/08/23 21:57:17.638
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:17.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:17.699
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 09/08/23 21:57:17.705
Sep  8 21:57:17.739: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2195" to be "running and ready"
Sep  8 21:57:17.757: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.80266ms
Sep  8 21:57:17.757: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:57:19.766: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026588283s
Sep  8 21:57:19.766: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Sep  8 21:57:19.766: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 09/08/23 21:57:19.778
Sep  8 21:57:19.808: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2195" to be "container debugger running"
Sep  8 21:57:19.818: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.603232ms
Sep  8 21:57:21.843: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.032957021s
Sep  8 21:57:23.826: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015184911s
Sep  8 21:57:23.826: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 09/08/23 21:57:23.826
Sep  8 21:57:23.827: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2195 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 21:57:23.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 21:57:23.828: INFO: ExecWithOptions: Clientset creation
Sep  8 21:57:23.828: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-2195/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Sep  8 21:57:23.936: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  8 21:57:23.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2195" for this suite. 09/08/23 21:57:23.987
------------------------------
â€¢ [SLOW TEST] [6.368 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:57:17.635
    Sep  8 21:57:17.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename ephemeral-containers-test 09/08/23 21:57:17.638
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:17.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:17.699
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 09/08/23 21:57:17.705
    Sep  8 21:57:17.739: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2195" to be "running and ready"
    Sep  8 21:57:17.757: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.80266ms
    Sep  8 21:57:17.757: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:57:19.766: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.026588283s
    Sep  8 21:57:19.766: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Sep  8 21:57:19.766: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 09/08/23 21:57:19.778
    Sep  8 21:57:19.808: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2195" to be "container debugger running"
    Sep  8 21:57:19.818: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.603232ms
    Sep  8 21:57:21.843: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.032957021s
    Sep  8 21:57:23.826: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015184911s
    Sep  8 21:57:23.826: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 09/08/23 21:57:23.826
    Sep  8 21:57:23.827: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2195 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 21:57:23.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 21:57:23.828: INFO: ExecWithOptions: Clientset creation
    Sep  8 21:57:23.828: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-2195/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Sep  8 21:57:23.936: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:57:23.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2195" for this suite. 09/08/23 21:57:23.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:57:24.004
Sep  8 21:57:24.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 21:57:24.005
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:24.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:24.046
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 09/08/23 21:57:24.054
Sep  8 21:57:24.078: INFO: Waiting up to 5m0s for pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab" in namespace "downward-api-1702" to be "Succeeded or Failed"
Sep  8 21:57:24.095: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.869919ms
Sep  8 21:57:26.109: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030180837s
Sep  8 21:57:28.114: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035507592s
STEP: Saw pod success 09/08/23 21:57:28.114
Sep  8 21:57:28.114: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab" satisfied condition "Succeeded or Failed"
Sep  8 21:57:28.125: INFO: Trying to get logs from node node-3 pod downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab container dapi-container: <nil>
STEP: delete the pod 09/08/23 21:57:28.151
Sep  8 21:57:28.194: INFO: Waiting for pod downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab to disappear
Sep  8 21:57:28.203: INFO: Pod downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  8 21:57:28.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1702" for this suite. 09/08/23 21:57:28.214
------------------------------
â€¢ [4.231 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:57:24.004
    Sep  8 21:57:24.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 21:57:24.005
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:24.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:24.046
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 09/08/23 21:57:24.054
    Sep  8 21:57:24.078: INFO: Waiting up to 5m0s for pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab" in namespace "downward-api-1702" to be "Succeeded or Failed"
    Sep  8 21:57:24.095: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.869919ms
    Sep  8 21:57:26.109: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030180837s
    Sep  8 21:57:28.114: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035507592s
    STEP: Saw pod success 09/08/23 21:57:28.114
    Sep  8 21:57:28.114: INFO: Pod "downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab" satisfied condition "Succeeded or Failed"
    Sep  8 21:57:28.125: INFO: Trying to get logs from node node-3 pod downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab container dapi-container: <nil>
    STEP: delete the pod 09/08/23 21:57:28.151
    Sep  8 21:57:28.194: INFO: Waiting for pod downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab to disappear
    Sep  8 21:57:28.203: INFO: Pod downward-api-9ae6091c-44b0-48e8-a451-0dd23e5de8ab no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:57:28.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1702" for this suite. 09/08/23 21:57:28.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:57:28.235
Sep  8 21:57:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubelet-test 09/08/23 21:57:28.237
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:28.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:28.268
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  8 21:57:32.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2707" for this suite. 09/08/23 21:57:32.351
------------------------------
â€¢ [4.128 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:57:28.235
    Sep  8 21:57:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubelet-test 09/08/23 21:57:28.237
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:28.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:28.268
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:57:32.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2707" for this suite. 09/08/23 21:57:32.351
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:57:32.366
Sep  8 21:57:32.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 21:57:32.368
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:32.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:32.424
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-jf69v" 09/08/23 21:57:32.446
Sep  8 21:57:32.471: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard cpu limit of 500m
Sep  8 21:57:32.471: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-jf69v" /status 09/08/23 21:57:32.471
STEP: Confirm /status for "e2e-rq-status-jf69v" resourceQuota via watch 09/08/23 21:57:32.495
Sep  8 21:57:32.498: INFO: observed resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList(nil)
Sep  8 21:57:32.498: INFO: Found resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  8 21:57:32.498: INFO: ResourceQuota "e2e-rq-status-jf69v" /status was updated
STEP: Patching hard spec values for cpu & memory 09/08/23 21:57:32.507
Sep  8 21:57:32.526: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard cpu limit of 1
Sep  8 21:57:32.526: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-jf69v" /status 09/08/23 21:57:32.526
STEP: Confirm /status for "e2e-rq-status-jf69v" resourceQuota via watch 09/08/23 21:57:32.548
Sep  8 21:57:32.551: INFO: observed resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Sep  8 21:57:32.551: INFO: Found resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Sep  8 21:57:32.551: INFO: ResourceQuota "e2e-rq-status-jf69v" /status was patched
STEP: Get "e2e-rq-status-jf69v" /status 09/08/23 21:57:32.551
Sep  8 21:57:32.565: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard cpu of 1
Sep  8 21:57:32.565: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-jf69v" /status before checking Spec is unchanged 09/08/23 21:57:32.575
Sep  8 21:57:32.590: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard cpu of 2
Sep  8 21:57:32.591: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard memory of 2Gi
Sep  8 21:57:32.599: INFO: Found resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Sep  8 21:59:57.621: INFO: ResourceQuota "e2e-rq-status-jf69v" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 21:59:57.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7826" for this suite. 09/08/23 21:59:57.637
------------------------------
â€¢ [SLOW TEST] [145.295 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:57:32.366
    Sep  8 21:57:32.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 21:57:32.368
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:57:32.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:57:32.424
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-jf69v" 09/08/23 21:57:32.446
    Sep  8 21:57:32.471: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard cpu limit of 500m
    Sep  8 21:57:32.471: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-jf69v" /status 09/08/23 21:57:32.471
    STEP: Confirm /status for "e2e-rq-status-jf69v" resourceQuota via watch 09/08/23 21:57:32.495
    Sep  8 21:57:32.498: INFO: observed resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList(nil)
    Sep  8 21:57:32.498: INFO: Found resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  8 21:57:32.498: INFO: ResourceQuota "e2e-rq-status-jf69v" /status was updated
    STEP: Patching hard spec values for cpu & memory 09/08/23 21:57:32.507
    Sep  8 21:57:32.526: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard cpu limit of 1
    Sep  8 21:57:32.526: INFO: Resource quota "e2e-rq-status-jf69v" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-jf69v" /status 09/08/23 21:57:32.526
    STEP: Confirm /status for "e2e-rq-status-jf69v" resourceQuota via watch 09/08/23 21:57:32.548
    Sep  8 21:57:32.551: INFO: observed resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Sep  8 21:57:32.551: INFO: Found resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Sep  8 21:57:32.551: INFO: ResourceQuota "e2e-rq-status-jf69v" /status was patched
    STEP: Get "e2e-rq-status-jf69v" /status 09/08/23 21:57:32.551
    Sep  8 21:57:32.565: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard cpu of 1
    Sep  8 21:57:32.565: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-jf69v" /status before checking Spec is unchanged 09/08/23 21:57:32.575
    Sep  8 21:57:32.590: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard cpu of 2
    Sep  8 21:57:32.591: INFO: Resourcequota "e2e-rq-status-jf69v" reports status: hard memory of 2Gi
    Sep  8 21:57:32.599: INFO: Found resourceQuota "e2e-rq-status-jf69v" in namespace "resourcequota-7826" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Sep  8 21:59:57.621: INFO: ResourceQuota "e2e-rq-status-jf69v" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 21:59:57.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7826" for this suite. 09/08/23 21:59:57.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 21:59:57.662
Sep  8 21:59:57.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 21:59:57.663
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:59:57.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:59:57.736
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-bc24c327-a872-469e-85ef-234b8f78542e 09/08/23 21:59:57.751
STEP: Creating configMap with name cm-test-opt-upd-4144c7db-6394-4fdf-8d6b-06a88b2a8410 09/08/23 21:59:57.763
STEP: Creating the pod 09/08/23 21:59:57.778
Sep  8 21:59:57.808: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a" in namespace "projected-6561" to be "running and ready"
Sep  8 21:59:57.843: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.432181ms
Sep  8 21:59:57.843: INFO: The phase of Pod pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a is Pending, waiting for it to be Running (with Ready = true)
Sep  8 21:59:59.854: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045350856s
Sep  8 21:59:59.854: INFO: The phase of Pod pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:00:01.854: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a": Phase="Running", Reason="", readiness=true. Elapsed: 4.045182265s
Sep  8 22:00:01.854: INFO: The phase of Pod pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a is Running (Ready = true)
Sep  8 22:00:01.854: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-bc24c327-a872-469e-85ef-234b8f78542e 09/08/23 22:00:01.931
STEP: Updating configmap cm-test-opt-upd-4144c7db-6394-4fdf-8d6b-06a88b2a8410 09/08/23 22:00:01.949
STEP: Creating configMap with name cm-test-opt-create-12ac4f97-d546-4dd0-bf72-63784ae80449 09/08/23 22:00:01.986
STEP: waiting to observe update in volume 09/08/23 22:00:02.005
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:01:14.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6561" for this suite. 09/08/23 22:01:15.009
------------------------------
â€¢ [SLOW TEST] [77.385 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 21:59:57.662
    Sep  8 21:59:57.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 21:59:57.663
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 21:59:57.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 21:59:57.736
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-bc24c327-a872-469e-85ef-234b8f78542e 09/08/23 21:59:57.751
    STEP: Creating configMap with name cm-test-opt-upd-4144c7db-6394-4fdf-8d6b-06a88b2a8410 09/08/23 21:59:57.763
    STEP: Creating the pod 09/08/23 21:59:57.778
    Sep  8 21:59:57.808: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a" in namespace "projected-6561" to be "running and ready"
    Sep  8 21:59:57.843: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.432181ms
    Sep  8 21:59:57.843: INFO: The phase of Pod pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 21:59:59.854: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045350856s
    Sep  8 21:59:59.854: INFO: The phase of Pod pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:00:01.854: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a": Phase="Running", Reason="", readiness=true. Elapsed: 4.045182265s
    Sep  8 22:00:01.854: INFO: The phase of Pod pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a is Running (Ready = true)
    Sep  8 22:00:01.854: INFO: Pod "pod-projected-configmaps-dfecd8f3-dd1f-4fd9-8bb2-32aa5d4ebc6a" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-bc24c327-a872-469e-85ef-234b8f78542e 09/08/23 22:00:01.931
    STEP: Updating configmap cm-test-opt-upd-4144c7db-6394-4fdf-8d6b-06a88b2a8410 09/08/23 22:00:01.949
    STEP: Creating configMap with name cm-test-opt-create-12ac4f97-d546-4dd0-bf72-63784ae80449 09/08/23 22:00:01.986
    STEP: waiting to observe update in volume 09/08/23 22:00:02.005
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:01:14.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6561" for this suite. 09/08/23 22:01:15.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:01:15.048
Sep  8 22:01:15.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 22:01:15.051
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:15.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:15.105
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 09/08/23 22:01:15.116
STEP: listing secrets in all namespaces to ensure that there are more than zero 09/08/23 22:01:15.142
STEP: patching the secret 09/08/23 22:01:15.164
STEP: deleting the secret using a LabelSelector 09/08/23 22:01:15.193
STEP: listing secrets in all namespaces, searching for label name and value in patch 09/08/23 22:01:15.219
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 22:01:15.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-846" for this suite. 09/08/23 22:01:15.256
------------------------------
â€¢ [0.224 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:01:15.048
    Sep  8 22:01:15.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 22:01:15.051
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:15.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:15.105
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 09/08/23 22:01:15.116
    STEP: listing secrets in all namespaces to ensure that there are more than zero 09/08/23 22:01:15.142
    STEP: patching the secret 09/08/23 22:01:15.164
    STEP: deleting the secret using a LabelSelector 09/08/23 22:01:15.193
    STEP: listing secrets in all namespaces, searching for label name and value in patch 09/08/23 22:01:15.219
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:01:15.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-846" for this suite. 09/08/23 22:01:15.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:01:15.279
Sep  8 22:01:15.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 22:01:15.281
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:15.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:15.322
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 09/08/23 22:01:15.328
Sep  8 22:01:15.357: INFO: Waiting up to 5m0s for pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888" in namespace "emptydir-3709" to be "Succeeded or Failed"
Sep  8 22:01:15.367: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221218ms
Sep  8 22:01:17.378: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020890328s
Sep  8 22:01:19.377: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020248375s
Sep  8 22:01:21.383: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025666171s
STEP: Saw pod success 09/08/23 22:01:21.383
Sep  8 22:01:21.383: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888" satisfied condition "Succeeded or Failed"
Sep  8 22:01:21.390: INFO: Trying to get logs from node node-3 pod pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888 container test-container: <nil>
STEP: delete the pod 09/08/23 22:01:21.409
Sep  8 22:01:21.455: INFO: Waiting for pod pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888 to disappear
Sep  8 22:01:21.482: INFO: Pod pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:01:21.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3709" for this suite. 09/08/23 22:01:21.502
------------------------------
â€¢ [SLOW TEST] [6.264 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:01:15.279
    Sep  8 22:01:15.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 22:01:15.281
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:15.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:15.322
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/08/23 22:01:15.328
    Sep  8 22:01:15.357: INFO: Waiting up to 5m0s for pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888" in namespace "emptydir-3709" to be "Succeeded or Failed"
    Sep  8 22:01:15.367: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221218ms
    Sep  8 22:01:17.378: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020890328s
    Sep  8 22:01:19.377: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020248375s
    Sep  8 22:01:21.383: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025666171s
    STEP: Saw pod success 09/08/23 22:01:21.383
    Sep  8 22:01:21.383: INFO: Pod "pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888" satisfied condition "Succeeded or Failed"
    Sep  8 22:01:21.390: INFO: Trying to get logs from node node-3 pod pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888 container test-container: <nil>
    STEP: delete the pod 09/08/23 22:01:21.409
    Sep  8 22:01:21.455: INFO: Waiting for pod pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888 to disappear
    Sep  8 22:01:21.482: INFO: Pod pod-5d8b819c-03f6-4a58-b577-0f8e74ebf888 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:01:21.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3709" for this suite. 09/08/23 22:01:21.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:01:21.544
Sep  8 22:01:21.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 22:01:21.545
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:21.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:21.589
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 22:01:21.634
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:01:22.587
STEP: Deploying the webhook pod 09/08/23 22:01:22.623
STEP: Wait for the deployment to be ready 09/08/23 22:01:22.653
Sep  8 22:01:22.675: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/08/23 22:01:24.708
STEP: Verifying the service has paired with the endpoint 09/08/23 22:01:24.757
Sep  8 22:01:25.757: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 09/08/23 22:01:25.94
STEP: Creating a configMap that should be mutated 09/08/23 22:01:25.973
STEP: Deleting the collection of validation webhooks 09/08/23 22:01:26.04
STEP: Creating a configMap that should not be mutated 09/08/23 22:01:26.199
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:01:26.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5088" for this suite. 09/08/23 22:01:26.369
STEP: Destroying namespace "webhook-5088-markers" for this suite. 09/08/23 22:01:26.394
------------------------------
â€¢ [4.876 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:01:21.544
    Sep  8 22:01:21.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 22:01:21.545
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:21.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:21.589
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 22:01:21.634
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:01:22.587
    STEP: Deploying the webhook pod 09/08/23 22:01:22.623
    STEP: Wait for the deployment to be ready 09/08/23 22:01:22.653
    Sep  8 22:01:22.675: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/08/23 22:01:24.708
    STEP: Verifying the service has paired with the endpoint 09/08/23 22:01:24.757
    Sep  8 22:01:25.757: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 09/08/23 22:01:25.94
    STEP: Creating a configMap that should be mutated 09/08/23 22:01:25.973
    STEP: Deleting the collection of validation webhooks 09/08/23 22:01:26.04
    STEP: Creating a configMap that should not be mutated 09/08/23 22:01:26.199
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:01:26.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5088" for this suite. 09/08/23 22:01:26.369
    STEP: Destroying namespace "webhook-5088-markers" for this suite. 09/08/23 22:01:26.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:01:26.427
Sep  8 22:01:26.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename prestop 09/08/23 22:01:26.428
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:26.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:26.478
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9455 09/08/23 22:01:26.486
STEP: Waiting for pods to come up. 09/08/23 22:01:26.509
Sep  8 22:01:26.509: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9455" to be "running"
Sep  8 22:01:26.526: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 17.152968ms
Sep  8 22:01:28.542: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032769983s
Sep  8 22:01:30.540: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.031307116s
Sep  8 22:01:30.540: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9455 09/08/23 22:01:30.549
Sep  8 22:01:30.566: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9455" to be "running"
Sep  8 22:01:30.591: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 24.051896ms
Sep  8 22:01:32.599: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031753547s
Sep  8 22:01:34.600: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.032707023s
Sep  8 22:01:34.600: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 09/08/23 22:01:34.6
Sep  8 22:01:39.668: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 09/08/23 22:01:39.669
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Sep  8 22:01:39.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-9455" for this suite. 09/08/23 22:01:39.738
------------------------------
â€¢ [SLOW TEST] [13.329 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:01:26.427
    Sep  8 22:01:26.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename prestop 09/08/23 22:01:26.428
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:26.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:26.478
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9455 09/08/23 22:01:26.486
    STEP: Waiting for pods to come up. 09/08/23 22:01:26.509
    Sep  8 22:01:26.509: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9455" to be "running"
    Sep  8 22:01:26.526: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 17.152968ms
    Sep  8 22:01:28.542: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032769983s
    Sep  8 22:01:30.540: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.031307116s
    Sep  8 22:01:30.540: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9455 09/08/23 22:01:30.549
    Sep  8 22:01:30.566: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9455" to be "running"
    Sep  8 22:01:30.591: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 24.051896ms
    Sep  8 22:01:32.599: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031753547s
    Sep  8 22:01:34.600: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.032707023s
    Sep  8 22:01:34.600: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 09/08/23 22:01:34.6
    Sep  8 22:01:39.668: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 09/08/23 22:01:39.669
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:01:39.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-9455" for this suite. 09/08/23 22:01:39.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:01:39.759
Sep  8 22:01:39.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 22:01:39.76
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:39.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:39.814
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Sep  8 22:01:39.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:01:48.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2919" for this suite. 09/08/23 22:01:48.091
------------------------------
â€¢ [SLOW TEST] [8.361 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:01:39.759
    Sep  8 22:01:39.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 22:01:39.76
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:39.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:39.814
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Sep  8 22:01:39.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:01:48.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2919" for this suite. 09/08/23 22:01:48.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:01:48.121
Sep  8 22:01:48.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 22:01:48.123
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:48.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:48.167
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 09/08/23 22:01:48.173
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7195;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7195;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +notcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_tcp@PTR;sleep 1; done
 09/08/23 22:01:48.226
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7195;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7195;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +notcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_tcp@PTR;sleep 1; done
 09/08/23 22:01:48.227
STEP: creating a pod to probe DNS 09/08/23 22:01:48.227
STEP: submitting the pod to kubernetes 09/08/23 22:01:48.228
Sep  8 22:01:48.257: INFO: Waiting up to 15m0s for pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb" in namespace "dns-7195" to be "running"
Sep  8 22:01:48.274: INFO: Pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.258648ms
Sep  8 22:01:50.285: INFO: Pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.027752639s
Sep  8 22:01:50.285: INFO: Pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb" satisfied condition "running"
STEP: retrieving the pod 09/08/23 22:01:50.286
STEP: looking for the results for each expected name from probers 09/08/23 22:01:50.293
Sep  8 22:01:50.303: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.319: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.331: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.340: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.349: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.358: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.440: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.452: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.463: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.472: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.483: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.492: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
Sep  8 22:01:50.556: INFO: Lookups using dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc]

Sep  8 22:01:55.848: INFO: DNS probes using dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb succeeded

STEP: deleting the pod 09/08/23 22:01:55.848
STEP: deleting the test service 09/08/23 22:01:55.887
STEP: deleting the test headless service 09/08/23 22:01:55.996
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 22:01:56.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7195" for this suite. 09/08/23 22:01:56.073
------------------------------
â€¢ [SLOW TEST] [7.973 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:01:48.121
    Sep  8 22:01:48.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 22:01:48.123
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:48.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:48.167
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 09/08/23 22:01:48.173
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7195;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7195;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +notcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_tcp@PTR;sleep 1; done
     09/08/23 22:01:48.226
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7195;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7195;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7195.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7195.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7195.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7195.svc;check="$$(dig +notcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.45.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.45.58_tcp@PTR;sleep 1; done
     09/08/23 22:01:48.227
    STEP: creating a pod to probe DNS 09/08/23 22:01:48.227
    STEP: submitting the pod to kubernetes 09/08/23 22:01:48.228
    Sep  8 22:01:48.257: INFO: Waiting up to 15m0s for pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb" in namespace "dns-7195" to be "running"
    Sep  8 22:01:48.274: INFO: Pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.258648ms
    Sep  8 22:01:50.285: INFO: Pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb": Phase="Running", Reason="", readiness=true. Elapsed: 2.027752639s
    Sep  8 22:01:50.285: INFO: Pod "dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 22:01:50.286
    STEP: looking for the results for each expected name from probers 09/08/23 22:01:50.293
    Sep  8 22:01:50.303: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.319: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.331: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.340: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.349: INFO: Unable to read wheezy_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.358: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.440: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.452: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.463: INFO: Unable to read jessie_udp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.472: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195 from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.483: INFO: Unable to read jessie_udp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.492: INFO: Unable to read jessie_tcp@dns-test-service.dns-7195.svc from pod dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb: the server could not find the requested resource (get pods dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb)
    Sep  8 22:01:50.556: INFO: Lookups using dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7195 wheezy_tcp@dns-test-service.dns-7195 wheezy_udp@dns-test-service.dns-7195.svc wheezy_tcp@dns-test-service.dns-7195.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7195 jessie_tcp@dns-test-service.dns-7195 jessie_udp@dns-test-service.dns-7195.svc jessie_tcp@dns-test-service.dns-7195.svc]

    Sep  8 22:01:55.848: INFO: DNS probes using dns-7195/dns-test-856d0834-6482-48a9-a5d3-c6a12c3ee1cb succeeded

    STEP: deleting the pod 09/08/23 22:01:55.848
    STEP: deleting the test service 09/08/23 22:01:55.887
    STEP: deleting the test headless service 09/08/23 22:01:55.996
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:01:56.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7195" for this suite. 09/08/23 22:01:56.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:01:56.099
Sep  8 22:01:56.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename subpath 09/08/23 22:01:56.1
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:56.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:56.152
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/08/23 22:01:56.161
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-ggkw 09/08/23 22:01:56.19
STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:01:56.19
Sep  8 22:01:56.211: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ggkw" in namespace "subpath-854" to be "Succeeded or Failed"
Sep  8 22:01:56.228: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Pending", Reason="", readiness=false. Elapsed: 17.177945ms
Sep  8 22:01:58.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.026556695s
Sep  8 22:02:00.239: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 4.028385053s
Sep  8 22:02:02.253: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 6.042466812s
Sep  8 22:02:04.244: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 8.03348878s
Sep  8 22:02:06.243: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 10.031969819s
Sep  8 22:02:08.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 12.026557825s
Sep  8 22:02:10.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 14.027454004s
Sep  8 22:02:12.241: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 16.030207377s
Sep  8 22:02:14.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 18.026875843s
Sep  8 22:02:16.247: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 20.035566088s
Sep  8 22:02:18.240: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=false. Elapsed: 22.029102958s
Sep  8 22:02:20.243: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031901507s
STEP: Saw pod success 09/08/23 22:02:20.243
Sep  8 22:02:20.243: INFO: Pod "pod-subpath-test-projected-ggkw" satisfied condition "Succeeded or Failed"
Sep  8 22:02:20.251: INFO: Trying to get logs from node node-3 pod pod-subpath-test-projected-ggkw container test-container-subpath-projected-ggkw: <nil>
STEP: delete the pod 09/08/23 22:02:20.289
Sep  8 22:02:20.325: INFO: Waiting for pod pod-subpath-test-projected-ggkw to disappear
Sep  8 22:02:20.334: INFO: Pod pod-subpath-test-projected-ggkw no longer exists
STEP: Deleting pod pod-subpath-test-projected-ggkw 09/08/23 22:02:20.334
Sep  8 22:02:20.335: INFO: Deleting pod "pod-subpath-test-projected-ggkw" in namespace "subpath-854"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  8 22:02:20.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-854" for this suite. 09/08/23 22:02:20.352
------------------------------
â€¢ [SLOW TEST] [24.268 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:01:56.099
    Sep  8 22:01:56.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename subpath 09/08/23 22:01:56.1
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:01:56.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:01:56.152
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/08/23 22:01:56.161
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-ggkw 09/08/23 22:01:56.19
    STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:01:56.19
    Sep  8 22:01:56.211: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-ggkw" in namespace "subpath-854" to be "Succeeded or Failed"
    Sep  8 22:01:56.228: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Pending", Reason="", readiness=false. Elapsed: 17.177945ms
    Sep  8 22:01:58.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.026556695s
    Sep  8 22:02:00.239: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 4.028385053s
    Sep  8 22:02:02.253: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 6.042466812s
    Sep  8 22:02:04.244: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 8.03348878s
    Sep  8 22:02:06.243: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 10.031969819s
    Sep  8 22:02:08.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 12.026557825s
    Sep  8 22:02:10.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 14.027454004s
    Sep  8 22:02:12.241: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 16.030207377s
    Sep  8 22:02:14.238: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 18.026875843s
    Sep  8 22:02:16.247: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=true. Elapsed: 20.035566088s
    Sep  8 22:02:18.240: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Running", Reason="", readiness=false. Elapsed: 22.029102958s
    Sep  8 22:02:20.243: INFO: Pod "pod-subpath-test-projected-ggkw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.031901507s
    STEP: Saw pod success 09/08/23 22:02:20.243
    Sep  8 22:02:20.243: INFO: Pod "pod-subpath-test-projected-ggkw" satisfied condition "Succeeded or Failed"
    Sep  8 22:02:20.251: INFO: Trying to get logs from node node-3 pod pod-subpath-test-projected-ggkw container test-container-subpath-projected-ggkw: <nil>
    STEP: delete the pod 09/08/23 22:02:20.289
    Sep  8 22:02:20.325: INFO: Waiting for pod pod-subpath-test-projected-ggkw to disappear
    Sep  8 22:02:20.334: INFO: Pod pod-subpath-test-projected-ggkw no longer exists
    STEP: Deleting pod pod-subpath-test-projected-ggkw 09/08/23 22:02:20.334
    Sep  8 22:02:20.335: INFO: Deleting pod "pod-subpath-test-projected-ggkw" in namespace "subpath-854"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:02:20.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-854" for this suite. 09/08/23 22:02:20.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:02:20.38
Sep  8 22:02:20.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 22:02:20.381
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:20.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:20.419
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-9417a851-2d03-4800-b477-e5e864b10032 09/08/23 22:02:20.424
STEP: Creating a pod to test consume configMaps 09/08/23 22:02:20.439
Sep  8 22:02:20.465: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374" in namespace "configmap-4058" to be "Succeeded or Failed"
Sep  8 22:02:20.487: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374": Phase="Pending", Reason="", readiness=false. Elapsed: 21.85819ms
Sep  8 22:02:22.498: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032677762s
Sep  8 22:02:24.498: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032939866s
STEP: Saw pod success 09/08/23 22:02:24.499
Sep  8 22:02:24.499: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374" satisfied condition "Succeeded or Failed"
Sep  8 22:02:24.506: INFO: Trying to get logs from node node-3 pod pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 22:02:24.523
Sep  8 22:02:24.555: INFO: Waiting for pod pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374 to disappear
Sep  8 22:02:24.563: INFO: Pod pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:02:24.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4058" for this suite. 09/08/23 22:02:24.575
------------------------------
â€¢ [4.212 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:02:20.38
    Sep  8 22:02:20.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 22:02:20.381
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:20.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:20.419
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-9417a851-2d03-4800-b477-e5e864b10032 09/08/23 22:02:20.424
    STEP: Creating a pod to test consume configMaps 09/08/23 22:02:20.439
    Sep  8 22:02:20.465: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374" in namespace "configmap-4058" to be "Succeeded or Failed"
    Sep  8 22:02:20.487: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374": Phase="Pending", Reason="", readiness=false. Elapsed: 21.85819ms
    Sep  8 22:02:22.498: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032677762s
    Sep  8 22:02:24.498: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032939866s
    STEP: Saw pod success 09/08/23 22:02:24.499
    Sep  8 22:02:24.499: INFO: Pod "pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374" satisfied condition "Succeeded or Failed"
    Sep  8 22:02:24.506: INFO: Trying to get logs from node node-3 pod pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 22:02:24.523
    Sep  8 22:02:24.555: INFO: Waiting for pod pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374 to disappear
    Sep  8 22:02:24.563: INFO: Pod pod-configmaps-2e0de163-5807-4410-91bb-a226a66fe374 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:02:24.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4058" for this suite. 09/08/23 22:02:24.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:02:24.595
Sep  8 22:02:24.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svcaccounts 09/08/23 22:02:24.597
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:24.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:24.639
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Sep  8 22:02:24.655: INFO: Got root ca configmap in namespace "svcaccounts-3207"
Sep  8 22:02:24.671: INFO: Deleted root ca configmap in namespace "svcaccounts-3207"
STEP: waiting for a new root ca configmap created 09/08/23 22:02:25.172
Sep  8 22:02:25.182: INFO: Recreated root ca configmap in namespace "svcaccounts-3207"
Sep  8 22:02:25.197: INFO: Updated root ca configmap in namespace "svcaccounts-3207"
STEP: waiting for the root ca configmap reconciled 09/08/23 22:02:25.698
Sep  8 22:02:25.708: INFO: Reconciled root ca configmap in namespace "svcaccounts-3207"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  8 22:02:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3207" for this suite. 09/08/23 22:02:25.722
------------------------------
â€¢ [1.142 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:02:24.595
    Sep  8 22:02:24.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svcaccounts 09/08/23 22:02:24.597
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:24.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:24.639
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Sep  8 22:02:24.655: INFO: Got root ca configmap in namespace "svcaccounts-3207"
    Sep  8 22:02:24.671: INFO: Deleted root ca configmap in namespace "svcaccounts-3207"
    STEP: waiting for a new root ca configmap created 09/08/23 22:02:25.172
    Sep  8 22:02:25.182: INFO: Recreated root ca configmap in namespace "svcaccounts-3207"
    Sep  8 22:02:25.197: INFO: Updated root ca configmap in namespace "svcaccounts-3207"
    STEP: waiting for the root ca configmap reconciled 09/08/23 22:02:25.698
    Sep  8 22:02:25.708: INFO: Reconciled root ca configmap in namespace "svcaccounts-3207"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:02:25.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3207" for this suite. 09/08/23 22:02:25.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:02:25.739
Sep  8 22:02:25.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 22:02:25.741
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:25.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:25.783
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/08/23 22:02:25.84
Sep  8 22:02:25.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:02:34.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:02:51.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3253" for this suite. 09/08/23 22:02:51.074
------------------------------
â€¢ [SLOW TEST] [25.351 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:02:25.739
    Sep  8 22:02:25.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 22:02:25.741
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:25.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:25.783
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 09/08/23 22:02:25.84
    Sep  8 22:02:25.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:02:34.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:02:51.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3253" for this suite. 09/08/23 22:02:51.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:02:51.091
Sep  8 22:02:51.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:02:51.092
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:51.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:51.153
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-0e1efc8a-e7f7-4ff2-a29b-52edecdd1909 09/08/23 22:02:51.158
STEP: Creating a pod to test consume secrets 09/08/23 22:02:51.174
Sep  8 22:02:51.200: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602" in namespace "projected-7566" to be "Succeeded or Failed"
Sep  8 22:02:51.217: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602": Phase="Pending", Reason="", readiness=false. Elapsed: 16.528111ms
Sep  8 22:02:53.233: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032793767s
Sep  8 22:02:55.231: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030054859s
STEP: Saw pod success 09/08/23 22:02:55.231
Sep  8 22:02:55.231: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602" satisfied condition "Succeeded or Failed"
Sep  8 22:02:55.244: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:02:55.276
Sep  8 22:02:55.304: INFO: Waiting for pod pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602 to disappear
Sep  8 22:02:55.316: INFO: Pod pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  8 22:02:55.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7566" for this suite. 09/08/23 22:02:55.331
------------------------------
â€¢ [4.256 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:02:51.091
    Sep  8 22:02:51.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:02:51.092
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:51.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:51.153
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-0e1efc8a-e7f7-4ff2-a29b-52edecdd1909 09/08/23 22:02:51.158
    STEP: Creating a pod to test consume secrets 09/08/23 22:02:51.174
    Sep  8 22:02:51.200: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602" in namespace "projected-7566" to be "Succeeded or Failed"
    Sep  8 22:02:51.217: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602": Phase="Pending", Reason="", readiness=false. Elapsed: 16.528111ms
    Sep  8 22:02:53.233: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032793767s
    Sep  8 22:02:55.231: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030054859s
    STEP: Saw pod success 09/08/23 22:02:55.231
    Sep  8 22:02:55.231: INFO: Pod "pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602" satisfied condition "Succeeded or Failed"
    Sep  8 22:02:55.244: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:02:55.276
    Sep  8 22:02:55.304: INFO: Waiting for pod pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602 to disappear
    Sep  8 22:02:55.316: INFO: Pod pod-projected-secrets-af5c23d9-89c5-4b0a-8c9f-8983a8a4d602 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:02:55.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7566" for this suite. 09/08/23 22:02:55.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:02:55.355
Sep  8 22:02:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename limitrange 09/08/23 22:02:55.356
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:55.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:55.416
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-ffsck" in namespace "limitrange-432" 09/08/23 22:02:55.424
STEP: Creating another limitRange in another namespace 09/08/23 22:02:55.443
Sep  8 22:02:55.492: INFO: Namespace "e2e-limitrange-ffsck-5173" created
Sep  8 22:02:55.493: INFO: Creating LimitRange "e2e-limitrange-ffsck" in namespace "e2e-limitrange-ffsck-5173"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-ffsck" 09/08/23 22:02:55.504
Sep  8 22:02:55.513: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-ffsck" in "limitrange-432" namespace 09/08/23 22:02:55.513
Sep  8 22:02:55.536: INFO: LimitRange "e2e-limitrange-ffsck" has been patched
STEP: Delete LimitRange "e2e-limitrange-ffsck" by Collection with labelSelector: "e2e-limitrange-ffsck=patched" 09/08/23 22:02:55.536
STEP: Confirm that the limitRange "e2e-limitrange-ffsck" has been deleted 09/08/23 22:02:55.576
Sep  8 22:02:55.577: INFO: Requesting list of LimitRange to confirm quantity
Sep  8 22:02:55.587: INFO: Found 0 LimitRange with label "e2e-limitrange-ffsck=patched"
Sep  8 22:02:55.587: INFO: LimitRange "e2e-limitrange-ffsck" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-ffsck" 09/08/23 22:02:55.587
Sep  8 22:02:55.596: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  8 22:02:55.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-432" for this suite. 09/08/23 22:02:55.611
STEP: Destroying namespace "e2e-limitrange-ffsck-5173" for this suite. 09/08/23 22:02:55.63
------------------------------
â€¢ [0.305 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:02:55.355
    Sep  8 22:02:55.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename limitrange 09/08/23 22:02:55.356
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:55.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:55.416
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-ffsck" in namespace "limitrange-432" 09/08/23 22:02:55.424
    STEP: Creating another limitRange in another namespace 09/08/23 22:02:55.443
    Sep  8 22:02:55.492: INFO: Namespace "e2e-limitrange-ffsck-5173" created
    Sep  8 22:02:55.493: INFO: Creating LimitRange "e2e-limitrange-ffsck" in namespace "e2e-limitrange-ffsck-5173"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-ffsck" 09/08/23 22:02:55.504
    Sep  8 22:02:55.513: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-ffsck" in "limitrange-432" namespace 09/08/23 22:02:55.513
    Sep  8 22:02:55.536: INFO: LimitRange "e2e-limitrange-ffsck" has been patched
    STEP: Delete LimitRange "e2e-limitrange-ffsck" by Collection with labelSelector: "e2e-limitrange-ffsck=patched" 09/08/23 22:02:55.536
    STEP: Confirm that the limitRange "e2e-limitrange-ffsck" has been deleted 09/08/23 22:02:55.576
    Sep  8 22:02:55.577: INFO: Requesting list of LimitRange to confirm quantity
    Sep  8 22:02:55.587: INFO: Found 0 LimitRange with label "e2e-limitrange-ffsck=patched"
    Sep  8 22:02:55.587: INFO: LimitRange "e2e-limitrange-ffsck" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-ffsck" 09/08/23 22:02:55.587
    Sep  8 22:02:55.596: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:02:55.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-432" for this suite. 09/08/23 22:02:55.611
    STEP: Destroying namespace "e2e-limitrange-ffsck-5173" for this suite. 09/08/23 22:02:55.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:02:55.663
Sep  8 22:02:55.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename daemonsets 09/08/23 22:02:55.665
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:55.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:55.709
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Sep  8 22:02:55.786: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:02:55.802
Sep  8 22:02:55.816: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:55.816: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:55.816: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:55.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:02:55.835: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:02:56.868: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:56.868: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:56.868: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:56.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:02:56.887: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:02:57.852: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:57.852: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:57.852: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:57.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:02:57.861: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:02:58.851: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:58.851: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:58.851: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:58.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 22:02:58.861: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 09/08/23 22:02:58.892
STEP: Check that daemon pods images are updated. 09/08/23 22:02:58.927
Sep  8 22:02:58.974: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  8 22:02:58.986: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:58.986: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:58.986: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:02:59.998: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  8 22:03:00.020: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:00.020: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:00.020: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:00.997: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  8 22:03:00.997: INFO: Pod daemon-set-pxdhl is not available
Sep  8 22:03:01.027: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:01.028: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:01.028: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:02.031: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Sep  8 22:03:02.031: INFO: Pod daemon-set-pxdhl is not available
Sep  8 22:03:02.057: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:02.057: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:02.057: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:03.041: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:03.041: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:03.041: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:04.013: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:04.013: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:04.013: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:04.998: INFO: Pod daemon-set-rl7sf is not available
Sep  8 22:03:05.015: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:05.015: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:05.015: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 09/08/23 22:03:05.015
Sep  8 22:03:05.034: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:05.034: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:05.034: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:05.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:03:05.043: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:03:06.064: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:06.064: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:06.064: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:06.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:03:06.077: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:03:07.059: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:07.059: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:07.059: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:03:07.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 22:03:07.069: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/08/23 22:03:07.141
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4952, will wait for the garbage collector to delete the pods 09/08/23 22:03:07.141
Sep  8 22:03:07.219: INFO: Deleting DaemonSet.extensions daemon-set took: 18.034695ms
Sep  8 22:03:07.320: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.304484ms
Sep  8 22:03:10.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:03:10.342: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  8 22:03:10.355: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33201"},"items":null}

Sep  8 22:03:10.370: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33201"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:03:10.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4952" for this suite. 09/08/23 22:03:10.436
------------------------------
â€¢ [SLOW TEST] [14.796 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:02:55.663
    Sep  8 22:02:55.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename daemonsets 09/08/23 22:02:55.665
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:02:55.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:02:55.709
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Sep  8 22:02:55.786: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:02:55.802
    Sep  8 22:02:55.816: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:55.816: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:55.816: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:55.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:02:55.835: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:02:56.868: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:56.868: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:56.868: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:56.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:02:56.887: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:02:57.852: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:57.852: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:57.852: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:57.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:02:57.861: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:02:58.851: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:58.851: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:58.851: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:58.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 22:02:58.861: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 09/08/23 22:02:58.892
    STEP: Check that daemon pods images are updated. 09/08/23 22:02:58.927
    Sep  8 22:02:58.974: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  8 22:02:58.986: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:58.986: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:58.986: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:02:59.998: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  8 22:03:00.020: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:00.020: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:00.020: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:00.997: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  8 22:03:00.997: INFO: Pod daemon-set-pxdhl is not available
    Sep  8 22:03:01.027: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:01.028: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:01.028: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:02.031: INFO: Wrong image for pod: daemon-set-8qfrg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Sep  8 22:03:02.031: INFO: Pod daemon-set-pxdhl is not available
    Sep  8 22:03:02.057: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:02.057: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:02.057: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:03.041: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:03.041: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:03.041: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:04.013: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:04.013: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:04.013: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:04.998: INFO: Pod daemon-set-rl7sf is not available
    Sep  8 22:03:05.015: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:05.015: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:05.015: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 09/08/23 22:03:05.015
    Sep  8 22:03:05.034: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:05.034: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:05.034: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:05.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:03:05.043: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:03:06.064: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:06.064: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:06.064: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:06.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:03:06.077: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:03:07.059: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:07.059: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:07.059: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:03:07.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 22:03:07.069: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/08/23 22:03:07.141
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4952, will wait for the garbage collector to delete the pods 09/08/23 22:03:07.141
    Sep  8 22:03:07.219: INFO: Deleting DaemonSet.extensions daemon-set took: 18.034695ms
    Sep  8 22:03:07.320: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.304484ms
    Sep  8 22:03:10.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:03:10.342: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  8 22:03:10.355: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33201"},"items":null}

    Sep  8 22:03:10.370: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33201"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:03:10.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4952" for this suite. 09/08/23 22:03:10.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:03:10.46
Sep  8 22:03:10.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 22:03:10.462
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:10.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:10.504
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 22:03:10.55
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:03:10.999
STEP: Deploying the webhook pod 09/08/23 22:03:11.018
STEP: Wait for the deployment to be ready 09/08/23 22:03:11.046
Sep  8 22:03:11.065: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 22:03:13.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 22:03:15.102
STEP: Verifying the service has paired with the endpoint 09/08/23 22:03:15.138
Sep  8 22:03:16.139: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Sep  8 22:03:16.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7623-crds.webhook.example.com via the AdmissionRegistration API 09/08/23 22:03:21.678
STEP: Creating a custom resource that should be mutated by the webhook 09/08/23 22:03:21.753
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:03:24.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3156" for this suite. 09/08/23 22:03:24.566
STEP: Destroying namespace "webhook-3156-markers" for this suite. 09/08/23 22:03:24.603
------------------------------
â€¢ [SLOW TEST] [14.166 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:03:10.46
    Sep  8 22:03:10.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 22:03:10.462
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:10.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:10.504
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 22:03:10.55
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:03:10.999
    STEP: Deploying the webhook pod 09/08/23 22:03:11.018
    STEP: Wait for the deployment to be ready 09/08/23 22:03:11.046
    Sep  8 22:03:11.065: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 22:03:13.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 3, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 22:03:15.102
    STEP: Verifying the service has paired with the endpoint 09/08/23 22:03:15.138
    Sep  8 22:03:16.139: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Sep  8 22:03:16.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7623-crds.webhook.example.com via the AdmissionRegistration API 09/08/23 22:03:21.678
    STEP: Creating a custom resource that should be mutated by the webhook 09/08/23 22:03:21.753
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:03:24.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3156" for this suite. 09/08/23 22:03:24.566
    STEP: Destroying namespace "webhook-3156-markers" for this suite. 09/08/23 22:03:24.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:03:24.629
Sep  8 22:03:24.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:03:24.633
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:24.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:24.696
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 09/08/23 22:03:24.707
Sep  8 22:03:24.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-4852 create -f -'
Sep  8 22:03:26.229: INFO: stderr: ""
Sep  8 22:03:26.229: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/08/23 22:03:26.229
Sep  8 22:03:27.241: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:03:27.241: INFO: Found 0 / 1
Sep  8 22:03:28.241: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:03:28.242: INFO: Found 0 / 1
Sep  8 22:03:29.239: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:03:29.240: INFO: Found 1 / 1
Sep  8 22:03:29.240: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 09/08/23 22:03:29.24
Sep  8 22:03:29.253: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:03:29.254: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  8 22:03:29.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-4852 patch pod agnhost-primary-fbpmc -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  8 22:03:29.408: INFO: stderr: ""
Sep  8 22:03:29.408: INFO: stdout: "pod/agnhost-primary-fbpmc patched\n"
STEP: checking annotations 09/08/23 22:03:29.408
Sep  8 22:03:29.416: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:03:29.416: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:03:29.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4852" for this suite. 09/08/23 22:03:29.427
------------------------------
â€¢ [4.824 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:03:24.629
    Sep  8 22:03:24.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:03:24.633
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:24.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:24.696
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 09/08/23 22:03:24.707
    Sep  8 22:03:24.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-4852 create -f -'
    Sep  8 22:03:26.229: INFO: stderr: ""
    Sep  8 22:03:26.229: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/08/23 22:03:26.229
    Sep  8 22:03:27.241: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:03:27.241: INFO: Found 0 / 1
    Sep  8 22:03:28.241: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:03:28.242: INFO: Found 0 / 1
    Sep  8 22:03:29.239: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:03:29.240: INFO: Found 1 / 1
    Sep  8 22:03:29.240: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 09/08/23 22:03:29.24
    Sep  8 22:03:29.253: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:03:29.254: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  8 22:03:29.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-4852 patch pod agnhost-primary-fbpmc -p {"metadata":{"annotations":{"x":"y"}}}'
    Sep  8 22:03:29.408: INFO: stderr: ""
    Sep  8 22:03:29.408: INFO: stdout: "pod/agnhost-primary-fbpmc patched\n"
    STEP: checking annotations 09/08/23 22:03:29.408
    Sep  8 22:03:29.416: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:03:29.416: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:03:29.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4852" for this suite. 09/08/23 22:03:29.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:03:29.458
Sep  8 22:03:29.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 22:03:29.459
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:29.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:29.509
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/08/23 22:03:29.532
Sep  8 22:03:29.567: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9514" to be "running and ready"
Sep  8 22:03:29.583: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.527955ms
Sep  8 22:03:29.583: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:03:31.601: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.033643103s
Sep  8 22:03:31.601: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  8 22:03:31.601: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 09/08/23 22:03:31.61
Sep  8 22:03:31.627: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9514" to be "running and ready"
Sep  8 22:03:31.638: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.829114ms
Sep  8 22:03:31.638: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:03:33.650: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.022986271s
Sep  8 22:03:33.650: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Sep  8 22:03:33.650: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 09/08/23 22:03:33.663
STEP: delete the pod with lifecycle hook 09/08/23 22:03:33.682
Sep  8 22:03:33.742: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  8 22:03:33.751: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  8 22:03:35.752: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  8 22:03:35.760: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  8 22:03:37.751: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  8 22:03:37.766: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  8 22:03:37.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9514" for this suite. 09/08/23 22:03:37.785
------------------------------
â€¢ [SLOW TEST] [8.362 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:03:29.458
    Sep  8 22:03:29.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 22:03:29.459
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:29.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:29.509
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/08/23 22:03:29.532
    Sep  8 22:03:29.567: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9514" to be "running and ready"
    Sep  8 22:03:29.583: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.527955ms
    Sep  8 22:03:29.583: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:03:31.601: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.033643103s
    Sep  8 22:03:31.601: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  8 22:03:31.601: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 09/08/23 22:03:31.61
    Sep  8 22:03:31.627: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9514" to be "running and ready"
    Sep  8 22:03:31.638: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.829114ms
    Sep  8 22:03:31.638: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:03:33.650: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.022986271s
    Sep  8 22:03:33.650: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Sep  8 22:03:33.650: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 09/08/23 22:03:33.663
    STEP: delete the pod with lifecycle hook 09/08/23 22:03:33.682
    Sep  8 22:03:33.742: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  8 22:03:33.751: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  8 22:03:35.752: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  8 22:03:35.760: INFO: Pod pod-with-poststart-exec-hook still exists
    Sep  8 22:03:37.751: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Sep  8 22:03:37.766: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:03:37.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9514" for this suite. 09/08/23 22:03:37.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:03:37.839
Sep  8 22:03:37.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename security-context 09/08/23 22:03:37.84
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:37.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:37.916
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/08/23 22:03:37.922
Sep  8 22:03:37.968: INFO: Waiting up to 5m0s for pod "security-context-9d82dd01-16d1-4414-b205-667713fac462" in namespace "security-context-1625" to be "Succeeded or Failed"
Sep  8 22:03:37.992: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462": Phase="Pending", Reason="", readiness=false. Elapsed: 22.926465ms
Sep  8 22:03:40.003: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034714676s
Sep  8 22:03:42.008: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039637359s
STEP: Saw pod success 09/08/23 22:03:42.008
Sep  8 22:03:42.009: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462" satisfied condition "Succeeded or Failed"
Sep  8 22:03:42.017: INFO: Trying to get logs from node node-3 pod security-context-9d82dd01-16d1-4414-b205-667713fac462 container test-container: <nil>
STEP: delete the pod 09/08/23 22:03:42.041
Sep  8 22:03:42.101: INFO: Waiting for pod security-context-9d82dd01-16d1-4414-b205-667713fac462 to disappear
Sep  8 22:03:42.110: INFO: Pod security-context-9d82dd01-16d1-4414-b205-667713fac462 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  8 22:03:42.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1625" for this suite. 09/08/23 22:03:42.132
------------------------------
â€¢ [4.318 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:03:37.839
    Sep  8 22:03:37.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename security-context 09/08/23 22:03:37.84
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:37.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:37.916
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/08/23 22:03:37.922
    Sep  8 22:03:37.968: INFO: Waiting up to 5m0s for pod "security-context-9d82dd01-16d1-4414-b205-667713fac462" in namespace "security-context-1625" to be "Succeeded or Failed"
    Sep  8 22:03:37.992: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462": Phase="Pending", Reason="", readiness=false. Elapsed: 22.926465ms
    Sep  8 22:03:40.003: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034714676s
    Sep  8 22:03:42.008: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039637359s
    STEP: Saw pod success 09/08/23 22:03:42.008
    Sep  8 22:03:42.009: INFO: Pod "security-context-9d82dd01-16d1-4414-b205-667713fac462" satisfied condition "Succeeded or Failed"
    Sep  8 22:03:42.017: INFO: Trying to get logs from node node-3 pod security-context-9d82dd01-16d1-4414-b205-667713fac462 container test-container: <nil>
    STEP: delete the pod 09/08/23 22:03:42.041
    Sep  8 22:03:42.101: INFO: Waiting for pod security-context-9d82dd01-16d1-4414-b205-667713fac462 to disappear
    Sep  8 22:03:42.110: INFO: Pod security-context-9d82dd01-16d1-4414-b205-667713fac462 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:03:42.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1625" for this suite. 09/08/23 22:03:42.132
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:03:42.16
Sep  8 22:03:42.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 22:03:42.163
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:42.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:42.219
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/08/23 22:03:42.249
Sep  8 22:03:42.282: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4306" to be "running and ready"
Sep  8 22:03:42.299: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.437653ms
Sep  8 22:03:42.299: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:03:44.308: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.025978435s
Sep  8 22:03:44.308: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  8 22:03:44.309: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 09/08/23 22:03:44.323
Sep  8 22:03:44.347: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4306" to be "running and ready"
Sep  8 22:03:44.357: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.127364ms
Sep  8 22:03:44.357: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:03:46.368: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.020390004s
Sep  8 22:03:46.368: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Sep  8 22:03:46.368: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/08/23 22:03:46.375
Sep  8 22:03:46.399: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  8 22:03:46.405: INFO: Pod pod-with-prestop-http-hook still exists
Sep  8 22:03:48.406: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  8 22:03:48.417: INFO: Pod pod-with-prestop-http-hook still exists
Sep  8 22:03:50.406: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  8 22:03:50.416: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 09/08/23 22:03:50.416
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  8 22:03:50.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4306" for this suite. 09/08/23 22:03:50.486
------------------------------
â€¢ [SLOW TEST] [8.361 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:03:42.16
    Sep  8 22:03:42.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 22:03:42.163
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:42.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:42.219
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/08/23 22:03:42.249
    Sep  8 22:03:42.282: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4306" to be "running and ready"
    Sep  8 22:03:42.299: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 16.437653ms
    Sep  8 22:03:42.299: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:03:44.308: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.025978435s
    Sep  8 22:03:44.308: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  8 22:03:44.309: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 09/08/23 22:03:44.323
    Sep  8 22:03:44.347: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4306" to be "running and ready"
    Sep  8 22:03:44.357: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.127364ms
    Sep  8 22:03:44.357: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:03:46.368: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.020390004s
    Sep  8 22:03:46.368: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Sep  8 22:03:46.368: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/08/23 22:03:46.375
    Sep  8 22:03:46.399: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  8 22:03:46.405: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  8 22:03:48.406: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  8 22:03:48.417: INFO: Pod pod-with-prestop-http-hook still exists
    Sep  8 22:03:50.406: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Sep  8 22:03:50.416: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 09/08/23 22:03:50.416
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:03:50.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4306" for this suite. 09/08/23 22:03:50.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:03:50.528
Sep  8 22:03:50.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 22:03:50.529
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:50.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:50.586
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad in namespace container-probe-1950 09/08/23 22:03:50.59
Sep  8 22:03:50.626: INFO: Waiting up to 5m0s for pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad" in namespace "container-probe-1950" to be "not pending"
Sep  8 22:03:50.637: INFO: Pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad": Phase="Pending", Reason="", readiness=false. Elapsed: 10.645411ms
Sep  8 22:03:52.657: INFO: Pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.03085147s
Sep  8 22:03:52.657: INFO: Pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad" satisfied condition "not pending"
Sep  8 22:03:52.657: INFO: Started pod test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad in namespace container-probe-1950
STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:03:52.657
Sep  8 22:03:52.672: INFO: Initial restart count of pod test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad is 0
STEP: deleting the pod 09/08/23 22:07:54.267
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 22:07:54.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1950" for this suite. 09/08/23 22:07:54.349
------------------------------
â€¢ [SLOW TEST] [243.837 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:03:50.528
    Sep  8 22:03:50.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 22:03:50.529
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:03:50.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:03:50.586
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad in namespace container-probe-1950 09/08/23 22:03:50.59
    Sep  8 22:03:50.626: INFO: Waiting up to 5m0s for pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad" in namespace "container-probe-1950" to be "not pending"
    Sep  8 22:03:50.637: INFO: Pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad": Phase="Pending", Reason="", readiness=false. Elapsed: 10.645411ms
    Sep  8 22:03:52.657: INFO: Pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.03085147s
    Sep  8 22:03:52.657: INFO: Pod "test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad" satisfied condition "not pending"
    Sep  8 22:03:52.657: INFO: Started pod test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad in namespace container-probe-1950
    STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:03:52.657
    Sep  8 22:03:52.672: INFO: Initial restart count of pod test-webserver-b41db1f2-b15f-4f5c-972e-2d0650fb50ad is 0
    STEP: deleting the pod 09/08/23 22:07:54.267
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:07:54.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1950" for this suite. 09/08/23 22:07:54.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:07:54.375
Sep  8 22:07:54.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:07:54.377
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:07:54.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:07:54.435
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:07:54.444
Sep  8 22:07:54.475: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967" in namespace "projected-8667" to be "Succeeded or Failed"
Sep  8 22:07:54.491: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Pending", Reason="", readiness=false. Elapsed: 16.016586ms
Sep  8 22:07:56.519: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043596653s
Sep  8 22:07:58.502: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026214119s
Sep  8 22:08:00.503: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027456022s
STEP: Saw pod success 09/08/23 22:08:00.503
Sep  8 22:08:00.503: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967" satisfied condition "Succeeded or Failed"
Sep  8 22:08:00.515: INFO: Trying to get logs from node node-3 pod downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967 container client-container: <nil>
STEP: delete the pod 09/08/23 22:08:00.554
Sep  8 22:08:00.598: INFO: Waiting for pod downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967 to disappear
Sep  8 22:08:00.603: INFO: Pod downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 22:08:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8667" for this suite. 09/08/23 22:08:00.615
------------------------------
â€¢ [SLOW TEST] [6.252 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:07:54.375
    Sep  8 22:07:54.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:07:54.377
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:07:54.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:07:54.435
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:07:54.444
    Sep  8 22:07:54.475: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967" in namespace "projected-8667" to be "Succeeded or Failed"
    Sep  8 22:07:54.491: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Pending", Reason="", readiness=false. Elapsed: 16.016586ms
    Sep  8 22:07:56.519: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043596653s
    Sep  8 22:07:58.502: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026214119s
    Sep  8 22:08:00.503: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027456022s
    STEP: Saw pod success 09/08/23 22:08:00.503
    Sep  8 22:08:00.503: INFO: Pod "downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967" satisfied condition "Succeeded or Failed"
    Sep  8 22:08:00.515: INFO: Trying to get logs from node node-3 pod downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967 container client-container: <nil>
    STEP: delete the pod 09/08/23 22:08:00.554
    Sep  8 22:08:00.598: INFO: Waiting for pod downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967 to disappear
    Sep  8 22:08:00.603: INFO: Pod downwardapi-volume-c375ab29-4ce8-4c35-a297-d7146519c967 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:08:00.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8667" for this suite. 09/08/23 22:08:00.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:08:00.633
Sep  8 22:08:00.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 22:08:00.635
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:00.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:00.684
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 in namespace container-probe-4706 09/08/23 22:08:00.69
Sep  8 22:08:00.713: INFO: Waiting up to 5m0s for pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388" in namespace "container-probe-4706" to be "not pending"
Sep  8 22:08:00.734: INFO: Pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388": Phase="Pending", Reason="", readiness=false. Elapsed: 20.959983ms
Sep  8 22:08:02.749: INFO: Pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388": Phase="Running", Reason="", readiness=true. Elapsed: 2.035957332s
Sep  8 22:08:02.749: INFO: Pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388" satisfied condition "not pending"
Sep  8 22:08:02.749: INFO: Started pod liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 in namespace container-probe-4706
STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:08:02.749
Sep  8 22:08:02.755: INFO: Initial restart count of pod liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 is 0
Sep  8 22:08:22.888: INFO: Restart count of pod container-probe-4706/liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 is now 1 (20.133549383s elapsed)
STEP: deleting the pod 09/08/23 22:08:22.888
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 22:08:22.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4706" for this suite. 09/08/23 22:08:22.928
------------------------------
â€¢ [SLOW TEST] [22.309 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:08:00.633
    Sep  8 22:08:00.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 22:08:00.635
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:00.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:00.684
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 in namespace container-probe-4706 09/08/23 22:08:00.69
    Sep  8 22:08:00.713: INFO: Waiting up to 5m0s for pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388" in namespace "container-probe-4706" to be "not pending"
    Sep  8 22:08:00.734: INFO: Pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388": Phase="Pending", Reason="", readiness=false. Elapsed: 20.959983ms
    Sep  8 22:08:02.749: INFO: Pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388": Phase="Running", Reason="", readiness=true. Elapsed: 2.035957332s
    Sep  8 22:08:02.749: INFO: Pod "liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388" satisfied condition "not pending"
    Sep  8 22:08:02.749: INFO: Started pod liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 in namespace container-probe-4706
    STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:08:02.749
    Sep  8 22:08:02.755: INFO: Initial restart count of pod liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 is 0
    Sep  8 22:08:22.888: INFO: Restart count of pod container-probe-4706/liveness-aedec2b2-835d-47f9-92e5-cc2d47c58388 is now 1 (20.133549383s elapsed)
    STEP: deleting the pod 09/08/23 22:08:22.888
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:08:22.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4706" for this suite. 09/08/23 22:08:22.928
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:08:22.943
Sep  8 22:08:22.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 22:08:22.945
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:22.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:22.992
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-9a25e629-e8c8-407a-98aa-f5333d49b142 09/08/23 22:08:23.009
STEP: Creating the pod 09/08/23 22:08:23.033
Sep  8 22:08:23.058: INFO: Waiting up to 5m0s for pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472" in namespace "configmap-1416" to be "running"
Sep  8 22:08:23.072: INFO: Pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472": Phase="Pending", Reason="", readiness=false. Elapsed: 14.500176ms
Sep  8 22:08:25.084: INFO: Pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472": Phase="Running", Reason="", readiness=false. Elapsed: 2.02583017s
Sep  8 22:08:25.084: INFO: Pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472" satisfied condition "running"
STEP: Waiting for pod with text data 09/08/23 22:08:25.084
STEP: Waiting for pod with binary data 09/08/23 22:08:25.104
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:08:25.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1416" for this suite. 09/08/23 22:08:25.133
------------------------------
â€¢ [2.232 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:08:22.943
    Sep  8 22:08:22.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 22:08:22.945
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:22.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:22.992
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-9a25e629-e8c8-407a-98aa-f5333d49b142 09/08/23 22:08:23.009
    STEP: Creating the pod 09/08/23 22:08:23.033
    Sep  8 22:08:23.058: INFO: Waiting up to 5m0s for pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472" in namespace "configmap-1416" to be "running"
    Sep  8 22:08:23.072: INFO: Pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472": Phase="Pending", Reason="", readiness=false. Elapsed: 14.500176ms
    Sep  8 22:08:25.084: INFO: Pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472": Phase="Running", Reason="", readiness=false. Elapsed: 2.02583017s
    Sep  8 22:08:25.084: INFO: Pod "pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472" satisfied condition "running"
    STEP: Waiting for pod with text data 09/08/23 22:08:25.084
    STEP: Waiting for pod with binary data 09/08/23 22:08:25.104
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:08:25.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1416" for this suite. 09/08/23 22:08:25.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:08:25.18
Sep  8 22:08:25.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-pred 09/08/23 22:08:25.183
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:25.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:25.236
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Sep  8 22:08:25.242: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  8 22:08:25.269: INFO: Waiting for terminating namespaces to be deleted...
Sep  8 22:08:25.278: INFO: 
Logging pods the apiserver thinks is on node node-3 before test
Sep  8 22:08:25.307: INFO: pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472 from configmap-1416 started at 2023-09-08 22:08:23 +0000 UTC (2 container statuses recorded)
Sep  8 22:08:25.307: INFO: 	Container agnhost-container ready: true, restart count 0
Sep  8 22:08:25.307: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Sep  8 22:08:25.307: INFO: cadvisor-lp4nn from default started at 2023-09-08 21:43:38 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.307: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 22:08:25.307: INFO: kube-prometheus-node-exporter-j7tkw from default started at 2023-09-08 21:43:35 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.307: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 22:08:25.307: INFO: netchecker-agent-hostnet-pthvh from default started at 2023-09-08 21:43:36 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.307: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 22:08:25.307: INFO: netchecker-agent-jkrf7 from default started at 2023-09-08 21:43:33 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 22:08:25.308: INFO: openebs-localpv-provisioner-5d6756bcd8-hwnnv from default started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 22:08:25.308: INFO: ingress-nginx-controller-rhxd4 from ingress-nginx started at 2023-09-08 21:43:41 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container controller ready: true, restart count 0
Sep  8 22:08:25.308: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 22:08:25.308: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 22:08:25.308: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 22:08:25.308: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 22:08:25.308: INFO: metallb-controller-77b687f97-8fnfx from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container controller ready: true, restart count 0
Sep  8 22:08:25.308: INFO: metallb-speaker-csr5m from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container speaker ready: true, restart count 0
Sep  8 22:08:25.308: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep  8 22:08:25.308: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 22:08:25.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 22:08:25.308: INFO: 	Container systemd-logs ready: true, restart count 0
Sep  8 22:08:25.308: INFO: 
Logging pods the apiserver thinks is on node node-4 before test
Sep  8 22:08:25.334: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.334: INFO: 	Container cadvisor ready: true, restart count 0
Sep  8 22:08:25.334: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.334: INFO: 	Container blackbox-exporter ready: true, restart count 0
Sep  8 22:08:25.334: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-hffhp from default started at 2023-09-08 21:43:25 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.334: INFO: 	Container kube-state-metrics ready: true, restart count 0
Sep  8 22:08:25.334: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.334: INFO: 	Container node-exporter ready: true, restart count 0
Sep  8 22:08:25.334: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.334: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 22:08:25.334: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.334: INFO: 	Container netchecker-agent ready: true, restart count 0
Sep  8 22:08:25.334: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container etcd ready: true, restart count 0
Sep  8 22:08:25.335: INFO: 	Container netchecker-server ready: true, restart count 1
Sep  8 22:08:25.335: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Sep  8 22:08:25.335: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container controller ready: true, restart count 0
Sep  8 22:08:25.335: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Sep  8 22:08:25.335: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container calico-node ready: true, restart count 0
Sep  8 22:08:25.335: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  8 22:08:25.335: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container metrics-server ready: true, restart count 0
Sep  8 22:08:25.335: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container nginx-proxy ready: true, restart count 0
Sep  8 22:08:25.335: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.335: INFO: 	Container node-cache ready: true, restart count 0
Sep  8 22:08:25.335: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.336: INFO: 	Container metallb-monitor ready: true, restart count 0
Sep  8 22:08:25.336: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
Sep  8 22:08:25.336: INFO: 	Container speaker ready: true, restart count 0
Sep  8 22:08:25.336: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
Sep  8 22:08:25.336: INFO: 	Container e2e ready: true, restart count 0
Sep  8 22:08:25.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 22:08:25.336: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
Sep  8 22:08:25.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep  8 22:08:25.336: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 09/08/23 22:08:25.336
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17830bd7971be337], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 09/08/23 22:08:25.414
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:08:26.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3797" for this suite. 09/08/23 22:08:26.436
------------------------------
â€¢ [1.271 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:08:25.18
    Sep  8 22:08:25.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-pred 09/08/23 22:08:25.183
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:25.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:25.236
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Sep  8 22:08:25.242: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Sep  8 22:08:25.269: INFO: Waiting for terminating namespaces to be deleted...
    Sep  8 22:08:25.278: INFO: 
    Logging pods the apiserver thinks is on node node-3 before test
    Sep  8 22:08:25.307: INFO: pod-configmaps-e69d91df-d9e8-4ab3-af58-4e4bc9385472 from configmap-1416 started at 2023-09-08 22:08:23 +0000 UTC (2 container statuses recorded)
    Sep  8 22:08:25.307: INFO: 	Container agnhost-container ready: true, restart count 0
    Sep  8 22:08:25.307: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
    Sep  8 22:08:25.307: INFO: cadvisor-lp4nn from default started at 2023-09-08 21:43:38 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.307: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 22:08:25.307: INFO: kube-prometheus-node-exporter-j7tkw from default started at 2023-09-08 21:43:35 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.307: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 22:08:25.307: INFO: netchecker-agent-hostnet-pthvh from default started at 2023-09-08 21:43:36 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.307: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 22:08:25.307: INFO: netchecker-agent-jkrf7 from default started at 2023-09-08 21:43:33 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: openebs-localpv-provisioner-5d6756bcd8-hwnnv from default started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: ingress-nginx-controller-rhxd4 from ingress-nginx started at 2023-09-08 21:43:41 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container controller ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: calico-node-46wp7 from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: kube-proxy-4nz2v from kube-system started at 2023-09-08 20:50:12 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: nginx-proxy-node-3 from kube-system started at 2023-09-08 20:49:52 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: nodelocaldns-j7fmg from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: metallb-controller-77b687f97-8fnfx from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container controller ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: metallb-speaker-csr5m from metallb-system started at 2023-09-08 21:43:26 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: sonobuoy from sonobuoy started at 2023-09-08 21:09:47 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-qwx4p from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 22:08:25.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: 	Container systemd-logs ready: true, restart count 0
    Sep  8 22:08:25.308: INFO: 
    Logging pods the apiserver thinks is on node node-4 before test
    Sep  8 22:08:25.334: INFO: cadvisor-2bq6z from default started at 2023-09-08 20:57:02 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.334: INFO: 	Container cadvisor ready: true, restart count 0
    Sep  8 22:08:25.334: INFO: kube-prometheus-blackbox-exporter-dc8c648c9-xdqrd from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.334: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Sep  8 22:08:25.334: INFO: kube-prometheus-kube-state-metrics-7787c6cfbc-hffhp from default started at 2023-09-08 21:43:25 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.334: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Sep  8 22:08:25.334: INFO: kube-prometheus-node-exporter-vzk9h from default started at 2023-09-08 20:56:29 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.334: INFO: 	Container node-exporter ready: true, restart count 0
    Sep  8 22:08:25.334: INFO: netchecker-agent-hostnet-47k69 from default started at 2023-09-08 20:52:21 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.334: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 22:08:25.334: INFO: netchecker-agent-t6llz from default started at 2023-09-08 20:52:20 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.334: INFO: 	Container netchecker-agent ready: true, restart count 0
    Sep  8 22:08:25.334: INFO: netchecker-server-57d55b464c-qrshp from default started at 2023-09-08 20:52:24 +0000 UTC (2 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container etcd ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: 	Container netchecker-server ready: true, restart count 1
    Sep  8 22:08:25.335: INFO: openebs-localpv-provisioner-5d6756bcd8-9d9sf from default started at 2023-09-08 20:56:16 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: ingress-nginx-controller-l2p2l from ingress-nginx started at 2023-09-08 20:56:06 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container controller ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: calico-kube-controllers-68fd66c797-c5t5m from kube-system started at 2023-09-08 20:51:32 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: calico-node-bvxmx from kube-system started at 2023-09-08 20:50:52 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container calico-node ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: kube-proxy-wjfp8 from kube-system started at 2023-09-08 20:50:09 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container kube-proxy ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: metrics-server-8468bb47f8-wjspw from kube-system started at 2023-09-08 20:53:05 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container metrics-server ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: nginx-proxy-node-4 from kube-system started at 2023-09-08 20:51:14 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container nginx-proxy ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: nodelocaldns-mqlmh from kube-system started at 2023-09-08 20:52:08 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.335: INFO: 	Container node-cache ready: true, restart count 0
    Sep  8 22:08:25.335: INFO: metallb-monitor-deployment-6947cfdbd8-kvjhq from metallb-system started at 2023-09-08 20:57:11 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.336: INFO: 	Container metallb-monitor ready: true, restart count 0
    Sep  8 22:08:25.336: INFO: metallb-speaker-srxnf from metallb-system started at 2023-09-08 20:56:04 +0000 UTC (1 container statuses recorded)
    Sep  8 22:08:25.336: INFO: 	Container speaker ready: true, restart count 0
    Sep  8 22:08:25.336: INFO: sonobuoy-e2e-job-5595c445240f482c from sonobuoy started at 2023-09-08 21:10:00 +0000 UTC (2 container statuses recorded)
    Sep  8 22:08:25.336: INFO: 	Container e2e ready: true, restart count 0
    Sep  8 22:08:25.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 22:08:25.336: INFO: sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-27l2v from sonobuoy started at 2023-09-08 21:10:01 +0000 UTC (2 container statuses recorded)
    Sep  8 22:08:25.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Sep  8 22:08:25.336: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 09/08/23 22:08:25.336
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17830bd7971be337], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 09/08/23 22:08:25.414
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:08:26.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3797" for this suite. 09/08/23 22:08:26.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:08:26.454
Sep  8 22:08:26.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svcaccounts 09/08/23 22:08:26.457
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:26.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:26.498
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 09/08/23 22:08:26.503
STEP: watching for the ServiceAccount to be added 09/08/23 22:08:26.524
STEP: patching the ServiceAccount 09/08/23 22:08:26.529
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/08/23 22:08:26.544
STEP: deleting the ServiceAccount 09/08/23 22:08:26.557
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  8 22:08:26.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6979" for this suite. 09/08/23 22:08:26.612
------------------------------
â€¢ [0.179 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:08:26.454
    Sep  8 22:08:26.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svcaccounts 09/08/23 22:08:26.457
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:26.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:26.498
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 09/08/23 22:08:26.503
    STEP: watching for the ServiceAccount to be added 09/08/23 22:08:26.524
    STEP: patching the ServiceAccount 09/08/23 22:08:26.529
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 09/08/23 22:08:26.544
    STEP: deleting the ServiceAccount 09/08/23 22:08:26.557
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:08:26.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6979" for this suite. 09/08/23 22:08:26.612
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:08:26.635
Sep  8 22:08:26.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:08:26.636
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:26.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:26.697
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:08:26.704
Sep  8 22:08:26.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  8 22:08:26.859: INFO: stderr: ""
Sep  8 22:08:26.859: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 09/08/23 22:08:26.859
STEP: verifying the pod e2e-test-httpd-pod was created 09/08/23 22:08:31.912
Sep  8 22:08:31.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 get pod e2e-test-httpd-pod -o json'
Sep  8 22:08:32.106: INFO: stderr: ""
Sep  8 22:08:32.106: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"dca881bf88aa9894687915acbdb52f4ecbc4c0b7a977863eb4af601c8a495dca\",\n            \"cni.projectcalico.org/podIP\": \"10.233.75.74/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.75.74/32\"\n        },\n        \"creationTimestamp\": \"2023-09-08T22:08:26Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6388\",\n        \"resourceVersion\": \"35020\",\n        \"uid\": \"77bb240c-f0cd-40fe-9b3e-5e31937a4d58\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5k7lp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5k7lp\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:26Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://669a8b9571bc05abd8ab11d660a3a760d85cffad1c96e316772ef1683d28d4a7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-08T22:08:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.100.19.129\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.75.74\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.75.74\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-08T22:08:26Z\"\n    }\n}\n"
STEP: replace the image in the pod 09/08/23 22:08:32.106
Sep  8 22:08:32.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 replace -f -'
Sep  8 22:08:33.721: INFO: stderr: ""
Sep  8 22:08:33.721: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/08/23 22:08:33.721
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Sep  8 22:08:33.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 delete pods e2e-test-httpd-pod'
Sep  8 22:08:35.861: INFO: stderr: ""
Sep  8 22:08:35.861: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:08:35.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6388" for this suite. 09/08/23 22:08:35.88
------------------------------
â€¢ [SLOW TEST] [9.273 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:08:26.635
    Sep  8 22:08:26.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:08:26.636
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:26.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:26.697
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:08:26.704
    Sep  8 22:08:26.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  8 22:08:26.859: INFO: stderr: ""
    Sep  8 22:08:26.859: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 09/08/23 22:08:26.859
    STEP: verifying the pod e2e-test-httpd-pod was created 09/08/23 22:08:31.912
    Sep  8 22:08:31.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 get pod e2e-test-httpd-pod -o json'
    Sep  8 22:08:32.106: INFO: stderr: ""
    Sep  8 22:08:32.106: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"dca881bf88aa9894687915acbdb52f4ecbc4c0b7a977863eb4af601c8a495dca\",\n            \"cni.projectcalico.org/podIP\": \"10.233.75.74/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.233.75.74/32\"\n        },\n        \"creationTimestamp\": \"2023-09-08T22:08:26Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6388\",\n        \"resourceVersion\": \"35020\",\n        \"uid\": \"77bb240c-f0cd-40fe-9b3e-5e31937a4d58\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5k7lp\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5k7lp\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:26Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-09-08T22:08:26Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://669a8b9571bc05abd8ab11d660a3a760d85cffad1c96e316772ef1683d28d4a7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-09-08T22:08:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.100.19.129\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.75.74\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.75.74\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-09-08T22:08:26Z\"\n    }\n}\n"
    STEP: replace the image in the pod 09/08/23 22:08:32.106
    Sep  8 22:08:32.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 replace -f -'
    Sep  8 22:08:33.721: INFO: stderr: ""
    Sep  8 22:08:33.721: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 09/08/23 22:08:33.721
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Sep  8 22:08:33.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6388 delete pods e2e-test-httpd-pod'
    Sep  8 22:08:35.861: INFO: stderr: ""
    Sep  8 22:08:35.861: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:08:35.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6388" for this suite. 09/08/23 22:08:35.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:08:35.909
Sep  8 22:08:35.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename cronjob 09/08/23 22:08:35.911
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:35.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:36.001
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 09/08/23 22:08:36.006
STEP: Ensuring a job is scheduled 09/08/23 22:08:36.018
STEP: Ensuring exactly one is scheduled 09/08/23 22:09:02.039
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/08/23 22:09:02.05
STEP: Ensuring the job is replaced with a new one 09/08/23 22:09:02.064
STEP: Removing cronjob 09/08/23 22:10:02.101
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  8 22:10:02.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4701" for this suite. 09/08/23 22:10:02.127
------------------------------
â€¢ [SLOW TEST] [86.246 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:08:35.909
    Sep  8 22:08:35.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename cronjob 09/08/23 22:08:35.911
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:08:35.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:08:36.001
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 09/08/23 22:08:36.006
    STEP: Ensuring a job is scheduled 09/08/23 22:08:36.018
    STEP: Ensuring exactly one is scheduled 09/08/23 22:09:02.039
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/08/23 22:09:02.05
    STEP: Ensuring the job is replaced with a new one 09/08/23 22:09:02.064
    STEP: Removing cronjob 09/08/23 22:10:02.101
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:10:02.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4701" for this suite. 09/08/23 22:10:02.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:10:02.158
Sep  8 22:10:02.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubelet-test 09/08/23 22:10:02.159
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:02.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:02.282
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 09/08/23 22:10:02.329
Sep  8 22:10:02.329: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1" in namespace "kubelet-test-9965" to be "completed"
Sep  8 22:10:02.371: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1": Phase="Pending", Reason="", readiness=false. Elapsed: 42.502556ms
Sep  8 22:10:04.382: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053096764s
Sep  8 22:10:06.383: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054645849s
Sep  8 22:10:06.383: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:10:06.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9965" for this suite. 09/08/23 22:10:06.426
------------------------------
â€¢ [4.287 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:10:02.158
    Sep  8 22:10:02.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubelet-test 09/08/23 22:10:02.159
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:02.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:02.282
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 09/08/23 22:10:02.329
    Sep  8 22:10:02.329: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1" in namespace "kubelet-test-9965" to be "completed"
    Sep  8 22:10:02.371: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1": Phase="Pending", Reason="", readiness=false. Elapsed: 42.502556ms
    Sep  8 22:10:04.382: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053096764s
    Sep  8 22:10:06.383: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054645849s
    Sep  8 22:10:06.383: INFO: Pod "agnhost-host-aliases4559aa91-1fbe-452e-8751-5171bfb320d1" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:10:06.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9965" for this suite. 09/08/23 22:10:06.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:10:06.445
Sep  8 22:10:06.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 22:10:06.447
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:06.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:06.505
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 09/08/23 22:10:06.513
STEP: Creating a ResourceQuota 09/08/23 22:10:11.533
STEP: Ensuring resource quota status is calculated 09/08/23 22:10:11.547
STEP: Creating a ReplicationController 09/08/23 22:10:13.561
STEP: Ensuring resource quota status captures replication controller creation 09/08/23 22:10:13.603
STEP: Deleting a ReplicationController 09/08/23 22:10:15.62
STEP: Ensuring resource quota status released usage 09/08/23 22:10:15.637
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 22:10:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-745" for this suite. 09/08/23 22:10:17.667
------------------------------
â€¢ [SLOW TEST] [11.244 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:10:06.445
    Sep  8 22:10:06.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 22:10:06.447
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:06.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:06.505
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 09/08/23 22:10:06.513
    STEP: Creating a ResourceQuota 09/08/23 22:10:11.533
    STEP: Ensuring resource quota status is calculated 09/08/23 22:10:11.547
    STEP: Creating a ReplicationController 09/08/23 22:10:13.561
    STEP: Ensuring resource quota status captures replication controller creation 09/08/23 22:10:13.603
    STEP: Deleting a ReplicationController 09/08/23 22:10:15.62
    STEP: Ensuring resource quota status released usage 09/08/23 22:10:15.637
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:10:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-745" for this suite. 09/08/23 22:10:17.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:10:17.692
Sep  8 22:10:17.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename csistoragecapacity 09/08/23 22:10:17.693
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:17.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:17.752
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 09/08/23 22:10:17.759
STEP: getting /apis/storage.k8s.io 09/08/23 22:10:17.767
STEP: getting /apis/storage.k8s.io/v1 09/08/23 22:10:17.77
STEP: creating 09/08/23 22:10:17.772
STEP: watching 09/08/23 22:10:17.833
Sep  8 22:10:17.833: INFO: starting watch
STEP: getting 09/08/23 22:10:17.86
STEP: listing in namespace 09/08/23 22:10:17.869
STEP: listing across namespaces 09/08/23 22:10:17.884
STEP: patching 09/08/23 22:10:17.933
STEP: updating 09/08/23 22:10:17.945
Sep  8 22:10:17.969: INFO: waiting for watch events with expected annotations in namespace
Sep  8 22:10:17.969: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 09/08/23 22:10:17.969
STEP: deleting a collection 09/08/23 22:10:18.044
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Sep  8 22:10:18.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-1426" for this suite. 09/08/23 22:10:18.14
------------------------------
â€¢ [0.475 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:10:17.692
    Sep  8 22:10:17.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename csistoragecapacity 09/08/23 22:10:17.693
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:17.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:17.752
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 09/08/23 22:10:17.759
    STEP: getting /apis/storage.k8s.io 09/08/23 22:10:17.767
    STEP: getting /apis/storage.k8s.io/v1 09/08/23 22:10:17.77
    STEP: creating 09/08/23 22:10:17.772
    STEP: watching 09/08/23 22:10:17.833
    Sep  8 22:10:17.833: INFO: starting watch
    STEP: getting 09/08/23 22:10:17.86
    STEP: listing in namespace 09/08/23 22:10:17.869
    STEP: listing across namespaces 09/08/23 22:10:17.884
    STEP: patching 09/08/23 22:10:17.933
    STEP: updating 09/08/23 22:10:17.945
    Sep  8 22:10:17.969: INFO: waiting for watch events with expected annotations in namespace
    Sep  8 22:10:17.969: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 09/08/23 22:10:17.969
    STEP: deleting a collection 09/08/23 22:10:18.044
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:10:18.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-1426" for this suite. 09/08/23 22:10:18.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:10:18.173
Sep  8 22:10:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename cronjob 09/08/23 22:10:18.175
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:18.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:18.216
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 09/08/23 22:10:18.233
STEP: Ensuring a job is scheduled 09/08/23 22:10:18.245
STEP: Ensuring exactly one is scheduled 09/08/23 22:11:00.254
STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/08/23 22:11:00.261
STEP: Ensuring no more jobs are scheduled 09/08/23 22:11:00.269
STEP: Removing cronjob 09/08/23 22:16:00.298
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  8 22:16:00.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7163" for this suite. 09/08/23 22:16:00.343
------------------------------
â€¢ [SLOW TEST] [342.220 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:10:18.173
    Sep  8 22:10:18.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename cronjob 09/08/23 22:10:18.175
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:10:18.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:10:18.216
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 09/08/23 22:10:18.233
    STEP: Ensuring a job is scheduled 09/08/23 22:10:18.245
    STEP: Ensuring exactly one is scheduled 09/08/23 22:11:00.254
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 09/08/23 22:11:00.261
    STEP: Ensuring no more jobs are scheduled 09/08/23 22:11:00.269
    STEP: Removing cronjob 09/08/23 22:16:00.298
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:16:00.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7163" for this suite. 09/08/23 22:16:00.343
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:16:00.394
Sep  8 22:16:00.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 22:16:00.395
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:16:00.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:16:00.462
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2118 09/08/23 22:16:00.468
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Sep  8 22:16:00.511: INFO: Found 0 stateful pods, waiting for 1
Sep  8 22:16:10.529: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 09/08/23 22:16:10.561
W0908 22:16:10.585504      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  8 22:16:10.607: INFO: Found 1 stateful pods, waiting for 2
Sep  8 22:16:20.618: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:16:20.618: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 09/08/23 22:16:20.643
STEP: Delete all of the StatefulSets 09/08/23 22:16:20.653
STEP: Verify that StatefulSets have been deleted 09/08/23 22:16:20.676
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 22:16:20.686: INFO: Deleting all statefulset in ns statefulset-2118
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:16:20.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2118" for this suite. 09/08/23 22:16:20.757
------------------------------
â€¢ [SLOW TEST] [20.401 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:16:00.394
    Sep  8 22:16:00.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 22:16:00.395
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:16:00.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:16:00.462
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2118 09/08/23 22:16:00.468
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Sep  8 22:16:00.511: INFO: Found 0 stateful pods, waiting for 1
    Sep  8 22:16:10.529: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 09/08/23 22:16:10.561
    W0908 22:16:10.585504      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  8 22:16:10.607: INFO: Found 1 stateful pods, waiting for 2
    Sep  8 22:16:20.618: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:16:20.618: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 09/08/23 22:16:20.643
    STEP: Delete all of the StatefulSets 09/08/23 22:16:20.653
    STEP: Verify that StatefulSets have been deleted 09/08/23 22:16:20.676
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 22:16:20.686: INFO: Deleting all statefulset in ns statefulset-2118
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:16:20.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2118" for this suite. 09/08/23 22:16:20.757
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:16:20.795
Sep  8 22:16:20.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename job 09/08/23 22:16:20.804
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:16:20.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:16:20.876
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 09/08/23 22:16:20.888
STEP: Ensuring active pods == parallelism 09/08/23 22:16:20.903
STEP: delete a job 09/08/23 22:16:24.927
STEP: deleting Job.batch foo in namespace job-3426, will wait for the garbage collector to delete the pods 09/08/23 22:16:24.927
Sep  8 22:16:25.006: INFO: Deleting Job.batch foo took: 21.026757ms
Sep  8 22:16:25.107: INFO: Terminating Job.batch foo pods took: 100.760201ms
STEP: Ensuring job was deleted 09/08/23 22:16:57.208
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  8 22:16:57.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3426" for this suite. 09/08/23 22:16:57.255
------------------------------
â€¢ [SLOW TEST] [36.481 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:16:20.795
    Sep  8 22:16:20.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename job 09/08/23 22:16:20.804
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:16:20.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:16:20.876
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 09/08/23 22:16:20.888
    STEP: Ensuring active pods == parallelism 09/08/23 22:16:20.903
    STEP: delete a job 09/08/23 22:16:24.927
    STEP: deleting Job.batch foo in namespace job-3426, will wait for the garbage collector to delete the pods 09/08/23 22:16:24.927
    Sep  8 22:16:25.006: INFO: Deleting Job.batch foo took: 21.026757ms
    Sep  8 22:16:25.107: INFO: Terminating Job.batch foo pods took: 100.760201ms
    STEP: Ensuring job was deleted 09/08/23 22:16:57.208
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:16:57.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3426" for this suite. 09/08/23 22:16:57.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:16:57.278
Sep  8 22:16:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename subpath 09/08/23 22:16:57.279
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:16:57.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:16:57.332
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/08/23 22:16:57.336
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-x6k9 09/08/23 22:16:57.366
STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:16:57.366
Sep  8 22:16:57.389: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x6k9" in namespace "subpath-2833" to be "Succeeded or Failed"
Sep  8 22:16:57.395: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.184453ms
Sep  8 22:16:59.405: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 2.015419416s
Sep  8 22:17:01.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017187349s
Sep  8 22:17:03.407: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 6.017504796s
Sep  8 22:17:05.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 8.01642718s
Sep  8 22:17:07.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 10.016674785s
Sep  8 22:17:09.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 12.016757891s
Sep  8 22:17:11.407: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 14.017989679s
Sep  8 22:17:13.405: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 16.015327118s
Sep  8 22:17:15.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 18.016261719s
Sep  8 22:17:17.404: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 20.014867021s
Sep  8 22:17:19.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=false. Elapsed: 22.016798156s
Sep  8 22:17:21.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016384053s
STEP: Saw pod success 09/08/23 22:17:21.406
Sep  8 22:17:21.406: INFO: Pod "pod-subpath-test-secret-x6k9" satisfied condition "Succeeded or Failed"
Sep  8 22:17:21.414: INFO: Trying to get logs from node node-3 pod pod-subpath-test-secret-x6k9 container test-container-subpath-secret-x6k9: <nil>
STEP: delete the pod 09/08/23 22:17:21.449
Sep  8 22:17:21.498: INFO: Waiting for pod pod-subpath-test-secret-x6k9 to disappear
Sep  8 22:17:21.508: INFO: Pod pod-subpath-test-secret-x6k9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-x6k9 09/08/23 22:17:21.508
Sep  8 22:17:21.508: INFO: Deleting pod "pod-subpath-test-secret-x6k9" in namespace "subpath-2833"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  8 22:17:21.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2833" for this suite. 09/08/23 22:17:21.539
------------------------------
â€¢ [SLOW TEST] [24.275 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:16:57.278
    Sep  8 22:16:57.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename subpath 09/08/23 22:16:57.279
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:16:57.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:16:57.332
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/08/23 22:16:57.336
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-x6k9 09/08/23 22:16:57.366
    STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:16:57.366
    Sep  8 22:16:57.389: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-x6k9" in namespace "subpath-2833" to be "Succeeded or Failed"
    Sep  8 22:16:57.395: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.184453ms
    Sep  8 22:16:59.405: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 2.015419416s
    Sep  8 22:17:01.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017187349s
    Sep  8 22:17:03.407: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 6.017504796s
    Sep  8 22:17:05.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 8.01642718s
    Sep  8 22:17:07.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 10.016674785s
    Sep  8 22:17:09.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 12.016757891s
    Sep  8 22:17:11.407: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 14.017989679s
    Sep  8 22:17:13.405: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 16.015327118s
    Sep  8 22:17:15.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 18.016261719s
    Sep  8 22:17:17.404: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=true. Elapsed: 20.014867021s
    Sep  8 22:17:19.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Running", Reason="", readiness=false. Elapsed: 22.016798156s
    Sep  8 22:17:21.406: INFO: Pod "pod-subpath-test-secret-x6k9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016384053s
    STEP: Saw pod success 09/08/23 22:17:21.406
    Sep  8 22:17:21.406: INFO: Pod "pod-subpath-test-secret-x6k9" satisfied condition "Succeeded or Failed"
    Sep  8 22:17:21.414: INFO: Trying to get logs from node node-3 pod pod-subpath-test-secret-x6k9 container test-container-subpath-secret-x6k9: <nil>
    STEP: delete the pod 09/08/23 22:17:21.449
    Sep  8 22:17:21.498: INFO: Waiting for pod pod-subpath-test-secret-x6k9 to disappear
    Sep  8 22:17:21.508: INFO: Pod pod-subpath-test-secret-x6k9 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-x6k9 09/08/23 22:17:21.508
    Sep  8 22:17:21.508: INFO: Deleting pod "pod-subpath-test-secret-x6k9" in namespace "subpath-2833"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:17:21.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2833" for this suite. 09/08/23 22:17:21.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:17:21.555
Sep  8 22:17:21.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 22:17:21.557
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:17:21.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:17:21.623
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 09/08/23 22:17:21.651
Sep  8 22:17:21.684: INFO: Waiting up to 5m0s for pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c" in namespace "pods-1081" to be "running and ready"
Sep  8 22:17:21.700: INFO: Pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.711512ms
Sep  8 22:17:21.700: INFO: The phase of Pod pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:17:23.714: INFO: Pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.03082209s
Sep  8 22:17:23.715: INFO: The phase of Pod pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c is Running (Ready = true)
Sep  8 22:17:23.715: INFO: Pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c" satisfied condition "running and ready"
Sep  8 22:17:23.735: INFO: Pod pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c has hostIP: 10.100.19.129
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 22:17:23.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1081" for this suite. 09/08/23 22:17:23.748
------------------------------
â€¢ [2.218 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:17:21.555
    Sep  8 22:17:21.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 22:17:21.557
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:17:21.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:17:21.623
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 09/08/23 22:17:21.651
    Sep  8 22:17:21.684: INFO: Waiting up to 5m0s for pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c" in namespace "pods-1081" to be "running and ready"
    Sep  8 22:17:21.700: INFO: Pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.711512ms
    Sep  8 22:17:21.700: INFO: The phase of Pod pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:17:23.714: INFO: Pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c": Phase="Running", Reason="", readiness=true. Elapsed: 2.03082209s
    Sep  8 22:17:23.715: INFO: The phase of Pod pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c is Running (Ready = true)
    Sep  8 22:17:23.715: INFO: Pod "pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c" satisfied condition "running and ready"
    Sep  8 22:17:23.735: INFO: Pod pod-hostip-c2259c71-fa74-4259-ade6-8893cea94a1c has hostIP: 10.100.19.129
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:17:23.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1081" for this suite. 09/08/23 22:17:23.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:17:23.774
Sep  8 22:17:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename taint-multiple-pods 09/08/23 22:17:23.775
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:17:23.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:17:23.847
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Sep  8 22:17:23.855: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  8 22:18:23.939: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Sep  8 22:18:23.948: INFO: Starting informer...
STEP: Starting pods... 09/08/23 22:18:23.948
Sep  8 22:18:24.209: INFO: Pod1 is running on node-3. Tainting Node
Sep  8 22:18:24.436: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6089" to be "running"
Sep  8 22:18:24.451: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.661597ms
Sep  8 22:18:26.471: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.034757744s
Sep  8 22:18:26.471: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Sep  8 22:18:26.471: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6089" to be "running"
Sep  8 22:18:26.480: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 8.660787ms
Sep  8 22:18:26.480: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Sep  8 22:18:26.481: INFO: Pod2 is running on node-3. Tainting Node
STEP: Trying to apply a taint on the Node 09/08/23 22:18:26.481
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 22:18:26.517
STEP: Waiting for Pod1 and Pod2 to be deleted 09/08/23 22:18:26.564
Sep  8 22:18:32.727: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  8 22:18:52.798: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 22:18:52.835
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:18:52.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-6089" for this suite. 09/08/23 22:18:52.883
------------------------------
â€¢ [SLOW TEST] [89.176 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:17:23.774
    Sep  8 22:17:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename taint-multiple-pods 09/08/23 22:17:23.775
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:17:23.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:17:23.847
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Sep  8 22:17:23.855: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  8 22:18:23.939: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Sep  8 22:18:23.948: INFO: Starting informer...
    STEP: Starting pods... 09/08/23 22:18:23.948
    Sep  8 22:18:24.209: INFO: Pod1 is running on node-3. Tainting Node
    Sep  8 22:18:24.436: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-6089" to be "running"
    Sep  8 22:18:24.451: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.661597ms
    Sep  8 22:18:26.471: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.034757744s
    Sep  8 22:18:26.471: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Sep  8 22:18:26.471: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-6089" to be "running"
    Sep  8 22:18:26.480: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 8.660787ms
    Sep  8 22:18:26.480: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Sep  8 22:18:26.481: INFO: Pod2 is running on node-3. Tainting Node
    STEP: Trying to apply a taint on the Node 09/08/23 22:18:26.481
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 22:18:26.517
    STEP: Waiting for Pod1 and Pod2 to be deleted 09/08/23 22:18:26.564
    Sep  8 22:18:32.727: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Sep  8 22:18:52.798: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 09/08/23 22:18:52.835
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:18:52.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-6089" for this suite. 09/08/23 22:18:52.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:18:52.954
Sep  8 22:18:52.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:18:52.956
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:18:53.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:18:53.125
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:18:53.132
Sep  8 22:18:53.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2675 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Sep  8 22:18:53.284: INFO: stderr: ""
Sep  8 22:18:53.284: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 09/08/23 22:18:53.284
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Sep  8 22:18:53.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2675 delete pods e2e-test-httpd-pod'
Sep  8 22:18:59.155: INFO: stderr: ""
Sep  8 22:18:59.155: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:18:59.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2675" for this suite. 09/08/23 22:18:59.174
------------------------------
â€¢ [SLOW TEST] [6.238 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:18:52.954
    Sep  8 22:18:52.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:18:52.956
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:18:53.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:18:53.125
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:18:53.132
    Sep  8 22:18:53.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2675 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Sep  8 22:18:53.284: INFO: stderr: ""
    Sep  8 22:18:53.284: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 09/08/23 22:18:53.284
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Sep  8 22:18:53.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2675 delete pods e2e-test-httpd-pod'
    Sep  8 22:18:59.155: INFO: stderr: ""
    Sep  8 22:18:59.155: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:18:59.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2675" for this suite. 09/08/23 22:18:59.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:18:59.193
Sep  8 22:18:59.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir-wrapper 09/08/23 22:18:59.196
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:18:59.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:18:59.244
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 09/08/23 22:18:59.26
STEP: Creating RC which spawns configmap-volume pods 09/08/23 22:19:00.028
Sep  8 22:19:00.066: INFO: Pod name wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1: Found 0 pods out of 5
Sep  8 22:19:05.089: INFO: Pod name wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/08/23 22:19:05.092
Sep  8 22:19:05.093: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:05.100: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.345055ms
Sep  8 22:19:07.113: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020364789s
Sep  8 22:19:09.114: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020994568s
Sep  8 22:19:11.133: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039442496s
Sep  8 22:19:13.121: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027591622s
Sep  8 22:19:15.137: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Running", Reason="", readiness=true. Elapsed: 10.044056713s
Sep  8 22:19:15.137: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf" satisfied condition "running"
Sep  8 22:19:15.137: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:15.160: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.273751ms
Sep  8 22:19:17.178: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb": Phase="Running", Reason="", readiness=true. Elapsed: 2.041079403s
Sep  8 22:19:17.178: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb" satisfied condition "running"
Sep  8 22:19:17.178: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-htzkj" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:17.186: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-htzkj": Phase="Running", Reason="", readiness=true. Elapsed: 7.765522ms
Sep  8 22:19:17.194: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-htzkj" satisfied condition "running"
Sep  8 22:19:17.200: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-lwnzn" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:17.213: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-lwnzn": Phase="Running", Reason="", readiness=true. Elapsed: 12.982995ms
Sep  8 22:19:17.213: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-lwnzn" satisfied condition "running"
Sep  8 22:19:17.213: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-rldgm" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:17.226: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-rldgm": Phase="Running", Reason="", readiness=true. Elapsed: 12.479813ms
Sep  8 22:19:17.226: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-rldgm" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1 in namespace emptydir-wrapper-1947, will wait for the garbage collector to delete the pods 09/08/23 22:19:17.226
Sep  8 22:19:17.313: INFO: Deleting ReplicationController wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1 took: 19.4616ms
Sep  8 22:19:17.413: INFO: Terminating ReplicationController wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1 pods took: 100.343391ms
STEP: Creating RC which spawns configmap-volume pods 09/08/23 22:19:21.03
Sep  8 22:19:21.067: INFO: Pod name wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b: Found 0 pods out of 5
Sep  8 22:19:26.097: INFO: Pod name wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/08/23 22:19:26.098
Sep  8 22:19:26.098: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:26.117: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 18.637697ms
Sep  8 22:19:28.131: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032765442s
Sep  8 22:19:30.134: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035961957s
Sep  8 22:19:32.141: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042688104s
Sep  8 22:19:34.133: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034850068s
Sep  8 22:19:36.129: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Running", Reason="", readiness=true. Elapsed: 10.030617941s
Sep  8 22:19:36.129: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245" satisfied condition "running"
Sep  8 22:19:36.129: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-8b8dk" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:36.140: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-8b8dk": Phase="Running", Reason="", readiness=true. Elapsed: 10.921928ms
Sep  8 22:19:36.140: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-8b8dk" satisfied condition "running"
Sep  8 22:19:36.140: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-b5pzp" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:36.148: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-b5pzp": Phase="Running", Reason="", readiness=true. Elapsed: 8.3524ms
Sep  8 22:19:36.148: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-b5pzp" satisfied condition "running"
Sep  8 22:19:36.148: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-bjg7g" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:36.162: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-bjg7g": Phase="Running", Reason="", readiness=true. Elapsed: 13.466601ms
Sep  8 22:19:36.162: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-bjg7g" satisfied condition "running"
Sep  8 22:19:36.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-c2dsf" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:36.174: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-c2dsf": Phase="Running", Reason="", readiness=true. Elapsed: 12.326386ms
Sep  8 22:19:36.174: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-c2dsf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b in namespace emptydir-wrapper-1947, will wait for the garbage collector to delete the pods 09/08/23 22:19:36.174
Sep  8 22:19:36.269: INFO: Deleting ReplicationController wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b took: 31.938656ms
Sep  8 22:19:36.370: INFO: Terminating ReplicationController wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b pods took: 101.321502ms
STEP: Creating RC which spawns configmap-volume pods 09/08/23 22:19:40.184
Sep  8 22:19:40.225: INFO: Pod name wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790: Found 0 pods out of 5
Sep  8 22:19:45.242: INFO: Pod name wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790: Found 5 pods out of 5
STEP: Ensuring each pod is running 09/08/23 22:19:45.242
Sep  8 22:19:45.243: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:45.253: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 9.514755ms
Sep  8 22:19:47.263: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019575054s
Sep  8 22:19:49.273: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029839945s
Sep  8 22:19:51.267: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023483303s
Sep  8 22:19:53.267: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02405206s
Sep  8 22:19:55.265: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Running", Reason="", readiness=true. Elapsed: 10.02221179s
Sep  8 22:19:55.275: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr" satisfied condition "running"
Sep  8 22:19:55.281: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-4g48d" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:55.290: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-4g48d": Phase="Running", Reason="", readiness=true. Elapsed: 9.432691ms
Sep  8 22:19:55.290: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-4g48d" satisfied condition "running"
Sep  8 22:19:55.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-mcwmx" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:55.303: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-mcwmx": Phase="Running", Reason="", readiness=true. Elapsed: 12.810192ms
Sep  8 22:19:55.303: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-mcwmx" satisfied condition "running"
Sep  8 22:19:55.303: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-th5wg" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:55.320: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-th5wg": Phase="Running", Reason="", readiness=true. Elapsed: 16.752248ms
Sep  8 22:19:55.320: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-th5wg" satisfied condition "running"
Sep  8 22:19:55.320: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-z848w" in namespace "emptydir-wrapper-1947" to be "running"
Sep  8 22:19:55.331: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-z848w": Phase="Running", Reason="", readiness=true. Elapsed: 10.854101ms
Sep  8 22:19:55.331: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-z848w" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790 in namespace emptydir-wrapper-1947, will wait for the garbage collector to delete the pods 09/08/23 22:19:55.331
Sep  8 22:19:55.436: INFO: Deleting ReplicationController wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790 took: 36.098932ms
Sep  8 22:19:55.637: INFO: Terminating ReplicationController wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790 pods took: 200.936089ms
STEP: Cleaning up the configMaps 09/08/23 22:20:00.238
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:20:01.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1947" for this suite. 09/08/23 22:20:01.177
------------------------------
â€¢ [SLOW TEST] [62.008 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:18:59.193
    Sep  8 22:18:59.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir-wrapper 09/08/23 22:18:59.196
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:18:59.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:18:59.244
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 09/08/23 22:18:59.26
    STEP: Creating RC which spawns configmap-volume pods 09/08/23 22:19:00.028
    Sep  8 22:19:00.066: INFO: Pod name wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1: Found 0 pods out of 5
    Sep  8 22:19:05.089: INFO: Pod name wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/08/23 22:19:05.092
    Sep  8 22:19:05.093: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:05.100: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.345055ms
    Sep  8 22:19:07.113: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020364789s
    Sep  8 22:19:09.114: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020994568s
    Sep  8 22:19:11.133: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.039442496s
    Sep  8 22:19:13.121: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027591622s
    Sep  8 22:19:15.137: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf": Phase="Running", Reason="", readiness=true. Elapsed: 10.044056713s
    Sep  8 22:19:15.137: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-c5bbf" satisfied condition "running"
    Sep  8 22:19:15.137: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:15.160: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.273751ms
    Sep  8 22:19:17.178: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb": Phase="Running", Reason="", readiness=true. Elapsed: 2.041079403s
    Sep  8 22:19:17.178: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-fffhb" satisfied condition "running"
    Sep  8 22:19:17.178: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-htzkj" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:17.186: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-htzkj": Phase="Running", Reason="", readiness=true. Elapsed: 7.765522ms
    Sep  8 22:19:17.194: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-htzkj" satisfied condition "running"
    Sep  8 22:19:17.200: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-lwnzn" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:17.213: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-lwnzn": Phase="Running", Reason="", readiness=true. Elapsed: 12.982995ms
    Sep  8 22:19:17.213: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-lwnzn" satisfied condition "running"
    Sep  8 22:19:17.213: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-rldgm" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:17.226: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-rldgm": Phase="Running", Reason="", readiness=true. Elapsed: 12.479813ms
    Sep  8 22:19:17.226: INFO: Pod "wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1-rldgm" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1 in namespace emptydir-wrapper-1947, will wait for the garbage collector to delete the pods 09/08/23 22:19:17.226
    Sep  8 22:19:17.313: INFO: Deleting ReplicationController wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1 took: 19.4616ms
    Sep  8 22:19:17.413: INFO: Terminating ReplicationController wrapped-volume-race-0f5f65ac-68c3-413d-9bb6-661de51dd8b1 pods took: 100.343391ms
    STEP: Creating RC which spawns configmap-volume pods 09/08/23 22:19:21.03
    Sep  8 22:19:21.067: INFO: Pod name wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b: Found 0 pods out of 5
    Sep  8 22:19:26.097: INFO: Pod name wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/08/23 22:19:26.098
    Sep  8 22:19:26.098: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:26.117: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 18.637697ms
    Sep  8 22:19:28.131: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032765442s
    Sep  8 22:19:30.134: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035961957s
    Sep  8 22:19:32.141: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 6.042688104s
    Sep  8 22:19:34.133: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034850068s
    Sep  8 22:19:36.129: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245": Phase="Running", Reason="", readiness=true. Elapsed: 10.030617941s
    Sep  8 22:19:36.129: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-7n245" satisfied condition "running"
    Sep  8 22:19:36.129: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-8b8dk" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:36.140: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-8b8dk": Phase="Running", Reason="", readiness=true. Elapsed: 10.921928ms
    Sep  8 22:19:36.140: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-8b8dk" satisfied condition "running"
    Sep  8 22:19:36.140: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-b5pzp" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:36.148: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-b5pzp": Phase="Running", Reason="", readiness=true. Elapsed: 8.3524ms
    Sep  8 22:19:36.148: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-b5pzp" satisfied condition "running"
    Sep  8 22:19:36.148: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-bjg7g" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:36.162: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-bjg7g": Phase="Running", Reason="", readiness=true. Elapsed: 13.466601ms
    Sep  8 22:19:36.162: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-bjg7g" satisfied condition "running"
    Sep  8 22:19:36.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-c2dsf" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:36.174: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-c2dsf": Phase="Running", Reason="", readiness=true. Elapsed: 12.326386ms
    Sep  8 22:19:36.174: INFO: Pod "wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b-c2dsf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b in namespace emptydir-wrapper-1947, will wait for the garbage collector to delete the pods 09/08/23 22:19:36.174
    Sep  8 22:19:36.269: INFO: Deleting ReplicationController wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b took: 31.938656ms
    Sep  8 22:19:36.370: INFO: Terminating ReplicationController wrapped-volume-race-6739908f-7af3-4e3f-a1c0-3a3d32bd1b9b pods took: 101.321502ms
    STEP: Creating RC which spawns configmap-volume pods 09/08/23 22:19:40.184
    Sep  8 22:19:40.225: INFO: Pod name wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790: Found 0 pods out of 5
    Sep  8 22:19:45.242: INFO: Pod name wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790: Found 5 pods out of 5
    STEP: Ensuring each pod is running 09/08/23 22:19:45.242
    Sep  8 22:19:45.243: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:45.253: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 9.514755ms
    Sep  8 22:19:47.263: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019575054s
    Sep  8 22:19:49.273: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029839945s
    Sep  8 22:19:51.267: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023483303s
    Sep  8 22:19:53.267: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02405206s
    Sep  8 22:19:55.265: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr": Phase="Running", Reason="", readiness=true. Elapsed: 10.02221179s
    Sep  8 22:19:55.275: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-2nrcr" satisfied condition "running"
    Sep  8 22:19:55.281: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-4g48d" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:55.290: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-4g48d": Phase="Running", Reason="", readiness=true. Elapsed: 9.432691ms
    Sep  8 22:19:55.290: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-4g48d" satisfied condition "running"
    Sep  8 22:19:55.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-mcwmx" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:55.303: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-mcwmx": Phase="Running", Reason="", readiness=true. Elapsed: 12.810192ms
    Sep  8 22:19:55.303: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-mcwmx" satisfied condition "running"
    Sep  8 22:19:55.303: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-th5wg" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:55.320: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-th5wg": Phase="Running", Reason="", readiness=true. Elapsed: 16.752248ms
    Sep  8 22:19:55.320: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-th5wg" satisfied condition "running"
    Sep  8 22:19:55.320: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-z848w" in namespace "emptydir-wrapper-1947" to be "running"
    Sep  8 22:19:55.331: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-z848w": Phase="Running", Reason="", readiness=true. Elapsed: 10.854101ms
    Sep  8 22:19:55.331: INFO: Pod "wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790-z848w" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790 in namespace emptydir-wrapper-1947, will wait for the garbage collector to delete the pods 09/08/23 22:19:55.331
    Sep  8 22:19:55.436: INFO: Deleting ReplicationController wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790 took: 36.098932ms
    Sep  8 22:19:55.637: INFO: Terminating ReplicationController wrapped-volume-race-222db26b-73fe-47d8-8597-c22ad8a0f790 pods took: 200.936089ms
    STEP: Cleaning up the configMaps 09/08/23 22:20:00.238
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:20:01.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1947" for this suite. 09/08/23 22:20:01.177
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:20:01.201
Sep  8 22:20:01.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename security-context-test 09/08/23 22:20:01.203
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:01.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:01.268
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Sep  8 22:20:01.309: INFO: Waiting up to 5m0s for pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4" in namespace "security-context-test-3577" to be "Succeeded or Failed"
Sep  8 22:20:01.328: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.786203ms
Sep  8 22:20:03.349: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04009229s
Sep  8 22:20:05.341: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032513153s
Sep  8 22:20:05.341: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  8 22:20:05.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3577" for this suite. 09/08/23 22:20:05.356
------------------------------
â€¢ [4.196 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:20:01.201
    Sep  8 22:20:01.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename security-context-test 09/08/23 22:20:01.203
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:01.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:01.268
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Sep  8 22:20:01.309: INFO: Waiting up to 5m0s for pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4" in namespace "security-context-test-3577" to be "Succeeded or Failed"
    Sep  8 22:20:01.328: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.786203ms
    Sep  8 22:20:03.349: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04009229s
    Sep  8 22:20:05.341: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032513153s
    Sep  8 22:20:05.341: INFO: Pod "busybox-user-65534-892d36ec-5401-4ab9-a78d-29e7d2b8b1b4" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:20:05.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3577" for this suite. 09/08/23 22:20:05.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:20:05.398
Sep  8 22:20:05.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:20:05.4
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:05.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:05.497
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:20:05.512
Sep  8 22:20:05.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae" in namespace "projected-1590" to be "Succeeded or Failed"
Sep  8 22:20:05.564: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae": Phase="Pending", Reason="", readiness=false. Elapsed: 24.103572ms
Sep  8 22:20:07.581: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04137854s
Sep  8 22:20:09.593: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052657535s
STEP: Saw pod success 09/08/23 22:20:09.593
Sep  8 22:20:09.593: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae" satisfied condition "Succeeded or Failed"
Sep  8 22:20:09.629: INFO: Trying to get logs from node node-3 pod downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae container client-container: <nil>
STEP: delete the pod 09/08/23 22:20:09.657
Sep  8 22:20:09.744: INFO: Waiting for pod downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae to disappear
Sep  8 22:20:09.758: INFO: Pod downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 22:20:09.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1590" for this suite. 09/08/23 22:20:09.788
------------------------------
â€¢ [4.412 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:20:05.398
    Sep  8 22:20:05.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:20:05.4
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:05.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:05.497
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:20:05.512
    Sep  8 22:20:05.540: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae" in namespace "projected-1590" to be "Succeeded or Failed"
    Sep  8 22:20:05.564: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae": Phase="Pending", Reason="", readiness=false. Elapsed: 24.103572ms
    Sep  8 22:20:07.581: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04137854s
    Sep  8 22:20:09.593: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052657535s
    STEP: Saw pod success 09/08/23 22:20:09.593
    Sep  8 22:20:09.593: INFO: Pod "downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae" satisfied condition "Succeeded or Failed"
    Sep  8 22:20:09.629: INFO: Trying to get logs from node node-3 pod downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae container client-container: <nil>
    STEP: delete the pod 09/08/23 22:20:09.657
    Sep  8 22:20:09.744: INFO: Waiting for pod downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae to disappear
    Sep  8 22:20:09.758: INFO: Pod downwardapi-volume-a8392425-70fc-4ddf-88be-383b7c1b92ae no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:20:09.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1590" for this suite. 09/08/23 22:20:09.788
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:20:09.811
Sep  8 22:20:09.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pod-network-test 09/08/23 22:20:09.814
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:09.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:09.869
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-6030 09/08/23 22:20:09.874
STEP: creating a selector 09/08/23 22:20:09.874
STEP: Creating the service pods in kubernetes 09/08/23 22:20:09.875
Sep  8 22:20:09.875: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  8 22:20:09.958: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6030" to be "running and ready"
Sep  8 22:20:09.979: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.804313ms
Sep  8 22:20:09.979: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:20:11.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037695415s
Sep  8 22:20:11.996: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:13.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.03250568s
Sep  8 22:20:13.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:15.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032362596s
Sep  8 22:20:15.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:17.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.032666637s
Sep  8 22:20:17.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:19.997: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038382017s
Sep  8 22:20:19.997: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:21.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.038154025s
Sep  8 22:20:21.996: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:23.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.029488833s
Sep  8 22:20:23.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:25.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029734115s
Sep  8 22:20:25.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:27.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.029901335s
Sep  8 22:20:27.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:29.989: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.031199153s
Sep  8 22:20:29.989: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:20:31.989: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.030789619s
Sep  8 22:20:31.989: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  8 22:20:31.989: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  8 22:20:32.001: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6030" to be "running and ready"
Sep  8 22:20:32.010: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.906455ms
Sep  8 22:20:32.010: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  8 22:20:32.010: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/08/23 22:20:32.017
Sep  8 22:20:32.053: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6030" to be "running"
Sep  8 22:20:32.072: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.657726ms
Sep  8 22:20:34.085: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031519357s
Sep  8 22:20:34.085: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  8 22:20:34.092: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6030" to be "running"
Sep  8 22:20:34.103: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.188865ms
Sep  8 22:20:34.103: INFO: Pod "host-test-container-pod" satisfied condition "running"
Sep  8 22:20:34.114: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  8 22:20:34.114: INFO: Going to poll 10.233.75.114 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  8 22:20:34.121: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.75.114:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:20:34.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:20:34.122: INFO: ExecWithOptions: Clientset creation
Sep  8 22:20:34.122: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6030/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.75.114%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  8 22:20:34.269: INFO: Found all 1 expected endpoints: [netserver-0]
Sep  8 22:20:34.269: INFO: Going to poll 10.233.103.127 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Sep  8 22:20:34.277: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.103.127:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:20:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:20:34.278: INFO: ExecWithOptions: Clientset creation
Sep  8 22:20:34.278: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6030/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.103.127%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  8 22:20:34.400: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  8 22:20:34.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6030" for this suite. 09/08/23 22:20:34.417
------------------------------
â€¢ [SLOW TEST] [24.622 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:20:09.811
    Sep  8 22:20:09.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pod-network-test 09/08/23 22:20:09.814
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:09.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:09.869
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-6030 09/08/23 22:20:09.874
    STEP: creating a selector 09/08/23 22:20:09.874
    STEP: Creating the service pods in kubernetes 09/08/23 22:20:09.875
    Sep  8 22:20:09.875: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  8 22:20:09.958: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6030" to be "running and ready"
    Sep  8 22:20:09.979: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.804313ms
    Sep  8 22:20:09.979: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:20:11.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.037695415s
    Sep  8 22:20:11.996: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:13.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.03250568s
    Sep  8 22:20:13.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:15.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.032362596s
    Sep  8 22:20:15.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:17.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.032666637s
    Sep  8 22:20:17.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:19.997: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.038382017s
    Sep  8 22:20:19.997: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:21.996: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.038154025s
    Sep  8 22:20:21.996: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:23.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.029488833s
    Sep  8 22:20:23.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:25.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029734115s
    Sep  8 22:20:25.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:27.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.029901335s
    Sep  8 22:20:27.988: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:29.989: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.031199153s
    Sep  8 22:20:29.989: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:20:31.989: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.030789619s
    Sep  8 22:20:31.989: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  8 22:20:31.989: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  8 22:20:32.001: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6030" to be "running and ready"
    Sep  8 22:20:32.010: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.906455ms
    Sep  8 22:20:32.010: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  8 22:20:32.010: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/08/23 22:20:32.017
    Sep  8 22:20:32.053: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6030" to be "running"
    Sep  8 22:20:32.072: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.657726ms
    Sep  8 22:20:34.085: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.031519357s
    Sep  8 22:20:34.085: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  8 22:20:34.092: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6030" to be "running"
    Sep  8 22:20:34.103: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.188865ms
    Sep  8 22:20:34.103: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Sep  8 22:20:34.114: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  8 22:20:34.114: INFO: Going to poll 10.233.75.114 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  8 22:20:34.121: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.75.114:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:20:34.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:20:34.122: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:20:34.122: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6030/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.75.114%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  8 22:20:34.269: INFO: Found all 1 expected endpoints: [netserver-0]
    Sep  8 22:20:34.269: INFO: Going to poll 10.233.103.127 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Sep  8 22:20:34.277: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.103.127:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6030 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:20:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:20:34.278: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:20:34.278: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6030/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.103.127%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  8 22:20:34.400: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:20:34.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6030" for this suite. 09/08/23 22:20:34.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:20:34.446
Sep  8 22:20:34.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:20:34.447
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:34.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:34.493
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-7964456d-8fc1-4633-882a-ddc62c5eb8d7 09/08/23 22:20:34.498
STEP: Creating a pod to test consume secrets 09/08/23 22:20:34.512
Sep  8 22:20:34.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7" in namespace "projected-7451" to be "Succeeded or Failed"
Sep  8 22:20:34.568: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.117654ms
Sep  8 22:20:36.583: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039536984s
Sep  8 22:20:38.583: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039445875s
STEP: Saw pod success 09/08/23 22:20:38.583
Sep  8 22:20:38.583: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7" satisfied condition "Succeeded or Failed"
Sep  8 22:20:38.592: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:20:38.626
Sep  8 22:20:38.668: INFO: Waiting for pod pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7 to disappear
Sep  8 22:20:38.675: INFO: Pod pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  8 22:20:38.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7451" for this suite. 09/08/23 22:20:38.683
------------------------------
â€¢ [4.255 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:20:34.446
    Sep  8 22:20:34.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:20:34.447
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:34.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:34.493
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-7964456d-8fc1-4633-882a-ddc62c5eb8d7 09/08/23 22:20:34.498
    STEP: Creating a pod to test consume secrets 09/08/23 22:20:34.512
    Sep  8 22:20:34.543: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7" in namespace "projected-7451" to be "Succeeded or Failed"
    Sep  8 22:20:34.568: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.117654ms
    Sep  8 22:20:36.583: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039536984s
    Sep  8 22:20:38.583: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039445875s
    STEP: Saw pod success 09/08/23 22:20:38.583
    Sep  8 22:20:38.583: INFO: Pod "pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7" satisfied condition "Succeeded or Failed"
    Sep  8 22:20:38.592: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:20:38.626
    Sep  8 22:20:38.668: INFO: Waiting for pod pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7 to disappear
    Sep  8 22:20:38.675: INFO: Pod pod-projected-secrets-c2a0ed42-4aa1-43f8-81a6-ae76724778d7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:20:38.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7451" for this suite. 09/08/23 22:20:38.683
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:20:38.702
Sep  8 22:20:38.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename gc 09/08/23 22:20:38.704
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:38.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:38.766
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 09/08/23 22:20:38.785
STEP: create the rc2 09/08/23 22:20:38.795
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/08/23 22:20:43.84
STEP: delete the rc simpletest-rc-to-be-deleted 09/08/23 22:20:45.247
STEP: wait for the rc to be deleted 09/08/23 22:20:45.3
Sep  8 22:20:50.398: INFO: 68 pods remaining
Sep  8 22:20:50.398: INFO: 68 pods has nil DeletionTimestamp
Sep  8 22:20:50.398: INFO: 
STEP: Gathering metrics 09/08/23 22:20:55.351
Sep  8 22:20:55.429: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
Sep  8 22:20:55.455: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 26.714726ms
Sep  8 22:20:55.456: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
Sep  8 22:20:55.456: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
Sep  8 22:20:55.585: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  8 22:20:55.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-286ld" in namespace "gc-6786"
Sep  8 22:20:55.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-28ww8" in namespace "gc-6786"
Sep  8 22:20:55.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xqwg" in namespace "gc-6786"
Sep  8 22:20:55.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ll84" in namespace "gc-6786"
Sep  8 22:20:55.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p958" in namespace "gc-6786"
Sep  8 22:20:55.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tk7n" in namespace "gc-6786"
Sep  8 22:20:55.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-558mq" in namespace "gc-6786"
Sep  8 22:20:55.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m775" in namespace "gc-6786"
Sep  8 22:20:56.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-65mfp" in namespace "gc-6786"
Sep  8 22:20:56.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c6zx" in namespace "gc-6786"
Sep  8 22:20:56.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jh76" in namespace "gc-6786"
Sep  8 22:20:56.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k7v6" in namespace "gc-6786"
Sep  8 22:20:56.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tnvx" in namespace "gc-6786"
Sep  8 22:20:56.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vmx5" in namespace "gc-6786"
Sep  8 22:20:56.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wsjf" in namespace "gc-6786"
Sep  8 22:20:56.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-7426h" in namespace "gc-6786"
Sep  8 22:20:56.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-7875v" in namespace "gc-6786"
Sep  8 22:20:56.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mk54" in namespace "gc-6786"
Sep  8 22:20:56.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q5q4" in namespace "gc-6786"
Sep  8 22:20:56.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w6nk" in namespace "gc-6786"
Sep  8 22:20:56.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ww84" in namespace "gc-6786"
Sep  8 22:20:57.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lg9r" in namespace "gc-6786"
Sep  8 22:20:57.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zbdd" in namespace "gc-6786"
Sep  8 22:20:57.227: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lhv4" in namespace "gc-6786"
Sep  8 22:20:57.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-9trwx" in namespace "gc-6786"
Sep  8 22:20:57.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8wn6" in namespace "gc-6786"
Sep  8 22:20:57.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcbwz" in namespace "gc-6786"
Sep  8 22:20:57.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgqr7" in namespace "gc-6786"
Sep  8 22:20:57.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhkqp" in namespace "gc-6786"
Sep  8 22:20:57.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5qp7" in namespace "gc-6786"
Sep  8 22:20:57.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6frt" in namespace "gc-6786"
Sep  8 22:20:58.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkhlh" in namespace "gc-6786"
Sep  8 22:20:58.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvr7v" in namespace "gc-6786"
Sep  8 22:20:58.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-f294q" in namespace "gc-6786"
Sep  8 22:20:58.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2m66" in namespace "gc-6786"
Sep  8 22:20:58.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5sp7" in namespace "gc-6786"
Sep  8 22:20:58.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbdsl" in namespace "gc-6786"
Sep  8 22:20:58.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-fldwz" in namespace "gc-6786"
Sep  8 22:20:58.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp64z" in namespace "gc-6786"
Sep  8 22:20:58.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvf7m" in namespace "gc-6786"
Sep  8 22:20:58.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjcs9" in namespace "gc-6786"
Sep  8 22:20:58.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkbnd" in namespace "gc-6786"
Sep  8 22:20:58.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-glnzn" in namespace "gc-6786"
Sep  8 22:20:58.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnskk" in namespace "gc-6786"
Sep  8 22:20:58.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsxg7" in namespace "gc-6786"
Sep  8 22:20:59.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvd7t" in namespace "gc-6786"
Sep  8 22:20:59.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-h45mj" in namespace "gc-6786"
Sep  8 22:20:59.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7jlz" in namespace "gc-6786"
Sep  8 22:20:59.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdwsb" in namespace "gc-6786"
Sep  8 22:20:59.278: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgbjj" in namespace "gc-6786"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  8 22:20:59.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6786" for this suite. 09/08/23 22:20:59.394
------------------------------
â€¢ [SLOW TEST] [20.732 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:20:38.702
    Sep  8 22:20:38.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename gc 09/08/23 22:20:38.704
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:38.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:38.766
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 09/08/23 22:20:38.785
    STEP: create the rc2 09/08/23 22:20:38.795
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 09/08/23 22:20:43.84
    STEP: delete the rc simpletest-rc-to-be-deleted 09/08/23 22:20:45.247
    STEP: wait for the rc to be deleted 09/08/23 22:20:45.3
    Sep  8 22:20:50.398: INFO: 68 pods remaining
    Sep  8 22:20:50.398: INFO: 68 pods has nil DeletionTimestamp
    Sep  8 22:20:50.398: INFO: 
    STEP: Gathering metrics 09/08/23 22:20:55.351
    Sep  8 22:20:55.429: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
    Sep  8 22:20:55.455: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 26.714726ms
    Sep  8 22:20:55.456: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
    Sep  8 22:20:55.456: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
    Sep  8 22:20:55.585: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  8 22:20:55.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-286ld" in namespace "gc-6786"
    Sep  8 22:20:55.621: INFO: Deleting pod "simpletest-rc-to-be-deleted-28ww8" in namespace "gc-6786"
    Sep  8 22:20:55.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xqwg" in namespace "gc-6786"
    Sep  8 22:20:55.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ll84" in namespace "gc-6786"
    Sep  8 22:20:55.776: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p958" in namespace "gc-6786"
    Sep  8 22:20:55.856: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tk7n" in namespace "gc-6786"
    Sep  8 22:20:55.901: INFO: Deleting pod "simpletest-rc-to-be-deleted-558mq" in namespace "gc-6786"
    Sep  8 22:20:55.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m775" in namespace "gc-6786"
    Sep  8 22:20:56.051: INFO: Deleting pod "simpletest-rc-to-be-deleted-65mfp" in namespace "gc-6786"
    Sep  8 22:20:56.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c6zx" in namespace "gc-6786"
    Sep  8 22:20:56.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jh76" in namespace "gc-6786"
    Sep  8 22:20:56.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k7v6" in namespace "gc-6786"
    Sep  8 22:20:56.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tnvx" in namespace "gc-6786"
    Sep  8 22:20:56.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vmx5" in namespace "gc-6786"
    Sep  8 22:20:56.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wsjf" in namespace "gc-6786"
    Sep  8 22:20:56.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-7426h" in namespace "gc-6786"
    Sep  8 22:20:56.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-7875v" in namespace "gc-6786"
    Sep  8 22:20:56.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mk54" in namespace "gc-6786"
    Sep  8 22:20:56.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q5q4" in namespace "gc-6786"
    Sep  8 22:20:56.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w6nk" in namespace "gc-6786"
    Sep  8 22:20:56.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ww84" in namespace "gc-6786"
    Sep  8 22:20:57.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lg9r" in namespace "gc-6786"
    Sep  8 22:20:57.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zbdd" in namespace "gc-6786"
    Sep  8 22:20:57.227: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lhv4" in namespace "gc-6786"
    Sep  8 22:20:57.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-9trwx" in namespace "gc-6786"
    Sep  8 22:20:57.394: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8wn6" in namespace "gc-6786"
    Sep  8 22:20:57.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-bcbwz" in namespace "gc-6786"
    Sep  8 22:20:57.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgqr7" in namespace "gc-6786"
    Sep  8 22:20:57.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhkqp" in namespace "gc-6786"
    Sep  8 22:20:57.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5qp7" in namespace "gc-6786"
    Sep  8 22:20:57.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6frt" in namespace "gc-6786"
    Sep  8 22:20:58.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkhlh" in namespace "gc-6786"
    Sep  8 22:20:58.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvr7v" in namespace "gc-6786"
    Sep  8 22:20:58.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-f294q" in namespace "gc-6786"
    Sep  8 22:20:58.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2m66" in namespace "gc-6786"
    Sep  8 22:20:58.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5sp7" in namespace "gc-6786"
    Sep  8 22:20:58.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbdsl" in namespace "gc-6786"
    Sep  8 22:20:58.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-fldwz" in namespace "gc-6786"
    Sep  8 22:20:58.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp64z" in namespace "gc-6786"
    Sep  8 22:20:58.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvf7m" in namespace "gc-6786"
    Sep  8 22:20:58.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjcs9" in namespace "gc-6786"
    Sep  8 22:20:58.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkbnd" in namespace "gc-6786"
    Sep  8 22:20:58.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-glnzn" in namespace "gc-6786"
    Sep  8 22:20:58.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnskk" in namespace "gc-6786"
    Sep  8 22:20:58.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsxg7" in namespace "gc-6786"
    Sep  8 22:20:59.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvd7t" in namespace "gc-6786"
    Sep  8 22:20:59.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-h45mj" in namespace "gc-6786"
    Sep  8 22:20:59.178: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7jlz" in namespace "gc-6786"
    Sep  8 22:20:59.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdwsb" in namespace "gc-6786"
    Sep  8 22:20:59.278: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgbjj" in namespace "gc-6786"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:20:59.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6786" for this suite. 09/08/23 22:20:59.394
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:20:59.435
Sep  8 22:20:59.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename endpointslice 09/08/23 22:20:59.443
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:59.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:59.54
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 09/08/23 22:20:59.571
STEP: getting /apis/discovery.k8s.io 09/08/23 22:20:59.591
STEP: getting /apis/discovery.k8s.iov1 09/08/23 22:20:59.602
STEP: creating 09/08/23 22:20:59.606
STEP: getting 09/08/23 22:20:59.672
STEP: listing 09/08/23 22:20:59.712
STEP: watching 09/08/23 22:20:59.74
Sep  8 22:20:59.740: INFO: starting watch
STEP: cluster-wide listing 09/08/23 22:20:59.746
STEP: cluster-wide watching 09/08/23 22:20:59.77
Sep  8 22:20:59.770: INFO: starting watch
STEP: patching 09/08/23 22:20:59.772
STEP: updating 09/08/23 22:20:59.792
Sep  8 22:20:59.824: INFO: waiting for watch events with expected annotations
Sep  8 22:20:59.824: INFO: saw patched and updated annotations
STEP: deleting 09/08/23 22:20:59.825
STEP: deleting a collection 09/08/23 22:20:59.904
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Sep  8 22:21:00.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5371" for this suite. 09/08/23 22:21:00.043
------------------------------
â€¢ [0.653 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:20:59.435
    Sep  8 22:20:59.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename endpointslice 09/08/23 22:20:59.443
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:20:59.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:20:59.54
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 09/08/23 22:20:59.571
    STEP: getting /apis/discovery.k8s.io 09/08/23 22:20:59.591
    STEP: getting /apis/discovery.k8s.iov1 09/08/23 22:20:59.602
    STEP: creating 09/08/23 22:20:59.606
    STEP: getting 09/08/23 22:20:59.672
    STEP: listing 09/08/23 22:20:59.712
    STEP: watching 09/08/23 22:20:59.74
    Sep  8 22:20:59.740: INFO: starting watch
    STEP: cluster-wide listing 09/08/23 22:20:59.746
    STEP: cluster-wide watching 09/08/23 22:20:59.77
    Sep  8 22:20:59.770: INFO: starting watch
    STEP: patching 09/08/23 22:20:59.772
    STEP: updating 09/08/23 22:20:59.792
    Sep  8 22:20:59.824: INFO: waiting for watch events with expected annotations
    Sep  8 22:20:59.824: INFO: saw patched and updated annotations
    STEP: deleting 09/08/23 22:20:59.825
    STEP: deleting a collection 09/08/23 22:20:59.904
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:21:00.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5371" for this suite. 09/08/23 22:21:00.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:21:00.091
Sep  8 22:21:00.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename security-context-test 09/08/23 22:21:00.092
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:00.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:00.179
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Sep  8 22:21:00.240: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65" in namespace "security-context-test-6988" to be "Succeeded or Failed"
Sep  8 22:21:00.269: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 28.884326ms
Sep  8 22:21:02.321: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080779105s
Sep  8 22:21:04.279: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038824878s
Sep  8 22:21:06.288: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04749863s
Sep  8 22:21:08.286: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046245453s
Sep  8 22:21:10.281: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040755971s
Sep  8 22:21:12.316: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.075556132s
Sep  8 22:21:12.316: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  8 22:21:12.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6988" for this suite. 09/08/23 22:21:12.334
------------------------------
â€¢ [SLOW TEST] [12.267 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:21:00.091
    Sep  8 22:21:00.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename security-context-test 09/08/23 22:21:00.092
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:00.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:00.179
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Sep  8 22:21:00.240: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65" in namespace "security-context-test-6988" to be "Succeeded or Failed"
    Sep  8 22:21:00.269: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 28.884326ms
    Sep  8 22:21:02.321: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080779105s
    Sep  8 22:21:04.279: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038824878s
    Sep  8 22:21:06.288: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04749863s
    Sep  8 22:21:08.286: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046245453s
    Sep  8 22:21:10.281: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040755971s
    Sep  8 22:21:12.316: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.075556132s
    Sep  8 22:21:12.316: INFO: Pod "busybox-readonly-false-e16ecbbe-b008-470f-813a-1727206b0c65" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:21:12.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6988" for this suite. 09/08/23 22:21:12.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:21:12.358
Sep  8 22:21:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:21:12.359
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:12.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:12.464
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 09/08/23 22:21:12.476
Sep  8 22:21:12.535: INFO: Waiting up to 5m0s for pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b" in namespace "projected-4919" to be "running and ready"
Sep  8 22:21:12.549: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.105088ms
Sep  8 22:21:12.549: INFO: The phase of Pod labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:21:14.560: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024864666s
Sep  8 22:21:14.560: INFO: The phase of Pod labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:21:16.579: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b": Phase="Running", Reason="", readiness=true. Elapsed: 4.044152442s
Sep  8 22:21:16.579: INFO: The phase of Pod labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b is Running (Ready = true)
Sep  8 22:21:16.579: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b" satisfied condition "running and ready"
Sep  8 22:21:17.154: INFO: Successfully updated pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 22:21:19.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4919" for this suite. 09/08/23 22:21:19.209
------------------------------
â€¢ [SLOW TEST] [6.881 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:21:12.358
    Sep  8 22:21:12.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:21:12.359
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:12.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:12.464
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 09/08/23 22:21:12.476
    Sep  8 22:21:12.535: INFO: Waiting up to 5m0s for pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b" in namespace "projected-4919" to be "running and ready"
    Sep  8 22:21:12.549: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.105088ms
    Sep  8 22:21:12.549: INFO: The phase of Pod labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:21:14.560: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024864666s
    Sep  8 22:21:14.560: INFO: The phase of Pod labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:21:16.579: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b": Phase="Running", Reason="", readiness=true. Elapsed: 4.044152442s
    Sep  8 22:21:16.579: INFO: The phase of Pod labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b is Running (Ready = true)
    Sep  8 22:21:16.579: INFO: Pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b" satisfied condition "running and ready"
    Sep  8 22:21:17.154: INFO: Successfully updated pod "labelsupdateb042ab7b-0991-4345-803d-505d0d4f8d6b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:21:19.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4919" for this suite. 09/08/23 22:21:19.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:21:19.24
Sep  8 22:21:19.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 22:21:19.241
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:19.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:19.306
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/08/23 22:21:19.316
Sep  8 22:21:19.368: INFO: Waiting up to 5m0s for pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7" in namespace "emptydir-3495" to be "Succeeded or Failed"
Sep  8 22:21:19.394: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.986877ms
Sep  8 22:21:21.410: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041095542s
Sep  8 22:21:23.409: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041031614s
Sep  8 22:21:25.406: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037485556s
STEP: Saw pod success 09/08/23 22:21:25.406
Sep  8 22:21:25.406: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7" satisfied condition "Succeeded or Failed"
Sep  8 22:21:25.417: INFO: Trying to get logs from node node-3 pod pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7 container test-container: <nil>
STEP: delete the pod 09/08/23 22:21:25.435
Sep  8 22:21:25.479: INFO: Waiting for pod pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7 to disappear
Sep  8 22:21:25.500: INFO: Pod pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:21:25.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3495" for this suite. 09/08/23 22:21:25.51
------------------------------
â€¢ [SLOW TEST] [6.284 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:21:19.24
    Sep  8 22:21:19.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 22:21:19.241
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:19.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:19.306
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/08/23 22:21:19.316
    Sep  8 22:21:19.368: INFO: Waiting up to 5m0s for pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7" in namespace "emptydir-3495" to be "Succeeded or Failed"
    Sep  8 22:21:19.394: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Pending", Reason="", readiness=false. Elapsed: 25.986877ms
    Sep  8 22:21:21.410: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041095542s
    Sep  8 22:21:23.409: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041031614s
    Sep  8 22:21:25.406: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037485556s
    STEP: Saw pod success 09/08/23 22:21:25.406
    Sep  8 22:21:25.406: INFO: Pod "pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7" satisfied condition "Succeeded or Failed"
    Sep  8 22:21:25.417: INFO: Trying to get logs from node node-3 pod pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7 container test-container: <nil>
    STEP: delete the pod 09/08/23 22:21:25.435
    Sep  8 22:21:25.479: INFO: Waiting for pod pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7 to disappear
    Sep  8 22:21:25.500: INFO: Pod pod-e1dcb1bf-87d7-426e-9033-17c7e99453b7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:21:25.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3495" for this suite. 09/08/23 22:21:25.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:21:25.526
Sep  8 22:21:25.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-preemption 09/08/23 22:21:25.527
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:25.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:25.574
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  8 22:21:25.624: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  8 22:22:25.701: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:25.712
Sep  8 22:22:25.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-preemption-path 09/08/23 22:22:25.714
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:25.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:25.769
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Sep  8 22:22:25.821: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Sep  8 22:22:25.833: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:25.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:25.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8425" for this suite. 09/08/23 22:22:26.127
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9294" for this suite. 09/08/23 22:22:26.14
------------------------------
â€¢ [SLOW TEST] [60.635 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:21:25.526
    Sep  8 22:21:25.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-preemption 09/08/23 22:21:25.527
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:21:25.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:21:25.574
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  8 22:21:25.624: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  8 22:22:25.701: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:25.712
    Sep  8 22:22:25.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-preemption-path 09/08/23 22:22:25.714
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:25.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:25.769
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Sep  8 22:22:25.821: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Sep  8 22:22:25.833: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:25.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:25.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8425" for this suite. 09/08/23 22:22:26.127
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9294" for this suite. 09/08/23 22:22:26.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:26.178
Sep  8 22:22:26.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir-wrapper 09/08/23 22:22:26.179
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:26.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:26.242
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Sep  8 22:22:26.304: INFO: Waiting up to 5m0s for pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9" in namespace "emptydir-wrapper-4149" to be "running and ready"
Sep  8 22:22:26.312: INFO: Pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.229471ms
Sep  8 22:22:26.313: INFO: The phase of Pod pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:22:28.326: INFO: Pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9": Phase="Running", Reason="", readiness=true. Elapsed: 2.021922723s
Sep  8 22:22:28.326: INFO: The phase of Pod pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9 is Running (Ready = true)
Sep  8 22:22:28.327: INFO: Pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9" satisfied condition "running and ready"
STEP: Cleaning up the secret 09/08/23 22:22:28.335
STEP: Cleaning up the configmap 09/08/23 22:22:28.354
STEP: Cleaning up the pod 09/08/23 22:22:28.382
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:28.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-4149" for this suite. 09/08/23 22:22:28.426
------------------------------
â€¢ [2.269 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:26.178
    Sep  8 22:22:26.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir-wrapper 09/08/23 22:22:26.179
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:26.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:26.242
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Sep  8 22:22:26.304: INFO: Waiting up to 5m0s for pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9" in namespace "emptydir-wrapper-4149" to be "running and ready"
    Sep  8 22:22:26.312: INFO: Pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.229471ms
    Sep  8 22:22:26.313: INFO: The phase of Pod pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:22:28.326: INFO: Pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9": Phase="Running", Reason="", readiness=true. Elapsed: 2.021922723s
    Sep  8 22:22:28.326: INFO: The phase of Pod pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9 is Running (Ready = true)
    Sep  8 22:22:28.327: INFO: Pod "pod-secrets-4680263d-2999-4a9a-a39c-97c71c9bbbe9" satisfied condition "running and ready"
    STEP: Cleaning up the secret 09/08/23 22:22:28.335
    STEP: Cleaning up the configmap 09/08/23 22:22:28.354
    STEP: Cleaning up the pod 09/08/23 22:22:28.382
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:28.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-4149" for this suite. 09/08/23 22:22:28.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:28.448
Sep  8 22:22:28.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 22:22:28.45
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:28.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:28.503
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 09/08/23 22:22:28.507
Sep  8 22:22:28.526: INFO: Waiting up to 5m0s for pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9" in namespace "emptydir-8411" to be "Succeeded or Failed"
Sep  8 22:22:28.534: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.635733ms
Sep  8 22:22:30.546: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019194571s
Sep  8 22:22:32.546: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01971355s
STEP: Saw pod success 09/08/23 22:22:32.546
Sep  8 22:22:32.546: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9" satisfied condition "Succeeded or Failed"
Sep  8 22:22:32.555: INFO: Trying to get logs from node node-3 pod pod-8a1f4132-cab4-4325-9ff0-05d1074259a9 container test-container: <nil>
STEP: delete the pod 09/08/23 22:22:32.58
Sep  8 22:22:32.611: INFO: Waiting for pod pod-8a1f4132-cab4-4325-9ff0-05d1074259a9 to disappear
Sep  8 22:22:32.620: INFO: Pod pod-8a1f4132-cab4-4325-9ff0-05d1074259a9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:32.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8411" for this suite. 09/08/23 22:22:32.632
------------------------------
â€¢ [4.198 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:28.448
    Sep  8 22:22:28.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 22:22:28.45
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:28.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:28.503
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 09/08/23 22:22:28.507
    Sep  8 22:22:28.526: INFO: Waiting up to 5m0s for pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9" in namespace "emptydir-8411" to be "Succeeded or Failed"
    Sep  8 22:22:28.534: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.635733ms
    Sep  8 22:22:30.546: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019194571s
    Sep  8 22:22:32.546: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01971355s
    STEP: Saw pod success 09/08/23 22:22:32.546
    Sep  8 22:22:32.546: INFO: Pod "pod-8a1f4132-cab4-4325-9ff0-05d1074259a9" satisfied condition "Succeeded or Failed"
    Sep  8 22:22:32.555: INFO: Trying to get logs from node node-3 pod pod-8a1f4132-cab4-4325-9ff0-05d1074259a9 container test-container: <nil>
    STEP: delete the pod 09/08/23 22:22:32.58
    Sep  8 22:22:32.611: INFO: Waiting for pod pod-8a1f4132-cab4-4325-9ff0-05d1074259a9 to disappear
    Sep  8 22:22:32.620: INFO: Pod pod-8a1f4132-cab4-4325-9ff0-05d1074259a9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:32.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8411" for this suite. 09/08/23 22:22:32.632
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:32.65
Sep  8 22:22:32.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 22:22:32.651
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:32.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:32.766
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 09/08/23 22:22:32.771
Sep  8 22:22:32.790: INFO: Waiting up to 5m0s for pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0" in namespace "emptydir-2973" to be "Succeeded or Failed"
Sep  8 22:22:32.807: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.472063ms
Sep  8 22:22:34.817: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026999256s
Sep  8 22:22:36.819: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029177323s
STEP: Saw pod success 09/08/23 22:22:36.819
Sep  8 22:22:36.819: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0" satisfied condition "Succeeded or Failed"
Sep  8 22:22:36.839: INFO: Trying to get logs from node node-3 pod pod-ff79eab1-5db2-4a19-a991-42cb38a217f0 container test-container: <nil>
STEP: delete the pod 09/08/23 22:22:36.867
Sep  8 22:22:36.908: INFO: Waiting for pod pod-ff79eab1-5db2-4a19-a991-42cb38a217f0 to disappear
Sep  8 22:22:36.918: INFO: Pod pod-ff79eab1-5db2-4a19-a991-42cb38a217f0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:36.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2973" for this suite. 09/08/23 22:22:36.941
------------------------------
â€¢ [4.310 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:32.65
    Sep  8 22:22:32.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 22:22:32.651
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:32.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:32.766
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 09/08/23 22:22:32.771
    Sep  8 22:22:32.790: INFO: Waiting up to 5m0s for pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0" in namespace "emptydir-2973" to be "Succeeded or Failed"
    Sep  8 22:22:32.807: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.472063ms
    Sep  8 22:22:34.817: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026999256s
    Sep  8 22:22:36.819: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029177323s
    STEP: Saw pod success 09/08/23 22:22:36.819
    Sep  8 22:22:36.819: INFO: Pod "pod-ff79eab1-5db2-4a19-a991-42cb38a217f0" satisfied condition "Succeeded or Failed"
    Sep  8 22:22:36.839: INFO: Trying to get logs from node node-3 pod pod-ff79eab1-5db2-4a19-a991-42cb38a217f0 container test-container: <nil>
    STEP: delete the pod 09/08/23 22:22:36.867
    Sep  8 22:22:36.908: INFO: Waiting for pod pod-ff79eab1-5db2-4a19-a991-42cb38a217f0 to disappear
    Sep  8 22:22:36.918: INFO: Pod pod-ff79eab1-5db2-4a19-a991-42cb38a217f0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:36.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2973" for this suite. 09/08/23 22:22:36.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:36.963
Sep  8 22:22:36.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 22:22:36.964
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:37.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:37.032
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-0cc1c996-9f6c-4d68-bc82-106e2647105a 09/08/23 22:22:37.038
STEP: Creating a pod to test consume secrets 09/08/23 22:22:37.05
Sep  8 22:22:37.088: INFO: Waiting up to 5m0s for pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a" in namespace "secrets-3547" to be "Succeeded or Failed"
Sep  8 22:22:37.097: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.10752ms
Sep  8 22:22:39.122: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033651463s
Sep  8 22:22:41.108: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020009979s
Sep  8 22:22:43.110: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02177252s
STEP: Saw pod success 09/08/23 22:22:43.11
Sep  8 22:22:43.110: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a" satisfied condition "Succeeded or Failed"
Sep  8 22:22:43.123: INFO: Trying to get logs from node node-3 pod pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:22:43.15
Sep  8 22:22:43.189: INFO: Waiting for pod pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a to disappear
Sep  8 22:22:43.195: INFO: Pod pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:43.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3547" for this suite. 09/08/23 22:22:43.207
------------------------------
â€¢ [SLOW TEST] [6.261 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:36.963
    Sep  8 22:22:36.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 22:22:36.964
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:37.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:37.032
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-0cc1c996-9f6c-4d68-bc82-106e2647105a 09/08/23 22:22:37.038
    STEP: Creating a pod to test consume secrets 09/08/23 22:22:37.05
    Sep  8 22:22:37.088: INFO: Waiting up to 5m0s for pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a" in namespace "secrets-3547" to be "Succeeded or Failed"
    Sep  8 22:22:37.097: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.10752ms
    Sep  8 22:22:39.122: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033651463s
    Sep  8 22:22:41.108: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020009979s
    Sep  8 22:22:43.110: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02177252s
    STEP: Saw pod success 09/08/23 22:22:43.11
    Sep  8 22:22:43.110: INFO: Pod "pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a" satisfied condition "Succeeded or Failed"
    Sep  8 22:22:43.123: INFO: Trying to get logs from node node-3 pod pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:22:43.15
    Sep  8 22:22:43.189: INFO: Waiting for pod pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a to disappear
    Sep  8 22:22:43.195: INFO: Pod pod-secrets-48afaeec-d1d5-484b-93a7-a6d2c1bb436a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:43.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3547" for this suite. 09/08/23 22:22:43.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:43.226
Sep  8 22:22:43.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svcaccounts 09/08/23 22:22:43.228
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:43.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:43.297
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-7pgw4"  09/08/23 22:22:43.307
Sep  8 22:22:43.321: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-7pgw4"  09/08/23 22:22:43.321
Sep  8 22:22:43.346: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:43.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1344" for this suite. 09/08/23 22:22:43.36
------------------------------
â€¢ [0.164 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:43.226
    Sep  8 22:22:43.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svcaccounts 09/08/23 22:22:43.228
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:43.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:43.297
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-7pgw4"  09/08/23 22:22:43.307
    Sep  8 22:22:43.321: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-7pgw4"  09/08/23 22:22:43.321
    Sep  8 22:22:43.346: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:43.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1344" for this suite. 09/08/23 22:22:43.36
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:43.391
Sep  8 22:22:43.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename watch 09/08/23 22:22:43.392
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:43.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:43.477
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 09/08/23 22:22:43.488
STEP: starting a background goroutine to produce watch events 09/08/23 22:22:43.51
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/08/23 22:22:43.51
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:46.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6692" for this suite. 09/08/23 22:22:46.26
------------------------------
â€¢ [2.922 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:43.391
    Sep  8 22:22:43.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename watch 09/08/23 22:22:43.392
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:43.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:43.477
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 09/08/23 22:22:43.488
    STEP: starting a background goroutine to produce watch events 09/08/23 22:22:43.51
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 09/08/23 22:22:43.51
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:46.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6692" for this suite. 09/08/23 22:22:46.26
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:46.313
Sep  8 22:22:46.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename cronjob 09/08/23 22:22:46.314
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:46.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:46.376
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 09/08/23 22:22:46.381
STEP: creating 09/08/23 22:22:46.381
STEP: getting 09/08/23 22:22:46.396
STEP: listing 09/08/23 22:22:46.404
STEP: watching 09/08/23 22:22:46.414
Sep  8 22:22:46.415: INFO: starting watch
STEP: cluster-wide listing 09/08/23 22:22:46.417
STEP: cluster-wide watching 09/08/23 22:22:46.432
Sep  8 22:22:46.432: INFO: starting watch
STEP: patching 09/08/23 22:22:46.437
STEP: updating 09/08/23 22:22:46.462
Sep  8 22:22:46.493: INFO: waiting for watch events with expected annotations
Sep  8 22:22:46.493: INFO: saw patched and updated annotations
STEP: patching /status 09/08/23 22:22:46.493
STEP: updating /status 09/08/23 22:22:46.511
STEP: get /status 09/08/23 22:22:46.539
STEP: deleting 09/08/23 22:22:46.553
STEP: deleting a collection 09/08/23 22:22:46.588
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:46.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1207" for this suite. 09/08/23 22:22:46.641
------------------------------
â€¢ [0.356 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:46.313
    Sep  8 22:22:46.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename cronjob 09/08/23 22:22:46.314
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:46.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:46.376
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 09/08/23 22:22:46.381
    STEP: creating 09/08/23 22:22:46.381
    STEP: getting 09/08/23 22:22:46.396
    STEP: listing 09/08/23 22:22:46.404
    STEP: watching 09/08/23 22:22:46.414
    Sep  8 22:22:46.415: INFO: starting watch
    STEP: cluster-wide listing 09/08/23 22:22:46.417
    STEP: cluster-wide watching 09/08/23 22:22:46.432
    Sep  8 22:22:46.432: INFO: starting watch
    STEP: patching 09/08/23 22:22:46.437
    STEP: updating 09/08/23 22:22:46.462
    Sep  8 22:22:46.493: INFO: waiting for watch events with expected annotations
    Sep  8 22:22:46.493: INFO: saw patched and updated annotations
    STEP: patching /status 09/08/23 22:22:46.493
    STEP: updating /status 09/08/23 22:22:46.511
    STEP: get /status 09/08/23 22:22:46.539
    STEP: deleting 09/08/23 22:22:46.553
    STEP: deleting a collection 09/08/23 22:22:46.588
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:46.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1207" for this suite. 09/08/23 22:22:46.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:46.671
Sep  8 22:22:46.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename podtemplate 09/08/23 22:22:46.673
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:46.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:46.72
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 09/08/23 22:22:46.725
STEP: Replace a pod template 09/08/23 22:22:46.753
Sep  8 22:22:46.781: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:46.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-515" for this suite. 09/08/23 22:22:46.791
------------------------------
â€¢ [0.145 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:46.671
    Sep  8 22:22:46.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename podtemplate 09/08/23 22:22:46.673
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:46.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:46.72
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 09/08/23 22:22:46.725
    STEP: Replace a pod template 09/08/23 22:22:46.753
    Sep  8 22:22:46.781: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:46.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-515" for this suite. 09/08/23 22:22:46.791
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:46.817
Sep  8 22:22:46.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:22:46.818
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:46.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:46.865
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-654a54c7-6621-427b-a883-04943e1c9d4b 09/08/23 22:22:46.872
STEP: Creating a pod to test consume secrets 09/08/23 22:22:46.888
Sep  8 22:22:46.916: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778" in namespace "projected-1488" to be "Succeeded or Failed"
Sep  8 22:22:46.929: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Pending", Reason="", readiness=false. Elapsed: 12.384857ms
Sep  8 22:22:48.938: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Running", Reason="", readiness=true. Elapsed: 2.02122039s
Sep  8 22:22:50.945: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Running", Reason="", readiness=false. Elapsed: 4.028799769s
Sep  8 22:22:52.940: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023509505s
STEP: Saw pod success 09/08/23 22:22:52.94
Sep  8 22:22:52.940: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778" satisfied condition "Succeeded or Failed"
Sep  8 22:22:52.947: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:22:52.964
Sep  8 22:22:53.000: INFO: Waiting for pod pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778 to disappear
Sep  8 22:22:53.008: INFO: Pod pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  8 22:22:53.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1488" for this suite. 09/08/23 22:22:53.02
------------------------------
â€¢ [SLOW TEST] [6.226 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:46.817
    Sep  8 22:22:46.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:22:46.818
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:46.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:46.865
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-654a54c7-6621-427b-a883-04943e1c9d4b 09/08/23 22:22:46.872
    STEP: Creating a pod to test consume secrets 09/08/23 22:22:46.888
    Sep  8 22:22:46.916: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778" in namespace "projected-1488" to be "Succeeded or Failed"
    Sep  8 22:22:46.929: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Pending", Reason="", readiness=false. Elapsed: 12.384857ms
    Sep  8 22:22:48.938: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Running", Reason="", readiness=true. Elapsed: 2.02122039s
    Sep  8 22:22:50.945: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Running", Reason="", readiness=false. Elapsed: 4.028799769s
    Sep  8 22:22:52.940: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023509505s
    STEP: Saw pod success 09/08/23 22:22:52.94
    Sep  8 22:22:52.940: INFO: Pod "pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778" satisfied condition "Succeeded or Failed"
    Sep  8 22:22:52.947: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:22:52.964
    Sep  8 22:22:53.000: INFO: Waiting for pod pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778 to disappear
    Sep  8 22:22:53.008: INFO: Pod pod-projected-secrets-71b147f2-acb5-4945-a9fe-a83cc3f06778 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:22:53.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1488" for this suite. 09/08/23 22:22:53.02
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:22:53.052
Sep  8 22:22:53.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pod-network-test 09/08/23 22:22:53.054
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:53.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:53.1
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-593 09/08/23 22:22:53.11
STEP: creating a selector 09/08/23 22:22:53.11
STEP: Creating the service pods in kubernetes 09/08/23 22:22:53.111
Sep  8 22:22:53.111: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  8 22:22:53.185: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-593" to be "running and ready"
Sep  8 22:22:53.201: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.195834ms
Sep  8 22:22:53.201: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:22:55.213: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027130843s
Sep  8 22:22:55.213: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:22:57.214: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.028909795s
Sep  8 22:22:57.214: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:22:59.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025743185s
Sep  8 22:22:59.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:01.213: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027289995s
Sep  8 22:23:01.213: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:03.215: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02926891s
Sep  8 22:23:03.215: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:05.210: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024140949s
Sep  8 22:23:05.210: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:07.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.025325421s
Sep  8 22:23:07.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:09.213: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027514108s
Sep  8 22:23:09.213: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:11.215: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.03003295s
Sep  8 22:23:11.215: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:13.224: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.038414286s
Sep  8 22:23:13.224: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:23:15.210: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024135394s
Sep  8 22:23:15.210: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  8 22:23:15.210: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  8 22:23:15.221: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-593" to be "running and ready"
Sep  8 22:23:15.233: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.503038ms
Sep  8 22:23:15.233: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  8 22:23:15.233: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/08/23 22:23:15.243
Sep  8 22:23:15.258: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-593" to be "running"
Sep  8 22:23:15.274: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.06098ms
Sep  8 22:23:17.284: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026578181s
Sep  8 22:23:19.284: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.026241764s
Sep  8 22:23:19.284: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  8 22:23:19.292: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  8 22:23:19.292: INFO: Breadth first check of 10.233.75.126 on host 10.100.19.129...
Sep  8 22:23:19.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.77:9080/dial?request=hostname&protocol=udp&host=10.233.75.126&port=8081&tries=1'] Namespace:pod-network-test-593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:23:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:23:19.304: INFO: ExecWithOptions: Clientset creation
Sep  8 22:23:19.304: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.75.126%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  8 22:23:19.431: INFO: Waiting for responses: map[]
Sep  8 22:23:19.431: INFO: reached 10.233.75.126 after 0/1 tries
Sep  8 22:23:19.431: INFO: Breadth first check of 10.233.103.124 on host 10.100.16.26...
Sep  8 22:23:19.440: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.77:9080/dial?request=hostname&protocol=udp&host=10.233.103.124&port=8081&tries=1'] Namespace:pod-network-test-593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:23:19.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:23:19.441: INFO: ExecWithOptions: Clientset creation
Sep  8 22:23:19.441: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.103.124%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  8 22:23:19.554: INFO: Waiting for responses: map[]
Sep  8 22:23:19.554: INFO: reached 10.233.103.124 after 0/1 tries
Sep  8 22:23:19.554: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  8 22:23:19.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-593" for this suite. 09/08/23 22:23:19.569
------------------------------
â€¢ [SLOW TEST] [26.532 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:22:53.052
    Sep  8 22:22:53.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pod-network-test 09/08/23 22:22:53.054
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:22:53.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:22:53.1
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-593 09/08/23 22:22:53.11
    STEP: creating a selector 09/08/23 22:22:53.11
    STEP: Creating the service pods in kubernetes 09/08/23 22:22:53.111
    Sep  8 22:22:53.111: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  8 22:22:53.185: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-593" to be "running and ready"
    Sep  8 22:22:53.201: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.195834ms
    Sep  8 22:22:53.201: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:22:55.213: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027130843s
    Sep  8 22:22:55.213: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:22:57.214: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.028909795s
    Sep  8 22:22:57.214: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:22:59.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025743185s
    Sep  8 22:22:59.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:01.213: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027289995s
    Sep  8 22:23:01.213: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:03.215: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.02926891s
    Sep  8 22:23:03.215: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:05.210: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.024140949s
    Sep  8 22:23:05.210: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:07.211: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.025325421s
    Sep  8 22:23:07.211: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:09.213: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027514108s
    Sep  8 22:23:09.213: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:11.215: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.03003295s
    Sep  8 22:23:11.215: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:13.224: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.038414286s
    Sep  8 22:23:13.224: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:23:15.210: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.024135394s
    Sep  8 22:23:15.210: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  8 22:23:15.210: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  8 22:23:15.221: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-593" to be "running and ready"
    Sep  8 22:23:15.233: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.503038ms
    Sep  8 22:23:15.233: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  8 22:23:15.233: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/08/23 22:23:15.243
    Sep  8 22:23:15.258: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-593" to be "running"
    Sep  8 22:23:15.274: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.06098ms
    Sep  8 22:23:17.284: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026578181s
    Sep  8 22:23:19.284: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.026241764s
    Sep  8 22:23:19.284: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  8 22:23:19.292: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  8 22:23:19.292: INFO: Breadth first check of 10.233.75.126 on host 10.100.19.129...
    Sep  8 22:23:19.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.77:9080/dial?request=hostname&protocol=udp&host=10.233.75.126&port=8081&tries=1'] Namespace:pod-network-test-593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:23:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:23:19.304: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:23:19.304: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.75.126%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  8 22:23:19.431: INFO: Waiting for responses: map[]
    Sep  8 22:23:19.431: INFO: reached 10.233.75.126 after 0/1 tries
    Sep  8 22:23:19.431: INFO: Breadth first check of 10.233.103.124 on host 10.100.16.26...
    Sep  8 22:23:19.440: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.77:9080/dial?request=hostname&protocol=udp&host=10.233.103.124&port=8081&tries=1'] Namespace:pod-network-test-593 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:23:19.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:23:19.441: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:23:19.441: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-593/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.103.124%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  8 22:23:19.554: INFO: Waiting for responses: map[]
    Sep  8 22:23:19.554: INFO: reached 10.233.103.124 after 0/1 tries
    Sep  8 22:23:19.554: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:23:19.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-593" for this suite. 09/08/23 22:23:19.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:23:19.585
Sep  8 22:23:19.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 22:23:19.586
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:19.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:19.634
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-9287 09/08/23 22:23:19.641
STEP: creating service affinity-clusterip-transition in namespace services-9287 09/08/23 22:23:19.641
STEP: creating replication controller affinity-clusterip-transition in namespace services-9287 09/08/23 22:23:19.674
I0908 22:23:19.725041      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9287, replica count: 3
I0908 22:23:22.777871      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 22:23:22.805: INFO: Creating new exec pod
Sep  8 22:23:22.825: INFO: Waiting up to 5m0s for pod "execpod-affinitysllsq" in namespace "services-9287" to be "running"
Sep  8 22:23:22.838: INFO: Pod "execpod-affinitysllsq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.44519ms
Sep  8 22:23:24.863: INFO: Pod "execpod-affinitysllsq": Phase="Running", Reason="", readiness=true. Elapsed: 2.037517021s
Sep  8 22:23:24.863: INFO: Pod "execpod-affinitysllsq" satisfied condition "running"
Sep  8 22:23:25.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Sep  8 22:23:26.111: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  8 22:23:26.111: INFO: stdout: ""
Sep  8 22:23:26.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c nc -v -z -w 2 10.233.7.143 80'
Sep  8 22:23:26.309: INFO: stderr: "+ nc -v -z -w 2 10.233.7.143 80\nConnection to 10.233.7.143 80 port [tcp/http] succeeded!\n"
Sep  8 22:23:26.309: INFO: stdout: ""
Sep  8 22:23:26.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.7.143:80/ ; done'
Sep  8 22:23:26.725: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n"
Sep  8 22:23:26.726: INFO: stdout: "\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5"
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
Sep  8 22:23:26.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.7.143:80/ ; done'
Sep  8 22:23:27.137: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n"
Sep  8 22:23:27.137: INFO: stdout: "\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb"
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
Sep  8 22:23:27.137: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9287, will wait for the garbage collector to delete the pods 09/08/23 22:23:27.165
Sep  8 22:23:27.244: INFO: Deleting ReplicationController affinity-clusterip-transition took: 18.076121ms
Sep  8 22:23:27.345: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.197155ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 22:23:29.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9287" for this suite. 09/08/23 22:23:29.847
------------------------------
â€¢ [SLOW TEST] [10.304 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:23:19.585
    Sep  8 22:23:19.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 22:23:19.586
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:19.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:19.634
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-9287 09/08/23 22:23:19.641
    STEP: creating service affinity-clusterip-transition in namespace services-9287 09/08/23 22:23:19.641
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9287 09/08/23 22:23:19.674
    I0908 22:23:19.725041      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9287, replica count: 3
    I0908 22:23:22.777871      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 22:23:22.805: INFO: Creating new exec pod
    Sep  8 22:23:22.825: INFO: Waiting up to 5m0s for pod "execpod-affinitysllsq" in namespace "services-9287" to be "running"
    Sep  8 22:23:22.838: INFO: Pod "execpod-affinitysllsq": Phase="Pending", Reason="", readiness=false. Elapsed: 12.44519ms
    Sep  8 22:23:24.863: INFO: Pod "execpod-affinitysllsq": Phase="Running", Reason="", readiness=true. Elapsed: 2.037517021s
    Sep  8 22:23:24.863: INFO: Pod "execpod-affinitysllsq" satisfied condition "running"
    Sep  8 22:23:25.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Sep  8 22:23:26.111: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Sep  8 22:23:26.111: INFO: stdout: ""
    Sep  8 22:23:26.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c nc -v -z -w 2 10.233.7.143 80'
    Sep  8 22:23:26.309: INFO: stderr: "+ nc -v -z -w 2 10.233.7.143 80\nConnection to 10.233.7.143 80 port [tcp/http] succeeded!\n"
    Sep  8 22:23:26.309: INFO: stdout: ""
    Sep  8 22:23:26.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.7.143:80/ ; done'
    Sep  8 22:23:26.725: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n"
    Sep  8 22:23:26.726: INFO: stdout: "\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5\naffinity-clusterip-transition-49995\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-jxvv5"
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-49995
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:26.726: INFO: Received response from host: affinity-clusterip-transition-jxvv5
    Sep  8 22:23:26.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-9287 exec execpod-affinitysllsq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.7.143:80/ ; done'
    Sep  8 22:23:27.137: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.7.143:80/\n"
    Sep  8 22:23:27.137: INFO: stdout: "\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb\naffinity-clusterip-transition-9lrmb"
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Received response from host: affinity-clusterip-transition-9lrmb
    Sep  8 22:23:27.137: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9287, will wait for the garbage collector to delete the pods 09/08/23 22:23:27.165
    Sep  8 22:23:27.244: INFO: Deleting ReplicationController affinity-clusterip-transition took: 18.076121ms
    Sep  8 22:23:27.345: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.197155ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:23:29.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9287" for this suite. 09/08/23 22:23:29.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:23:29.897
Sep  8 22:23:29.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 22:23:29.898
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:29.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:29.983
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 09/08/23 22:23:29.997
STEP: Creating a ResourceQuota 09/08/23 22:23:35.006
STEP: Ensuring resource quota status is calculated 09/08/23 22:23:35.021
STEP: Creating a Service 09/08/23 22:23:37.034
STEP: Creating a NodePort Service 09/08/23 22:23:37.094
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/08/23 22:23:37.189
STEP: Ensuring resource quota status captures service creation 09/08/23 22:23:37.269
STEP: Deleting Services 09/08/23 22:23:39.285
STEP: Ensuring resource quota status released usage 09/08/23 22:23:39.44
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 22:23:41.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1557" for this suite. 09/08/23 22:23:41.461
------------------------------
â€¢ [SLOW TEST] [11.583 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:23:29.897
    Sep  8 22:23:29.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 22:23:29.898
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:29.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:29.983
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 09/08/23 22:23:29.997
    STEP: Creating a ResourceQuota 09/08/23 22:23:35.006
    STEP: Ensuring resource quota status is calculated 09/08/23 22:23:35.021
    STEP: Creating a Service 09/08/23 22:23:37.034
    STEP: Creating a NodePort Service 09/08/23 22:23:37.094
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 09/08/23 22:23:37.189
    STEP: Ensuring resource quota status captures service creation 09/08/23 22:23:37.269
    STEP: Deleting Services 09/08/23 22:23:39.285
    STEP: Ensuring resource quota status released usage 09/08/23 22:23:39.44
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:23:41.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1557" for this suite. 09/08/23 22:23:41.461
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:23:41.481
Sep  8 22:23:41.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 22:23:41.482
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:41.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:41.57
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 09/08/23 22:23:41.576
Sep  8 22:23:41.601: INFO: Waiting up to 5m0s for pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908" in namespace "downward-api-7613" to be "Succeeded or Failed"
Sep  8 22:23:41.614: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908": Phase="Pending", Reason="", readiness=false. Elapsed: 13.563838ms
Sep  8 22:23:43.627: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026562181s
Sep  8 22:23:45.625: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023941119s
STEP: Saw pod success 09/08/23 22:23:45.625
Sep  8 22:23:45.625: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908" satisfied condition "Succeeded or Failed"
Sep  8 22:23:45.640: INFO: Trying to get logs from node node-3 pod downward-api-8607f695-fb09-4ea3-8238-b4271d59f908 container dapi-container: <nil>
STEP: delete the pod 09/08/23 22:23:45.658
Sep  8 22:23:45.685: INFO: Waiting for pod downward-api-8607f695-fb09-4ea3-8238-b4271d59f908 to disappear
Sep  8 22:23:45.694: INFO: Pod downward-api-8607f695-fb09-4ea3-8238-b4271d59f908 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  8 22:23:45.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7613" for this suite. 09/08/23 22:23:45.706
------------------------------
â€¢ [4.242 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:23:41.481
    Sep  8 22:23:41.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 22:23:41.482
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:41.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:41.57
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 09/08/23 22:23:41.576
    Sep  8 22:23:41.601: INFO: Waiting up to 5m0s for pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908" in namespace "downward-api-7613" to be "Succeeded or Failed"
    Sep  8 22:23:41.614: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908": Phase="Pending", Reason="", readiness=false. Elapsed: 13.563838ms
    Sep  8 22:23:43.627: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026562181s
    Sep  8 22:23:45.625: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023941119s
    STEP: Saw pod success 09/08/23 22:23:45.625
    Sep  8 22:23:45.625: INFO: Pod "downward-api-8607f695-fb09-4ea3-8238-b4271d59f908" satisfied condition "Succeeded or Failed"
    Sep  8 22:23:45.640: INFO: Trying to get logs from node node-3 pod downward-api-8607f695-fb09-4ea3-8238-b4271d59f908 container dapi-container: <nil>
    STEP: delete the pod 09/08/23 22:23:45.658
    Sep  8 22:23:45.685: INFO: Waiting for pod downward-api-8607f695-fb09-4ea3-8238-b4271d59f908 to disappear
    Sep  8 22:23:45.694: INFO: Pod downward-api-8607f695-fb09-4ea3-8238-b4271d59f908 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:23:45.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7613" for this suite. 09/08/23 22:23:45.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:23:45.723
Sep  8 22:23:45.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 22:23:45.726
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:45.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:45.77
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:23:45.778
Sep  8 22:23:45.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65" in namespace "downward-api-5908" to be "Succeeded or Failed"
Sep  8 22:23:45.828: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65": Phase="Pending", Reason="", readiness=false. Elapsed: 24.925857ms
Sep  8 22:23:47.838: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035355937s
Sep  8 22:23:49.839: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036150252s
STEP: Saw pod success 09/08/23 22:23:49.839
Sep  8 22:23:49.839: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65" satisfied condition "Succeeded or Failed"
Sep  8 22:23:49.848: INFO: Trying to get logs from node node-3 pod downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65 container client-container: <nil>
STEP: delete the pod 09/08/23 22:23:49.866
Sep  8 22:23:49.922: INFO: Waiting for pod downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65 to disappear
Sep  8 22:23:49.943: INFO: Pod downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 22:23:49.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5908" for this suite. 09/08/23 22:23:49.952
------------------------------
â€¢ [4.241 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:23:45.723
    Sep  8 22:23:45.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 22:23:45.726
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:45.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:45.77
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:23:45.778
    Sep  8 22:23:45.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65" in namespace "downward-api-5908" to be "Succeeded or Failed"
    Sep  8 22:23:45.828: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65": Phase="Pending", Reason="", readiness=false. Elapsed: 24.925857ms
    Sep  8 22:23:47.838: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035355937s
    Sep  8 22:23:49.839: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036150252s
    STEP: Saw pod success 09/08/23 22:23:49.839
    Sep  8 22:23:49.839: INFO: Pod "downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65" satisfied condition "Succeeded or Failed"
    Sep  8 22:23:49.848: INFO: Trying to get logs from node node-3 pod downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65 container client-container: <nil>
    STEP: delete the pod 09/08/23 22:23:49.866
    Sep  8 22:23:49.922: INFO: Waiting for pod downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65 to disappear
    Sep  8 22:23:49.943: INFO: Pod downwardapi-volume-77f70755-9fa8-4a4f-9e52-79eaf4d56a65 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:23:49.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5908" for this suite. 09/08/23 22:23:49.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:23:49.965
Sep  8 22:23:49.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename namespaces 09/08/23 22:23:49.966
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:50.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:50.015
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 09/08/23 22:23:50.018
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:50.055
STEP: Creating a pod in the namespace 09/08/23 22:23:50.06
STEP: Waiting for the pod to have running status 09/08/23 22:23:50.08
Sep  8 22:23:50.080: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8335" to be "running"
Sep  8 22:23:50.094: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.859683ms
Sep  8 22:23:52.106: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02516815s
Sep  8 22:23:52.106: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 09/08/23 22:23:52.106
STEP: Waiting for the namespace to be removed. 09/08/23 22:23:52.138
STEP: Recreating the namespace 09/08/23 22:24:04.146
STEP: Verifying there are no pods in the namespace 09/08/23 22:24:04.192
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:04.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7118" for this suite. 09/08/23 22:24:04.222
STEP: Destroying namespace "nsdeletetest-8335" for this suite. 09/08/23 22:24:04.242
Sep  8 22:24:04.251: INFO: Namespace nsdeletetest-8335 was already deleted
STEP: Destroying namespace "nsdeletetest-648" for this suite. 09/08/23 22:24:04.251
------------------------------
â€¢ [SLOW TEST] [14.307 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:23:49.965
    Sep  8 22:23:49.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename namespaces 09/08/23 22:23:49.966
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:50.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:23:50.015
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 09/08/23 22:23:50.018
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:23:50.055
    STEP: Creating a pod in the namespace 09/08/23 22:23:50.06
    STEP: Waiting for the pod to have running status 09/08/23 22:23:50.08
    Sep  8 22:23:50.080: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8335" to be "running"
    Sep  8 22:23:50.094: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.859683ms
    Sep  8 22:23:52.106: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02516815s
    Sep  8 22:23:52.106: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 09/08/23 22:23:52.106
    STEP: Waiting for the namespace to be removed. 09/08/23 22:23:52.138
    STEP: Recreating the namespace 09/08/23 22:24:04.146
    STEP: Verifying there are no pods in the namespace 09/08/23 22:24:04.192
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:04.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7118" for this suite. 09/08/23 22:24:04.222
    STEP: Destroying namespace "nsdeletetest-8335" for this suite. 09/08/23 22:24:04.242
    Sep  8 22:24:04.251: INFO: Namespace nsdeletetest-8335 was already deleted
    STEP: Destroying namespace "nsdeletetest-648" for this suite. 09/08/23 22:24:04.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:04.288
Sep  8 22:24:04.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 22:24:04.294
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:04.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:04.355
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 09/08/23 22:24:04.361
Sep  8 22:24:04.380: INFO: Waiting up to 5m0s for pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5" in namespace "var-expansion-4607" to be "Succeeded or Failed"
Sep  8 22:24:04.394: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.771676ms
Sep  8 22:24:06.410: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029890583s
Sep  8 22:24:08.405: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.024483662s
Sep  8 22:24:10.404: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023558157s
STEP: Saw pod success 09/08/23 22:24:10.404
Sep  8 22:24:10.405: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5" satisfied condition "Succeeded or Failed"
Sep  8 22:24:10.413: INFO: Trying to get logs from node node-3 pod var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5 container dapi-container: <nil>
STEP: delete the pod 09/08/23 22:24:10.43
Sep  8 22:24:10.461: INFO: Waiting for pod var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5 to disappear
Sep  8 22:24:10.468: INFO: Pod var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:10.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4607" for this suite. 09/08/23 22:24:10.485
------------------------------
â€¢ [SLOW TEST] [6.212 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:04.288
    Sep  8 22:24:04.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 22:24:04.294
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:04.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:04.355
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 09/08/23 22:24:04.361
    Sep  8 22:24:04.380: INFO: Waiting up to 5m0s for pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5" in namespace "var-expansion-4607" to be "Succeeded or Failed"
    Sep  8 22:24:04.394: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.771676ms
    Sep  8 22:24:06.410: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.029890583s
    Sep  8 22:24:08.405: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.024483662s
    Sep  8 22:24:10.404: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023558157s
    STEP: Saw pod success 09/08/23 22:24:10.404
    Sep  8 22:24:10.405: INFO: Pod "var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5" satisfied condition "Succeeded or Failed"
    Sep  8 22:24:10.413: INFO: Trying to get logs from node node-3 pod var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5 container dapi-container: <nil>
    STEP: delete the pod 09/08/23 22:24:10.43
    Sep  8 22:24:10.461: INFO: Waiting for pod var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5 to disappear
    Sep  8 22:24:10.468: INFO: Pod var-expansion-0b9db919-0603-49e4-9822-874e1ed0f6c5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:10.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4607" for this suite. 09/08/23 22:24:10.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:10.503
Sep  8 22:24:10.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 22:24:10.505
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:10.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:10.55
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2365 09/08/23 22:24:10.554
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 09/08/23 22:24:10.568
STEP: Creating pod with conflicting port in namespace statefulset-2365 09/08/23 22:24:10.593
STEP: Waiting until pod test-pod will start running in namespace statefulset-2365 09/08/23 22:24:10.622
Sep  8 22:24:10.623: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2365" to be "running"
Sep  8 22:24:10.647: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.629388ms
Sep  8 22:24:12.656: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033225039s
Sep  8 22:24:14.658: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.034728028s
Sep  8 22:24:14.658: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-2365 09/08/23 22:24:14.658
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2365 09/08/23 22:24:14.673
Sep  8 22:24:14.703: INFO: Observed stateful pod in namespace: statefulset-2365, name: ss-0, uid: e1c0e1d3-288c-40df-b39a-4f874be3ad8e, status phase: Pending. Waiting for statefulset controller to delete.
Sep  8 22:24:14.738: INFO: Observed stateful pod in namespace: statefulset-2365, name: ss-0, uid: e1c0e1d3-288c-40df-b39a-4f874be3ad8e, status phase: Failed. Waiting for statefulset controller to delete.
Sep  8 22:24:14.772: INFO: Observed stateful pod in namespace: statefulset-2365, name: ss-0, uid: e1c0e1d3-288c-40df-b39a-4f874be3ad8e, status phase: Failed. Waiting for statefulset controller to delete.
Sep  8 22:24:14.827: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2365
STEP: Removing pod with conflicting port in namespace statefulset-2365 09/08/23 22:24:14.827
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2365 and will be in running state 09/08/23 22:24:14.898
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 22:24:16.925: INFO: Deleting all statefulset in ns statefulset-2365
Sep  8 22:24:16.934: INFO: Scaling statefulset ss to 0
Sep  8 22:24:26.976: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 22:24:26.985: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:27.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2365" for this suite. 09/08/23 22:24:27.058
------------------------------
â€¢ [SLOW TEST] [16.576 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:10.503
    Sep  8 22:24:10.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 22:24:10.505
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:10.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:10.55
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2365 09/08/23 22:24:10.554
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 09/08/23 22:24:10.568
    STEP: Creating pod with conflicting port in namespace statefulset-2365 09/08/23 22:24:10.593
    STEP: Waiting until pod test-pod will start running in namespace statefulset-2365 09/08/23 22:24:10.622
    Sep  8 22:24:10.623: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2365" to be "running"
    Sep  8 22:24:10.647: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.629388ms
    Sep  8 22:24:12.656: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033225039s
    Sep  8 22:24:14.658: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.034728028s
    Sep  8 22:24:14.658: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-2365 09/08/23 22:24:14.658
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2365 09/08/23 22:24:14.673
    Sep  8 22:24:14.703: INFO: Observed stateful pod in namespace: statefulset-2365, name: ss-0, uid: e1c0e1d3-288c-40df-b39a-4f874be3ad8e, status phase: Pending. Waiting for statefulset controller to delete.
    Sep  8 22:24:14.738: INFO: Observed stateful pod in namespace: statefulset-2365, name: ss-0, uid: e1c0e1d3-288c-40df-b39a-4f874be3ad8e, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  8 22:24:14.772: INFO: Observed stateful pod in namespace: statefulset-2365, name: ss-0, uid: e1c0e1d3-288c-40df-b39a-4f874be3ad8e, status phase: Failed. Waiting for statefulset controller to delete.
    Sep  8 22:24:14.827: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2365
    STEP: Removing pod with conflicting port in namespace statefulset-2365 09/08/23 22:24:14.827
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2365 and will be in running state 09/08/23 22:24:14.898
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 22:24:16.925: INFO: Deleting all statefulset in ns statefulset-2365
    Sep  8 22:24:16.934: INFO: Scaling statefulset ss to 0
    Sep  8 22:24:26.976: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 22:24:26.985: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:27.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2365" for this suite. 09/08/23 22:24:27.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:27.081
Sep  8 22:24:27.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sysctl 09/08/23 22:24:27.082
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:27.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:27.124
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 09/08/23 22:24:27.13
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:27.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2757" for this suite. 09/08/23 22:24:27.153
------------------------------
â€¢ [0.097 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:27.081
    Sep  8 22:24:27.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sysctl 09/08/23 22:24:27.082
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:27.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:27.124
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 09/08/23 22:24:27.13
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:27.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2757" for this suite. 09/08/23 22:24:27.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:27.178
Sep  8 22:24:27.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:24:27.179
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:27.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:27.229
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 09/08/23 22:24:27.237
Sep  8 22:24:27.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 create -f -'
Sep  8 22:24:27.572: INFO: stderr: ""
Sep  8 22:24:27.572: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 22:24:27.572
Sep  8 22:24:27.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 22:24:27.710: INFO: stderr: ""
Sep  8 22:24:27.710: INFO: stdout: "update-demo-nautilus-6rgbc update-demo-nautilus-8pvlr "
Sep  8 22:24:27.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-6rgbc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 22:24:27.827: INFO: stderr: ""
Sep  8 22:24:27.827: INFO: stdout: ""
Sep  8 22:24:27.827: INFO: update-demo-nautilus-6rgbc is created but not running
Sep  8 22:24:32.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  8 22:24:32.940: INFO: stderr: ""
Sep  8 22:24:32.940: INFO: stdout: "update-demo-nautilus-6rgbc update-demo-nautilus-8pvlr "
Sep  8 22:24:32.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-6rgbc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 22:24:33.043: INFO: stderr: ""
Sep  8 22:24:33.043: INFO: stdout: "true"
Sep  8 22:24:33.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-6rgbc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 22:24:33.163: INFO: stderr: ""
Sep  8 22:24:33.163: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 22:24:33.163: INFO: validating pod update-demo-nautilus-6rgbc
Sep  8 22:24:33.179: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 22:24:33.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 22:24:33.180: INFO: update-demo-nautilus-6rgbc is verified up and running
Sep  8 22:24:33.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-8pvlr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  8 22:24:33.275: INFO: stderr: ""
Sep  8 22:24:33.275: INFO: stdout: "true"
Sep  8 22:24:33.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-8pvlr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  8 22:24:33.390: INFO: stderr: ""
Sep  8 22:24:33.390: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Sep  8 22:24:33.390: INFO: validating pod update-demo-nautilus-8pvlr
Sep  8 22:24:33.410: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  8 22:24:33.410: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  8 22:24:33.410: INFO: update-demo-nautilus-8pvlr is verified up and running
STEP: using delete to clean up resources 09/08/23 22:24:33.411
Sep  8 22:24:33.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 delete --grace-period=0 --force -f -'
Sep  8 22:24:33.574: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 22:24:33.575: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  8 22:24:33.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get rc,svc -l name=update-demo --no-headers'
Sep  8 22:24:33.764: INFO: stderr: "No resources found in kubectl-2412 namespace.\n"
Sep  8 22:24:33.764: INFO: stdout: ""
Sep  8 22:24:33.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  8 22:24:33.898: INFO: stderr: ""
Sep  8 22:24:33.898: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2412" for this suite. 09/08/23 22:24:33.916
------------------------------
â€¢ [SLOW TEST] [6.760 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:27.178
    Sep  8 22:24:27.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:24:27.179
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:27.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:27.229
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 09/08/23 22:24:27.237
    Sep  8 22:24:27.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 create -f -'
    Sep  8 22:24:27.572: INFO: stderr: ""
    Sep  8 22:24:27.572: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 09/08/23 22:24:27.572
    Sep  8 22:24:27.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 22:24:27.710: INFO: stderr: ""
    Sep  8 22:24:27.710: INFO: stdout: "update-demo-nautilus-6rgbc update-demo-nautilus-8pvlr "
    Sep  8 22:24:27.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-6rgbc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 22:24:27.827: INFO: stderr: ""
    Sep  8 22:24:27.827: INFO: stdout: ""
    Sep  8 22:24:27.827: INFO: update-demo-nautilus-6rgbc is created but not running
    Sep  8 22:24:32.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Sep  8 22:24:32.940: INFO: stderr: ""
    Sep  8 22:24:32.940: INFO: stdout: "update-demo-nautilus-6rgbc update-demo-nautilus-8pvlr "
    Sep  8 22:24:32.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-6rgbc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 22:24:33.043: INFO: stderr: ""
    Sep  8 22:24:33.043: INFO: stdout: "true"
    Sep  8 22:24:33.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-6rgbc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 22:24:33.163: INFO: stderr: ""
    Sep  8 22:24:33.163: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 22:24:33.163: INFO: validating pod update-demo-nautilus-6rgbc
    Sep  8 22:24:33.179: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 22:24:33.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 22:24:33.180: INFO: update-demo-nautilus-6rgbc is verified up and running
    Sep  8 22:24:33.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-8pvlr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Sep  8 22:24:33.275: INFO: stderr: ""
    Sep  8 22:24:33.275: INFO: stdout: "true"
    Sep  8 22:24:33.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods update-demo-nautilus-8pvlr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Sep  8 22:24:33.390: INFO: stderr: ""
    Sep  8 22:24:33.390: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Sep  8 22:24:33.390: INFO: validating pod update-demo-nautilus-8pvlr
    Sep  8 22:24:33.410: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Sep  8 22:24:33.410: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Sep  8 22:24:33.410: INFO: update-demo-nautilus-8pvlr is verified up and running
    STEP: using delete to clean up resources 09/08/23 22:24:33.411
    Sep  8 22:24:33.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 delete --grace-period=0 --force -f -'
    Sep  8 22:24:33.574: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 22:24:33.575: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Sep  8 22:24:33.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get rc,svc -l name=update-demo --no-headers'
    Sep  8 22:24:33.764: INFO: stderr: "No resources found in kubectl-2412 namespace.\n"
    Sep  8 22:24:33.764: INFO: stdout: ""
    Sep  8 22:24:33.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2412 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Sep  8 22:24:33.898: INFO: stderr: ""
    Sep  8 22:24:33.898: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2412" for this suite. 09/08/23 22:24:33.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:33.939
Sep  8 22:24:33.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 22:24:33.944
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:33.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:33.987
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 09/08/23 22:24:33.993
Sep  8 22:24:34.020: INFO: Waiting up to 5m0s for pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9" in namespace "var-expansion-7884" to be "Succeeded or Failed"
Sep  8 22:24:34.051: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 31.101935ms
Sep  8 22:24:36.061: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040866303s
Sep  8 22:24:38.063: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0431508s
STEP: Saw pod success 09/08/23 22:24:38.073
Sep  8 22:24:38.075: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9" satisfied condition "Succeeded or Failed"
Sep  8 22:24:38.084: INFO: Trying to get logs from node node-3 pod var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9 container dapi-container: <nil>
STEP: delete the pod 09/08/23 22:24:38.146
Sep  8 22:24:38.189: INFO: Waiting for pod var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9 to disappear
Sep  8 22:24:38.198: INFO: Pod var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:38.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7884" for this suite. 09/08/23 22:24:38.217
------------------------------
â€¢ [4.310 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:33.939
    Sep  8 22:24:33.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 22:24:33.944
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:33.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:33.987
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 09/08/23 22:24:33.993
    Sep  8 22:24:34.020: INFO: Waiting up to 5m0s for pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9" in namespace "var-expansion-7884" to be "Succeeded or Failed"
    Sep  8 22:24:34.051: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 31.101935ms
    Sep  8 22:24:36.061: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040866303s
    Sep  8 22:24:38.063: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0431508s
    STEP: Saw pod success 09/08/23 22:24:38.073
    Sep  8 22:24:38.075: INFO: Pod "var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9" satisfied condition "Succeeded or Failed"
    Sep  8 22:24:38.084: INFO: Trying to get logs from node node-3 pod var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9 container dapi-container: <nil>
    STEP: delete the pod 09/08/23 22:24:38.146
    Sep  8 22:24:38.189: INFO: Waiting for pod var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9 to disappear
    Sep  8 22:24:38.198: INFO: Pod var-expansion-b59fde23-afaa-4934-8713-a16cc77b3ff9 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:38.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7884" for this suite. 09/08/23 22:24:38.217
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:38.249
Sep  8 22:24:38.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 22:24:38.25
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:38.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:38.314
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 09/08/23 22:24:38.336
STEP: waiting for Deployment to be created 09/08/23 22:24:38.356
STEP: waiting for all Replicas to be Ready 09/08/23 22:24:38.363
Sep  8 22:24:38.366: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:38.366: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:38.382: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:38.382: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:38.454: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:38.454: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:38.518: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:38.518: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  8 22:24:39.928: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  8 22:24:39.928: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  8 22:24:40.013: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 09/08/23 22:24:40.013
W0908 22:24:40.036964      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  8 22:24:40.039: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 09/08/23 22:24:40.039
Sep  8 22:24:40.043: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.043: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.044: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.046: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
Sep  8 22:24:40.046: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:40.047: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.073: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.073: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.158: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.158: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:40.203: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:40.203: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:40.245: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:40.245: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:41.984: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:41.984: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:42.056: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
STEP: listing Deployments 09/08/23 22:24:42.057
Sep  8 22:24:42.087: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 09/08/23 22:24:42.087
Sep  8 22:24:42.136: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 09/08/23 22:24:42.139
Sep  8 22:24:42.196: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:42.196: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:42.234: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:42.358: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:42.408: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:43.994: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:44.043: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:44.071: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:44.116: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep  8 22:24:46.122: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 09/08/23 22:24:46.203
STEP: fetching the DeploymentStatus 09/08/23 22:24:46.22
Sep  8 22:24:46.236: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 3
STEP: deleting the Deployment 09/08/23 22:24:46.238
Sep  8 22:24:46.265: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.266: INFO: observed event type MODIFIED
Sep  8 22:24:46.267: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 22:24:46.275: INFO: Log out all the ReplicaSets if there is no deployment created
Sep  8 22:24:46.286: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-366  f31204c6-0507-415d-9c27-fa44ec67ee01 44299 2 2023-09-08 22:24:42 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment b2e22c10-3406-43af-8f7e-104aa8741251 0xc00401dfc7 0xc00401dfc8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e22c10-3406-43af-8f7e-104aa8741251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003c0960 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep  8 22:24:46.297: INFO: pod: "test-deployment-7b7876f9d6-97pxc":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-97pxc test-deployment-7b7876f9d6- deployment-366  edcaafbc-a7d5-4831-b16d-cc1922b3bace 44298 0 2023-09-08 22:24:44 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:5d582502c225238081291b77a4b2ca4cbbca8892ac09e9db6efe7014e4be59d8 cni.projectcalico.org/podIP:10.233.103.109/32 cni.projectcalico.org/podIPs:10.233.103.109/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f31204c6-0507-415d-9c27-fa44ec67ee01 0xc007c5c3f7 0xc007c5c3f8}] [] [{calico Update v1 2023-09-08 22:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31204c6-0507-415d-9c27-fa44ec67ee01\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk82b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk82b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.109,StartTime:2023-09-08 22:24:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://06b83c81ec35c704ec9bfd368bb949078a61f387a06459df07a70a6710108595,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  8 22:24:46.297: INFO: pod: "test-deployment-7b7876f9d6-xpxp8":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-xpxp8 test-deployment-7b7876f9d6- deployment-366  b5256e47-d3d0-482c-9f1c-6f66abee1b47 44255 0 2023-09-08 22:24:42 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:354ef09f223421e38e37a86185c62643b5f3b659602995ed0e149b5b442750a7 cni.projectcalico.org/podIP:10.233.75.90/32 cni.projectcalico.org/podIPs:10.233.75.90/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f31204c6-0507-415d-9c27-fa44ec67ee01 0xc007c5c627 0xc007c5c628}] [] [{kube-controller-manager Update v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31204c6-0507-415d-9c27-fa44ec67ee01\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:24:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmscb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmscb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.90,StartTime:2023-09-08 22:24:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7e542d1863728f46fa02142115d81e9ea3d18f4a2743cacf8412e008a8aa0572,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  8 22:24:46.298: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-366  6b51d9d7-1f89-4022-8d3e-aae2faf64d60 44307 4 2023-09-08 22:24:40 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment b2e22c10-3406-43af-8f7e-104aa8741251 0xc0003c17e7 0xc0003c17e8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e22c10-3406-43af-8f7e-104aa8741251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007c5c040 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep  8 22:24:46.308: INFO: pod: "test-deployment-7df74c55ff-ffwmk":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-ffwmk test-deployment-7df74c55ff- deployment-366  5631fa81-c749-44bc-943d-7710308120be 44303 0 2023-09-08 22:24:40 +0000 UTC 2023-09-08 22:24:47 +0000 UTC 0xc007c5da28 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:72c35e913474ac6f762a898c340548c9516ea8cd914270aa7866b7c3c20d1d08 cni.projectcalico.org/podIP:10.233.75.102/32 cni.projectcalico.org/podIPs:10.233.75.102/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 6b51d9d7-1f89-4022-8d3e-aae2faf64d60 0xc007c5db77 0xc007c5db78}] [] [{calico Update v1 2023-09-08 22:24:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:24:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b51d9d7-1f89-4022-8d3e-aae2faf64d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:24:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8zx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8zx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.102,StartTime:2023-09-08 22:24:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://61a292426e84bbf0748e99043d64ac8902a25e99ac250b63c361325b436f131a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  8 22:24:46.308: INFO: pod: "test-deployment-7df74c55ff-qsrbg":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-qsrbg test-deployment-7df74c55ff- deployment-366  96df6dc1-1060-4e5a-8250-f37a6bfd4b29 44268 0 2023-09-08 22:24:42 +0000 UTC 2023-09-08 22:24:44 +0000 UTC 0xc007c5df00 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:e82c690dd0caf47221722f48d3e11dfd4db85be2f0af20793f9e75e3b94a0df8 cni.projectcalico.org/podIP:10.233.103.82/32 cni.projectcalico.org/podIPs:10.233.103.82/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 6b51d9d7-1f89-4022-8d3e-aae2faf64d60 0xc007c5df57 0xc007c5df58}] [] [{calico Update v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b51d9d7-1f89-4022-8d3e-aae2faf64d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:24:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhjjz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhjjz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.82,StartTime:2023-09-08 22:24:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://22abf169fa75298d2ef3bf62e241b220f55c904ad33b5c4945af240899b93c44,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  8 22:24:46.308: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-366  2ee258bc-cd12-4139-81a1-46598403fa1d 44186 3 2023-09-08 22:24:38 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment b2e22c10-3406-43af-8f7e-104aa8741251 0xc007c5c0a7 0xc007c5c0a8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e22c10-3406-43af-8f7e-104aa8741251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007c5c130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:46.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-366" for this suite. 09/08/23 22:24:46.349
------------------------------
â€¢ [SLOW TEST] [8.122 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:38.249
    Sep  8 22:24:38.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 22:24:38.25
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:38.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:38.314
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 09/08/23 22:24:38.336
    STEP: waiting for Deployment to be created 09/08/23 22:24:38.356
    STEP: waiting for all Replicas to be Ready 09/08/23 22:24:38.363
    Sep  8 22:24:38.366: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:38.366: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:38.382: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:38.382: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:38.454: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:38.454: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:38.518: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:38.518: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Sep  8 22:24:39.928: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  8 22:24:39.928: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Sep  8 22:24:40.013: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 09/08/23 22:24:40.013
    W0908 22:24:40.036964      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  8 22:24:40.039: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 09/08/23 22:24:40.039
    Sep  8 22:24:40.043: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.043: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.044: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.045: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.046: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 0
    Sep  8 22:24:40.046: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:40.047: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.048: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.073: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.073: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.158: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.158: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:40.203: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:40.203: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:40.245: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:40.245: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:41.984: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:41.984: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:42.056: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    STEP: listing Deployments 09/08/23 22:24:42.057
    Sep  8 22:24:42.087: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 09/08/23 22:24:42.087
    Sep  8 22:24:42.136: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 09/08/23 22:24:42.139
    Sep  8 22:24:42.196: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:42.196: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:42.234: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:42.358: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:42.408: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:43.994: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:44.043: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:44.071: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:44.116: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Sep  8 22:24:46.122: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 09/08/23 22:24:46.203
    STEP: fetching the DeploymentStatus 09/08/23 22:24:46.22
    Sep  8 22:24:46.236: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:46.237: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 1
    Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 2
    Sep  8 22:24:46.238: INFO: observed Deployment test-deployment in namespace deployment-366 with ReadyReplicas 3
    STEP: deleting the Deployment 09/08/23 22:24:46.238
    Sep  8 22:24:46.265: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.266: INFO: observed event type MODIFIED
    Sep  8 22:24:46.267: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 22:24:46.275: INFO: Log out all the ReplicaSets if there is no deployment created
    Sep  8 22:24:46.286: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-366  f31204c6-0507-415d-9c27-fa44ec67ee01 44299 2 2023-09-08 22:24:42 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment b2e22c10-3406-43af-8f7e-104aa8741251 0xc00401dfc7 0xc00401dfc8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e22c10-3406-43af-8f7e-104aa8741251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003c0960 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Sep  8 22:24:46.297: INFO: pod: "test-deployment-7b7876f9d6-97pxc":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-97pxc test-deployment-7b7876f9d6- deployment-366  edcaafbc-a7d5-4831-b16d-cc1922b3bace 44298 0 2023-09-08 22:24:44 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:5d582502c225238081291b77a4b2ca4cbbca8892ac09e9db6efe7014e4be59d8 cni.projectcalico.org/podIP:10.233.103.109/32 cni.projectcalico.org/podIPs:10.233.103.109/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f31204c6-0507-415d-9c27-fa44ec67ee01 0xc007c5c3f7 0xc007c5c3f8}] [] [{calico Update v1 2023-09-08 22:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:24:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31204c6-0507-415d-9c27-fa44ec67ee01\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mk82b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mk82b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.109,StartTime:2023-09-08 22:24:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://06b83c81ec35c704ec9bfd368bb949078a61f387a06459df07a70a6710108595,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  8 22:24:46.297: INFO: pod: "test-deployment-7b7876f9d6-xpxp8":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-xpxp8 test-deployment-7b7876f9d6- deployment-366  b5256e47-d3d0-482c-9f1c-6f66abee1b47 44255 0 2023-09-08 22:24:42 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:354ef09f223421e38e37a86185c62643b5f3b659602995ed0e149b5b442750a7 cni.projectcalico.org/podIP:10.233.75.90/32 cni.projectcalico.org/podIPs:10.233.75.90/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 f31204c6-0507-415d-9c27-fa44ec67ee01 0xc007c5c627 0xc007c5c628}] [] [{kube-controller-manager Update v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f31204c6-0507-415d-9c27-fa44ec67ee01\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:24:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:24:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmscb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmscb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.90,StartTime:2023-09-08 22:24:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7e542d1863728f46fa02142115d81e9ea3d18f4a2743cacf8412e008a8aa0572,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  8 22:24:46.298: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-366  6b51d9d7-1f89-4022-8d3e-aae2faf64d60 44307 4 2023-09-08 22:24:40 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment b2e22c10-3406-43af-8f7e-104aa8741251 0xc0003c17e7 0xc0003c17e8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e22c10-3406-43af-8f7e-104aa8741251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:24:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007c5c040 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Sep  8 22:24:46.308: INFO: pod: "test-deployment-7df74c55ff-ffwmk":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-ffwmk test-deployment-7df74c55ff- deployment-366  5631fa81-c749-44bc-943d-7710308120be 44303 0 2023-09-08 22:24:40 +0000 UTC 2023-09-08 22:24:47 +0000 UTC 0xc007c5da28 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:72c35e913474ac6f762a898c340548c9516ea8cd914270aa7866b7c3c20d1d08 cni.projectcalico.org/podIP:10.233.75.102/32 cni.projectcalico.org/podIPs:10.233.75.102/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 6b51d9d7-1f89-4022-8d3e-aae2faf64d60 0xc007c5db77 0xc007c5db78}] [] [{calico Update v1 2023-09-08 22:24:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:24:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b51d9d7-1f89-4022-8d3e-aae2faf64d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:24:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8zx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8zx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.102,StartTime:2023-09-08 22:24:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://61a292426e84bbf0748e99043d64ac8902a25e99ac250b63c361325b436f131a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  8 22:24:46.308: INFO: pod: "test-deployment-7df74c55ff-qsrbg":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-qsrbg test-deployment-7df74c55ff- deployment-366  96df6dc1-1060-4e5a-8250-f37a6bfd4b29 44268 0 2023-09-08 22:24:42 +0000 UTC 2023-09-08 22:24:44 +0000 UTC 0xc007c5df00 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:e82c690dd0caf47221722f48d3e11dfd4db85be2f0af20793f9e75e3b94a0df8 cni.projectcalico.org/podIP:10.233.103.82/32 cni.projectcalico.org/podIPs:10.233.103.82/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 6b51d9d7-1f89-4022-8d3e-aae2faf64d60 0xc007c5df57 0xc007c5df58}] [] [{calico Update v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b51d9d7-1f89-4022-8d3e-aae2faf64d60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:24:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhjjz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhjjz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:24:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.82,StartTime:2023-09-08 22:24:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:24:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://22abf169fa75298d2ef3bf62e241b220f55c904ad33b5c4945af240899b93c44,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Sep  8 22:24:46.308: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-366  2ee258bc-cd12-4139-81a1-46598403fa1d 44186 3 2023-09-08 22:24:38 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment b2e22c10-3406-43af-8f7e-104aa8741251 0xc007c5c0a7 0xc007c5c0a8}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:24:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2e22c10-3406-43af-8f7e-104aa8741251\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:24:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007c5c130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:46.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-366" for this suite. 09/08/23 22:24:46.349
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:46.372
Sep  8 22:24:46.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 22:24:46.373
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:46.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:46.442
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-ed2bfc05-c73e-4f78-b9ee-4f5690b880d1 09/08/23 22:24:46.457
STEP: Creating a pod to test consume secrets 09/08/23 22:24:46.474
Sep  8 22:24:46.529: INFO: Waiting up to 5m0s for pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59" in namespace "secrets-446" to be "Succeeded or Failed"
Sep  8 22:24:46.546: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Pending", Reason="", readiness=false. Elapsed: 16.816239ms
Sep  8 22:24:48.559: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Running", Reason="", readiness=true. Elapsed: 2.029897048s
Sep  8 22:24:50.556: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Running", Reason="", readiness=false. Elapsed: 4.02667783s
Sep  8 22:24:52.560: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030676856s
STEP: Saw pod success 09/08/23 22:24:52.56
Sep  8 22:24:52.560: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59" satisfied condition "Succeeded or Failed"
Sep  8 22:24:52.574: INFO: Trying to get logs from node node-3 pod pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59 container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:24:52.639
Sep  8 22:24:52.736: INFO: Waiting for pod pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59 to disappear
Sep  8 22:24:52.742: INFO: Pod pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:52.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-446" for this suite. 09/08/23 22:24:52.752
------------------------------
â€¢ [SLOW TEST] [6.403 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:46.372
    Sep  8 22:24:46.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 22:24:46.373
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:46.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:46.442
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-ed2bfc05-c73e-4f78-b9ee-4f5690b880d1 09/08/23 22:24:46.457
    STEP: Creating a pod to test consume secrets 09/08/23 22:24:46.474
    Sep  8 22:24:46.529: INFO: Waiting up to 5m0s for pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59" in namespace "secrets-446" to be "Succeeded or Failed"
    Sep  8 22:24:46.546: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Pending", Reason="", readiness=false. Elapsed: 16.816239ms
    Sep  8 22:24:48.559: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Running", Reason="", readiness=true. Elapsed: 2.029897048s
    Sep  8 22:24:50.556: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Running", Reason="", readiness=false. Elapsed: 4.02667783s
    Sep  8 22:24:52.560: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030676856s
    STEP: Saw pod success 09/08/23 22:24:52.56
    Sep  8 22:24:52.560: INFO: Pod "pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59" satisfied condition "Succeeded or Failed"
    Sep  8 22:24:52.574: INFO: Trying to get logs from node node-3 pod pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59 container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:24:52.639
    Sep  8 22:24:52.736: INFO: Waiting for pod pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59 to disappear
    Sep  8 22:24:52.742: INFO: Pod pod-secrets-95e8de46-5efd-4bab-96ec-fac147f6ea59 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:52.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-446" for this suite. 09/08/23 22:24:52.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:52.78
Sep  8 22:24:52.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 22:24:52.781
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:52.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:52.83
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/08/23 22:24:52.839
Sep  8 22:24:52.861: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8278  e681bd8f-c120-4904-b27e-b6980f43687d 44451 0 2023-09-08 22:24:52 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-08 22:24:52 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7tqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7tqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:24:52.861: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8278" to be "running and ready"
Sep  8 22:24:52.877: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 15.75074ms
Sep  8 22:24:52.877: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:24:54.888: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027218837s
Sep  8 22:24:54.888: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:24:56.887: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.025991507s
Sep  8 22:24:56.887: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Sep  8 22:24:56.887: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 09/08/23 22:24:56.887
Sep  8 22:24:56.887: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8278 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:24:56.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:24:56.889: INFO: ExecWithOptions: Clientset creation
Sep  8 22:24:56.889: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-8278/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 09/08/23 22:24:57.028
Sep  8 22:24:57.028: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8278 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:24:57.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:24:57.029: INFO: ExecWithOptions: Clientset creation
Sep  8 22:24:57.029: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-8278/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Sep  8 22:24:57.183: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:57.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8278" for this suite. 09/08/23 22:24:57.236
------------------------------
â€¢ [4.474 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:52.78
    Sep  8 22:24:52.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 22:24:52.781
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:52.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:52.83
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 09/08/23 22:24:52.839
    Sep  8 22:24:52.861: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8278  e681bd8f-c120-4904-b27e-b6980f43687d 44451 0 2023-09-08 22:24:52 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-09-08 22:24:52 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7tqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7tqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:24:52.861: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8278" to be "running and ready"
    Sep  8 22:24:52.877: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 15.75074ms
    Sep  8 22:24:52.877: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:24:54.888: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027218837s
    Sep  8 22:24:54.888: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:24:56.887: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.025991507s
    Sep  8 22:24:56.887: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Sep  8 22:24:56.887: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 09/08/23 22:24:56.887
    Sep  8 22:24:56.887: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8278 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:24:56.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:24:56.889: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:24:56.889: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-8278/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 09/08/23 22:24:57.028
    Sep  8 22:24:57.028: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8278 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:24:57.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:24:57.029: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:24:57.029: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-8278/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Sep  8 22:24:57.183: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:57.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8278" for this suite. 09/08/23 22:24:57.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:57.255
Sep  8 22:24:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename namespaces 09/08/23 22:24:57.256
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:57.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:57.313
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-6986" 09/08/23 22:24:57.319
Sep  8 22:24:57.338: INFO: Namespace "namespaces-6986" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3af7af1c-08fb-48ad-95c3-714ce808d119", "kubernetes.io/metadata.name":"namespaces-6986", "namespaces-6986":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:24:57.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6986" for this suite. 09/08/23 22:24:57.35
------------------------------
â€¢ [0.117 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:57.255
    Sep  8 22:24:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename namespaces 09/08/23 22:24:57.256
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:57.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:57.313
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-6986" 09/08/23 22:24:57.319
    Sep  8 22:24:57.338: INFO: Namespace "namespaces-6986" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3af7af1c-08fb-48ad-95c3-714ce808d119", "kubernetes.io/metadata.name":"namespaces-6986", "namespaces-6986":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:24:57.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6986" for this suite. 09/08/23 22:24:57.35
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:24:57.372
Sep  8 22:24:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 22:24:57.375
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:57.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:57.417
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 22:24:57.455
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:24:57.922
STEP: Deploying the webhook pod 09/08/23 22:24:57.943
STEP: Wait for the deployment to be ready 09/08/23 22:24:57.979
Sep  8 22:24:58.005: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 22:25:00.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 22:25:02.064
STEP: Verifying the service has paired with the endpoint 09/08/23 22:25:02.132
Sep  8 22:25:03.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/08/23 22:25:03.143
STEP: create a pod that should be updated by the webhook 09/08/23 22:25:03.18
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:25:03.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6155" for this suite. 09/08/23 22:25:03.439
STEP: Destroying namespace "webhook-6155-markers" for this suite. 09/08/23 22:25:03.489
------------------------------
â€¢ [SLOW TEST] [6.203 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:24:57.372
    Sep  8 22:24:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 22:24:57.375
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:24:57.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:24:57.417
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 22:24:57.455
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:24:57.922
    STEP: Deploying the webhook pod 09/08/23 22:24:57.943
    STEP: Wait for the deployment to be ready 09/08/23 22:24:57.979
    Sep  8 22:24:58.005: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 22:25:00.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 24, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 22:25:02.064
    STEP: Verifying the service has paired with the endpoint 09/08/23 22:25:02.132
    Sep  8 22:25:03.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 09/08/23 22:25:03.143
    STEP: create a pod that should be updated by the webhook 09/08/23 22:25:03.18
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:25:03.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6155" for this suite. 09/08/23 22:25:03.439
    STEP: Destroying namespace "webhook-6155-markers" for this suite. 09/08/23 22:25:03.489
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:25:03.585
Sep  8 22:25:03.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 22:25:03.586
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:03.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:03.669
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 09/08/23 22:25:03.698
Sep  8 22:25:03.746: INFO: Waiting up to 5m0s for pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f" in namespace "var-expansion-5794" to be "Succeeded or Failed"
Sep  8 22:25:03.789: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Pending", Reason="", readiness=false. Elapsed: 43.056159ms
Sep  8 22:25:05.806: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059837677s
Sep  8 22:25:07.800: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054564597s
Sep  8 22:25:09.821: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075134524s
STEP: Saw pod success 09/08/23 22:25:09.821
Sep  8 22:25:09.821: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f" satisfied condition "Succeeded or Failed"
Sep  8 22:25:09.838: INFO: Trying to get logs from node node-3 pod var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f container dapi-container: <nil>
STEP: delete the pod 09/08/23 22:25:09.853
Sep  8 22:25:09.924: INFO: Waiting for pod var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f to disappear
Sep  8 22:25:09.968: INFO: Pod var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 22:25:09.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5794" for this suite. 09/08/23 22:25:09.991
------------------------------
â€¢ [SLOW TEST] [6.442 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:25:03.585
    Sep  8 22:25:03.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 22:25:03.586
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:03.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:03.669
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 09/08/23 22:25:03.698
    Sep  8 22:25:03.746: INFO: Waiting up to 5m0s for pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f" in namespace "var-expansion-5794" to be "Succeeded or Failed"
    Sep  8 22:25:03.789: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Pending", Reason="", readiness=false. Elapsed: 43.056159ms
    Sep  8 22:25:05.806: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059837677s
    Sep  8 22:25:07.800: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054564597s
    Sep  8 22:25:09.821: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075134524s
    STEP: Saw pod success 09/08/23 22:25:09.821
    Sep  8 22:25:09.821: INFO: Pod "var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f" satisfied condition "Succeeded or Failed"
    Sep  8 22:25:09.838: INFO: Trying to get logs from node node-3 pod var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f container dapi-container: <nil>
    STEP: delete the pod 09/08/23 22:25:09.853
    Sep  8 22:25:09.924: INFO: Waiting for pod var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f to disappear
    Sep  8 22:25:09.968: INFO: Pod var-expansion-04c7f404-3e3c-469c-acb8-dc5470fa133f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:25:09.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5794" for this suite. 09/08/23 22:25:09.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:25:10.018
Sep  8 22:25:10.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:25:10.02
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:10.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:10.122
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 09/08/23 22:25:10.132
Sep  8 22:25:10.133: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  8 22:25:10.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
Sep  8 22:25:10.520: INFO: stderr: ""
Sep  8 22:25:10.520: INFO: stdout: "service/agnhost-replica created\n"
Sep  8 22:25:10.520: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  8 22:25:10.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
Sep  8 22:25:12.289: INFO: stderr: ""
Sep  8 22:25:12.289: INFO: stdout: "service/agnhost-primary created\n"
Sep  8 22:25:12.289: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  8 22:25:12.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
Sep  8 22:25:12.793: INFO: stderr: ""
Sep  8 22:25:12.793: INFO: stdout: "service/frontend created\n"
Sep  8 22:25:12.793: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  8 22:25:12.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
Sep  8 22:25:13.160: INFO: stderr: ""
Sep  8 22:25:13.160: INFO: stdout: "deployment.apps/frontend created\n"
Sep  8 22:25:13.160: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  8 22:25:13.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
Sep  8 22:25:13.547: INFO: stderr: ""
Sep  8 22:25:13.547: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  8 22:25:13.547: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  8 22:25:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
Sep  8 22:25:14.034: INFO: stderr: ""
Sep  8 22:25:14.034: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 09/08/23 22:25:14.034
Sep  8 22:25:14.035: INFO: Waiting for all frontend pods to be Running.
Sep  8 22:25:19.086: INFO: Waiting for frontend to serve content.
Sep  8 22:25:19.110: INFO: Trying to add a new entry to the guestbook.
Sep  8 22:25:19.143: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 09/08/23 22:25:19.164
Sep  8 22:25:19.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
Sep  8 22:25:19.371: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 22:25:19.371: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 09/08/23 22:25:19.372
Sep  8 22:25:19.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
Sep  8 22:25:19.571: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 22:25:19.572: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/08/23 22:25:19.572
Sep  8 22:25:19.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
Sep  8 22:25:19.891: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 22:25:19.892: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/08/23 22:25:19.892
Sep  8 22:25:19.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
Sep  8 22:25:20.122: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 22:25:20.122: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 09/08/23 22:25:20.122
Sep  8 22:25:20.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
Sep  8 22:25:20.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 22:25:20.340: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 09/08/23 22:25:20.34
Sep  8 22:25:20.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
Sep  8 22:25:20.493: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  8 22:25:20.493: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:25:20.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1862" for this suite. 09/08/23 22:25:20.511
------------------------------
â€¢ [SLOW TEST] [10.523 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:25:10.018
    Sep  8 22:25:10.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:25:10.02
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:10.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:10.122
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 09/08/23 22:25:10.132
    Sep  8 22:25:10.133: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Sep  8 22:25:10.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
    Sep  8 22:25:10.520: INFO: stderr: ""
    Sep  8 22:25:10.520: INFO: stdout: "service/agnhost-replica created\n"
    Sep  8 22:25:10.520: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Sep  8 22:25:10.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
    Sep  8 22:25:12.289: INFO: stderr: ""
    Sep  8 22:25:12.289: INFO: stdout: "service/agnhost-primary created\n"
    Sep  8 22:25:12.289: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Sep  8 22:25:12.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
    Sep  8 22:25:12.793: INFO: stderr: ""
    Sep  8 22:25:12.793: INFO: stdout: "service/frontend created\n"
    Sep  8 22:25:12.793: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Sep  8 22:25:12.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
    Sep  8 22:25:13.160: INFO: stderr: ""
    Sep  8 22:25:13.160: INFO: stdout: "deployment.apps/frontend created\n"
    Sep  8 22:25:13.160: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  8 22:25:13.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
    Sep  8 22:25:13.547: INFO: stderr: ""
    Sep  8 22:25:13.547: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Sep  8 22:25:13.547: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Sep  8 22:25:13.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 create -f -'
    Sep  8 22:25:14.034: INFO: stderr: ""
    Sep  8 22:25:14.034: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 09/08/23 22:25:14.034
    Sep  8 22:25:14.035: INFO: Waiting for all frontend pods to be Running.
    Sep  8 22:25:19.086: INFO: Waiting for frontend to serve content.
    Sep  8 22:25:19.110: INFO: Trying to add a new entry to the guestbook.
    Sep  8 22:25:19.143: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 09/08/23 22:25:19.164
    Sep  8 22:25:19.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
    Sep  8 22:25:19.371: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 22:25:19.371: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 09/08/23 22:25:19.372
    Sep  8 22:25:19.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
    Sep  8 22:25:19.571: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 22:25:19.572: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/08/23 22:25:19.572
    Sep  8 22:25:19.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
    Sep  8 22:25:19.891: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 22:25:19.892: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/08/23 22:25:19.892
    Sep  8 22:25:19.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
    Sep  8 22:25:20.122: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 22:25:20.122: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 09/08/23 22:25:20.122
    Sep  8 22:25:20.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
    Sep  8 22:25:20.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 22:25:20.340: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 09/08/23 22:25:20.34
    Sep  8 22:25:20.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-1862 delete --grace-period=0 --force -f -'
    Sep  8 22:25:20.493: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Sep  8 22:25:20.493: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:25:20.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1862" for this suite. 09/08/23 22:25:20.511
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:25:20.541
Sep  8 22:25:20.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename watch 09/08/23 22:25:20.548
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:20.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:20.66
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 09/08/23 22:25:20.677
STEP: creating a watch on configmaps with label B 09/08/23 22:25:20.692
STEP: creating a watch on configmaps with label A or B 09/08/23 22:25:20.695
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/08/23 22:25:20.698
Sep  8 22:25:20.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44904 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:25:20.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44904 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/08/23 22:25:20.715
Sep  8 22:25:20.749: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44905 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:25:20.750: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44905 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/08/23 22:25:20.75
Sep  8 22:25:20.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44906 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:25:20.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44906 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/08/23 22:25:20.795
Sep  8 22:25:20.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44908 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:25:20.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44908 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/08/23 22:25:20.825
Sep  8 22:25:20.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 44909 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:25:20.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 44909 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/08/23 22:25:30.839
Sep  8 22:25:30.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 45037 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:25:30.864: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 45037 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  8 22:25:40.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-259" for this suite. 09/08/23 22:25:40.882
------------------------------
â€¢ [SLOW TEST] [20.373 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:25:20.541
    Sep  8 22:25:20.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename watch 09/08/23 22:25:20.548
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:20.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:20.66
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 09/08/23 22:25:20.677
    STEP: creating a watch on configmaps with label B 09/08/23 22:25:20.692
    STEP: creating a watch on configmaps with label A or B 09/08/23 22:25:20.695
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 09/08/23 22:25:20.698
    Sep  8 22:25:20.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44904 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:25:20.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44904 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 09/08/23 22:25:20.715
    Sep  8 22:25:20.749: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44905 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:25:20.750: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44905 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 09/08/23 22:25:20.75
    Sep  8 22:25:20.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44906 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:25:20.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44906 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 09/08/23 22:25:20.795
    Sep  8 22:25:20.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44908 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:25:20.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-259  4586f9bc-9935-4b2b-a72a-270c35e8acfe 44908 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 09/08/23 22:25:20.825
    Sep  8 22:25:20.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 44909 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:25:20.838: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 44909 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 09/08/23 22:25:30.839
    Sep  8 22:25:30.863: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 45037 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:25:30.864: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-259  b4f846ce-9845-4074-bf6e-311f09abd0d1 45037 0 2023-09-08 22:25:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-09-08 22:25:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:25:40.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-259" for this suite. 09/08/23 22:25:40.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:25:40.915
Sep  8 22:25:40.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename namespaces 09/08/23 22:25:40.917
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:40.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:40.978
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-jfxcs" 09/08/23 22:25:40.983
Sep  8 22:25:41.025: INFO: Namespace "e2e-ns-jfxcs-7122" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-jfxcs-7122" 09/08/23 22:25:41.025
Sep  8 22:25:41.051: INFO: Namespace "e2e-ns-jfxcs-7122" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-jfxcs-7122" 09/08/23 22:25:41.051
Sep  8 22:25:41.076: INFO: Namespace "e2e-ns-jfxcs-7122" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:25:41.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9451" for this suite. 09/08/23 22:25:41.094
STEP: Destroying namespace "e2e-ns-jfxcs-7122" for this suite. 09/08/23 22:25:41.108
------------------------------
â€¢ [0.209 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:25:40.915
    Sep  8 22:25:40.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename namespaces 09/08/23 22:25:40.917
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:40.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:40.978
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-jfxcs" 09/08/23 22:25:40.983
    Sep  8 22:25:41.025: INFO: Namespace "e2e-ns-jfxcs-7122" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-jfxcs-7122" 09/08/23 22:25:41.025
    Sep  8 22:25:41.051: INFO: Namespace "e2e-ns-jfxcs-7122" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-jfxcs-7122" 09/08/23 22:25:41.051
    Sep  8 22:25:41.076: INFO: Namespace "e2e-ns-jfxcs-7122" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:25:41.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9451" for this suite. 09/08/23 22:25:41.094
    STEP: Destroying namespace "e2e-ns-jfxcs-7122" for this suite. 09/08/23 22:25:41.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:25:41.126
Sep  8 22:25:41.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:25:41.128
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:41.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:41.184
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-e7b566fb-7ac3-4e09-b5f2-e2fa060b9bf5 09/08/23 22:25:41.195
STEP: Creating a pod to test consume secrets 09/08/23 22:25:41.212
Sep  8 22:25:41.247: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3" in namespace "projected-6229" to be "Succeeded or Failed"
Sep  8 22:25:41.262: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.340134ms
Sep  8 22:25:43.272: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024811603s
Sep  8 22:25:45.293: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045941548s
Sep  8 22:25:47.275: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028175898s
STEP: Saw pod success 09/08/23 22:25:47.275
Sep  8 22:25:47.275: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3" satisfied condition "Succeeded or Failed"
Sep  8 22:25:47.296: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:25:47.333
Sep  8 22:25:47.370: INFO: Waiting for pod pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3 to disappear
Sep  8 22:25:47.377: INFO: Pod pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  8 22:25:47.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6229" for this suite. 09/08/23 22:25:47.39
------------------------------
â€¢ [SLOW TEST] [6.281 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:25:41.126
    Sep  8 22:25:41.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:25:41.128
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:41.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:41.184
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-e7b566fb-7ac3-4e09-b5f2-e2fa060b9bf5 09/08/23 22:25:41.195
    STEP: Creating a pod to test consume secrets 09/08/23 22:25:41.212
    Sep  8 22:25:41.247: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3" in namespace "projected-6229" to be "Succeeded or Failed"
    Sep  8 22:25:41.262: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.340134ms
    Sep  8 22:25:43.272: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024811603s
    Sep  8 22:25:45.293: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045941548s
    Sep  8 22:25:47.275: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028175898s
    STEP: Saw pod success 09/08/23 22:25:47.275
    Sep  8 22:25:47.275: INFO: Pod "pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3" satisfied condition "Succeeded or Failed"
    Sep  8 22:25:47.296: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:25:47.333
    Sep  8 22:25:47.370: INFO: Waiting for pod pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3 to disappear
    Sep  8 22:25:47.377: INFO: Pod pod-projected-secrets-96f48ced-b576-4901-a1a4-56b0212503b3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:25:47.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6229" for this suite. 09/08/23 22:25:47.39
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:25:47.409
Sep  8 22:25:47.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pod-network-test 09/08/23 22:25:47.413
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:47.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:47.469
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9026 09/08/23 22:25:47.489
STEP: creating a selector 09/08/23 22:25:47.489
STEP: Creating the service pods in kubernetes 09/08/23 22:25:47.489
Sep  8 22:25:47.489: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  8 22:25:47.564: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9026" to be "running and ready"
Sep  8 22:25:47.581: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.869018ms
Sep  8 22:25:47.581: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:25:49.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02583712s
Sep  8 22:25:49.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:25:51.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.036374172s
Sep  8 22:25:51.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:25:53.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030635161s
Sep  8 22:25:53.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:25:55.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025886979s
Sep  8 22:25:55.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:25:57.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030529128s
Sep  8 22:25:57.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:25:59.591: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.02716499s
Sep  8 22:25:59.591: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:26:01.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.032833593s
Sep  8 22:26:01.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:26:03.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025728442s
Sep  8 22:26:03.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:26:05.591: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026966215s
Sep  8 22:26:05.591: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:26:07.591: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.027360302s
Sep  8 22:26:07.591: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Sep  8 22:26:09.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.026068084s
Sep  8 22:26:09.590: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Sep  8 22:26:09.590: INFO: Pod "netserver-0" satisfied condition "running and ready"
Sep  8 22:26:09.597: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9026" to be "running and ready"
Sep  8 22:26:09.607: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.801335ms
Sep  8 22:26:09.607: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Sep  8 22:26:09.607: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 09/08/23 22:26:09.618
Sep  8 22:26:09.635: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9026" to be "running"
Sep  8 22:26:09.645: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.305817ms
Sep  8 22:26:11.660: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02574498s
Sep  8 22:26:11.660: INFO: Pod "test-container-pod" satisfied condition "running"
Sep  8 22:26:11.668: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Sep  8 22:26:11.668: INFO: Breadth first check of 10.233.75.74 on host 10.100.19.129...
Sep  8 22:26:11.675: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.94:9080/dial?request=hostname&protocol=http&host=10.233.75.74&port=8083&tries=1'] Namespace:pod-network-test-9026 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:26:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:26:11.677: INFO: ExecWithOptions: Clientset creation
Sep  8 22:26:11.677: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9026/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.75.74%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  8 22:26:11.830: INFO: Waiting for responses: map[]
Sep  8 22:26:11.830: INFO: reached 10.233.75.74 after 0/1 tries
Sep  8 22:26:11.830: INFO: Breadth first check of 10.233.103.87 on host 10.100.16.26...
Sep  8 22:26:11.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.94:9080/dial?request=hostname&protocol=http&host=10.233.103.87&port=8083&tries=1'] Namespace:pod-network-test-9026 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  8 22:26:11.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
Sep  8 22:26:11.855: INFO: ExecWithOptions: Clientset creation
Sep  8 22:26:11.856: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9026/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.103.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Sep  8 22:26:11.988: INFO: Waiting for responses: map[]
Sep  8 22:26:11.988: INFO: reached 10.233.103.87 after 0/1 tries
Sep  8 22:26:11.988: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Sep  8 22:26:11.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9026" for this suite. 09/08/23 22:26:12.004
------------------------------
â€¢ [SLOW TEST] [24.612 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:25:47.409
    Sep  8 22:25:47.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pod-network-test 09/08/23 22:25:47.413
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:25:47.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:25:47.469
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9026 09/08/23 22:25:47.489
    STEP: creating a selector 09/08/23 22:25:47.489
    STEP: Creating the service pods in kubernetes 09/08/23 22:25:47.489
    Sep  8 22:25:47.489: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Sep  8 22:25:47.564: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9026" to be "running and ready"
    Sep  8 22:25:47.581: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.869018ms
    Sep  8 22:25:47.581: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:25:49.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02583712s
    Sep  8 22:25:49.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:25:51.600: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.036374172s
    Sep  8 22:25:51.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:25:53.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.030635161s
    Sep  8 22:25:53.595: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:25:55.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025886979s
    Sep  8 22:25:55.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:25:57.594: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030529128s
    Sep  8 22:25:57.594: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:25:59.591: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.02716499s
    Sep  8 22:25:59.591: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:26:01.596: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.032833593s
    Sep  8 22:26:01.597: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:26:03.589: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025728442s
    Sep  8 22:26:03.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:26:05.591: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.026966215s
    Sep  8 22:26:05.591: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:26:07.591: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.027360302s
    Sep  8 22:26:07.591: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Sep  8 22:26:09.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.026068084s
    Sep  8 22:26:09.590: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Sep  8 22:26:09.590: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Sep  8 22:26:09.597: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9026" to be "running and ready"
    Sep  8 22:26:09.607: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.801335ms
    Sep  8 22:26:09.607: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Sep  8 22:26:09.607: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 09/08/23 22:26:09.618
    Sep  8 22:26:09.635: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9026" to be "running"
    Sep  8 22:26:09.645: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.305817ms
    Sep  8 22:26:11.660: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02574498s
    Sep  8 22:26:11.660: INFO: Pod "test-container-pod" satisfied condition "running"
    Sep  8 22:26:11.668: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Sep  8 22:26:11.668: INFO: Breadth first check of 10.233.75.74 on host 10.100.19.129...
    Sep  8 22:26:11.675: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.94:9080/dial?request=hostname&protocol=http&host=10.233.75.74&port=8083&tries=1'] Namespace:pod-network-test-9026 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:26:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:26:11.677: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:26:11.677: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9026/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.75.74%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  8 22:26:11.830: INFO: Waiting for responses: map[]
    Sep  8 22:26:11.830: INFO: reached 10.233.75.74 after 0/1 tries
    Sep  8 22:26:11.830: INFO: Breadth first check of 10.233.103.87 on host 10.100.16.26...
    Sep  8 22:26:11.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.75.94:9080/dial?request=hostname&protocol=http&host=10.233.103.87&port=8083&tries=1'] Namespace:pod-network-test-9026 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Sep  8 22:26:11.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    Sep  8 22:26:11.855: INFO: ExecWithOptions: Clientset creation
    Sep  8 22:26:11.856: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9026/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.75.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.103.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Sep  8 22:26:11.988: INFO: Waiting for responses: map[]
    Sep  8 22:26:11.988: INFO: reached 10.233.103.87 after 0/1 tries
    Sep  8 22:26:11.988: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:26:11.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9026" for this suite. 09/08/23 22:26:12.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:26:12.028
Sep  8 22:26:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename daemonsets 09/08/23 22:26:12.029
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:12.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:12.087
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 09/08/23 22:26:12.137
STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:26:12.157
Sep  8 22:26:12.177: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:12.177: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:12.177: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:12.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:26:12.184: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:26:13.196: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:13.196: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:13.196: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:13.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:26:13.213: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:26:14.194: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:14.195: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:14.195: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:14.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 22:26:14.205: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/08/23 22:26:14.212
Sep  8 22:26:14.254: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:14.254: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:14.254: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:14.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:26:14.269: INFO: Node node-4 is running 0 daemon pod, expected 1
Sep  8 22:26:15.288: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:15.288: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:15.288: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:15.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:26:15.297: INFO: Node node-4 is running 0 daemon pod, expected 1
Sep  8 22:26:16.279: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:16.279: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:16.279: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:26:16.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 22:26:16.291: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 09/08/23 22:26:16.291
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/08/23 22:26:16.308
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1871, will wait for the garbage collector to delete the pods 09/08/23 22:26:16.308
Sep  8 22:26:16.400: INFO: Deleting DaemonSet.extensions daemon-set took: 22.727853ms
Sep  8 22:26:16.501: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.078554ms
Sep  8 22:26:18.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:26:18.813: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  8 22:26:18.821: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45459"},"items":null}

Sep  8 22:26:18.828: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45459"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:26:18.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1871" for this suite. 09/08/23 22:26:18.87
------------------------------
â€¢ [SLOW TEST] [6.867 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:26:12.028
    Sep  8 22:26:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename daemonsets 09/08/23 22:26:12.029
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:12.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:12.087
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 09/08/23 22:26:12.137
    STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:26:12.157
    Sep  8 22:26:12.177: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:12.177: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:12.177: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:12.184: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:26:12.184: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:26:13.196: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:13.196: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:13.196: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:13.213: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:26:13.213: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:26:14.194: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:14.195: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:14.195: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:14.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 22:26:14.205: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 09/08/23 22:26:14.212
    Sep  8 22:26:14.254: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:14.254: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:14.254: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:14.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:26:14.269: INFO: Node node-4 is running 0 daemon pod, expected 1
    Sep  8 22:26:15.288: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:15.288: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:15.288: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:15.297: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:26:15.297: INFO: Node node-4 is running 0 daemon pod, expected 1
    Sep  8 22:26:16.279: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:16.279: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:16.279: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:26:16.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 22:26:16.291: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 09/08/23 22:26:16.291
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/08/23 22:26:16.308
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1871, will wait for the garbage collector to delete the pods 09/08/23 22:26:16.308
    Sep  8 22:26:16.400: INFO: Deleting DaemonSet.extensions daemon-set took: 22.727853ms
    Sep  8 22:26:16.501: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.078554ms
    Sep  8 22:26:18.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:26:18.813: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  8 22:26:18.821: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45459"},"items":null}

    Sep  8 22:26:18.828: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45459"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:26:18.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1871" for this suite. 09/08/23 22:26:18.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:26:18.896
Sep  8 22:26:18.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replication-controller 09/08/23 22:26:18.898
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:18.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:18.98
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 09/08/23 22:26:18.986
STEP: When the matched label of one of its pods change 09/08/23 22:26:18.999
Sep  8 22:26:19.009: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  8 22:26:24.018: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 09/08/23 22:26:24.047
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  8 22:26:25.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1184" for this suite. 09/08/23 22:26:25.096
------------------------------
â€¢ [SLOW TEST] [6.232 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:26:18.896
    Sep  8 22:26:18.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replication-controller 09/08/23 22:26:18.898
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:18.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:18.98
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 09/08/23 22:26:18.986
    STEP: When the matched label of one of its pods change 09/08/23 22:26:18.999
    Sep  8 22:26:19.009: INFO: Pod name pod-release: Found 0 pods out of 1
    Sep  8 22:26:24.018: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 09/08/23 22:26:24.047
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:26:25.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1184" for this suite. 09/08/23 22:26:25.096
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:26:25.13
Sep  8 22:26:25.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 22:26:25.132
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:25.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:25.191
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 09/08/23 22:26:25.198
Sep  8 22:26:25.219: INFO: Waiting up to 5m0s for pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a" in namespace "downward-api-7788" to be "Succeeded or Failed"
Sep  8 22:26:25.244: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.625806ms
Sep  8 22:26:27.261: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041482594s
Sep  8 22:26:29.259: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039934017s
STEP: Saw pod success 09/08/23 22:26:29.259
Sep  8 22:26:29.260: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a" satisfied condition "Succeeded or Failed"
Sep  8 22:26:29.276: INFO: Trying to get logs from node node-3 pod downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a container dapi-container: <nil>
STEP: delete the pod 09/08/23 22:26:29.308
Sep  8 22:26:29.355: INFO: Waiting for pod downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a to disappear
Sep  8 22:26:29.374: INFO: Pod downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  8 22:26:29.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7788" for this suite. 09/08/23 22:26:29.39
------------------------------
â€¢ [4.280 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:26:25.13
    Sep  8 22:26:25.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 22:26:25.132
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:25.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:25.191
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 09/08/23 22:26:25.198
    Sep  8 22:26:25.219: INFO: Waiting up to 5m0s for pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a" in namespace "downward-api-7788" to be "Succeeded or Failed"
    Sep  8 22:26:25.244: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.625806ms
    Sep  8 22:26:27.261: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041482594s
    Sep  8 22:26:29.259: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039934017s
    STEP: Saw pod success 09/08/23 22:26:29.259
    Sep  8 22:26:29.260: INFO: Pod "downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a" satisfied condition "Succeeded or Failed"
    Sep  8 22:26:29.276: INFO: Trying to get logs from node node-3 pod downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a container dapi-container: <nil>
    STEP: delete the pod 09/08/23 22:26:29.308
    Sep  8 22:26:29.355: INFO: Waiting for pod downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a to disappear
    Sep  8 22:26:29.374: INFO: Pod downward-api-b20157f8-8f1b-42a1-b834-b69335a3382a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:26:29.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7788" for this suite. 09/08/23 22:26:29.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:26:29.416
Sep  8 22:26:29.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 22:26:29.417
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:29.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:29.489
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 09/08/23 22:26:29.494
Sep  8 22:26:29.514: INFO: Waiting up to 2m0s for pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" in namespace "var-expansion-9365" to be "running"
Sep  8 22:26:29.522: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.694062ms
Sep  8 22:26:31.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027446494s
Sep  8 22:26:33.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016440701s
Sep  8 22:26:35.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022623676s
Sep  8 22:26:37.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02679878s
Sep  8 22:26:39.545: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030501143s
Sep  8 22:26:41.542: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02815651s
Sep  8 22:26:43.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.018992587s
Sep  8 22:26:45.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022836774s
Sep  8 22:26:47.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.018552628s
Sep  8 22:26:49.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026423754s
Sep  8 22:26:51.538: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02406402s
Sep  8 22:26:53.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017025724s
Sep  8 22:26:55.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.016518249s
Sep  8 22:26:57.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018655463s
Sep  8 22:26:59.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020166045s
Sep  8 22:27:01.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021481383s
Sep  8 22:27:03.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.017431206s
Sep  8 22:27:05.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019870081s
Sep  8 22:27:07.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.020206011s
Sep  8 22:27:09.536: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022458136s
Sep  8 22:27:11.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026483221s
Sep  8 22:27:13.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023373591s
Sep  8 22:27:15.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018998066s
Sep  8 22:27:17.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.018906177s
Sep  8 22:27:19.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018452615s
Sep  8 22:27:21.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.018488252s
Sep  8 22:27:23.545: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.03060787s
Sep  8 22:27:25.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.019416128s
Sep  8 22:27:27.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0206467s
Sep  8 22:27:29.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015905825s
Sep  8 22:27:31.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.018133487s
Sep  8 22:27:33.548: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.033811742s
Sep  8 22:27:35.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01756479s
Sep  8 22:27:37.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.020705235s
Sep  8 22:27:39.536: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022095219s
Sep  8 22:27:41.538: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.0235537s
Sep  8 22:27:43.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.025722592s
Sep  8 22:27:45.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018197167s
Sep  8 22:27:47.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016384306s
Sep  8 22:27:49.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018424029s
Sep  8 22:27:51.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.018976231s
Sep  8 22:27:53.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.025511151s
Sep  8 22:27:55.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018424917s
Sep  8 22:27:57.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018005471s
Sep  8 22:27:59.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018284643s
Sep  8 22:28:01.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.02052644s
Sep  8 22:28:03.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.025498976s
Sep  8 22:28:05.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017413985s
Sep  8 22:28:07.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.017667812s
Sep  8 22:28:09.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016675242s
Sep  8 22:28:11.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023139941s
Sep  8 22:28:13.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.020250388s
Sep  8 22:28:15.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015867182s
Sep  8 22:28:17.536: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.022108592s
Sep  8 22:28:19.538: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023979448s
Sep  8 22:28:21.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018393323s
Sep  8 22:28:23.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023401856s
Sep  8 22:28:25.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016489797s
Sep  8 22:28:27.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017698945s
Sep  8 22:28:29.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027447564s
Sep  8 22:28:29.549: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.034835915s
STEP: updating the pod 09/08/23 22:28:29.549
Sep  8 22:28:30.076: INFO: Successfully updated pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5"
STEP: waiting for pod running 09/08/23 22:28:30.076
Sep  8 22:28:30.077: INFO: Waiting up to 2m0s for pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" in namespace "var-expansion-9365" to be "running"
Sep  8 22:28:30.087: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.238852ms
Sep  8 22:28:32.096: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019418159s
Sep  8 22:28:32.096: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" satisfied condition "running"
STEP: deleting the pod gracefully 09/08/23 22:28:32.096
Sep  8 22:28:32.096: INFO: Deleting pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" in namespace "var-expansion-9365"
Sep  8 22:28:32.123: INFO: Wait up to 5m0s for pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 22:29:04.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9365" for this suite. 09/08/23 22:29:04.163
------------------------------
â€¢ [SLOW TEST] [154.773 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:26:29.416
    Sep  8 22:26:29.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 22:26:29.417
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:26:29.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:26:29.489
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 09/08/23 22:26:29.494
    Sep  8 22:26:29.514: INFO: Waiting up to 2m0s for pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" in namespace "var-expansion-9365" to be "running"
    Sep  8 22:26:29.522: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.694062ms
    Sep  8 22:26:31.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027446494s
    Sep  8 22:26:33.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016440701s
    Sep  8 22:26:35.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022623676s
    Sep  8 22:26:37.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.02679878s
    Sep  8 22:26:39.545: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030501143s
    Sep  8 22:26:41.542: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02815651s
    Sep  8 22:26:43.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.018992587s
    Sep  8 22:26:45.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022836774s
    Sep  8 22:26:47.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.018552628s
    Sep  8 22:26:49.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026423754s
    Sep  8 22:26:51.538: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.02406402s
    Sep  8 22:26:53.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.017025724s
    Sep  8 22:26:55.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.016518249s
    Sep  8 22:26:57.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018655463s
    Sep  8 22:26:59.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.020166045s
    Sep  8 22:27:01.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.021481383s
    Sep  8 22:27:03.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.017431206s
    Sep  8 22:27:05.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019870081s
    Sep  8 22:27:07.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.020206011s
    Sep  8 22:27:09.536: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022458136s
    Sep  8 22:27:11.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.026483221s
    Sep  8 22:27:13.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.023373591s
    Sep  8 22:27:15.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018998066s
    Sep  8 22:27:17.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.018906177s
    Sep  8 22:27:19.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.018452615s
    Sep  8 22:27:21.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.018488252s
    Sep  8 22:27:23.545: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.03060787s
    Sep  8 22:27:25.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.019416128s
    Sep  8 22:27:27.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.0206467s
    Sep  8 22:27:29.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015905825s
    Sep  8 22:27:31.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.018133487s
    Sep  8 22:27:33.548: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.033811742s
    Sep  8 22:27:35.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.01756479s
    Sep  8 22:27:37.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.020705235s
    Sep  8 22:27:39.536: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022095219s
    Sep  8 22:27:41.538: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.0235537s
    Sep  8 22:27:43.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.025722592s
    Sep  8 22:27:45.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018197167s
    Sep  8 22:27:47.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.016384306s
    Sep  8 22:27:49.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018424029s
    Sep  8 22:27:51.533: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.018976231s
    Sep  8 22:27:53.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.025511151s
    Sep  8 22:27:55.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018424917s
    Sep  8 22:27:57.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018005471s
    Sep  8 22:27:59.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.018284643s
    Sep  8 22:28:01.535: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.02052644s
    Sep  8 22:28:03.540: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.025498976s
    Sep  8 22:28:05.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017413985s
    Sep  8 22:28:07.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.017667812s
    Sep  8 22:28:09.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.016675242s
    Sep  8 22:28:11.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.023139941s
    Sep  8 22:28:13.534: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.020250388s
    Sep  8 22:28:15.530: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015867182s
    Sep  8 22:28:17.536: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.022108592s
    Sep  8 22:28:19.538: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.023979448s
    Sep  8 22:28:21.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018393323s
    Sep  8 22:28:23.537: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.023401856s
    Sep  8 22:28:25.531: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.016489797s
    Sep  8 22:28:27.532: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017698945s
    Sep  8 22:28:29.541: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027447564s
    Sep  8 22:28:29.549: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.034835915s
    STEP: updating the pod 09/08/23 22:28:29.549
    Sep  8 22:28:30.076: INFO: Successfully updated pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5"
    STEP: waiting for pod running 09/08/23 22:28:30.076
    Sep  8 22:28:30.077: INFO: Waiting up to 2m0s for pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" in namespace "var-expansion-9365" to be "running"
    Sep  8 22:28:30.087: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.238852ms
    Sep  8 22:28:32.096: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.019418159s
    Sep  8 22:28:32.096: INFO: Pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" satisfied condition "running"
    STEP: deleting the pod gracefully 09/08/23 22:28:32.096
    Sep  8 22:28:32.096: INFO: Deleting pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" in namespace "var-expansion-9365"
    Sep  8 22:28:32.123: INFO: Wait up to 5m0s for pod "var-expansion-11cbb5e7-71f3-4a5e-bb5f-69007de523f5" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:29:04.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9365" for this suite. 09/08/23 22:29:04.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:29:04.193
Sep  8 22:29:04.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 22:29:04.194
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:29:04.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:29:04.259
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-98771c63-4ba6-4de1-8c18-59a011d94c77 09/08/23 22:29:04.332
STEP: Creating a pod to test consume secrets 09/08/23 22:29:04.343
Sep  8 22:29:04.374: INFO: Waiting up to 5m0s for pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e" in namespace "secrets-8632" to be "Succeeded or Failed"
Sep  8 22:29:04.387: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.624264ms
Sep  8 22:29:06.398: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02354893s
Sep  8 22:29:08.412: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037491128s
STEP: Saw pod success 09/08/23 22:29:08.412
Sep  8 22:29:08.413: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e" satisfied condition "Succeeded or Failed"
Sep  8 22:29:08.423: INFO: Trying to get logs from node node-3 pod pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:29:08.457
Sep  8 22:29:08.487: INFO: Waiting for pod pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e to disappear
Sep  8 22:29:08.496: INFO: Pod pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 22:29:08.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8632" for this suite. 09/08/23 22:29:08.515
STEP: Destroying namespace "secret-namespace-6451" for this suite. 09/08/23 22:29:08.535
------------------------------
â€¢ [4.368 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:29:04.193
    Sep  8 22:29:04.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 22:29:04.194
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:29:04.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:29:04.259
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-98771c63-4ba6-4de1-8c18-59a011d94c77 09/08/23 22:29:04.332
    STEP: Creating a pod to test consume secrets 09/08/23 22:29:04.343
    Sep  8 22:29:04.374: INFO: Waiting up to 5m0s for pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e" in namespace "secrets-8632" to be "Succeeded or Failed"
    Sep  8 22:29:04.387: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.624264ms
    Sep  8 22:29:06.398: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02354893s
    Sep  8 22:29:08.412: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037491128s
    STEP: Saw pod success 09/08/23 22:29:08.412
    Sep  8 22:29:08.413: INFO: Pod "pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e" satisfied condition "Succeeded or Failed"
    Sep  8 22:29:08.423: INFO: Trying to get logs from node node-3 pod pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:29:08.457
    Sep  8 22:29:08.487: INFO: Waiting for pod pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e to disappear
    Sep  8 22:29:08.496: INFO: Pod pod-secrets-8494ae26-9c25-4401-96f6-bd83c1eca18e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:29:08.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8632" for this suite. 09/08/23 22:29:08.515
    STEP: Destroying namespace "secret-namespace-6451" for this suite. 09/08/23 22:29:08.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:29:08.563
Sep  8 22:29:08.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 22:29:08.564
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:29:08.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:29:08.618
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-baf14b0a-e2f0-4b81-a62a-91fefcba494d 09/08/23 22:29:08.647
STEP: Creating secret with name s-test-opt-upd-d1f80434-3562-4ee7-8aa5-72df33e74015 09/08/23 22:29:08.662
STEP: Creating the pod 09/08/23 22:29:08.674
Sep  8 22:29:08.695: INFO: Waiting up to 5m0s for pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3" in namespace "secrets-7022" to be "running and ready"
Sep  8 22:29:08.720: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.584164ms
Sep  8 22:29:08.720: INFO: The phase of Pod pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:29:10.734: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039187359s
Sep  8 22:29:10.734: INFO: The phase of Pod pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:29:12.736: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.04130905s
Sep  8 22:29:12.736: INFO: The phase of Pod pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3 is Running (Ready = true)
Sep  8 22:29:12.736: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-baf14b0a-e2f0-4b81-a62a-91fefcba494d 09/08/23 22:29:12.79
STEP: Updating secret s-test-opt-upd-d1f80434-3562-4ee7-8aa5-72df33e74015 09/08/23 22:29:12.809
STEP: Creating secret with name s-test-opt-create-961a7bd5-66ae-4cb0-b6e8-d72a8a8020d1 09/08/23 22:29:12.83
STEP: waiting to observe update in volume 09/08/23 22:29:12.842
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 22:30:33.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7022" for this suite. 09/08/23 22:30:34.006
------------------------------
â€¢ [SLOW TEST] [85.457 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:29:08.563
    Sep  8 22:29:08.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 22:29:08.564
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:29:08.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:29:08.618
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-baf14b0a-e2f0-4b81-a62a-91fefcba494d 09/08/23 22:29:08.647
    STEP: Creating secret with name s-test-opt-upd-d1f80434-3562-4ee7-8aa5-72df33e74015 09/08/23 22:29:08.662
    STEP: Creating the pod 09/08/23 22:29:08.674
    Sep  8 22:29:08.695: INFO: Waiting up to 5m0s for pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3" in namespace "secrets-7022" to be "running and ready"
    Sep  8 22:29:08.720: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.584164ms
    Sep  8 22:29:08.720: INFO: The phase of Pod pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:29:10.734: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039187359s
    Sep  8 22:29:10.734: INFO: The phase of Pod pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:29:12.736: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.04130905s
    Sep  8 22:29:12.736: INFO: The phase of Pod pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3 is Running (Ready = true)
    Sep  8 22:29:12.736: INFO: Pod "pod-secrets-85ddb3e4-4253-40dd-a33a-7d3db0e353d3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-baf14b0a-e2f0-4b81-a62a-91fefcba494d 09/08/23 22:29:12.79
    STEP: Updating secret s-test-opt-upd-d1f80434-3562-4ee7-8aa5-72df33e74015 09/08/23 22:29:12.809
    STEP: Creating secret with name s-test-opt-create-961a7bd5-66ae-4cb0-b6e8-d72a8a8020d1 09/08/23 22:29:12.83
    STEP: waiting to observe update in volume 09/08/23 22:29:12.842
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:30:33.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7022" for this suite. 09/08/23 22:30:34.006
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:30:34.02
Sep  8 22:30:34.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 22:30:34.022
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:34.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:34.078
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 22:30:34.122
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:30:34.468
STEP: Deploying the webhook pod 09/08/23 22:30:34.493
STEP: Wait for the deployment to be ready 09/08/23 22:30:34.545
Sep  8 22:30:34.582: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 22:30:36.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 22:30:38.63
STEP: Verifying the service has paired with the endpoint 09/08/23 22:30:38.663
Sep  8 22:30:39.664: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 09/08/23 22:30:39.674
STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 22:30:39.703
STEP: Updating a validating webhook configuration's rules to not include the create operation 09/08/23 22:30:39.719
STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 22:30:39.743
STEP: Patching a validating webhook configuration's rules to include the create operation 09/08/23 22:30:39.776
STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 22:30:39.793
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:30:39.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2142" for this suite. 09/08/23 22:30:40.179
STEP: Destroying namespace "webhook-2142-markers" for this suite. 09/08/23 22:30:40.216
------------------------------
â€¢ [SLOW TEST] [6.239 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:30:34.02
    Sep  8 22:30:34.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 22:30:34.022
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:34.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:34.078
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 22:30:34.122
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:30:34.468
    STEP: Deploying the webhook pod 09/08/23 22:30:34.493
    STEP: Wait for the deployment to be ready 09/08/23 22:30:34.545
    Sep  8 22:30:34.582: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 22:30:36.621: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 30, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 22:30:38.63
    STEP: Verifying the service has paired with the endpoint 09/08/23 22:30:38.663
    Sep  8 22:30:39.664: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 09/08/23 22:30:39.674
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 22:30:39.703
    STEP: Updating a validating webhook configuration's rules to not include the create operation 09/08/23 22:30:39.719
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 22:30:39.743
    STEP: Patching a validating webhook configuration's rules to include the create operation 09/08/23 22:30:39.776
    STEP: Creating a configMap that does not comply to the validation webhook rules 09/08/23 22:30:39.793
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:30:39.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2142" for this suite. 09/08/23 22:30:40.179
    STEP: Destroying namespace "webhook-2142-markers" for this suite. 09/08/23 22:30:40.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:30:40.264
Sep  8 22:30:40.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 22:30:40.265
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:40.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:40.326
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-5580 09/08/23 22:30:40.339
STEP: creating service affinity-clusterip in namespace services-5580 09/08/23 22:30:40.34
STEP: creating replication controller affinity-clusterip in namespace services-5580 09/08/23 22:30:40.383
I0908 22:30:40.396707      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5580, replica count: 3
I0908 22:30:43.449659      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 22:30:43.474: INFO: Creating new exec pod
Sep  8 22:30:43.510: INFO: Waiting up to 5m0s for pod "execpod-affinityzkgfz" in namespace "services-5580" to be "running"
Sep  8 22:30:43.529: INFO: Pod "execpod-affinityzkgfz": Phase="Pending", Reason="", readiness=false. Elapsed: 19.599243ms
Sep  8 22:30:45.545: INFO: Pod "execpod-affinityzkgfz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035616412s
Sep  8 22:30:47.538: INFO: Pod "execpod-affinityzkgfz": Phase="Running", Reason="", readiness=true. Elapsed: 4.0285536s
Sep  8 22:30:47.538: INFO: Pod "execpod-affinityzkgfz" satisfied condition "running"
Sep  8 22:30:48.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5580 exec execpod-affinityzkgfz -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Sep  8 22:30:48.771: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  8 22:30:48.771: INFO: stdout: ""
Sep  8 22:30:48.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5580 exec execpod-affinityzkgfz -- /bin/sh -x -c nc -v -z -w 2 10.233.31.119 80'
Sep  8 22:30:49.050: INFO: stderr: "+ nc -v -z -w 2 10.233.31.119 80\nConnection to 10.233.31.119 80 port [tcp/http] succeeded!\n"
Sep  8 22:30:49.050: INFO: stdout: ""
Sep  8 22:30:49.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5580 exec execpod-affinityzkgfz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.31.119:80/ ; done'
Sep  8 22:30:49.349: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n"
Sep  8 22:30:49.349: INFO: stdout: "\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f"
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
Sep  8 22:30:49.349: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5580, will wait for the garbage collector to delete the pods 09/08/23 22:30:49.397
Sep  8 22:30:49.474: INFO: Deleting ReplicationController affinity-clusterip took: 16.289677ms
Sep  8 22:30:49.574: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.292717ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 22:30:52.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5580" for this suite. 09/08/23 22:30:52.349
------------------------------
â€¢ [SLOW TEST] [12.121 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:30:40.264
    Sep  8 22:30:40.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 22:30:40.265
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:40.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:40.326
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-5580 09/08/23 22:30:40.339
    STEP: creating service affinity-clusterip in namespace services-5580 09/08/23 22:30:40.34
    STEP: creating replication controller affinity-clusterip in namespace services-5580 09/08/23 22:30:40.383
    I0908 22:30:40.396707      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5580, replica count: 3
    I0908 22:30:43.449659      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 22:30:43.474: INFO: Creating new exec pod
    Sep  8 22:30:43.510: INFO: Waiting up to 5m0s for pod "execpod-affinityzkgfz" in namespace "services-5580" to be "running"
    Sep  8 22:30:43.529: INFO: Pod "execpod-affinityzkgfz": Phase="Pending", Reason="", readiness=false. Elapsed: 19.599243ms
    Sep  8 22:30:45.545: INFO: Pod "execpod-affinityzkgfz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035616412s
    Sep  8 22:30:47.538: INFO: Pod "execpod-affinityzkgfz": Phase="Running", Reason="", readiness=true. Elapsed: 4.0285536s
    Sep  8 22:30:47.538: INFO: Pod "execpod-affinityzkgfz" satisfied condition "running"
    Sep  8 22:30:48.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5580 exec execpod-affinityzkgfz -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Sep  8 22:30:48.771: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Sep  8 22:30:48.771: INFO: stdout: ""
    Sep  8 22:30:48.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5580 exec execpod-affinityzkgfz -- /bin/sh -x -c nc -v -z -w 2 10.233.31.119 80'
    Sep  8 22:30:49.050: INFO: stderr: "+ nc -v -z -w 2 10.233.31.119 80\nConnection to 10.233.31.119 80 port [tcp/http] succeeded!\n"
    Sep  8 22:30:49.050: INFO: stdout: ""
    Sep  8 22:30:49.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-5580 exec execpod-affinityzkgfz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.31.119:80/ ; done'
    Sep  8 22:30:49.349: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.31.119:80/\n"
    Sep  8 22:30:49.349: INFO: stdout: "\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f\naffinity-clusterip-hdk9f"
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Received response from host: affinity-clusterip-hdk9f
    Sep  8 22:30:49.349: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-5580, will wait for the garbage collector to delete the pods 09/08/23 22:30:49.397
    Sep  8 22:30:49.474: INFO: Deleting ReplicationController affinity-clusterip took: 16.289677ms
    Sep  8 22:30:49.574: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.292717ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:30:52.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5580" for this suite. 09/08/23 22:30:52.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:30:52.393
Sep  8 22:30:52.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:30:52.395
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:52.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:52.477
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:30:52.485
Sep  8 22:30:52.527: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7" in namespace "projected-7188" to be "Succeeded or Failed"
Sep  8 22:30:52.557: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005138ms
Sep  8 22:30:54.570: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043396926s
Sep  8 22:30:56.567: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040448935s
STEP: Saw pod success 09/08/23 22:30:56.567
Sep  8 22:30:56.568: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7" satisfied condition "Succeeded or Failed"
Sep  8 22:30:56.577: INFO: Trying to get logs from node node-3 pod downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7 container client-container: <nil>
STEP: delete the pod 09/08/23 22:30:56.604
Sep  8 22:30:56.648: INFO: Waiting for pod downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7 to disappear
Sep  8 22:30:56.654: INFO: Pod downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 22:30:56.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7188" for this suite. 09/08/23 22:30:56.67
------------------------------
â€¢ [4.298 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:30:52.393
    Sep  8 22:30:52.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:30:52.395
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:52.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:52.477
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:30:52.485
    Sep  8 22:30:52.527: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7" in namespace "projected-7188" to be "Succeeded or Failed"
    Sep  8 22:30:52.557: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005138ms
    Sep  8 22:30:54.570: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043396926s
    Sep  8 22:30:56.567: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040448935s
    STEP: Saw pod success 09/08/23 22:30:56.567
    Sep  8 22:30:56.568: INFO: Pod "downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7" satisfied condition "Succeeded or Failed"
    Sep  8 22:30:56.577: INFO: Trying to get logs from node node-3 pod downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7 container client-container: <nil>
    STEP: delete the pod 09/08/23 22:30:56.604
    Sep  8 22:30:56.648: INFO: Waiting for pod downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7 to disappear
    Sep  8 22:30:56.654: INFO: Pod downwardapi-volume-19d761c1-3526-463b-a104-566062640fb7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:30:56.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7188" for this suite. 09/08/23 22:30:56.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:30:56.692
Sep  8 22:30:56.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 22:30:56.693
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:56.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:56.748
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5161 09/08/23 22:30:56.755
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 09/08/23 22:30:56.782
Sep  8 22:30:56.806: INFO: Found 0 stateful pods, waiting for 3
Sep  8 22:31:06.815: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:31:06.815: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:31:06.815: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/08/23 22:31:06.842
Sep  8 22:31:06.885: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/08/23 22:31:06.885
STEP: Not applying an update when the partition is greater than the number of replicas 09/08/23 22:31:16.941
STEP: Performing a canary update 09/08/23 22:31:16.941
Sep  8 22:31:16.973: INFO: Updating stateful set ss2
Sep  8 22:31:16.998: INFO: Waiting for Pod statefulset-5161/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 09/08/23 22:31:27.027
Sep  8 22:31:27.144: INFO: Found 1 stateful pods, waiting for 3
Sep  8 22:31:37.157: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:31:37.157: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:31:37.157: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 09/08/23 22:31:37.192
Sep  8 22:31:37.230: INFO: Updating stateful set ss2
Sep  8 22:31:37.271: INFO: Waiting for Pod statefulset-5161/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep  8 22:31:47.328: INFO: Updating stateful set ss2
Sep  8 22:31:47.346: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
Sep  8 22:31:47.346: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Sep  8 22:31:57.375: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
Sep  8 22:31:57.375: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 22:32:07.367: INFO: Deleting all statefulset in ns statefulset-5161
Sep  8 22:32:07.374: INFO: Scaling statefulset ss2 to 0
Sep  8 22:32:17.417: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 22:32:17.426: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:32:17.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5161" for this suite. 09/08/23 22:32:17.49
------------------------------
â€¢ [SLOW TEST] [80.827 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:30:56.692
    Sep  8 22:30:56.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 22:30:56.693
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:30:56.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:30:56.748
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5161 09/08/23 22:30:56.755
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 09/08/23 22:30:56.782
    Sep  8 22:30:56.806: INFO: Found 0 stateful pods, waiting for 3
    Sep  8 22:31:06.815: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:31:06.815: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:31:06.815: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/08/23 22:31:06.842
    Sep  8 22:31:06.885: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/08/23 22:31:06.885
    STEP: Not applying an update when the partition is greater than the number of replicas 09/08/23 22:31:16.941
    STEP: Performing a canary update 09/08/23 22:31:16.941
    Sep  8 22:31:16.973: INFO: Updating stateful set ss2
    Sep  8 22:31:16.998: INFO: Waiting for Pod statefulset-5161/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 09/08/23 22:31:27.027
    Sep  8 22:31:27.144: INFO: Found 1 stateful pods, waiting for 3
    Sep  8 22:31:37.157: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:31:37.157: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:31:37.157: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 09/08/23 22:31:37.192
    Sep  8 22:31:37.230: INFO: Updating stateful set ss2
    Sep  8 22:31:37.271: INFO: Waiting for Pod statefulset-5161/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep  8 22:31:47.328: INFO: Updating stateful set ss2
    Sep  8 22:31:47.346: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
    Sep  8 22:31:47.346: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Sep  8 22:31:57.375: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
    Sep  8 22:31:57.375: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 22:32:07.367: INFO: Deleting all statefulset in ns statefulset-5161
    Sep  8 22:32:07.374: INFO: Scaling statefulset ss2 to 0
    Sep  8 22:32:17.417: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 22:32:17.426: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:32:17.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5161" for this suite. 09/08/23 22:32:17.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:32:17.52
Sep  8 22:32:17.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-preemption 09/08/23 22:32:17.521
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:32:17.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:32:17.583
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  8 22:32:17.630: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  8 22:33:17.726: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:33:17.745
Sep  8 22:33:17.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-preemption-path 09/08/23 22:33:17.747
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:17.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:17.824
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 09/08/23 22:33:17.827
STEP: Trying to launch a pod without a label to get a node which can launch it. 09/08/23 22:33:17.827
Sep  8 22:33:17.853: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3908" to be "running"
Sep  8 22:33:17.865: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511328ms
Sep  8 22:33:19.877: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023656306s
Sep  8 22:33:19.877: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 09/08/23 22:33:19.888
Sep  8 22:33:19.946: INFO: found a healthy node: node-3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Sep  8 22:33:26.221: INFO: pods created so far: [1 1 1]
Sep  8 22:33:26.221: INFO: length of pods created so far: 3
Sep  8 22:33:30.250: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Sep  8 22:33:37.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:33:37.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3908" for this suite. 09/08/23 22:33:37.46
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2390" for this suite. 09/08/23 22:33:37.476
------------------------------
â€¢ [SLOW TEST] [79.983 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:32:17.52
    Sep  8 22:32:17.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-preemption 09/08/23 22:32:17.521
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:32:17.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:32:17.583
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  8 22:32:17.630: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  8 22:33:17.726: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:33:17.745
    Sep  8 22:33:17.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-preemption-path 09/08/23 22:33:17.747
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:17.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:17.824
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 09/08/23 22:33:17.827
    STEP: Trying to launch a pod without a label to get a node which can launch it. 09/08/23 22:33:17.827
    Sep  8 22:33:17.853: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3908" to be "running"
    Sep  8 22:33:17.865: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511328ms
    Sep  8 22:33:19.877: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.023656306s
    Sep  8 22:33:19.877: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 09/08/23 22:33:19.888
    Sep  8 22:33:19.946: INFO: found a healthy node: node-3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Sep  8 22:33:26.221: INFO: pods created so far: [1 1 1]
    Sep  8 22:33:26.221: INFO: length of pods created so far: 3
    Sep  8 22:33:30.250: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:33:37.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:33:37.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3908" for this suite. 09/08/23 22:33:37.46
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2390" for this suite. 09/08/23 22:33:37.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:33:37.516
Sep  8 22:33:37.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:33:37.517
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:37.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:37.557
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-754959c0-78ef-40c7-bf09-6065af69962c 09/08/23 22:33:37.562
STEP: Creating a pod to test consume configMaps 09/08/23 22:33:37.571
Sep  8 22:33:37.592: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125" in namespace "projected-4149" to be "Succeeded or Failed"
Sep  8 22:33:37.602: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Pending", Reason="", readiness=false. Elapsed: 10.472248ms
Sep  8 22:33:39.612: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Running", Reason="", readiness=true. Elapsed: 2.019912599s
Sep  8 22:33:41.614: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Running", Reason="", readiness=false. Elapsed: 4.022855802s
Sep  8 22:33:43.612: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020106169s
STEP: Saw pod success 09/08/23 22:33:43.612
Sep  8 22:33:43.612: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125" satisfied condition "Succeeded or Failed"
Sep  8 22:33:43.622: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 22:33:43.662
Sep  8 22:33:43.702: INFO: Waiting for pod pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125 to disappear
Sep  8 22:33:43.709: INFO: Pod pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:33:43.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4149" for this suite. 09/08/23 22:33:43.722
------------------------------
â€¢ [SLOW TEST] [6.230 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:33:37.516
    Sep  8 22:33:37.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:33:37.517
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:37.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:37.557
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-754959c0-78ef-40c7-bf09-6065af69962c 09/08/23 22:33:37.562
    STEP: Creating a pod to test consume configMaps 09/08/23 22:33:37.571
    Sep  8 22:33:37.592: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125" in namespace "projected-4149" to be "Succeeded or Failed"
    Sep  8 22:33:37.602: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Pending", Reason="", readiness=false. Elapsed: 10.472248ms
    Sep  8 22:33:39.612: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Running", Reason="", readiness=true. Elapsed: 2.019912599s
    Sep  8 22:33:41.614: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Running", Reason="", readiness=false. Elapsed: 4.022855802s
    Sep  8 22:33:43.612: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020106169s
    STEP: Saw pod success 09/08/23 22:33:43.612
    Sep  8 22:33:43.612: INFO: Pod "pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125" satisfied condition "Succeeded or Failed"
    Sep  8 22:33:43.622: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 22:33:43.662
    Sep  8 22:33:43.702: INFO: Waiting for pod pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125 to disappear
    Sep  8 22:33:43.709: INFO: Pod pod-projected-configmaps-b0852f82-f567-456b-8788-cb50007b8125 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:33:43.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4149" for this suite. 09/08/23 22:33:43.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:33:43.748
Sep  8 22:33:43.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 22:33:43.749
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:43.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:43.795
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-1d2aadce-579d-4568-973b-df9b21a1eb05 09/08/23 22:33:43.802
STEP: Creating a pod to test consume configMaps 09/08/23 22:33:43.817
Sep  8 22:33:43.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17" in namespace "configmap-5580" to be "Succeeded or Failed"
Sep  8 22:33:43.857: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Pending", Reason="", readiness=false. Elapsed: 17.621013ms
Sep  8 22:33:45.867: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02759507s
Sep  8 22:33:47.872: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032706352s
Sep  8 22:33:49.875: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036458303s
STEP: Saw pod success 09/08/23 22:33:49.876
Sep  8 22:33:49.876: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17" satisfied condition "Succeeded or Failed"
Sep  8 22:33:49.884: INFO: Trying to get logs from node node-3 pod pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 22:33:49.905
Sep  8 22:33:49.948: INFO: Waiting for pod pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17 to disappear
Sep  8 22:33:49.954: INFO: Pod pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:33:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5580" for this suite. 09/08/23 22:33:49.963
------------------------------
â€¢ [SLOW TEST] [6.237 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:33:43.748
    Sep  8 22:33:43.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 22:33:43.749
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:43.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:43.795
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-1d2aadce-579d-4568-973b-df9b21a1eb05 09/08/23 22:33:43.802
    STEP: Creating a pod to test consume configMaps 09/08/23 22:33:43.817
    Sep  8 22:33:43.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17" in namespace "configmap-5580" to be "Succeeded or Failed"
    Sep  8 22:33:43.857: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Pending", Reason="", readiness=false. Elapsed: 17.621013ms
    Sep  8 22:33:45.867: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02759507s
    Sep  8 22:33:47.872: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032706352s
    Sep  8 22:33:49.875: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036458303s
    STEP: Saw pod success 09/08/23 22:33:49.876
    Sep  8 22:33:49.876: INFO: Pod "pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17" satisfied condition "Succeeded or Failed"
    Sep  8 22:33:49.884: INFO: Trying to get logs from node node-3 pod pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 22:33:49.905
    Sep  8 22:33:49.948: INFO: Waiting for pod pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17 to disappear
    Sep  8 22:33:49.954: INFO: Pod pod-configmaps-20c5aaeb-ea7a-44fe-a0a0-646a4f13de17 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:33:49.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5580" for this suite. 09/08/23 22:33:49.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:33:49.987
Sep  8 22:33:49.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename daemonsets 09/08/23 22:33:49.99
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:50.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:50.048
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 09/08/23 22:33:50.106
STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:33:50.117
Sep  8 22:33:50.140: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:50.140: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:50.141: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:50.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:33:50.149: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:33:51.169: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:51.169: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:51.169: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:51.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:33:51.182: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:33:52.159: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:52.159: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:52.160: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:52.170: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 22:33:52.170: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 09/08/23 22:33:52.18
Sep  8 22:33:52.232: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:52.232: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:52.233: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:52.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:33:52.245: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:33:53.255: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:53.255: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:53.255: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:53.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:33:53.271: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:33:54.261: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:54.261: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:54.261: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:54.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:33:54.277: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:33:55.260: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:55.260: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:55.260: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:55.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:33:55.271: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:33:56.259: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:56.259: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:56.259: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:56.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Sep  8 22:33:56.268: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:33:57.260: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:57.260: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:57.261: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:33:57.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Sep  8 22:33:57.272: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 09/08/23 22:33:57.279
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9250, will wait for the garbage collector to delete the pods 09/08/23 22:33:57.279
Sep  8 22:33:57.349: INFO: Deleting DaemonSet.extensions daemon-set took: 11.479218ms
Sep  8 22:33:57.450: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.464993ms
Sep  8 22:34:00.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Sep  8 22:34:00.161: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Sep  8 22:34:00.181: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48691"},"items":null}

Sep  8 22:34:00.189: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48692"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:34:00.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9250" for this suite. 09/08/23 22:34:00.223
------------------------------
â€¢ [SLOW TEST] [10.262 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:33:49.987
    Sep  8 22:33:49.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename daemonsets 09/08/23 22:33:49.99
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:33:50.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:33:50.048
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 09/08/23 22:33:50.106
    STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:33:50.117
    Sep  8 22:33:50.140: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:50.140: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:50.141: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:50.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:33:50.149: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:33:51.169: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:51.169: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:51.169: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:51.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:33:51.182: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:33:52.159: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:52.159: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:52.160: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:52.170: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 22:33:52.170: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 09/08/23 22:33:52.18
    Sep  8 22:33:52.232: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:52.232: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:52.233: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:52.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:33:52.245: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:33:53.255: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:53.255: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:53.255: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:53.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:33:53.271: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:33:54.261: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:54.261: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:54.261: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:54.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:33:54.277: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:33:55.260: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:55.260: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:55.260: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:55.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:33:55.271: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:33:56.259: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:56.259: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:56.259: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:56.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Sep  8 22:33:56.268: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:33:57.260: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:57.260: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:57.261: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:33:57.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Sep  8 22:33:57.272: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 09/08/23 22:33:57.279
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9250, will wait for the garbage collector to delete the pods 09/08/23 22:33:57.279
    Sep  8 22:33:57.349: INFO: Deleting DaemonSet.extensions daemon-set took: 11.479218ms
    Sep  8 22:33:57.450: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.464993ms
    Sep  8 22:34:00.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Sep  8 22:34:00.161: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Sep  8 22:34:00.181: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48691"},"items":null}

    Sep  8 22:34:00.189: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48692"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:34:00.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9250" for this suite. 09/08/23 22:34:00.223
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:34:00.249
Sep  8 22:34:00.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 22:34:00.25
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:00.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:00.294
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:34:00.299
Sep  8 22:34:00.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab" in namespace "downward-api-9999" to be "Succeeded or Failed"
Sep  8 22:34:00.338: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab": Phase="Pending", Reason="", readiness=false. Elapsed: 15.395889ms
Sep  8 22:34:02.349: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026356026s
Sep  8 22:34:04.350: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026845338s
STEP: Saw pod success 09/08/23 22:34:04.35
Sep  8 22:34:04.350: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab" satisfied condition "Succeeded or Failed"
Sep  8 22:34:04.370: INFO: Trying to get logs from node node-3 pod downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab container client-container: <nil>
STEP: delete the pod 09/08/23 22:34:04.392
Sep  8 22:34:04.426: INFO: Waiting for pod downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab to disappear
Sep  8 22:34:04.434: INFO: Pod downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 22:34:04.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9999" for this suite. 09/08/23 22:34:04.448
------------------------------
â€¢ [4.219 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:34:00.249
    Sep  8 22:34:00.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 22:34:00.25
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:00.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:00.294
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:34:00.299
    Sep  8 22:34:00.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab" in namespace "downward-api-9999" to be "Succeeded or Failed"
    Sep  8 22:34:00.338: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab": Phase="Pending", Reason="", readiness=false. Elapsed: 15.395889ms
    Sep  8 22:34:02.349: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026356026s
    Sep  8 22:34:04.350: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026845338s
    STEP: Saw pod success 09/08/23 22:34:04.35
    Sep  8 22:34:04.350: INFO: Pod "downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab" satisfied condition "Succeeded or Failed"
    Sep  8 22:34:04.370: INFO: Trying to get logs from node node-3 pod downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab container client-container: <nil>
    STEP: delete the pod 09/08/23 22:34:04.392
    Sep  8 22:34:04.426: INFO: Waiting for pod downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab to disappear
    Sep  8 22:34:04.434: INFO: Pod downwardapi-volume-6971f011-27e4-48ed-a0ce-b19a2d5f5aab no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:34:04.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9999" for this suite. 09/08/23 22:34:04.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:34:04.472
Sep  8 22:34:04.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename watch 09/08/23 22:34:04.473
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:04.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:04.524
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 09/08/23 22:34:04.536
STEP: creating a new configmap 09/08/23 22:34:04.54
STEP: modifying the configmap once 09/08/23 22:34:04.584
STEP: closing the watch once it receives two notifications 09/08/23 22:34:04.61
Sep  8 22:34:04.610: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48738 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:34:04.611: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48740 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 09/08/23 22:34:04.611
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/08/23 22:34:04.634
STEP: deleting the configmap 09/08/23 22:34:04.639
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/08/23 22:34:04.653
Sep  8 22:34:04.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48741 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  8 22:34:04.658: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48742 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Sep  8 22:34:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3284" for this suite. 09/08/23 22:34:04.67
------------------------------
â€¢ [0.218 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:34:04.472
    Sep  8 22:34:04.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename watch 09/08/23 22:34:04.473
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:04.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:04.524
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 09/08/23 22:34:04.536
    STEP: creating a new configmap 09/08/23 22:34:04.54
    STEP: modifying the configmap once 09/08/23 22:34:04.584
    STEP: closing the watch once it receives two notifications 09/08/23 22:34:04.61
    Sep  8 22:34:04.610: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48738 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:34:04.611: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48740 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 09/08/23 22:34:04.611
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 09/08/23 22:34:04.634
    STEP: deleting the configmap 09/08/23 22:34:04.639
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 09/08/23 22:34:04.653
    Sep  8 22:34:04.657: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48741 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Sep  8 22:34:04.658: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3284  2d77cc7c-697a-4e1f-ab40-0442f27c5530 48742 0 2023-09-08 22:34:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-09-08 22:34:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:34:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3284" for this suite. 09/08/23 22:34:04.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:34:04.691
Sep  8 22:34:04.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 22:34:04.693
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:04.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:04.751
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Sep  8 22:34:04.801: INFO: Waiting up to 5m0s for pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839" in namespace "pods-2281" to be "running and ready"
Sep  8 22:34:04.818: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839": Phase="Pending", Reason="", readiness=false. Elapsed: 17.183055ms
Sep  8 22:34:04.818: INFO: The phase of Pod server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:34:06.828: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027227944s
Sep  8 22:34:06.828: INFO: The phase of Pod server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:34:08.827: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839": Phase="Running", Reason="", readiness=true. Elapsed: 4.026332091s
Sep  8 22:34:08.827: INFO: The phase of Pod server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839 is Running (Ready = true)
Sep  8 22:34:08.827: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839" satisfied condition "running and ready"
Sep  8 22:34:08.925: INFO: Waiting up to 5m0s for pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999" in namespace "pods-2281" to be "Succeeded or Failed"
Sep  8 22:34:08.955: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Pending", Reason="", readiness=false. Elapsed: 30.141363ms
Sep  8 22:34:10.964: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039539215s
Sep  8 22:34:12.965: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040011596s
Sep  8 22:34:14.974: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049071184s
STEP: Saw pod success 09/08/23 22:34:14.982
Sep  8 22:34:14.983: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999" satisfied condition "Succeeded or Failed"
Sep  8 22:34:14.997: INFO: Trying to get logs from node node-3 pod client-envvars-f9b3797e-055a-4575-aee5-2cc163884999 container env3cont: <nil>
STEP: delete the pod 09/08/23 22:34:15.024
Sep  8 22:34:15.061: INFO: Waiting for pod client-envvars-f9b3797e-055a-4575-aee5-2cc163884999 to disappear
Sep  8 22:34:15.081: INFO: Pod client-envvars-f9b3797e-055a-4575-aee5-2cc163884999 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 22:34:15.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2281" for this suite. 09/08/23 22:34:15.093
------------------------------
â€¢ [SLOW TEST] [10.421 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:34:04.691
    Sep  8 22:34:04.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 22:34:04.693
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:04.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:04.751
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Sep  8 22:34:04.801: INFO: Waiting up to 5m0s for pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839" in namespace "pods-2281" to be "running and ready"
    Sep  8 22:34:04.818: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839": Phase="Pending", Reason="", readiness=false. Elapsed: 17.183055ms
    Sep  8 22:34:04.818: INFO: The phase of Pod server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:34:06.828: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027227944s
    Sep  8 22:34:06.828: INFO: The phase of Pod server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:34:08.827: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839": Phase="Running", Reason="", readiness=true. Elapsed: 4.026332091s
    Sep  8 22:34:08.827: INFO: The phase of Pod server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839 is Running (Ready = true)
    Sep  8 22:34:08.827: INFO: Pod "server-envvars-fb24aacf-5695-4977-beeb-ceb22745f839" satisfied condition "running and ready"
    Sep  8 22:34:08.925: INFO: Waiting up to 5m0s for pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999" in namespace "pods-2281" to be "Succeeded or Failed"
    Sep  8 22:34:08.955: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Pending", Reason="", readiness=false. Elapsed: 30.141363ms
    Sep  8 22:34:10.964: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039539215s
    Sep  8 22:34:12.965: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040011596s
    Sep  8 22:34:14.974: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049071184s
    STEP: Saw pod success 09/08/23 22:34:14.982
    Sep  8 22:34:14.983: INFO: Pod "client-envvars-f9b3797e-055a-4575-aee5-2cc163884999" satisfied condition "Succeeded or Failed"
    Sep  8 22:34:14.997: INFO: Trying to get logs from node node-3 pod client-envvars-f9b3797e-055a-4575-aee5-2cc163884999 container env3cont: <nil>
    STEP: delete the pod 09/08/23 22:34:15.024
    Sep  8 22:34:15.061: INFO: Waiting for pod client-envvars-f9b3797e-055a-4575-aee5-2cc163884999 to disappear
    Sep  8 22:34:15.081: INFO: Pod client-envvars-f9b3797e-055a-4575-aee5-2cc163884999 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:34:15.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2281" for this suite. 09/08/23 22:34:15.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:34:15.114
Sep  8 22:34:15.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 22:34:15.116
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:15.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:15.172
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-6176 09/08/23 22:34:15.177
STEP: creating service affinity-nodeport-transition in namespace services-6176 09/08/23 22:34:15.178
STEP: creating replication controller affinity-nodeport-transition in namespace services-6176 09/08/23 22:34:15.24
I0908 22:34:15.276502      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6176, replica count: 3
I0908 22:34:18.328549      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 22:34:18.359: INFO: Creating new exec pod
Sep  8 22:34:18.386: INFO: Waiting up to 5m0s for pod "execpod-affinityfzcg8" in namespace "services-6176" to be "running"
Sep  8 22:34:18.412: INFO: Pod "execpod-affinityfzcg8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.105811ms
Sep  8 22:34:20.424: INFO: Pod "execpod-affinityfzcg8": Phase="Running", Reason="", readiness=true. Elapsed: 2.038061429s
Sep  8 22:34:20.424: INFO: Pod "execpod-affinityfzcg8" satisfied condition "running"
Sep  8 22:34:21.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Sep  8 22:34:21.725: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep  8 22:34:21.725: INFO: stdout: ""
Sep  8 22:34:21.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 10.233.3.17 80'
Sep  8 22:34:22.027: INFO: stderr: "+ nc -v -z -w 2 10.233.3.17 80\nConnection to 10.233.3.17 80 port [tcp/http] succeeded!\n"
Sep  8 22:34:22.027: INFO: stdout: ""
Sep  8 22:34:22.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 32374'
Sep  8 22:34:22.279: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 32374\nConnection to 10.100.19.129 32374 port [tcp/*] succeeded!\n"
Sep  8 22:34:22.279: INFO: stdout: ""
Sep  8 22:34:22.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 32374'
Sep  8 22:34:22.512: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 32374\nConnection to 10.100.16.26 32374 port [tcp/*] succeeded!\n"
Sep  8 22:34:22.512: INFO: stdout: ""
Sep  8 22:34:22.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.19.129:32374/ ; done'
Sep  8 22:34:22.914: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n"
Sep  8 22:34:22.914: INFO: stdout: "\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b"
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
Sep  8 22:34:22.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.19.129:32374/ ; done'
Sep  8 22:34:23.363: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n"
Sep  8 22:34:23.363: INFO: stdout: "\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h"
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.364: INFO: Received response from host: affinity-nodeport-transition-4hg9h
Sep  8 22:34:23.364: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6176, will wait for the garbage collector to delete the pods 09/08/23 22:34:23.389
Sep  8 22:34:23.472: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.28549ms
Sep  8 22:34:23.574: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.827386ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 22:34:26.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6176" for this suite. 09/08/23 22:34:26.51
------------------------------
â€¢ [SLOW TEST] [11.424 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:34:15.114
    Sep  8 22:34:15.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 22:34:15.116
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:15.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:15.172
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-6176 09/08/23 22:34:15.177
    STEP: creating service affinity-nodeport-transition in namespace services-6176 09/08/23 22:34:15.178
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6176 09/08/23 22:34:15.24
    I0908 22:34:15.276502      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6176, replica count: 3
    I0908 22:34:18.328549      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 22:34:18.359: INFO: Creating new exec pod
    Sep  8 22:34:18.386: INFO: Waiting up to 5m0s for pod "execpod-affinityfzcg8" in namespace "services-6176" to be "running"
    Sep  8 22:34:18.412: INFO: Pod "execpod-affinityfzcg8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.105811ms
    Sep  8 22:34:20.424: INFO: Pod "execpod-affinityfzcg8": Phase="Running", Reason="", readiness=true. Elapsed: 2.038061429s
    Sep  8 22:34:20.424: INFO: Pod "execpod-affinityfzcg8" satisfied condition "running"
    Sep  8 22:34:21.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Sep  8 22:34:21.725: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Sep  8 22:34:21.725: INFO: stdout: ""
    Sep  8 22:34:21.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 10.233.3.17 80'
    Sep  8 22:34:22.027: INFO: stderr: "+ nc -v -z -w 2 10.233.3.17 80\nConnection to 10.233.3.17 80 port [tcp/http] succeeded!\n"
    Sep  8 22:34:22.027: INFO: stdout: ""
    Sep  8 22:34:22.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 10.100.19.129 32374'
    Sep  8 22:34:22.279: INFO: stderr: "+ nc -v -z -w 2 10.100.19.129 32374\nConnection to 10.100.19.129 32374 port [tcp/*] succeeded!\n"
    Sep  8 22:34:22.279: INFO: stdout: ""
    Sep  8 22:34:22.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c nc -v -z -w 2 10.100.16.26 32374'
    Sep  8 22:34:22.512: INFO: stderr: "+ nc -v -z -w 2 10.100.16.26 32374\nConnection to 10.100.16.26 32374 port [tcp/*] succeeded!\n"
    Sep  8 22:34:22.512: INFO: stdout: ""
    Sep  8 22:34:22.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.19.129:32374/ ; done'
    Sep  8 22:34:22.914: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n"
    Sep  8 22:34:22.914: INFO: stdout: "\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b\naffinity-nodeport-transition-ptgkm\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-5f54b"
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-ptgkm
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:22.914: INFO: Received response from host: affinity-nodeport-transition-5f54b
    Sep  8 22:34:22.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6176 exec execpod-affinityfzcg8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.19.129:32374/ ; done'
    Sep  8 22:34:23.363: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.19.129:32374/\n"
    Sep  8 22:34:23.363: INFO: stdout: "\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h\naffinity-nodeport-transition-4hg9h"
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.363: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.364: INFO: Received response from host: affinity-nodeport-transition-4hg9h
    Sep  8 22:34:23.364: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6176, will wait for the garbage collector to delete the pods 09/08/23 22:34:23.389
    Sep  8 22:34:23.472: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.28549ms
    Sep  8 22:34:23.574: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.827386ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:34:26.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6176" for this suite. 09/08/23 22:34:26.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:34:26.557
Sep  8 22:34:26.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 22:34:26.559
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:26.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:26.619
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 09/08/23 22:34:26.656
STEP: watching for Pod to be ready 09/08/23 22:34:26.677
Sep  8 22:34:26.680: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep  8 22:34:26.686: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
Sep  8 22:34:26.724: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
Sep  8 22:34:27.430: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
Sep  8 22:34:28.251: INFO: Found Pod pod-test in namespace pods-1935 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 09/08/23 22:34:28.263
STEP: getting the Pod and ensuring that it's patched 09/08/23 22:34:28.29
STEP: replacing the Pod's status Ready condition to False 09/08/23 22:34:28.298
STEP: check the Pod again to ensure its Ready conditions are False 09/08/23 22:34:28.331
STEP: deleting the Pod via a Collection with a LabelSelector 09/08/23 22:34:28.331
STEP: watching for the Pod to be deleted 09/08/23 22:34:28.359
Sep  8 22:34:28.362: INFO: observed event type MODIFIED
Sep  8 22:34:30.251: INFO: observed event type MODIFIED
Sep  8 22:34:30.673: INFO: observed event type MODIFIED
Sep  8 22:34:31.287: INFO: observed event type MODIFIED
Sep  8 22:34:31.307: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 22:34:31.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1935" for this suite. 09/08/23 22:34:31.358
------------------------------
â€¢ [4.817 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:34:26.557
    Sep  8 22:34:26.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 22:34:26.559
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:26.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:26.619
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 09/08/23 22:34:26.656
    STEP: watching for Pod to be ready 09/08/23 22:34:26.677
    Sep  8 22:34:26.680: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Sep  8 22:34:26.686: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
    Sep  8 22:34:26.724: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
    Sep  8 22:34:27.430: INFO: observed Pod pod-test in namespace pods-1935 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
    Sep  8 22:34:28.251: INFO: Found Pod pod-test in namespace pods-1935 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:34:26 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 09/08/23 22:34:28.263
    STEP: getting the Pod and ensuring that it's patched 09/08/23 22:34:28.29
    STEP: replacing the Pod's status Ready condition to False 09/08/23 22:34:28.298
    STEP: check the Pod again to ensure its Ready conditions are False 09/08/23 22:34:28.331
    STEP: deleting the Pod via a Collection with a LabelSelector 09/08/23 22:34:28.331
    STEP: watching for the Pod to be deleted 09/08/23 22:34:28.359
    Sep  8 22:34:28.362: INFO: observed event type MODIFIED
    Sep  8 22:34:30.251: INFO: observed event type MODIFIED
    Sep  8 22:34:30.673: INFO: observed event type MODIFIED
    Sep  8 22:34:31.287: INFO: observed event type MODIFIED
    Sep  8 22:34:31.307: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:34:31.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1935" for this suite. 09/08/23 22:34:31.358
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:34:31.374
Sep  8 22:34:31.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 22:34:31.376
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:31.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:31.412
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 09/08/23 22:34:31.425
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8986.svc.cluster.local;sleep 1; done
 09/08/23 22:34:31.454
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8986.svc.cluster.local;sleep 1; done
 09/08/23 22:34:31.454
STEP: creating a pod to probe DNS 09/08/23 22:34:31.454
STEP: submitting the pod to kubernetes 09/08/23 22:34:31.454
Sep  8 22:34:31.506: INFO: Waiting up to 15m0s for pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b" in namespace "dns-8986" to be "running"
Sep  8 22:34:31.522: INFO: Pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.708101ms
Sep  8 22:34:33.532: INFO: Pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.0260254s
Sep  8 22:34:33.532: INFO: Pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b" satisfied condition "running"
STEP: retrieving the pod 09/08/23 22:34:33.532
STEP: looking for the results for each expected name from probers 09/08/23 22:34:33.541
Sep  8 22:34:33.560: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.568: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.578: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.590: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.602: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.612: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.636: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.649: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
Sep  8 22:34:33.649: INFO: Lookups using dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8986.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8986.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local jessie_udp@dns-test-service-2.dns-8986.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8986.svc.cluster.local]

Sep  8 22:34:38.795: INFO: DNS probes using dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b succeeded

STEP: deleting the pod 09/08/23 22:34:38.795
STEP: deleting the test headless service 09/08/23 22:34:38.854
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 22:34:38.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8986" for this suite. 09/08/23 22:34:38.93
------------------------------
â€¢ [SLOW TEST] [7.574 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:34:31.374
    Sep  8 22:34:31.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 22:34:31.376
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:31.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:31.412
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 09/08/23 22:34:31.425
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8986.svc.cluster.local;sleep 1; done
     09/08/23 22:34:31.454
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8986.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8986.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8986.svc.cluster.local;sleep 1; done
     09/08/23 22:34:31.454
    STEP: creating a pod to probe DNS 09/08/23 22:34:31.454
    STEP: submitting the pod to kubernetes 09/08/23 22:34:31.454
    Sep  8 22:34:31.506: INFO: Waiting up to 15m0s for pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b" in namespace "dns-8986" to be "running"
    Sep  8 22:34:31.522: INFO: Pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.708101ms
    Sep  8 22:34:33.532: INFO: Pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.0260254s
    Sep  8 22:34:33.532: INFO: Pod "dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 22:34:33.532
    STEP: looking for the results for each expected name from probers 09/08/23 22:34:33.541
    Sep  8 22:34:33.560: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.568: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.578: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.590: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.602: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.612: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.636: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.649: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8986.svc.cluster.local from pod dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b: the server could not find the requested resource (get pods dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b)
    Sep  8 22:34:33.649: INFO: Lookups using dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8986.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8986.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8986.svc.cluster.local jessie_udp@dns-test-service-2.dns-8986.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8986.svc.cluster.local]

    Sep  8 22:34:38.795: INFO: DNS probes using dns-8986/dns-test-4c8e3d7f-8096-4667-9690-27967e683a4b succeeded

    STEP: deleting the pod 09/08/23 22:34:38.795
    STEP: deleting the test headless service 09/08/23 22:34:38.854
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:34:38.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8986" for this suite. 09/08/23 22:34:38.93
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:34:38.951
Sep  8 22:34:38.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename gc 09/08/23 22:34:38.953
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:38.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:39.01
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 09/08/23 22:34:39.033
STEP: delete the rc 09/08/23 22:34:44.062
STEP: wait for the rc to be deleted 09/08/23 22:34:44.104
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/08/23 22:34:49.122
STEP: Gathering metrics 09/08/23 22:35:19.157
Sep  8 22:35:19.224: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
Sep  8 22:35:19.235: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.494867ms
Sep  8 22:35:19.235: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
Sep  8 22:35:19.236: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
Sep  8 22:35:19.328: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Sep  8 22:35:19.328: INFO: Deleting pod "simpletest.rc-2kccp" in namespace "gc-1215"
Sep  8 22:35:19.429: INFO: Deleting pod "simpletest.rc-46vm6" in namespace "gc-1215"
Sep  8 22:35:19.486: INFO: Deleting pod "simpletest.rc-4dfh6" in namespace "gc-1215"
Sep  8 22:35:19.571: INFO: Deleting pod "simpletest.rc-57xw6" in namespace "gc-1215"
Sep  8 22:35:19.606: INFO: Deleting pod "simpletest.rc-5d2pj" in namespace "gc-1215"
Sep  8 22:35:19.646: INFO: Deleting pod "simpletest.rc-5px8p" in namespace "gc-1215"
Sep  8 22:35:19.686: INFO: Deleting pod "simpletest.rc-5qck2" in namespace "gc-1215"
Sep  8 22:35:19.741: INFO: Deleting pod "simpletest.rc-674td" in namespace "gc-1215"
Sep  8 22:35:19.781: INFO: Deleting pod "simpletest.rc-6cwc5" in namespace "gc-1215"
Sep  8 22:35:19.876: INFO: Deleting pod "simpletest.rc-6hm48" in namespace "gc-1215"
Sep  8 22:35:19.953: INFO: Deleting pod "simpletest.rc-6lcbd" in namespace "gc-1215"
Sep  8 22:35:20.000: INFO: Deleting pod "simpletest.rc-6n7sg" in namespace "gc-1215"
Sep  8 22:35:20.045: INFO: Deleting pod "simpletest.rc-6v6d6" in namespace "gc-1215"
Sep  8 22:35:20.097: INFO: Deleting pod "simpletest.rc-74tdb" in namespace "gc-1215"
Sep  8 22:35:20.193: INFO: Deleting pod "simpletest.rc-75qnr" in namespace "gc-1215"
Sep  8 22:35:20.238: INFO: Deleting pod "simpletest.rc-7gjqv" in namespace "gc-1215"
Sep  8 22:35:20.277: INFO: Deleting pod "simpletest.rc-7hfxr" in namespace "gc-1215"
Sep  8 22:35:20.324: INFO: Deleting pod "simpletest.rc-7s6g5" in namespace "gc-1215"
Sep  8 22:35:20.388: INFO: Deleting pod "simpletest.rc-85xl6" in namespace "gc-1215"
Sep  8 22:35:20.421: INFO: Deleting pod "simpletest.rc-8dqzn" in namespace "gc-1215"
Sep  8 22:35:20.485: INFO: Deleting pod "simpletest.rc-8qfsd" in namespace "gc-1215"
Sep  8 22:35:20.525: INFO: Deleting pod "simpletest.rc-8sv5r" in namespace "gc-1215"
Sep  8 22:35:20.572: INFO: Deleting pod "simpletest.rc-9qgzh" in namespace "gc-1215"
Sep  8 22:35:20.615: INFO: Deleting pod "simpletest.rc-b8qhd" in namespace "gc-1215"
Sep  8 22:35:20.659: INFO: Deleting pod "simpletest.rc-bn5st" in namespace "gc-1215"
Sep  8 22:35:20.716: INFO: Deleting pod "simpletest.rc-bx4jk" in namespace "gc-1215"
Sep  8 22:35:20.761: INFO: Deleting pod "simpletest.rc-c4kh4" in namespace "gc-1215"
Sep  8 22:35:20.799: INFO: Deleting pod "simpletest.rc-c8mml" in namespace "gc-1215"
Sep  8 22:35:20.869: INFO: Deleting pod "simpletest.rc-cskth" in namespace "gc-1215"
Sep  8 22:35:20.924: INFO: Deleting pod "simpletest.rc-cstcs" in namespace "gc-1215"
Sep  8 22:35:20.969: INFO: Deleting pod "simpletest.rc-cvfmb" in namespace "gc-1215"
Sep  8 22:35:21.041: INFO: Deleting pod "simpletest.rc-dvnrs" in namespace "gc-1215"
Sep  8 22:35:21.206: INFO: Deleting pod "simpletest.rc-dxj9l" in namespace "gc-1215"
Sep  8 22:35:21.299: INFO: Deleting pod "simpletest.rc-fcx5d" in namespace "gc-1215"
Sep  8 22:35:21.356: INFO: Deleting pod "simpletest.rc-fnsqr" in namespace "gc-1215"
Sep  8 22:35:21.418: INFO: Deleting pod "simpletest.rc-fpf4d" in namespace "gc-1215"
Sep  8 22:35:21.464: INFO: Deleting pod "simpletest.rc-g6rxr" in namespace "gc-1215"
Sep  8 22:35:21.510: INFO: Deleting pod "simpletest.rc-gvxqs" in namespace "gc-1215"
Sep  8 22:35:21.649: INFO: Deleting pod "simpletest.rc-gx2tv" in namespace "gc-1215"
Sep  8 22:35:21.771: INFO: Deleting pod "simpletest.rc-h62pv" in namespace "gc-1215"
Sep  8 22:35:21.841: INFO: Deleting pod "simpletest.rc-h928t" in namespace "gc-1215"
Sep  8 22:35:21.918: INFO: Deleting pod "simpletest.rc-hb86k" in namespace "gc-1215"
Sep  8 22:35:22.022: INFO: Deleting pod "simpletest.rc-hfpp5" in namespace "gc-1215"
Sep  8 22:35:22.086: INFO: Deleting pod "simpletest.rc-hvdnv" in namespace "gc-1215"
Sep  8 22:35:22.181: INFO: Deleting pod "simpletest.rc-hwgd9" in namespace "gc-1215"
Sep  8 22:35:22.298: INFO: Deleting pod "simpletest.rc-j9pfj" in namespace "gc-1215"
Sep  8 22:35:22.373: INFO: Deleting pod "simpletest.rc-jhqwb" in namespace "gc-1215"
Sep  8 22:35:22.454: INFO: Deleting pod "simpletest.rc-jjj79" in namespace "gc-1215"
Sep  8 22:35:22.540: INFO: Deleting pod "simpletest.rc-jwqvm" in namespace "gc-1215"
Sep  8 22:35:22.595: INFO: Deleting pod "simpletest.rc-jxf8r" in namespace "gc-1215"
Sep  8 22:35:22.664: INFO: Deleting pod "simpletest.rc-k2rsx" in namespace "gc-1215"
Sep  8 22:35:22.732: INFO: Deleting pod "simpletest.rc-k7wds" in namespace "gc-1215"
Sep  8 22:35:22.798: INFO: Deleting pod "simpletest.rc-ks6pg" in namespace "gc-1215"
Sep  8 22:35:22.886: INFO: Deleting pod "simpletest.rc-ktxhv" in namespace "gc-1215"
Sep  8 22:35:22.943: INFO: Deleting pod "simpletest.rc-l9zjj" in namespace "gc-1215"
Sep  8 22:35:23.006: INFO: Deleting pod "simpletest.rc-ldj56" in namespace "gc-1215"
Sep  8 22:35:23.092: INFO: Deleting pod "simpletest.rc-llcjh" in namespace "gc-1215"
Sep  8 22:35:23.140: INFO: Deleting pod "simpletest.rc-lw4qw" in namespace "gc-1215"
Sep  8 22:35:23.182: INFO: Deleting pod "simpletest.rc-lz2tb" in namespace "gc-1215"
Sep  8 22:35:23.263: INFO: Deleting pod "simpletest.rc-m5pjm" in namespace "gc-1215"
Sep  8 22:35:23.354: INFO: Deleting pod "simpletest.rc-m7t9s" in namespace "gc-1215"
Sep  8 22:35:23.440: INFO: Deleting pod "simpletest.rc-m99hg" in namespace "gc-1215"
Sep  8 22:35:23.492: INFO: Deleting pod "simpletest.rc-mjrln" in namespace "gc-1215"
Sep  8 22:35:23.612: INFO: Deleting pod "simpletest.rc-mnswj" in namespace "gc-1215"
Sep  8 22:35:23.687: INFO: Deleting pod "simpletest.rc-mqmm4" in namespace "gc-1215"
Sep  8 22:35:23.745: INFO: Deleting pod "simpletest.rc-mqmzd" in namespace "gc-1215"
Sep  8 22:35:23.790: INFO: Deleting pod "simpletest.rc-mst97" in namespace "gc-1215"
Sep  8 22:35:23.868: INFO: Deleting pod "simpletest.rc-mxk2s" in namespace "gc-1215"
Sep  8 22:35:23.942: INFO: Deleting pod "simpletest.rc-n774q" in namespace "gc-1215"
Sep  8 22:35:23.999: INFO: Deleting pod "simpletest.rc-nqsfj" in namespace "gc-1215"
Sep  8 22:35:24.038: INFO: Deleting pod "simpletest.rc-nvj44" in namespace "gc-1215"
Sep  8 22:35:24.088: INFO: Deleting pod "simpletest.rc-nxsvt" in namespace "gc-1215"
Sep  8 22:35:24.137: INFO: Deleting pod "simpletest.rc-pf962" in namespace "gc-1215"
Sep  8 22:35:24.200: INFO: Deleting pod "simpletest.rc-pfnvx" in namespace "gc-1215"
Sep  8 22:35:24.261: INFO: Deleting pod "simpletest.rc-ph4rw" in namespace "gc-1215"
Sep  8 22:35:24.294: INFO: Deleting pod "simpletest.rc-pwl5w" in namespace "gc-1215"
Sep  8 22:35:24.335: INFO: Deleting pod "simpletest.rc-pxsl5" in namespace "gc-1215"
Sep  8 22:35:24.382: INFO: Deleting pod "simpletest.rc-pz74m" in namespace "gc-1215"
Sep  8 22:35:24.434: INFO: Deleting pod "simpletest.rc-q54hl" in namespace "gc-1215"
Sep  8 22:35:24.477: INFO: Deleting pod "simpletest.rc-r4rvj" in namespace "gc-1215"
Sep  8 22:35:24.550: INFO: Deleting pod "simpletest.rc-r7lc5" in namespace "gc-1215"
Sep  8 22:35:24.624: INFO: Deleting pod "simpletest.rc-r8jq2" in namespace "gc-1215"
Sep  8 22:35:24.702: INFO: Deleting pod "simpletest.rc-r8ksm" in namespace "gc-1215"
Sep  8 22:35:24.786: INFO: Deleting pod "simpletest.rc-rj8gl" in namespace "gc-1215"
Sep  8 22:35:24.836: INFO: Deleting pod "simpletest.rc-rlqkb" in namespace "gc-1215"
Sep  8 22:35:24.912: INFO: Deleting pod "simpletest.rc-s77zj" in namespace "gc-1215"
Sep  8 22:35:25.094: INFO: Deleting pod "simpletest.rc-sm8fq" in namespace "gc-1215"
Sep  8 22:35:25.251: INFO: Deleting pod "simpletest.rc-tbwxs" in namespace "gc-1215"
Sep  8 22:35:25.311: INFO: Deleting pod "simpletest.rc-tfhvb" in namespace "gc-1215"
Sep  8 22:35:25.360: INFO: Deleting pod "simpletest.rc-tk2h7" in namespace "gc-1215"
Sep  8 22:35:25.408: INFO: Deleting pod "simpletest.rc-v2g67" in namespace "gc-1215"
Sep  8 22:35:25.455: INFO: Deleting pod "simpletest.rc-vn5d7" in namespace "gc-1215"
Sep  8 22:35:25.520: INFO: Deleting pod "simpletest.rc-vqb5v" in namespace "gc-1215"
Sep  8 22:35:25.564: INFO: Deleting pod "simpletest.rc-xb7gv" in namespace "gc-1215"
Sep  8 22:35:25.608: INFO: Deleting pod "simpletest.rc-xfmnc" in namespace "gc-1215"
Sep  8 22:35:25.655: INFO: Deleting pod "simpletest.rc-xq558" in namespace "gc-1215"
Sep  8 22:35:25.691: INFO: Deleting pod "simpletest.rc-xrqq6" in namespace "gc-1215"
Sep  8 22:35:25.750: INFO: Deleting pod "simpletest.rc-zpnzx" in namespace "gc-1215"
Sep  8 22:35:25.844: INFO: Deleting pod "simpletest.rc-zt9zd" in namespace "gc-1215"
Sep  8 22:35:25.922: INFO: Deleting pod "simpletest.rc-ztq55" in namespace "gc-1215"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  8 22:35:25.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1215" for this suite. 09/08/23 22:35:26.002
------------------------------
â€¢ [SLOW TEST] [47.078 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:34:38.951
    Sep  8 22:34:38.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename gc 09/08/23 22:34:38.953
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:34:38.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:34:39.01
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 09/08/23 22:34:39.033
    STEP: delete the rc 09/08/23 22:34:44.062
    STEP: wait for the rc to be deleted 09/08/23 22:34:44.104
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 09/08/23 22:34:49.122
    STEP: Gathering metrics 09/08/23 22:35:19.157
    Sep  8 22:35:19.224: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
    Sep  8 22:35:19.235: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.494867ms
    Sep  8 22:35:19.235: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
    Sep  8 22:35:19.236: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
    Sep  8 22:35:19.328: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Sep  8 22:35:19.328: INFO: Deleting pod "simpletest.rc-2kccp" in namespace "gc-1215"
    Sep  8 22:35:19.429: INFO: Deleting pod "simpletest.rc-46vm6" in namespace "gc-1215"
    Sep  8 22:35:19.486: INFO: Deleting pod "simpletest.rc-4dfh6" in namespace "gc-1215"
    Sep  8 22:35:19.571: INFO: Deleting pod "simpletest.rc-57xw6" in namespace "gc-1215"
    Sep  8 22:35:19.606: INFO: Deleting pod "simpletest.rc-5d2pj" in namespace "gc-1215"
    Sep  8 22:35:19.646: INFO: Deleting pod "simpletest.rc-5px8p" in namespace "gc-1215"
    Sep  8 22:35:19.686: INFO: Deleting pod "simpletest.rc-5qck2" in namespace "gc-1215"
    Sep  8 22:35:19.741: INFO: Deleting pod "simpletest.rc-674td" in namespace "gc-1215"
    Sep  8 22:35:19.781: INFO: Deleting pod "simpletest.rc-6cwc5" in namespace "gc-1215"
    Sep  8 22:35:19.876: INFO: Deleting pod "simpletest.rc-6hm48" in namespace "gc-1215"
    Sep  8 22:35:19.953: INFO: Deleting pod "simpletest.rc-6lcbd" in namespace "gc-1215"
    Sep  8 22:35:20.000: INFO: Deleting pod "simpletest.rc-6n7sg" in namespace "gc-1215"
    Sep  8 22:35:20.045: INFO: Deleting pod "simpletest.rc-6v6d6" in namespace "gc-1215"
    Sep  8 22:35:20.097: INFO: Deleting pod "simpletest.rc-74tdb" in namespace "gc-1215"
    Sep  8 22:35:20.193: INFO: Deleting pod "simpletest.rc-75qnr" in namespace "gc-1215"
    Sep  8 22:35:20.238: INFO: Deleting pod "simpletest.rc-7gjqv" in namespace "gc-1215"
    Sep  8 22:35:20.277: INFO: Deleting pod "simpletest.rc-7hfxr" in namespace "gc-1215"
    Sep  8 22:35:20.324: INFO: Deleting pod "simpletest.rc-7s6g5" in namespace "gc-1215"
    Sep  8 22:35:20.388: INFO: Deleting pod "simpletest.rc-85xl6" in namespace "gc-1215"
    Sep  8 22:35:20.421: INFO: Deleting pod "simpletest.rc-8dqzn" in namespace "gc-1215"
    Sep  8 22:35:20.485: INFO: Deleting pod "simpletest.rc-8qfsd" in namespace "gc-1215"
    Sep  8 22:35:20.525: INFO: Deleting pod "simpletest.rc-8sv5r" in namespace "gc-1215"
    Sep  8 22:35:20.572: INFO: Deleting pod "simpletest.rc-9qgzh" in namespace "gc-1215"
    Sep  8 22:35:20.615: INFO: Deleting pod "simpletest.rc-b8qhd" in namespace "gc-1215"
    Sep  8 22:35:20.659: INFO: Deleting pod "simpletest.rc-bn5st" in namespace "gc-1215"
    Sep  8 22:35:20.716: INFO: Deleting pod "simpletest.rc-bx4jk" in namespace "gc-1215"
    Sep  8 22:35:20.761: INFO: Deleting pod "simpletest.rc-c4kh4" in namespace "gc-1215"
    Sep  8 22:35:20.799: INFO: Deleting pod "simpletest.rc-c8mml" in namespace "gc-1215"
    Sep  8 22:35:20.869: INFO: Deleting pod "simpletest.rc-cskth" in namespace "gc-1215"
    Sep  8 22:35:20.924: INFO: Deleting pod "simpletest.rc-cstcs" in namespace "gc-1215"
    Sep  8 22:35:20.969: INFO: Deleting pod "simpletest.rc-cvfmb" in namespace "gc-1215"
    Sep  8 22:35:21.041: INFO: Deleting pod "simpletest.rc-dvnrs" in namespace "gc-1215"
    Sep  8 22:35:21.206: INFO: Deleting pod "simpletest.rc-dxj9l" in namespace "gc-1215"
    Sep  8 22:35:21.299: INFO: Deleting pod "simpletest.rc-fcx5d" in namespace "gc-1215"
    Sep  8 22:35:21.356: INFO: Deleting pod "simpletest.rc-fnsqr" in namespace "gc-1215"
    Sep  8 22:35:21.418: INFO: Deleting pod "simpletest.rc-fpf4d" in namespace "gc-1215"
    Sep  8 22:35:21.464: INFO: Deleting pod "simpletest.rc-g6rxr" in namespace "gc-1215"
    Sep  8 22:35:21.510: INFO: Deleting pod "simpletest.rc-gvxqs" in namespace "gc-1215"
    Sep  8 22:35:21.649: INFO: Deleting pod "simpletest.rc-gx2tv" in namespace "gc-1215"
    Sep  8 22:35:21.771: INFO: Deleting pod "simpletest.rc-h62pv" in namespace "gc-1215"
    Sep  8 22:35:21.841: INFO: Deleting pod "simpletest.rc-h928t" in namespace "gc-1215"
    Sep  8 22:35:21.918: INFO: Deleting pod "simpletest.rc-hb86k" in namespace "gc-1215"
    Sep  8 22:35:22.022: INFO: Deleting pod "simpletest.rc-hfpp5" in namespace "gc-1215"
    Sep  8 22:35:22.086: INFO: Deleting pod "simpletest.rc-hvdnv" in namespace "gc-1215"
    Sep  8 22:35:22.181: INFO: Deleting pod "simpletest.rc-hwgd9" in namespace "gc-1215"
    Sep  8 22:35:22.298: INFO: Deleting pod "simpletest.rc-j9pfj" in namespace "gc-1215"
    Sep  8 22:35:22.373: INFO: Deleting pod "simpletest.rc-jhqwb" in namespace "gc-1215"
    Sep  8 22:35:22.454: INFO: Deleting pod "simpletest.rc-jjj79" in namespace "gc-1215"
    Sep  8 22:35:22.540: INFO: Deleting pod "simpletest.rc-jwqvm" in namespace "gc-1215"
    Sep  8 22:35:22.595: INFO: Deleting pod "simpletest.rc-jxf8r" in namespace "gc-1215"
    Sep  8 22:35:22.664: INFO: Deleting pod "simpletest.rc-k2rsx" in namespace "gc-1215"
    Sep  8 22:35:22.732: INFO: Deleting pod "simpletest.rc-k7wds" in namespace "gc-1215"
    Sep  8 22:35:22.798: INFO: Deleting pod "simpletest.rc-ks6pg" in namespace "gc-1215"
    Sep  8 22:35:22.886: INFO: Deleting pod "simpletest.rc-ktxhv" in namespace "gc-1215"
    Sep  8 22:35:22.943: INFO: Deleting pod "simpletest.rc-l9zjj" in namespace "gc-1215"
    Sep  8 22:35:23.006: INFO: Deleting pod "simpletest.rc-ldj56" in namespace "gc-1215"
    Sep  8 22:35:23.092: INFO: Deleting pod "simpletest.rc-llcjh" in namespace "gc-1215"
    Sep  8 22:35:23.140: INFO: Deleting pod "simpletest.rc-lw4qw" in namespace "gc-1215"
    Sep  8 22:35:23.182: INFO: Deleting pod "simpletest.rc-lz2tb" in namespace "gc-1215"
    Sep  8 22:35:23.263: INFO: Deleting pod "simpletest.rc-m5pjm" in namespace "gc-1215"
    Sep  8 22:35:23.354: INFO: Deleting pod "simpletest.rc-m7t9s" in namespace "gc-1215"
    Sep  8 22:35:23.440: INFO: Deleting pod "simpletest.rc-m99hg" in namespace "gc-1215"
    Sep  8 22:35:23.492: INFO: Deleting pod "simpletest.rc-mjrln" in namespace "gc-1215"
    Sep  8 22:35:23.612: INFO: Deleting pod "simpletest.rc-mnswj" in namespace "gc-1215"
    Sep  8 22:35:23.687: INFO: Deleting pod "simpletest.rc-mqmm4" in namespace "gc-1215"
    Sep  8 22:35:23.745: INFO: Deleting pod "simpletest.rc-mqmzd" in namespace "gc-1215"
    Sep  8 22:35:23.790: INFO: Deleting pod "simpletest.rc-mst97" in namespace "gc-1215"
    Sep  8 22:35:23.868: INFO: Deleting pod "simpletest.rc-mxk2s" in namespace "gc-1215"
    Sep  8 22:35:23.942: INFO: Deleting pod "simpletest.rc-n774q" in namespace "gc-1215"
    Sep  8 22:35:23.999: INFO: Deleting pod "simpletest.rc-nqsfj" in namespace "gc-1215"
    Sep  8 22:35:24.038: INFO: Deleting pod "simpletest.rc-nvj44" in namespace "gc-1215"
    Sep  8 22:35:24.088: INFO: Deleting pod "simpletest.rc-nxsvt" in namespace "gc-1215"
    Sep  8 22:35:24.137: INFO: Deleting pod "simpletest.rc-pf962" in namespace "gc-1215"
    Sep  8 22:35:24.200: INFO: Deleting pod "simpletest.rc-pfnvx" in namespace "gc-1215"
    Sep  8 22:35:24.261: INFO: Deleting pod "simpletest.rc-ph4rw" in namespace "gc-1215"
    Sep  8 22:35:24.294: INFO: Deleting pod "simpletest.rc-pwl5w" in namespace "gc-1215"
    Sep  8 22:35:24.335: INFO: Deleting pod "simpletest.rc-pxsl5" in namespace "gc-1215"
    Sep  8 22:35:24.382: INFO: Deleting pod "simpletest.rc-pz74m" in namespace "gc-1215"
    Sep  8 22:35:24.434: INFO: Deleting pod "simpletest.rc-q54hl" in namespace "gc-1215"
    Sep  8 22:35:24.477: INFO: Deleting pod "simpletest.rc-r4rvj" in namespace "gc-1215"
    Sep  8 22:35:24.550: INFO: Deleting pod "simpletest.rc-r7lc5" in namespace "gc-1215"
    Sep  8 22:35:24.624: INFO: Deleting pod "simpletest.rc-r8jq2" in namespace "gc-1215"
    Sep  8 22:35:24.702: INFO: Deleting pod "simpletest.rc-r8ksm" in namespace "gc-1215"
    Sep  8 22:35:24.786: INFO: Deleting pod "simpletest.rc-rj8gl" in namespace "gc-1215"
    Sep  8 22:35:24.836: INFO: Deleting pod "simpletest.rc-rlqkb" in namespace "gc-1215"
    Sep  8 22:35:24.912: INFO: Deleting pod "simpletest.rc-s77zj" in namespace "gc-1215"
    Sep  8 22:35:25.094: INFO: Deleting pod "simpletest.rc-sm8fq" in namespace "gc-1215"
    Sep  8 22:35:25.251: INFO: Deleting pod "simpletest.rc-tbwxs" in namespace "gc-1215"
    Sep  8 22:35:25.311: INFO: Deleting pod "simpletest.rc-tfhvb" in namespace "gc-1215"
    Sep  8 22:35:25.360: INFO: Deleting pod "simpletest.rc-tk2h7" in namespace "gc-1215"
    Sep  8 22:35:25.408: INFO: Deleting pod "simpletest.rc-v2g67" in namespace "gc-1215"
    Sep  8 22:35:25.455: INFO: Deleting pod "simpletest.rc-vn5d7" in namespace "gc-1215"
    Sep  8 22:35:25.520: INFO: Deleting pod "simpletest.rc-vqb5v" in namespace "gc-1215"
    Sep  8 22:35:25.564: INFO: Deleting pod "simpletest.rc-xb7gv" in namespace "gc-1215"
    Sep  8 22:35:25.608: INFO: Deleting pod "simpletest.rc-xfmnc" in namespace "gc-1215"
    Sep  8 22:35:25.655: INFO: Deleting pod "simpletest.rc-xq558" in namespace "gc-1215"
    Sep  8 22:35:25.691: INFO: Deleting pod "simpletest.rc-xrqq6" in namespace "gc-1215"
    Sep  8 22:35:25.750: INFO: Deleting pod "simpletest.rc-zpnzx" in namespace "gc-1215"
    Sep  8 22:35:25.844: INFO: Deleting pod "simpletest.rc-zt9zd" in namespace "gc-1215"
    Sep  8 22:35:25.922: INFO: Deleting pod "simpletest.rc-ztq55" in namespace "gc-1215"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:35:25.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1215" for this suite. 09/08/23 22:35:26.002
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:35:26.029
Sep  8 22:35:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 22:35:26.049
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:35:26.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:35:26.218
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6237 09/08/23 22:35:26.24
STEP: changing the ExternalName service to type=ClusterIP 09/08/23 22:35:26.272
STEP: creating replication controller externalname-service in namespace services-6237 09/08/23 22:35:26.378
I0908 22:35:26.422798      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6237, replica count: 2
I0908 22:35:29.476619      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0908 22:35:32.477272      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0908 22:35:35.477860      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0908 22:35:38.478674      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0908 22:35:41.480497      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 22:35:41.480: INFO: Creating new exec pod
Sep  8 22:35:41.510: INFO: Waiting up to 5m0s for pod "execpodbcl8m" in namespace "services-6237" to be "running"
Sep  8 22:35:41.540: INFO: Pod "execpodbcl8m": Phase="Pending", Reason="", readiness=false. Elapsed: 29.901254ms
Sep  8 22:35:43.552: INFO: Pod "execpodbcl8m": Phase="Running", Reason="", readiness=true. Elapsed: 2.041027321s
Sep  8 22:35:43.552: INFO: Pod "execpodbcl8m" satisfied condition "running"
Sep  8 22:35:44.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6237 exec execpodbcl8m -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Sep  8 22:35:44.789: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  8 22:35:44.789: INFO: stdout: ""
Sep  8 22:35:44.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6237 exec execpodbcl8m -- /bin/sh -x -c nc -v -z -w 2 10.233.38.242 80'
Sep  8 22:35:45.025: INFO: stderr: "+ nc -v -z -w 2 10.233.38.242 80\nConnection to 10.233.38.242 80 port [tcp/http] succeeded!\n"
Sep  8 22:35:45.025: INFO: stdout: ""
Sep  8 22:35:45.025: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 22:35:45.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6237" for this suite. 09/08/23 22:35:45.133
------------------------------
â€¢ [SLOW TEST] [19.125 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:35:26.029
    Sep  8 22:35:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 22:35:26.049
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:35:26.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:35:26.218
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6237 09/08/23 22:35:26.24
    STEP: changing the ExternalName service to type=ClusterIP 09/08/23 22:35:26.272
    STEP: creating replication controller externalname-service in namespace services-6237 09/08/23 22:35:26.378
    I0908 22:35:26.422798      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6237, replica count: 2
    I0908 22:35:29.476619      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0908 22:35:32.477272      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0908 22:35:35.477860      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0908 22:35:38.478674      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0908 22:35:41.480497      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 22:35:41.480: INFO: Creating new exec pod
    Sep  8 22:35:41.510: INFO: Waiting up to 5m0s for pod "execpodbcl8m" in namespace "services-6237" to be "running"
    Sep  8 22:35:41.540: INFO: Pod "execpodbcl8m": Phase="Pending", Reason="", readiness=false. Elapsed: 29.901254ms
    Sep  8 22:35:43.552: INFO: Pod "execpodbcl8m": Phase="Running", Reason="", readiness=true. Elapsed: 2.041027321s
    Sep  8 22:35:43.552: INFO: Pod "execpodbcl8m" satisfied condition "running"
    Sep  8 22:35:44.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6237 exec execpodbcl8m -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Sep  8 22:35:44.789: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Sep  8 22:35:44.789: INFO: stdout: ""
    Sep  8 22:35:44.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-6237 exec execpodbcl8m -- /bin/sh -x -c nc -v -z -w 2 10.233.38.242 80'
    Sep  8 22:35:45.025: INFO: stderr: "+ nc -v -z -w 2 10.233.38.242 80\nConnection to 10.233.38.242 80 port [tcp/http] succeeded!\n"
    Sep  8 22:35:45.025: INFO: stdout: ""
    Sep  8 22:35:45.025: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:35:45.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6237" for this suite. 09/08/23 22:35:45.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:35:45.155
Sep  8 22:35:45.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 22:35:45.158
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:35:45.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:35:45.209
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 09/08/23 22:35:45.215
STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:35:45.237
STEP: Creating a ResourceQuota with not terminating scope 09/08/23 22:35:47.27
STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:35:47.282
STEP: Creating a long running pod 09/08/23 22:35:49.293
STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/08/23 22:35:49.343
STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/08/23 22:35:51.352
STEP: Deleting the pod 09/08/23 22:35:53.361
STEP: Ensuring resource quota status released the pod usage 09/08/23 22:35:53.399
STEP: Creating a terminating pod 09/08/23 22:35:55.411
STEP: Ensuring resource quota with terminating scope captures the pod usage 09/08/23 22:35:55.455
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/08/23 22:35:57.465
STEP: Deleting the pod 09/08/23 22:35:59.479
STEP: Ensuring resource quota status released the pod usage 09/08/23 22:35:59.525
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 22:36:01.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2823" for this suite. 09/08/23 22:36:01.57
------------------------------
â€¢ [SLOW TEST] [16.444 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:35:45.155
    Sep  8 22:35:45.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 22:35:45.158
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:35:45.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:35:45.209
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 09/08/23 22:35:45.215
    STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:35:45.237
    STEP: Creating a ResourceQuota with not terminating scope 09/08/23 22:35:47.27
    STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:35:47.282
    STEP: Creating a long running pod 09/08/23 22:35:49.293
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 09/08/23 22:35:49.343
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 09/08/23 22:35:51.352
    STEP: Deleting the pod 09/08/23 22:35:53.361
    STEP: Ensuring resource quota status released the pod usage 09/08/23 22:35:53.399
    STEP: Creating a terminating pod 09/08/23 22:35:55.411
    STEP: Ensuring resource quota with terminating scope captures the pod usage 09/08/23 22:35:55.455
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 09/08/23 22:35:57.465
    STEP: Deleting the pod 09/08/23 22:35:59.479
    STEP: Ensuring resource quota status released the pod usage 09/08/23 22:35:59.525
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:36:01.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2823" for this suite. 09/08/23 22:36:01.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:36:01.601
Sep  8 22:36:01.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 22:36:01.602
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:01.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:01.678
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 09/08/23 22:36:01.691
Sep  8 22:36:01.743: INFO: Waiting up to 5m0s for pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82" in namespace "emptydir-3528" to be "Succeeded or Failed"
Sep  8 22:36:01.776: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Pending", Reason="", readiness=false. Elapsed: 33.45062ms
Sep  8 22:36:03.791: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Running", Reason="", readiness=true. Elapsed: 2.048737063s
Sep  8 22:36:05.790: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Running", Reason="", readiness=false. Elapsed: 4.046983079s
Sep  8 22:36:07.795: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052386632s
STEP: Saw pod success 09/08/23 22:36:07.795
Sep  8 22:36:07.795: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82" satisfied condition "Succeeded or Failed"
Sep  8 22:36:07.807: INFO: Trying to get logs from node node-3 pod pod-196d6f55-db69-4c7a-b49b-224b29994c82 container test-container: <nil>
STEP: delete the pod 09/08/23 22:36:07.866
Sep  8 22:36:07.901: INFO: Waiting for pod pod-196d6f55-db69-4c7a-b49b-224b29994c82 to disappear
Sep  8 22:36:07.912: INFO: Pod pod-196d6f55-db69-4c7a-b49b-224b29994c82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:36:07.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3528" for this suite. 09/08/23 22:36:07.924
------------------------------
â€¢ [SLOW TEST] [6.347 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:36:01.601
    Sep  8 22:36:01.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 22:36:01.602
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:01.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:01.678
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 09/08/23 22:36:01.691
    Sep  8 22:36:01.743: INFO: Waiting up to 5m0s for pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82" in namespace "emptydir-3528" to be "Succeeded or Failed"
    Sep  8 22:36:01.776: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Pending", Reason="", readiness=false. Elapsed: 33.45062ms
    Sep  8 22:36:03.791: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Running", Reason="", readiness=true. Elapsed: 2.048737063s
    Sep  8 22:36:05.790: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Running", Reason="", readiness=false. Elapsed: 4.046983079s
    Sep  8 22:36:07.795: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052386632s
    STEP: Saw pod success 09/08/23 22:36:07.795
    Sep  8 22:36:07.795: INFO: Pod "pod-196d6f55-db69-4c7a-b49b-224b29994c82" satisfied condition "Succeeded or Failed"
    Sep  8 22:36:07.807: INFO: Trying to get logs from node node-3 pod pod-196d6f55-db69-4c7a-b49b-224b29994c82 container test-container: <nil>
    STEP: delete the pod 09/08/23 22:36:07.866
    Sep  8 22:36:07.901: INFO: Waiting for pod pod-196d6f55-db69-4c7a-b49b-224b29994c82 to disappear
    Sep  8 22:36:07.912: INFO: Pod pod-196d6f55-db69-4c7a-b49b-224b29994c82 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:36:07.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3528" for this suite. 09/08/23 22:36:07.924
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:36:07.95
Sep  8 22:36:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename deployment 09/08/23 22:36:07.951
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:08.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:08.048
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Sep  8 22:36:08.058: INFO: Creating deployment "webserver-deployment"
Sep  8 22:36:08.075: INFO: Waiting for observed generation 1
Sep  8 22:36:10.116: INFO: Waiting for all required pods to come up
Sep  8 22:36:10.127: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 09/08/23 22:36:10.128
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xt8zf" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6dfc4" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-h2xkn" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mhpsh" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6tsft" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rv24s" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tcvzw" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qth8t" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-22vjl" in namespace "deployment-651" to be "running"
Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh": Phase="Pending", Reason="", readiness=false. Elapsed: 16.765795ms
Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn": Phase="Pending", Reason="", readiness=false. Elapsed: 16.868667ms
Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-xt8zf": Phase="Pending", Reason="", readiness=false. Elapsed: 17.104197ms
Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw": Phase="Pending", Reason="", readiness=false. Elapsed: 17.042022ms
Sep  8 22:36:10.146: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft": Phase="Pending", Reason="", readiness=false. Elapsed: 18.370511ms
Sep  8 22:36:10.157: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.781405ms
Sep  8 22:36:10.157: INFO: Pod "webserver-deployment-7f5969cbc7-22vjl": Phase="Pending", Reason="", readiness=false. Elapsed: 27.699124ms
Sep  8 22:36:10.157: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t": Phase="Pending", Reason="", readiness=false. Elapsed: 28.590859ms
Sep  8 22:36:10.164: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s": Phase="Pending", Reason="", readiness=false. Elapsed: 35.78146ms
Sep  8 22:36:12.154: INFO: Pod "webserver-deployment-7f5969cbc7-xt8zf": Phase="Running", Reason="", readiness=true. Elapsed: 2.026629537s
Sep  8 22:36:12.154: INFO: Pod "webserver-deployment-7f5969cbc7-xt8zf" satisfied condition "running"
Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t": Phase="Running", Reason="", readiness=true. Elapsed: 2.05055163s
Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t" satisfied condition "running"
Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh": Phase="Running", Reason="", readiness=true. Elapsed: 2.051088142s
Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh" satisfied condition "running"
Sep  8 22:36:12.180: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft": Phase="Running", Reason="", readiness=true. Elapsed: 2.051466798s
Sep  8 22:36:12.180: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft" satisfied condition "running"
Sep  8 22:36:12.186: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn": Phase="Running", Reason="", readiness=true. Elapsed: 2.05816925s
Sep  8 22:36:12.186: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn" satisfied condition "running"
Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw": Phase="Running", Reason="", readiness=true. Elapsed: 2.058306697s
Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw" satisfied condition "running"
Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-22vjl": Phase="Running", Reason="", readiness=true. Elapsed: 2.05786356s
Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-22vjl" satisfied condition "running"
Sep  8 22:36:12.193: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.064934769s
Sep  8 22:36:12.193: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4" satisfied condition "running"
Sep  8 22:36:12.196: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s": Phase="Running", Reason="", readiness=true. Elapsed: 2.06817689s
Sep  8 22:36:12.196: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s" satisfied condition "running"
Sep  8 22:36:12.196: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  8 22:36:12.243: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  8 22:36:12.294: INFO: Updating deployment webserver-deployment
Sep  8 22:36:12.295: INFO: Waiting for observed generation 2
Sep  8 22:36:14.336: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  8 22:36:14.344: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  8 22:36:14.356: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  8 22:36:14.394: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  8 22:36:14.394: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  8 22:36:14.406: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  8 22:36:14.428: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  8 22:36:14.428: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  8 22:36:14.461: INFO: Updating deployment webserver-deployment
Sep  8 22:36:14.461: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  8 22:36:14.491: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  8 22:36:16.531: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Sep  8 22:36:16.569: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-651  7ace6889-9be1-4a9e-8fe2-a2796c399a88 52464 3 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006fbee08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-08 22:36:14 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-08 22:36:14 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  8 22:36:16.602: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-651  c73ba0d5-769a-4dd4-9b74-9316ffd6c555 52461 3 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7ace6889-9be1-4a9e-8fe2-a2796c399a88 0xc0079ac967 0xc0079ac968}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ace6889-9be1-4a9e-8fe2-a2796c399a88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079aca08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  8 22:36:16.602: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  8 22:36:16.602: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-651  f5350933-23ea-45bf-bad5-aa86608fd626 52452 3 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7ace6889-9be1-4a9e-8fe2-a2796c399a88 0xc0079ac877 0xc0079ac878}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ace6889-9be1-4a9e-8fe2-a2796c399a88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079ac908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  8 22:36:16.627: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6dfc4 webserver-deployment-7f5969cbc7- deployment-651  a5615df7-b517-4905-bd89-f804cbb9a387 52233 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8931f94b75e311eceb54f81672179efc6132cd26e176dee47df7f319c606ef2b cni.projectcalico.org/podIP:10.233.75.112/32 cni.projectcalico.org/podIPs:10.233.75.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf227 0xc006fbf228}] [] [{calico Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7c5sg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7c5sg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.112,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0801f24c0d5da72d4c8bb2eeeedfc95dbc23b805371ebf26037e1731eb30f6e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.628: INFO: Pod "webserver-deployment-7f5969cbc7-6msnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6msnj webserver-deployment-7f5969cbc7- deployment-651  17532794-a278-4f97-8069-5ba97cf2ca24 52488 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b36a55479ac59521a14f5720186132c8d8e886dcd1e0edf823f9ffe1aa9df5b9 cni.projectcalico.org/podIP:10.233.75.107/32 cni.projectcalico.org/podIPs:10.233.75.107/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf447 0xc006fbf448}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stgxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stgxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.628: INFO: Pod "webserver-deployment-7f5969cbc7-6sn5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6sn5f webserver-deployment-7f5969cbc7- deployment-651  9f7b3d38-9d77-4d50-a983-6b8fbdcaeba5 52498 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1d6bb2ad19709e8ba27a304627acfab13ccadecee997b19c93ff0e4c26869a5f cni.projectcalico.org/podIP:10.233.75.127/32 cni.projectcalico.org/podIPs:10.233.75.127/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf657 0xc006fbf658}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmqc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmqc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6tsft webserver-deployment-7f5969cbc7- deployment-651  dc9747b6-39b6-40ec-a400-23b5507f30ea 52272 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:da6902feed151752840f2edf8f5ff54aa0083b72ce8a5987ef2576a52b74e9cf cni.projectcalico.org/podIP:10.233.75.105/32 cni.projectcalico.org/podIPs:10.233.75.105/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf877 0xc006fbf878}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v42vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v42vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.105,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cba459d436e5959c520e922e814707d1416db8510d1990e0bb0d10a10267b73b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-7n7r5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7n7r5 webserver-deployment-7f5969cbc7- deployment-651  4cefc38b-884a-4fab-ae35-18528fdf1707 52492 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c15e5aa954605e1b1e61b9ab011d137109356fd658a2327e59ccfc6eb78a8d0f cni.projectcalico.org/podIP:10.233.103.92/32 cni.projectcalico.org/podIPs:10.233.103.92/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbfaa7 0xc006fbfaa8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cxck2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cxck2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-clmbr" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-clmbr webserver-deployment-7f5969cbc7- deployment-651  8f3a609f-5729-46d4-94d1-74b774f18189 52438 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbfc97 0xc006fbfc98}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zcg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zcg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-cncnp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cncnp webserver-deployment-7f5969cbc7- deployment-651  0880b631-953e-4b03-9046-d046128191b4 52436 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbfe00 0xc006fbfe01}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lwcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lwcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-fpr5s" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fpr5s webserver-deployment-7f5969cbc7- deployment-651  53be9a2e-d7c9-49ca-bfdf-15fd783b6044 52207 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:766dbe084921a7c13160eb6607a1894eda6d0d695d14fb7d3c0e3bb44ca699a8 cni.projectcalico.org/podIP:10.233.103.107/32 cni.projectcalico.org/podIPs:10.233.103.107/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbff60 0xc006fbff61}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2dgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2dgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.107,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1534decd9371881ea6dd1cdf816ffa4fb96f5e5579c45d141cbe2650427e23dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-gvmb2" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gvmb2 webserver-deployment-7f5969cbc7- deployment-651  9985e9df-75d3-4521-8b67-9deedc7d8c77 52467 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0167 0xc0052d0168}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wdqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wdqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.630: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h2xkn webserver-deployment-7f5969cbc7- deployment-651  6bcc88f4-e090-4ac1-ad38-0556c89e4899 52278 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0931d0e1cf87e241e135e15f7966ec9121301919f0c962105aac858efb17891c cni.projectcalico.org/podIP:10.233.103.114/32 cni.projectcalico.org/podIPs:10.233.103.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0347 0xc0052d0348}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtbcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtbcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.114,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://704335240a82cc694105d8bee8dffd8942e9dbc19a1dae093ff0ec83a9e79d9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.630: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mhpsh webserver-deployment-7f5969cbc7- deployment-651  eda9b7b4-1c20-4670-83ac-496d985a3266 52275 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0f10f22c888710f7b7384f71e1f716797c00021fb9c8e143a270e2ff44780b09 cni.projectcalico.org/podIP:10.233.103.94/32 cni.projectcalico.org/podIPs:10.233.103.94/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0587 0xc0052d0588}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drjfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drjfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.94,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://57ebcd8bdd932c16734638dd94d67bda5c4b27f78eb86c2935dd647b91dfae29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.634: INFO: Pod "webserver-deployment-7f5969cbc7-mlmp6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mlmp6 webserver-deployment-7f5969cbc7- deployment-651  8f952141-f0e6-4a33-8e45-7d50cd1e7f49 52407 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d07b7 0xc0052d07b8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpdtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpdtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.635: INFO: Pod "webserver-deployment-7f5969cbc7-nl7rt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nl7rt webserver-deployment-7f5969cbc7- deployment-651  02c78440-db4c-49d6-9d1d-c915829f7509 52530 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c35cba04001257871a30a2258f9795c53b5d9e77946fc8fe4b65334d89aaa361 cni.projectcalico.org/podIP:10.233.75.121/32 cni.projectcalico.org/podIPs:10.233.75.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0950 0xc0052d0951}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ksrqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ksrqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.640: INFO: Pod "webserver-deployment-7f5969cbc7-q4vdd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q4vdd webserver-deployment-7f5969cbc7- deployment-651  1d0e4b34-120b-4a04-bb76-b61e203057ff 52435 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0ad0 0xc0052d0ad1}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwjbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwjbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.641: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qth8t webserver-deployment-7f5969cbc7- deployment-651  f5bc2d5d-e7fd-4eb7-af27-0b8cf97cf5a9 52281 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0594fee6872f72446b7319cc120528e036c9fe2b63c851119be5f9d327525e6b cni.projectcalico.org/podIP:10.233.103.116/32 cni.projectcalico.org/podIPs:10.233.103.116/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0c40 0xc0052d0c41}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbfc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbfc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.116,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0dca94f800737d81b08316522be547ac183f0ccd8216d1d3df90901735ef6507,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.641: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rv24s webserver-deployment-7f5969cbc7- deployment-651  27fac720-1904-481d-b773-80b6b4d45efc 52249 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cc81f9673d0cd4125c9861417260fbbd81ae57bd0786a08dc9fd930da121be46 cni.projectcalico.org/podIP:10.233.103.112/32 cni.projectcalico.org/podIPs:10.233.103.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0e47 0xc0052d0e48}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tdjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tdjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.112,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c0cd5bcea47a4a18e7d75960d75a151a0d86185cc066bd48869186bedccfa621,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.642: INFO: Pod "webserver-deployment-7f5969cbc7-sqnqn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sqnqn webserver-deployment-7f5969cbc7- deployment-651  6619e477-3d2a-41fa-a45b-2734973c330d 52430 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d1067 0xc0052d1068}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdhx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdhx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.642: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tcvzw webserver-deployment-7f5969cbc7- deployment-651  292fc726-435d-4eeb-ab2d-fe04fffd4d68 52230 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0b82bc4a3a2aca83b0e35aea90d5d60a8e6d086bd08079883214e968be3baf41 cni.projectcalico.org/podIP:10.233.75.126/32 cni.projectcalico.org/podIPs:10.233.75.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d11f0 0xc0052d11f1}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r5kw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r5kw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.126,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://51aaca772eaca47ed86c2a4371d77591b5bbed42fb280976565ec0bc58a1baea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.642: INFO: Pod "webserver-deployment-7f5969cbc7-vq6gv" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vq6gv webserver-deployment-7f5969cbc7- deployment-651  d1e73866-38f2-4386-8357-d347f1423fd7 52445 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d1407 0xc0052d1408}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rv68n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rv68n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.643: INFO: Pod "webserver-deployment-7f5969cbc7-z5z6t" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z5z6t webserver-deployment-7f5969cbc7- deployment-651  2ca5bf6f-0b2c-4271-9c2f-d9e8f3d7fa06 52420 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d1570 0xc0052d1571}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dngjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dngjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.643: INFO: Pod "webserver-deployment-d9f79cb5-5tsdp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5tsdp webserver-deployment-d9f79cb5- deployment-651  d8f8af5c-6ac5-49f6-b11b-c9e6d4b77010 52347 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c779e277ac38634bcf2af0fded9c73134b3fa86fd829a27f1dbb1b6c6adcfda3 cni.projectcalico.org/podIP:10.233.75.125/32 cni.projectcalico.org/podIPs:10.233.75.125/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d16cf 0xc0052d1700}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nstxb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nstxb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.643: INFO: Pod "webserver-deployment-d9f79cb5-5zm8s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5zm8s webserver-deployment-d9f79cb5- deployment-651  b35bbe49-1e72-4116-ac6d-47bd3b277d38 52368 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c290e27ee2446d070e7d2b8fa77be7773ab2e97be35d7c8f18226fe9002108d3 cni.projectcalico.org/podIP:10.233.75.91/32 cni.projectcalico.org/podIPs:10.233.75.91/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1927 0xc0052d1928}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-254gp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-254gp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.644: INFO: Pod "webserver-deployment-d9f79cb5-6t84d" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6t84d webserver-deployment-d9f79cb5- deployment-651  f29aa444-16e1-4e70-9dbb-bd8fa04c0045 52505 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b0f87a07f55b6a1aa7bd90f6ae6c20961d1746c2ab186c1742268991484b7697 cni.projectcalico.org/podIP:10.233.103.85/32 cni.projectcalico.org/podIPs:10.233.103.85/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1b77 0xc0052d1b78}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwd76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwd76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.644: INFO: Pod "webserver-deployment-d9f79cb5-7kkdd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7kkdd webserver-deployment-d9f79cb5- deployment-651  12ddd652-6270-4a82-b9d9-36fc2d59f3a7 52427 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1d87 0xc0052d1d88}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9thv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9thv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-gw2wr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gw2wr webserver-deployment-d9f79cb5- deployment-651  82b630c3-6cba-49cd-bc05-e209ba12ffed 52352 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b72043a82dea5d4067060405d91a871028300e6f8b6dfb9fa2cecf8ad5c6c124 cni.projectcalico.org/podIP:10.233.103.127/32 cni.projectcalico.org/podIPs:10.233.103.127/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1eef 0xc0052d1f00}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdt6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdt6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-hm4n8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hm4n8 webserver-deployment-d9f79cb5- deployment-651  68fcad15-b7cc-4a2d-9300-b76cfd6c79db 52360 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1d8e14b2903f24f37f2204e40e51e31eb3e109359e850cb7496e1f9cc9984028 cni.projectcalico.org/podIP:10.233.103.76/32 cni.projectcalico.org/podIPs:10.233.103.76/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4147 0xc0043f4148}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jdj58,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jdj58,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-j5hcp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j5hcp webserver-deployment-d9f79cb5- deployment-651  c8aa4621-b1c4-42c9-9866-ccf6df9ddc83 52416 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4367 0xc0043f4368}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65fbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65fbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-kcnh4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kcnh4 webserver-deployment-d9f79cb5- deployment-651  b2ac5cd8-5ac6-402e-881c-1d4d1071c3fe 52413 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f479f 0xc0043f47b0}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nk5kz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nk5kz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.646: INFO: Pod "webserver-deployment-d9f79cb5-mqh62" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mqh62 webserver-deployment-d9f79cb5- deployment-651  81e75ba9-e576-40dd-ba4b-469226e4b205 52447 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4a2f 0xc0043f4a40}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5g7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5g7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.646: INFO: Pod "webserver-deployment-d9f79cb5-plxp5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-plxp5 webserver-deployment-d9f79cb5- deployment-651  b137c1de-74a6-4279-85ed-4090ea7c078e 52534 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d0a373844643cdb5b5d652ae78165cfe3b186112627413e5c788072a319d6efe cni.projectcalico.org/podIP:10.233.75.108/32 cni.projectcalico.org/podIPs:10.233.75.108/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4b9f 0xc0043f4bd0}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkqj2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkqj2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.646: INFO: Pod "webserver-deployment-d9f79cb5-tpq77" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tpq77 webserver-deployment-d9f79cb5- deployment-651  f356c5e1-eaf6-4c69-bab1-a503cfcc61f5 52364 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0f410327d008920443a0c066a410858a80c177e3b0f97a6eef07bab604a2f89c cni.projectcalico.org/podIP:10.233.75.82/32 cni.projectcalico.org/podIPs:10.233.75.82/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4df7 0xc0043f4df8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbwsw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbwsw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.647: INFO: Pod "webserver-deployment-d9f79cb5-vcn54" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vcn54 webserver-deployment-d9f79cb5- deployment-651  03b6b225-4b04-4d25-9c88-11901f6581db 52515 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a138a5239ec72811b2805c3fe950cb27bf1e8d75b6773556401b0cccee2113b9 cni.projectcalico.org/podIP:10.233.103.104/32 cni.projectcalico.org/podIPs:10.233.103.104/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f5277 0xc0043f5278}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2vpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2vpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  8 22:36:16.648: INFO: Pod "webserver-deployment-d9f79cb5-wmz96" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wmz96 webserver-deployment-d9f79cb5- deployment-651  fb4c3b21-b991-478a-91ed-2567fe189dd6 52523 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c4f8f6af758d3f0e6c5cddd7d1a284781fab74e63edef0a27984f019396af9c8 cni.projectcalico.org/podIP:10.233.103.89/32 cni.projectcalico.org/podIPs:10.233.103.89/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f54a7 0xc0043f54a8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blqns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blqns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Sep  8 22:36:16.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-651" for this suite. 09/08/23 22:36:16.685
------------------------------
â€¢ [SLOW TEST] [8.764 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:36:07.95
    Sep  8 22:36:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename deployment 09/08/23 22:36:07.951
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:08.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:08.048
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Sep  8 22:36:08.058: INFO: Creating deployment "webserver-deployment"
    Sep  8 22:36:08.075: INFO: Waiting for observed generation 1
    Sep  8 22:36:10.116: INFO: Waiting for all required pods to come up
    Sep  8 22:36:10.127: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 09/08/23 22:36:10.128
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xt8zf" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6dfc4" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-h2xkn" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mhpsh" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6tsft" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rv24s" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-tcvzw" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qth8t" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.128: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-22vjl" in namespace "deployment-651" to be "running"
    Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh": Phase="Pending", Reason="", readiness=false. Elapsed: 16.765795ms
    Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn": Phase="Pending", Reason="", readiness=false. Elapsed: 16.868667ms
    Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-xt8zf": Phase="Pending", Reason="", readiness=false. Elapsed: 17.104197ms
    Sep  8 22:36:10.145: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw": Phase="Pending", Reason="", readiness=false. Elapsed: 17.042022ms
    Sep  8 22:36:10.146: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft": Phase="Pending", Reason="", readiness=false. Elapsed: 18.370511ms
    Sep  8 22:36:10.157: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.781405ms
    Sep  8 22:36:10.157: INFO: Pod "webserver-deployment-7f5969cbc7-22vjl": Phase="Pending", Reason="", readiness=false. Elapsed: 27.699124ms
    Sep  8 22:36:10.157: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t": Phase="Pending", Reason="", readiness=false. Elapsed: 28.590859ms
    Sep  8 22:36:10.164: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s": Phase="Pending", Reason="", readiness=false. Elapsed: 35.78146ms
    Sep  8 22:36:12.154: INFO: Pod "webserver-deployment-7f5969cbc7-xt8zf": Phase="Running", Reason="", readiness=true. Elapsed: 2.026629537s
    Sep  8 22:36:12.154: INFO: Pod "webserver-deployment-7f5969cbc7-xt8zf" satisfied condition "running"
    Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t": Phase="Running", Reason="", readiness=true. Elapsed: 2.05055163s
    Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t" satisfied condition "running"
    Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh": Phase="Running", Reason="", readiness=true. Elapsed: 2.051088142s
    Sep  8 22:36:12.179: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh" satisfied condition "running"
    Sep  8 22:36:12.180: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft": Phase="Running", Reason="", readiness=true. Elapsed: 2.051466798s
    Sep  8 22:36:12.180: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft" satisfied condition "running"
    Sep  8 22:36:12.186: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn": Phase="Running", Reason="", readiness=true. Elapsed: 2.05816925s
    Sep  8 22:36:12.186: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn" satisfied condition "running"
    Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw": Phase="Running", Reason="", readiness=true. Elapsed: 2.058306697s
    Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw" satisfied condition "running"
    Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-22vjl": Phase="Running", Reason="", readiness=true. Elapsed: 2.05786356s
    Sep  8 22:36:12.187: INFO: Pod "webserver-deployment-7f5969cbc7-22vjl" satisfied condition "running"
    Sep  8 22:36:12.193: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.064934769s
    Sep  8 22:36:12.193: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4" satisfied condition "running"
    Sep  8 22:36:12.196: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s": Phase="Running", Reason="", readiness=true. Elapsed: 2.06817689s
    Sep  8 22:36:12.196: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s" satisfied condition "running"
    Sep  8 22:36:12.196: INFO: Waiting for deployment "webserver-deployment" to complete
    Sep  8 22:36:12.243: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Sep  8 22:36:12.294: INFO: Updating deployment webserver-deployment
    Sep  8 22:36:12.295: INFO: Waiting for observed generation 2
    Sep  8 22:36:14.336: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Sep  8 22:36:14.344: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Sep  8 22:36:14.356: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  8 22:36:14.394: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Sep  8 22:36:14.394: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Sep  8 22:36:14.406: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Sep  8 22:36:14.428: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Sep  8 22:36:14.428: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Sep  8 22:36:14.461: INFO: Updating deployment webserver-deployment
    Sep  8 22:36:14.461: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Sep  8 22:36:14.491: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Sep  8 22:36:16.531: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Sep  8 22:36:16.569: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-651  7ace6889-9be1-4a9e-8fe2-a2796c399a88 52464 3 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006fbee08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-09-08 22:36:14 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-09-08 22:36:14 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Sep  8 22:36:16.602: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-651  c73ba0d5-769a-4dd4-9b74-9316ffd6c555 52461 3 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7ace6889-9be1-4a9e-8fe2-a2796c399a88 0xc0079ac967 0xc0079ac968}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ace6889-9be1-4a9e-8fe2-a2796c399a88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079aca08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 22:36:16.602: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Sep  8 22:36:16.602: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-651  f5350933-23ea-45bf-bad5-aa86608fd626 52452 3 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7ace6889-9be1-4a9e-8fe2-a2796c399a88 0xc0079ac877 0xc0079ac878}] [] [{kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ace6889-9be1-4a9e-8fe2-a2796c399a88\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0079ac908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Sep  8 22:36:16.627: INFO: Pod "webserver-deployment-7f5969cbc7-6dfc4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6dfc4 webserver-deployment-7f5969cbc7- deployment-651  a5615df7-b517-4905-bd89-f804cbb9a387 52233 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8931f94b75e311eceb54f81672179efc6132cd26e176dee47df7f319c606ef2b cni.projectcalico.org/podIP:10.233.75.112/32 cni.projectcalico.org/podIPs:10.233.75.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf227 0xc006fbf228}] [] [{calico Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7c5sg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7c5sg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.112,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0801f24c0d5da72d4c8bb2eeeedfc95dbc23b805371ebf26037e1731eb30f6e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.628: INFO: Pod "webserver-deployment-7f5969cbc7-6msnj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6msnj webserver-deployment-7f5969cbc7- deployment-651  17532794-a278-4f97-8069-5ba97cf2ca24 52488 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b36a55479ac59521a14f5720186132c8d8e886dcd1e0edf823f9ffe1aa9df5b9 cni.projectcalico.org/podIP:10.233.75.107/32 cni.projectcalico.org/podIPs:10.233.75.107/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf447 0xc006fbf448}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stgxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stgxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.628: INFO: Pod "webserver-deployment-7f5969cbc7-6sn5f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6sn5f webserver-deployment-7f5969cbc7- deployment-651  9f7b3d38-9d77-4d50-a983-6b8fbdcaeba5 52498 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1d6bb2ad19709e8ba27a304627acfab13ccadecee997b19c93ff0e4c26869a5f cni.projectcalico.org/podIP:10.233.75.127/32 cni.projectcalico.org/podIPs:10.233.75.127/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf657 0xc006fbf658}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmqc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmqc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-6tsft" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6tsft webserver-deployment-7f5969cbc7- deployment-651  dc9747b6-39b6-40ec-a400-23b5507f30ea 52272 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:da6902feed151752840f2edf8f5ff54aa0083b72ce8a5987ef2576a52b74e9cf cni.projectcalico.org/podIP:10.233.75.105/32 cni.projectcalico.org/podIPs:10.233.75.105/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbf877 0xc006fbf878}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v42vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v42vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.105,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cba459d436e5959c520e922e814707d1416db8510d1990e0bb0d10a10267b73b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-7n7r5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7n7r5 webserver-deployment-7f5969cbc7- deployment-651  4cefc38b-884a-4fab-ae35-18528fdf1707 52492 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c15e5aa954605e1b1e61b9ab011d137109356fd658a2327e59ccfc6eb78a8d0f cni.projectcalico.org/podIP:10.233.103.92/32 cni.projectcalico.org/podIPs:10.233.103.92/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbfaa7 0xc006fbfaa8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cxck2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cxck2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-clmbr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-clmbr webserver-deployment-7f5969cbc7- deployment-651  8f3a609f-5729-46d4-94d1-74b774f18189 52438 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbfc97 0xc006fbfc98}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zcg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zcg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-cncnp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cncnp webserver-deployment-7f5969cbc7- deployment-651  0880b631-953e-4b03-9046-d046128191b4 52436 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbfe00 0xc006fbfe01}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lwcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lwcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-fpr5s" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fpr5s webserver-deployment-7f5969cbc7- deployment-651  53be9a2e-d7c9-49ca-bfdf-15fd783b6044 52207 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:766dbe084921a7c13160eb6607a1894eda6d0d695d14fb7d3c0e3bb44ca699a8 cni.projectcalico.org/podIP:10.233.103.107/32 cni.projectcalico.org/podIPs:10.233.103.107/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc006fbff60 0xc006fbff61}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2dgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2dgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.107,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1534decd9371881ea6dd1cdf816ffa4fb96f5e5579c45d141cbe2650427e23dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.107,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.629: INFO: Pod "webserver-deployment-7f5969cbc7-gvmb2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gvmb2 webserver-deployment-7f5969cbc7- deployment-651  9985e9df-75d3-4521-8b67-9deedc7d8c77 52467 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0167 0xc0052d0168}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wdqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wdqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.630: INFO: Pod "webserver-deployment-7f5969cbc7-h2xkn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h2xkn webserver-deployment-7f5969cbc7- deployment-651  6bcc88f4-e090-4ac1-ad38-0556c89e4899 52278 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0931d0e1cf87e241e135e15f7966ec9121301919f0c962105aac858efb17891c cni.projectcalico.org/podIP:10.233.103.114/32 cni.projectcalico.org/podIPs:10.233.103.114/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0347 0xc0052d0348}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtbcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtbcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.114,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://704335240a82cc694105d8bee8dffd8942e9dbc19a1dae093ff0ec83a9e79d9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.630: INFO: Pod "webserver-deployment-7f5969cbc7-mhpsh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mhpsh webserver-deployment-7f5969cbc7- deployment-651  eda9b7b4-1c20-4670-83ac-496d985a3266 52275 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0f10f22c888710f7b7384f71e1f716797c00021fb9c8e143a270e2ff44780b09 cni.projectcalico.org/podIP:10.233.103.94/32 cni.projectcalico.org/podIPs:10.233.103.94/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0587 0xc0052d0588}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drjfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drjfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.94,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://57ebcd8bdd932c16734638dd94d67bda5c4b27f78eb86c2935dd647b91dfae29,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.634: INFO: Pod "webserver-deployment-7f5969cbc7-mlmp6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mlmp6 webserver-deployment-7f5969cbc7- deployment-651  8f952141-f0e6-4a33-8e45-7d50cd1e7f49 52407 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d07b7 0xc0052d07b8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpdtv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpdtv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.635: INFO: Pod "webserver-deployment-7f5969cbc7-nl7rt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nl7rt webserver-deployment-7f5969cbc7- deployment-651  02c78440-db4c-49d6-9d1d-c915829f7509 52530 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c35cba04001257871a30a2258f9795c53b5d9e77946fc8fe4b65334d89aaa361 cni.projectcalico.org/podIP:10.233.75.121/32 cni.projectcalico.org/podIPs:10.233.75.121/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0950 0xc0052d0951}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ksrqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ksrqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.640: INFO: Pod "webserver-deployment-7f5969cbc7-q4vdd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q4vdd webserver-deployment-7f5969cbc7- deployment-651  1d0e4b34-120b-4a04-bb76-b61e203057ff 52435 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0ad0 0xc0052d0ad1}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwjbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwjbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.641: INFO: Pod "webserver-deployment-7f5969cbc7-qth8t" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qth8t webserver-deployment-7f5969cbc7- deployment-651  f5bc2d5d-e7fd-4eb7-af27-0b8cf97cf5a9 52281 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0594fee6872f72446b7319cc120528e036c9fe2b63c851119be5f9d327525e6b cni.projectcalico.org/podIP:10.233.103.116/32 cni.projectcalico.org/podIPs:10.233.103.116/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0c40 0xc0052d0c41}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbfc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbfc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.116,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0dca94f800737d81b08316522be547ac183f0ccd8216d1d3df90901735ef6507,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.641: INFO: Pod "webserver-deployment-7f5969cbc7-rv24s" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rv24s webserver-deployment-7f5969cbc7- deployment-651  27fac720-1904-481d-b773-80b6b4d45efc 52249 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cc81f9673d0cd4125c9861417260fbbd81ae57bd0786a08dc9fd930da121be46 cni.projectcalico.org/podIP:10.233.103.112/32 cni.projectcalico.org/podIPs:10.233.103.112/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d0e47 0xc0052d0e48}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.103.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tdjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tdjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:10.233.103.112,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c0cd5bcea47a4a18e7d75960d75a151a0d86185cc066bd48869186bedccfa621,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.103.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.642: INFO: Pod "webserver-deployment-7f5969cbc7-sqnqn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sqnqn webserver-deployment-7f5969cbc7- deployment-651  6619e477-3d2a-41fa-a45b-2734973c330d 52430 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d1067 0xc0052d1068}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gdhx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gdhx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.642: INFO: Pod "webserver-deployment-7f5969cbc7-tcvzw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tcvzw webserver-deployment-7f5969cbc7- deployment-651  292fc726-435d-4eeb-ab2d-fe04fffd4d68 52230 0 2023-09-08 22:36:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0b82bc4a3a2aca83b0e35aea90d5d60a8e6d086bd08079883214e968be3baf41 cni.projectcalico.org/podIP:10.233.75.126/32 cni.projectcalico.org/podIPs:10.233.75.126/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d11f0 0xc0052d11f1}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.75.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r5kw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r5kw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:10.233.75.126,StartTime:2023-09-08 22:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-09-08 22:36:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://51aaca772eaca47ed86c2a4371d77591b5bbed42fb280976565ec0bc58a1baea,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.75.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.642: INFO: Pod "webserver-deployment-7f5969cbc7-vq6gv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vq6gv webserver-deployment-7f5969cbc7- deployment-651  d1e73866-38f2-4386-8357-d347f1423fd7 52445 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d1407 0xc0052d1408}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rv68n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rv68n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.643: INFO: Pod "webserver-deployment-7f5969cbc7-z5z6t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-z5z6t webserver-deployment-7f5969cbc7- deployment-651  2ca5bf6f-0b2c-4271-9c2f-d9e8f3d7fa06 52420 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 f5350933-23ea-45bf-bad5-aa86608fd626 0xc0052d1570 0xc0052d1571}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f5350933-23ea-45bf-bad5-aa86608fd626\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dngjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dngjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.643: INFO: Pod "webserver-deployment-d9f79cb5-5tsdp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5tsdp webserver-deployment-d9f79cb5- deployment-651  d8f8af5c-6ac5-49f6-b11b-c9e6d4b77010 52347 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c779e277ac38634bcf2af0fded9c73134b3fa86fd829a27f1dbb1b6c6adcfda3 cni.projectcalico.org/podIP:10.233.75.125/32 cni.projectcalico.org/podIPs:10.233.75.125/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d16cf 0xc0052d1700}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nstxb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nstxb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.643: INFO: Pod "webserver-deployment-d9f79cb5-5zm8s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5zm8s webserver-deployment-d9f79cb5- deployment-651  b35bbe49-1e72-4116-ac6d-47bd3b277d38 52368 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c290e27ee2446d070e7d2b8fa77be7773ab2e97be35d7c8f18226fe9002108d3 cni.projectcalico.org/podIP:10.233.75.91/32 cni.projectcalico.org/podIPs:10.233.75.91/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1927 0xc0052d1928}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-254gp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-254gp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.644: INFO: Pod "webserver-deployment-d9f79cb5-6t84d" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6t84d webserver-deployment-d9f79cb5- deployment-651  f29aa444-16e1-4e70-9dbb-bd8fa04c0045 52505 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b0f87a07f55b6a1aa7bd90f6ae6c20961d1746c2ab186c1742268991484b7697 cni.projectcalico.org/podIP:10.233.103.85/32 cni.projectcalico.org/podIPs:10.233.103.85/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1b77 0xc0052d1b78}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwd76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwd76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.644: INFO: Pod "webserver-deployment-d9f79cb5-7kkdd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7kkdd webserver-deployment-d9f79cb5- deployment-651  12ddd652-6270-4a82-b9d9-36fc2d59f3a7 52427 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1d87 0xc0052d1d88}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w9thv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w9thv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-gw2wr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gw2wr webserver-deployment-d9f79cb5- deployment-651  82b630c3-6cba-49cd-bc05-e209ba12ffed 52352 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:b72043a82dea5d4067060405d91a871028300e6f8b6dfb9fa2cecf8ad5c6c124 cni.projectcalico.org/podIP:10.233.103.127/32 cni.projectcalico.org/podIPs:10.233.103.127/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0052d1eef 0xc0052d1f00}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sdt6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sdt6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-hm4n8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-hm4n8 webserver-deployment-d9f79cb5- deployment-651  68fcad15-b7cc-4a2d-9300-b76cfd6c79db 52360 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:1d8e14b2903f24f37f2204e40e51e31eb3e109359e850cb7496e1f9cc9984028 cni.projectcalico.org/podIP:10.233.103.76/32 cni.projectcalico.org/podIPs:10.233.103.76/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4147 0xc0043f4148}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jdj58,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jdj58,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-j5hcp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j5hcp webserver-deployment-d9f79cb5- deployment-651  c8aa4621-b1c4-42c9-9866-ccf6df9ddc83 52416 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4367 0xc0043f4368}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65fbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65fbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.645: INFO: Pod "webserver-deployment-d9f79cb5-kcnh4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kcnh4 webserver-deployment-d9f79cb5- deployment-651  b2ac5cd8-5ac6-402e-881c-1d4d1071c3fe 52413 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f479f 0xc0043f47b0}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nk5kz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nk5kz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.646: INFO: Pod "webserver-deployment-d9f79cb5-mqh62" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mqh62 webserver-deployment-d9f79cb5- deployment-651  81e75ba9-e576-40dd-ba4b-469226e4b205 52447 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4a2f 0xc0043f4a40}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5g7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5g7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.646: INFO: Pod "webserver-deployment-d9f79cb5-plxp5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-plxp5 webserver-deployment-d9f79cb5- deployment-651  b137c1de-74a6-4279-85ed-4090ea7c078e 52534 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d0a373844643cdb5b5d652ae78165cfe3b186112627413e5c788072a319d6efe cni.projectcalico.org/podIP:10.233.75.108/32 cni.projectcalico.org/podIPs:10.233.75.108/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4b9f 0xc0043f4bd0}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkqj2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkqj2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.646: INFO: Pod "webserver-deployment-d9f79cb5-tpq77" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tpq77 webserver-deployment-d9f79cb5- deployment-651  f356c5e1-eaf6-4c69-bab1-a503cfcc61f5 52364 0 2023-09-08 22:36:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0f410327d008920443a0c066a410858a80c177e3b0f97a6eef07bab604a2f89c cni.projectcalico.org/podIP:10.233.75.82/32 cni.projectcalico.org/podIPs:10.233.75.82/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f4df7 0xc0043f4df8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbwsw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbwsw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.19.129,PodIP:,StartTime:2023-09-08 22:36:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.647: INFO: Pod "webserver-deployment-d9f79cb5-vcn54" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vcn54 webserver-deployment-d9f79cb5- deployment-651  03b6b225-4b04-4d25-9c88-11901f6581db 52515 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a138a5239ec72811b2805c3fe950cb27bf1e8d75b6773556401b0cccee2113b9 cni.projectcalico.org/podIP:10.233.103.104/32 cni.projectcalico.org/podIPs:10.233.103.104/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f5277 0xc0043f5278}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2vpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2vpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Sep  8 22:36:16.648: INFO: Pod "webserver-deployment-d9f79cb5-wmz96" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wmz96 webserver-deployment-d9f79cb5- deployment-651  fb4c3b21-b991-478a-91ed-2567fe189dd6 52523 0 2023-09-08 22:36:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:c4f8f6af758d3f0e6c5cddd7d1a284781fab74e63edef0a27984f019396af9c8 cni.projectcalico.org/podIP:10.233.103.89/32 cni.projectcalico.org/podIPs:10.233.103.89/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c73ba0d5-769a-4dd4-9b74-9316ffd6c555 0xc0043f54a7 0xc0043f54a8}] [] [{kube-controller-manager Update v1 2023-09-08 22:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c73ba0d5-769a-4dd4-9b74-9316ffd6c555\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-09-08 22:36:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-09-08 22:36:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blqns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blqns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node-4,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-09-08 22:36:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.100.16.26,PodIP:,StartTime:2023-09-08 22:36:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:36:16.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-651" for this suite. 09/08/23 22:36:16.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:36:16.719
Sep  8 22:36:16.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:36:16.72
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:16.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:16.785
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-aae0fd8b-dfef-4e39-b422-0ed54036ed49 09/08/23 22:36:16.823
STEP: Creating secret with name secret-projected-all-test-volume-fc6dc6bc-79e1-476a-951d-e88cea2f913c 09/08/23 22:36:16.864
STEP: Creating a pod to test Check all projections for projected volume plugin 09/08/23 22:36:16.885
Sep  8 22:36:16.925: INFO: Waiting up to 5m0s for pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6" in namespace "projected-1278" to be "Succeeded or Failed"
Sep  8 22:36:16.947: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.972134ms
Sep  8 22:36:18.961: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035524401s
Sep  8 22:36:20.972: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046717482s
Sep  8 22:36:23.016: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090527741s
Sep  8 22:36:25.024: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.09866914s
STEP: Saw pod success 09/08/23 22:36:25.024
Sep  8 22:36:25.024: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6" satisfied condition "Succeeded or Failed"
Sep  8 22:36:25.046: INFO: Trying to get logs from node node-3 pod projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6 container projected-all-volume-test: <nil>
STEP: delete the pod 09/08/23 22:36:25.123
Sep  8 22:36:25.180: INFO: Waiting for pod projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6 to disappear
Sep  8 22:36:25.186: INFO: Pod projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Sep  8 22:36:25.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1278" for this suite. 09/08/23 22:36:25.22
------------------------------
â€¢ [SLOW TEST] [8.529 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:36:16.719
    Sep  8 22:36:16.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:36:16.72
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:16.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:16.785
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-aae0fd8b-dfef-4e39-b422-0ed54036ed49 09/08/23 22:36:16.823
    STEP: Creating secret with name secret-projected-all-test-volume-fc6dc6bc-79e1-476a-951d-e88cea2f913c 09/08/23 22:36:16.864
    STEP: Creating a pod to test Check all projections for projected volume plugin 09/08/23 22:36:16.885
    Sep  8 22:36:16.925: INFO: Waiting up to 5m0s for pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6" in namespace "projected-1278" to be "Succeeded or Failed"
    Sep  8 22:36:16.947: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.972134ms
    Sep  8 22:36:18.961: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035524401s
    Sep  8 22:36:20.972: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046717482s
    Sep  8 22:36:23.016: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090527741s
    Sep  8 22:36:25.024: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.09866914s
    STEP: Saw pod success 09/08/23 22:36:25.024
    Sep  8 22:36:25.024: INFO: Pod "projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6" satisfied condition "Succeeded or Failed"
    Sep  8 22:36:25.046: INFO: Trying to get logs from node node-3 pod projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6 container projected-all-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:36:25.123
    Sep  8 22:36:25.180: INFO: Waiting for pod projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6 to disappear
    Sep  8 22:36:25.186: INFO: Pod projected-volume-742f22a5-681c-441b-b03f-f3e02eea43f6 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:36:25.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1278" for this suite. 09/08/23 22:36:25.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:36:25.25
Sep  8 22:36:25.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename init-container 09/08/23 22:36:25.253
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:25.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:25.316
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 09/08/23 22:36:25.339
Sep  8 22:36:25.339: INFO: PodSpec: initContainers in spec.initContainers
Sep  8 22:37:13.033: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-daa28e04-947b-4c1c-9223-5a28c4dd8f61", GenerateName:"", Namespace:"init-container-3594", SelfLink:"", UID:"d2cd6cfc-2f33-48cb-b78f-167505bc1e0a", ResourceVersion:"53218", Generation:0, CreationTimestamp:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"339971016"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"28210b592b54b829e833aed15792e6d730c4b7c3a1964b61b0b7cf49076b17d4", "cni.projectcalico.org/podIP":"10.233.75.114/32", "cni.projectcalico.org/podIPs":"10.233.75.114/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000efc570), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 8, 22, 36, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000efc630), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 8, 22, 37, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000efc660), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bslt7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e72960), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bslt7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bslt7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bslt7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003e967a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0087aa310), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003e96830)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003e96850)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003e96858), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003e9685c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000ff9230), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.100.19.129", PodIP:"10.233.75.114", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.75.114"}}, StartTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0087aa3f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0087aa460)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://ffa4712f47ea7e1d61b3dc944a624bbe9d37252f4cc9660842f891b138bfb4b9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e72a00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e729e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003e968d4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:37:13.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3594" for this suite. 09/08/23 22:37:13.051
------------------------------
â€¢ [SLOW TEST] [47.823 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:36:25.25
    Sep  8 22:36:25.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename init-container 09/08/23 22:36:25.253
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:36:25.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:36:25.316
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 09/08/23 22:36:25.339
    Sep  8 22:36:25.339: INFO: PodSpec: initContainers in spec.initContainers
    Sep  8 22:37:13.033: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-daa28e04-947b-4c1c-9223-5a28c4dd8f61", GenerateName:"", Namespace:"init-container-3594", SelfLink:"", UID:"d2cd6cfc-2f33-48cb-b78f-167505bc1e0a", ResourceVersion:"53218", Generation:0, CreationTimestamp:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"339971016"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"28210b592b54b829e833aed15792e6d730c4b7c3a1964b61b0b7cf49076b17d4", "cni.projectcalico.org/podIP":"10.233.75.114/32", "cni.projectcalico.org/podIPs":"10.233.75.114/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000efc570), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 8, 22, 36, 28, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000efc630), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.September, 8, 22, 37, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000efc660), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-bslt7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e72960), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bslt7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bslt7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-bslt7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003e967a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0087aa310), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003e96830)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003e96850)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003e96858), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003e9685c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000ff9230), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.100.19.129", PodIP:"10.233.75.114", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.75.114"}}, StartTime:time.Date(2023, time.September, 8, 22, 36, 25, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0087aa3f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0087aa460)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://ffa4712f47ea7e1d61b3dc944a624bbe9d37252f4cc9660842f891b138bfb4b9", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e72a00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e729e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003e968d4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:37:13.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3594" for this suite. 09/08/23 22:37:13.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:37:13.077
Sep  8 22:37:13.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename namespaces 09/08/23 22:37:13.078
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:37:13.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:37:13.131
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 09/08/23 22:37:13.137
STEP: patching the Namespace 09/08/23 22:37:13.176
STEP: get the Namespace and ensuring it has the label 09/08/23 22:37:13.196
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:37:13.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3456" for this suite. 09/08/23 22:37:13.227
STEP: Destroying namespace "nspatchtest-dc063a19-64d0-4bdc-af6d-200c47b3346e-9660" for this suite. 09/08/23 22:37:13.243
------------------------------
â€¢ [0.197 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:37:13.077
    Sep  8 22:37:13.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename namespaces 09/08/23 22:37:13.078
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:37:13.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:37:13.131
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 09/08/23 22:37:13.137
    STEP: patching the Namespace 09/08/23 22:37:13.176
    STEP: get the Namespace and ensuring it has the label 09/08/23 22:37:13.196
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:37:13.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3456" for this suite. 09/08/23 22:37:13.227
    STEP: Destroying namespace "nspatchtest-dc063a19-64d0-4bdc-af6d-200c47b3346e-9660" for this suite. 09/08/23 22:37:13.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:37:13.274
Sep  8 22:37:13.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 22:37:13.275
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:37:13.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:37:13.327
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9690 09/08/23 22:37:13.333
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 09/08/23 22:37:13.364
Sep  8 22:37:13.411: INFO: Found 0 stateful pods, waiting for 3
Sep  8 22:37:23.424: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:37:23.424: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:37:23.424: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:37:23.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 22:37:23.741: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 22:37:23.741: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 22:37:23.741: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/08/23 22:37:33.785
Sep  8 22:37:33.819: INFO: Updating stateful set ss2
STEP: Creating a new revision 09/08/23 22:37:33.82
STEP: Updating Pods in reverse ordinal order 09/08/23 22:37:43.873
Sep  8 22:37:43.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 22:37:44.144: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  8 22:37:44.144: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 22:37:44.144: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 22:37:54.216: INFO: Waiting for StatefulSet statefulset-9690/ss2 to complete update
STEP: Rolling back to a previous revision 09/08/23 22:38:04.236
Sep  8 22:38:04.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 22:38:04.530: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 22:38:04.530: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 22:38:04.530: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 22:38:14.607: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 09/08/23 22:38:24.671
Sep  8 22:38:24.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 22:38:24.945: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  8 22:38:24.945: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 22:38:24.945: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 22:38:35.004: INFO: Waiting for StatefulSet statefulset-9690/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 22:38:45.029: INFO: Deleting all statefulset in ns statefulset-9690
Sep  8 22:38:45.046: INFO: Scaling statefulset ss2 to 0
Sep  8 22:38:55.100: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 22:38:55.113: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:38:55.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9690" for this suite. 09/08/23 22:38:55.164
------------------------------
â€¢ [SLOW TEST] [101.918 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:37:13.274
    Sep  8 22:37:13.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 22:37:13.275
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:37:13.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:37:13.327
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9690 09/08/23 22:37:13.333
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 09/08/23 22:37:13.364
    Sep  8 22:37:13.411: INFO: Found 0 stateful pods, waiting for 3
    Sep  8 22:37:23.424: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:37:23.424: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:37:23.424: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:37:23.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 22:37:23.741: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 22:37:23.741: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 22:37:23.741: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 09/08/23 22:37:33.785
    Sep  8 22:37:33.819: INFO: Updating stateful set ss2
    STEP: Creating a new revision 09/08/23 22:37:33.82
    STEP: Updating Pods in reverse ordinal order 09/08/23 22:37:43.873
    Sep  8 22:37:43.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 22:37:44.144: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  8 22:37:44.144: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 22:37:44.144: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 22:37:54.216: INFO: Waiting for StatefulSet statefulset-9690/ss2 to complete update
    STEP: Rolling back to a previous revision 09/08/23 22:38:04.236
    Sep  8 22:38:04.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 22:38:04.530: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 22:38:04.530: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 22:38:04.530: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 22:38:14.607: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 09/08/23 22:38:24.671
    Sep  8 22:38:24.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-9690 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 22:38:24.945: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  8 22:38:24.945: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 22:38:24.945: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 22:38:35.004: INFO: Waiting for StatefulSet statefulset-9690/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 22:38:45.029: INFO: Deleting all statefulset in ns statefulset-9690
    Sep  8 22:38:45.046: INFO: Scaling statefulset ss2 to 0
    Sep  8 22:38:55.100: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 22:38:55.113: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:38:55.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9690" for this suite. 09/08/23 22:38:55.164
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:38:55.192
Sep  8 22:38:55.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 22:38:55.193
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:38:55.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:38:55.27
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8353 09/08/23 22:38:55.277
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-8353 09/08/23 22:38:55.294
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8353 09/08/23 22:38:55.314
Sep  8 22:38:55.322: INFO: Found 0 stateful pods, waiting for 1
Sep  8 22:39:05.336: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/08/23 22:39:05.337
Sep  8 22:39:05.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 22:39:05.604: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 22:39:05.604: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 22:39:05.604: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 22:39:05.615: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  8 22:39:15.627: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 22:39:15.627: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 22:39:15.670: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  8 22:39:15.670: INFO: ss-0  node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  }]
Sep  8 22:39:15.670: INFO: ss-1          Pending         []
Sep  8 22:39:15.670: INFO: 
Sep  8 22:39:15.671: INFO: StatefulSet ss has not reached scale 3, at 2
Sep  8 22:39:16.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986627056s
Sep  8 22:39:17.711: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968583013s
Sep  8 22:39:18.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.944798184s
Sep  8 22:39:19.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.916036015s
Sep  8 22:39:20.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.905135709s
Sep  8 22:39:21.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.896867205s
Sep  8 22:39:22.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.888144874s
Sep  8 22:39:23.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.877432078s
Sep  8 22:39:24.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 862.222545ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8353 09/08/23 22:39:25.805
Sep  8 22:39:25.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 22:39:26.094: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  8 22:39:26.094: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 22:39:26.094: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 22:39:26.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 22:39:26.347: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  8 22:39:26.347: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 22:39:26.347: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 22:39:26.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  8 22:39:26.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  8 22:39:26.579: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  8 22:39:26.579: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  8 22:39:26.591: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep  8 22:39:36.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:39:36.603: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  8 22:39:36.603: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 09/08/23 22:39:36.603
Sep  8 22:39:36.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 22:39:36.853: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 22:39:36.853: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 22:39:36.853: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 22:39:36.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 22:39:37.073: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 22:39:37.073: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 22:39:37.073: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 22:39:37.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  8 22:39:37.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  8 22:39:37.307: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  8 22:39:37.307: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  8 22:39:37.307: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 22:39:37.317: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  8 22:39:47.334: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 22:39:47.334: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 22:39:47.334: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  8 22:39:47.367: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  8 22:39:47.367: INFO: ss-0  node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  }]
Sep  8 22:39:47.367: INFO: ss-1  node-4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
Sep  8 22:39:47.367: INFO: ss-2  node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
Sep  8 22:39:47.367: INFO: 
Sep  8 22:39:47.367: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  8 22:39:48.382: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Sep  8 22:39:48.382: INFO: ss-0  node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  }]
Sep  8 22:39:48.382: INFO: ss-1  node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
Sep  8 22:39:48.382: INFO: ss-2  node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
Sep  8 22:39:48.382: INFO: 
Sep  8 22:39:48.382: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  8 22:39:49.393: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.975091355s
Sep  8 22:39:50.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.964136776s
Sep  8 22:39:51.416: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.954922608s
Sep  8 22:39:52.425: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.941190534s
Sep  8 22:39:53.445: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.932372355s
Sep  8 22:39:54.460: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.912694773s
Sep  8 22:39:55.469: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.897273718s
Sep  8 22:39:56.478: INFO: Verifying statefulset ss doesn't scale past 0 for another 888.969719ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8353 09/08/23 22:39:57.478
Sep  8 22:39:57.486: INFO: Scaling statefulset ss to 0
Sep  8 22:39:57.513: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 22:39:57.521: INFO: Deleting all statefulset in ns statefulset-8353
Sep  8 22:39:57.529: INFO: Scaling statefulset ss to 0
Sep  8 22:39:57.559: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 22:39:57.566: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:39:57.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8353" for this suite. 09/08/23 22:39:57.628
------------------------------
â€¢ [SLOW TEST] [62.450 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:38:55.192
    Sep  8 22:38:55.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 22:38:55.193
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:38:55.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:38:55.27
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8353 09/08/23 22:38:55.277
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-8353 09/08/23 22:38:55.294
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8353 09/08/23 22:38:55.314
    Sep  8 22:38:55.322: INFO: Found 0 stateful pods, waiting for 1
    Sep  8 22:39:05.336: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 09/08/23 22:39:05.337
    Sep  8 22:39:05.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 22:39:05.604: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 22:39:05.604: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 22:39:05.604: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 22:39:05.615: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Sep  8 22:39:15.627: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 22:39:15.627: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 22:39:15.670: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  8 22:39:15.670: INFO: ss-0  node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  }]
    Sep  8 22:39:15.670: INFO: ss-1          Pending         []
    Sep  8 22:39:15.670: INFO: 
    Sep  8 22:39:15.671: INFO: StatefulSet ss has not reached scale 3, at 2
    Sep  8 22:39:16.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986627056s
    Sep  8 22:39:17.711: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.968583013s
    Sep  8 22:39:18.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.944798184s
    Sep  8 22:39:19.752: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.916036015s
    Sep  8 22:39:20.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.905135709s
    Sep  8 22:39:21.769: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.896867205s
    Sep  8 22:39:22.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.888144874s
    Sep  8 22:39:23.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.877432078s
    Sep  8 22:39:24.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 862.222545ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8353 09/08/23 22:39:25.805
    Sep  8 22:39:25.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 22:39:26.094: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Sep  8 22:39:26.094: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 22:39:26.094: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 22:39:26.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 22:39:26.347: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  8 22:39:26.347: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 22:39:26.347: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 22:39:26.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Sep  8 22:39:26.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Sep  8 22:39:26.579: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Sep  8 22:39:26.579: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Sep  8 22:39:26.591: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Sep  8 22:39:36.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:39:36.603: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Sep  8 22:39:36.603: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 09/08/23 22:39:36.603
    Sep  8 22:39:36.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 22:39:36.853: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 22:39:36.853: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 22:39:36.853: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 22:39:36.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 22:39:37.073: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 22:39:37.073: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 22:39:37.073: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 22:39:37.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=statefulset-8353 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Sep  8 22:39:37.306: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Sep  8 22:39:37.307: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Sep  8 22:39:37.307: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Sep  8 22:39:37.307: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 22:39:37.317: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Sep  8 22:39:47.334: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 22:39:47.334: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 22:39:47.334: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Sep  8 22:39:47.367: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  8 22:39:47.367: INFO: ss-0  node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  }]
    Sep  8 22:39:47.367: INFO: ss-1  node-4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
    Sep  8 22:39:47.367: INFO: ss-2  node-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
    Sep  8 22:39:47.367: INFO: 
    Sep  8 22:39:47.367: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  8 22:39:48.382: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
    Sep  8 22:39:48.382: INFO: ss-0  node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:38:55 +0000 UTC  }]
    Sep  8 22:39:48.382: INFO: ss-1  node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
    Sep  8 22:39:48.382: INFO: ss-2  node-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-09-08 22:39:15 +0000 UTC  }]
    Sep  8 22:39:48.382: INFO: 
    Sep  8 22:39:48.382: INFO: StatefulSet ss has not reached scale 0, at 3
    Sep  8 22:39:49.393: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.975091355s
    Sep  8 22:39:50.402: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.964136776s
    Sep  8 22:39:51.416: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.954922608s
    Sep  8 22:39:52.425: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.941190534s
    Sep  8 22:39:53.445: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.932372355s
    Sep  8 22:39:54.460: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.912694773s
    Sep  8 22:39:55.469: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.897273718s
    Sep  8 22:39:56.478: INFO: Verifying statefulset ss doesn't scale past 0 for another 888.969719ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8353 09/08/23 22:39:57.478
    Sep  8 22:39:57.486: INFO: Scaling statefulset ss to 0
    Sep  8 22:39:57.513: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 22:39:57.521: INFO: Deleting all statefulset in ns statefulset-8353
    Sep  8 22:39:57.529: INFO: Scaling statefulset ss to 0
    Sep  8 22:39:57.559: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 22:39:57.566: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:39:57.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8353" for this suite. 09/08/23 22:39:57.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:39:57.649
Sep  8 22:39:57.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename ingress 09/08/23 22:39:57.65
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:39:57.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:39:57.701
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 09/08/23 22:39:57.706
STEP: getting /apis/networking.k8s.io 09/08/23 22:39:57.709
STEP: getting /apis/networking.k8s.iov1 09/08/23 22:39:57.711
STEP: creating 09/08/23 22:39:57.713
STEP: getting 09/08/23 22:39:57.78
STEP: listing 09/08/23 22:39:57.796
STEP: watching 09/08/23 22:39:57.807
Sep  8 22:39:57.807: INFO: starting watch
STEP: cluster-wide listing 09/08/23 22:39:57.809
STEP: cluster-wide watching 09/08/23 22:39:57.815
Sep  8 22:39:57.816: INFO: starting watch
STEP: patching 09/08/23 22:39:57.818
STEP: updating 09/08/23 22:39:57.834
Sep  8 22:39:57.861: INFO: waiting for watch events with expected annotations
Sep  8 22:39:57.861: INFO: saw patched and updated annotations
STEP: patching /status 09/08/23 22:39:57.861
STEP: updating /status 09/08/23 22:39:57.879
STEP: get /status 09/08/23 22:39:57.936
STEP: deleting 09/08/23 22:39:57.949
STEP: deleting a collection 09/08/23 22:39:58.015
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Sep  8 22:39:58.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-3698" for this suite. 09/08/23 22:39:58.07
------------------------------
â€¢ [0.437 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:39:57.649
    Sep  8 22:39:57.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename ingress 09/08/23 22:39:57.65
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:39:57.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:39:57.701
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 09/08/23 22:39:57.706
    STEP: getting /apis/networking.k8s.io 09/08/23 22:39:57.709
    STEP: getting /apis/networking.k8s.iov1 09/08/23 22:39:57.711
    STEP: creating 09/08/23 22:39:57.713
    STEP: getting 09/08/23 22:39:57.78
    STEP: listing 09/08/23 22:39:57.796
    STEP: watching 09/08/23 22:39:57.807
    Sep  8 22:39:57.807: INFO: starting watch
    STEP: cluster-wide listing 09/08/23 22:39:57.809
    STEP: cluster-wide watching 09/08/23 22:39:57.815
    Sep  8 22:39:57.816: INFO: starting watch
    STEP: patching 09/08/23 22:39:57.818
    STEP: updating 09/08/23 22:39:57.834
    Sep  8 22:39:57.861: INFO: waiting for watch events with expected annotations
    Sep  8 22:39:57.861: INFO: saw patched and updated annotations
    STEP: patching /status 09/08/23 22:39:57.861
    STEP: updating /status 09/08/23 22:39:57.879
    STEP: get /status 09/08/23 22:39:57.936
    STEP: deleting 09/08/23 22:39:57.949
    STEP: deleting a collection 09/08/23 22:39:58.015
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:39:58.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-3698" for this suite. 09/08/23 22:39:58.07
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:39:58.087
Sep  8 22:39:58.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:39:58.089
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:39:58.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:39:58.145
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 09/08/23 22:39:58.151
Sep  8 22:39:58.151: INFO: namespace kubectl-5206
Sep  8 22:39:58.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 create -f -'
Sep  8 22:39:58.507: INFO: stderr: ""
Sep  8 22:39:58.507: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/08/23 22:39:58.507
Sep  8 22:39:59.521: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:39:59.521: INFO: Found 0 / 1
Sep  8 22:40:00.533: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:40:00.533: INFO: Found 1 / 1
Sep  8 22:40:00.533: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  8 22:40:00.541: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:40:00.542: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  8 22:40:00.542: INFO: wait on agnhost-primary startup in kubectl-5206 
Sep  8 22:40:00.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 logs agnhost-primary-t49fj agnhost-primary'
Sep  8 22:40:00.681: INFO: stderr: ""
Sep  8 22:40:00.681: INFO: stdout: "Paused\n"
STEP: exposing RC 09/08/23 22:40:00.681
Sep  8 22:40:00.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  8 22:40:00.822: INFO: stderr: ""
Sep  8 22:40:00.822: INFO: stdout: "service/rm2 exposed\n"
Sep  8 22:40:00.845: INFO: Service rm2 in namespace kubectl-5206 found.
STEP: exposing service 09/08/23 22:40:02.868
Sep  8 22:40:02.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  8 22:40:03.048: INFO: stderr: ""
Sep  8 22:40:03.048: INFO: stdout: "service/rm3 exposed\n"
Sep  8 22:40:03.091: INFO: Service rm3 in namespace kubectl-5206 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:05.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5206" for this suite. 09/08/23 22:40:05.118
------------------------------
â€¢ [SLOW TEST] [7.047 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:39:58.087
    Sep  8 22:39:58.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:39:58.089
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:39:58.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:39:58.145
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 09/08/23 22:39:58.151
    Sep  8 22:39:58.151: INFO: namespace kubectl-5206
    Sep  8 22:39:58.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 create -f -'
    Sep  8 22:39:58.507: INFO: stderr: ""
    Sep  8 22:39:58.507: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/08/23 22:39:58.507
    Sep  8 22:39:59.521: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:39:59.521: INFO: Found 0 / 1
    Sep  8 22:40:00.533: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:40:00.533: INFO: Found 1 / 1
    Sep  8 22:40:00.533: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  8 22:40:00.541: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:40:00.542: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  8 22:40:00.542: INFO: wait on agnhost-primary startup in kubectl-5206 
    Sep  8 22:40:00.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 logs agnhost-primary-t49fj agnhost-primary'
    Sep  8 22:40:00.681: INFO: stderr: ""
    Sep  8 22:40:00.681: INFO: stdout: "Paused\n"
    STEP: exposing RC 09/08/23 22:40:00.681
    Sep  8 22:40:00.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Sep  8 22:40:00.822: INFO: stderr: ""
    Sep  8 22:40:00.822: INFO: stdout: "service/rm2 exposed\n"
    Sep  8 22:40:00.845: INFO: Service rm2 in namespace kubectl-5206 found.
    STEP: exposing service 09/08/23 22:40:02.868
    Sep  8 22:40:02.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5206 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Sep  8 22:40:03.048: INFO: stderr: ""
    Sep  8 22:40:03.048: INFO: stdout: "service/rm3 exposed\n"
    Sep  8 22:40:03.091: INFO: Service rm3 in namespace kubectl-5206 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:05.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5206" for this suite. 09/08/23 22:40:05.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:05.135
Sep  8 22:40:05.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:40:05.137
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:05.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:05.175
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:40:05.18
Sep  8 22:40:05.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2392 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Sep  8 22:40:05.326: INFO: stderr: ""
Sep  8 22:40:05.326: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 09/08/23 22:40:05.326
Sep  8 22:40:05.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2392 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Sep  8 22:40:05.641: INFO: stderr: ""
Sep  8 22:40:05.641: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:40:05.641
Sep  8 22:40:05.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2392 delete pods e2e-test-httpd-pod'
Sep  8 22:40:07.996: INFO: stderr: ""
Sep  8 22:40:07.996: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:07.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2392" for this suite. 09/08/23 22:40:08.013
------------------------------
â€¢ [2.900 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:05.135
    Sep  8 22:40:05.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:40:05.137
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:05.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:05.175
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:40:05.18
    Sep  8 22:40:05.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2392 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Sep  8 22:40:05.326: INFO: stderr: ""
    Sep  8 22:40:05.326: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 09/08/23 22:40:05.326
    Sep  8 22:40:05.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2392 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Sep  8 22:40:05.641: INFO: stderr: ""
    Sep  8 22:40:05.641: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 09/08/23 22:40:05.641
    Sep  8 22:40:05.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2392 delete pods e2e-test-httpd-pod'
    Sep  8 22:40:07.996: INFO: stderr: ""
    Sep  8 22:40:07.996: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:07.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2392" for this suite. 09/08/23 22:40:08.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:08.035
Sep  8 22:40:08.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sysctl 09/08/23 22:40:08.037
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:08.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:08.099
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/08/23 22:40:08.106
STEP: Watching for error events or started pod 09/08/23 22:40:08.13
STEP: Waiting for pod completion 09/08/23 22:40:10.142
Sep  8 22:40:10.142: INFO: Waiting up to 3m0s for pod "sysctl-c787e847-371c-4439-bb81-e52cde725247" in namespace "sysctl-8937" to be "completed"
Sep  8 22:40:10.150: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247": Phase="Pending", Reason="", readiness=false. Elapsed: 8.190929ms
Sep  8 22:40:12.160: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018461821s
Sep  8 22:40:14.160: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018036265s
Sep  8 22:40:14.160: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247" satisfied condition "completed"
STEP: Checking that the pod succeeded 09/08/23 22:40:14.177
STEP: Getting logs from the pod 09/08/23 22:40:14.177
STEP: Checking that the sysctl is actually updated 09/08/23 22:40:14.197
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:14.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8937" for this suite. 09/08/23 22:40:14.218
------------------------------
â€¢ [SLOW TEST] [6.205 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:08.035
    Sep  8 22:40:08.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sysctl 09/08/23 22:40:08.037
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:08.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:08.099
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 09/08/23 22:40:08.106
    STEP: Watching for error events or started pod 09/08/23 22:40:08.13
    STEP: Waiting for pod completion 09/08/23 22:40:10.142
    Sep  8 22:40:10.142: INFO: Waiting up to 3m0s for pod "sysctl-c787e847-371c-4439-bb81-e52cde725247" in namespace "sysctl-8937" to be "completed"
    Sep  8 22:40:10.150: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247": Phase="Pending", Reason="", readiness=false. Elapsed: 8.190929ms
    Sep  8 22:40:12.160: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018461821s
    Sep  8 22:40:14.160: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018036265s
    Sep  8 22:40:14.160: INFO: Pod "sysctl-c787e847-371c-4439-bb81-e52cde725247" satisfied condition "completed"
    STEP: Checking that the pod succeeded 09/08/23 22:40:14.177
    STEP: Getting logs from the pod 09/08/23 22:40:14.177
    STEP: Checking that the sysctl is actually updated 09/08/23 22:40:14.197
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:14.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8937" for this suite. 09/08/23 22:40:14.218
  << End Captured GinkgoWriter Output
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:14.241
Sep  8 22:40:14.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename conformance-tests 09/08/23 22:40:14.242
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:14.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:14.321
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 09/08/23 22:40:14.337
Sep  8 22:40:14.337: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:14.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-7778" for this suite. 09/08/23 22:40:14.379
------------------------------
â€¢ [0.157 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:14.241
    Sep  8 22:40:14.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename conformance-tests 09/08/23 22:40:14.242
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:14.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:14.321
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 09/08/23 22:40:14.337
    Sep  8 22:40:14.337: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:14.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-7778" for this suite. 09/08/23 22:40:14.379
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:14.4
Sep  8 22:40:14.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename dns 09/08/23 22:40:14.402
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:14.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:14.477
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 09/08/23 22:40:14.482
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
 09/08/23 22:40:14.498
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
 09/08/23 22:40:14.499
STEP: creating a pod to probe DNS 09/08/23 22:40:14.499
STEP: submitting the pod to kubernetes 09/08/23 22:40:14.499
Sep  8 22:40:14.523: INFO: Waiting up to 15m0s for pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0" in namespace "dns-7397" to be "running"
Sep  8 22:40:14.555: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.035379ms
Sep  8 22:40:16.570: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047424647s
Sep  8 22:40:18.566: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0": Phase="Running", Reason="", readiness=true. Elapsed: 4.043038951s
Sep  8 22:40:18.566: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0" satisfied condition "running"
STEP: retrieving the pod 09/08/23 22:40:18.566
STEP: looking for the results for each expected name from probers 09/08/23 22:40:18.584
Sep  8 22:40:18.616: INFO: DNS probes using dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0 succeeded

STEP: deleting the pod 09/08/23 22:40:18.616
STEP: changing the externalName to bar.example.com 09/08/23 22:40:18.66
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
 09/08/23 22:40:18.686
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
 09/08/23 22:40:18.687
STEP: creating a second pod to probe DNS 09/08/23 22:40:18.687
STEP: submitting the pod to kubernetes 09/08/23 22:40:18.687
Sep  8 22:40:18.713: INFO: Waiting up to 15m0s for pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81" in namespace "dns-7397" to be "running"
Sep  8 22:40:18.726: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81": Phase="Pending", Reason="", readiness=false. Elapsed: 13.782227ms
Sep  8 22:40:20.742: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029414085s
Sep  8 22:40:22.737: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81": Phase="Running", Reason="", readiness=true. Elapsed: 4.024508468s
Sep  8 22:40:22.737: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81" satisfied condition "running"
STEP: retrieving the pod 09/08/23 22:40:22.738
STEP: looking for the results for each expected name from probers 09/08/23 22:40:22.752
Sep  8 22:40:22.802: INFO: DNS probes using dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81 succeeded

STEP: deleting the pod 09/08/23 22:40:22.802
STEP: changing the service to type=ClusterIP 09/08/23 22:40:22.849
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
 09/08/23 22:40:22.902
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
 09/08/23 22:40:22.902
STEP: creating a third pod to probe DNS 09/08/23 22:40:22.902
STEP: submitting the pod to kubernetes 09/08/23 22:40:22.921
Sep  8 22:40:22.941: INFO: Waiting up to 15m0s for pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e" in namespace "dns-7397" to be "running"
Sep  8 22:40:22.961: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.780945ms
Sep  8 22:40:24.969: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02797499s
Sep  8 22:40:26.972: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e": Phase="Running", Reason="", readiness=true. Elapsed: 4.030675661s
Sep  8 22:40:26.972: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e" satisfied condition "running"
STEP: retrieving the pod 09/08/23 22:40:26.972
STEP: looking for the results for each expected name from probers 09/08/23 22:40:26.978
Sep  8 22:40:27.013: INFO: DNS probes using dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e succeeded

STEP: deleting the pod 09/08/23 22:40:27.013
STEP: deleting the test externalName service 09/08/23 22:40:27.053
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:27.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7397" for this suite. 09/08/23 22:40:27.115
------------------------------
â€¢ [SLOW TEST] [12.734 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:14.4
    Sep  8 22:40:14.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename dns 09/08/23 22:40:14.402
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:14.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:14.477
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 09/08/23 22:40:14.482
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
     09/08/23 22:40:14.498
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
     09/08/23 22:40:14.499
    STEP: creating a pod to probe DNS 09/08/23 22:40:14.499
    STEP: submitting the pod to kubernetes 09/08/23 22:40:14.499
    Sep  8 22:40:14.523: INFO: Waiting up to 15m0s for pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0" in namespace "dns-7397" to be "running"
    Sep  8 22:40:14.555: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.035379ms
    Sep  8 22:40:16.570: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047424647s
    Sep  8 22:40:18.566: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0": Phase="Running", Reason="", readiness=true. Elapsed: 4.043038951s
    Sep  8 22:40:18.566: INFO: Pod "dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 22:40:18.566
    STEP: looking for the results for each expected name from probers 09/08/23 22:40:18.584
    Sep  8 22:40:18.616: INFO: DNS probes using dns-test-a9ce6b6f-4744-4ed8-8c1e-3e8660f17be0 succeeded

    STEP: deleting the pod 09/08/23 22:40:18.616
    STEP: changing the externalName to bar.example.com 09/08/23 22:40:18.66
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
     09/08/23 22:40:18.686
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
     09/08/23 22:40:18.687
    STEP: creating a second pod to probe DNS 09/08/23 22:40:18.687
    STEP: submitting the pod to kubernetes 09/08/23 22:40:18.687
    Sep  8 22:40:18.713: INFO: Waiting up to 15m0s for pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81" in namespace "dns-7397" to be "running"
    Sep  8 22:40:18.726: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81": Phase="Pending", Reason="", readiness=false. Elapsed: 13.782227ms
    Sep  8 22:40:20.742: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029414085s
    Sep  8 22:40:22.737: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81": Phase="Running", Reason="", readiness=true. Elapsed: 4.024508468s
    Sep  8 22:40:22.737: INFO: Pod "dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 22:40:22.738
    STEP: looking for the results for each expected name from probers 09/08/23 22:40:22.752
    Sep  8 22:40:22.802: INFO: DNS probes using dns-test-e70f816c-1ce1-49c8-8084-68068be4ce81 succeeded

    STEP: deleting the pod 09/08/23 22:40:22.802
    STEP: changing the service to type=ClusterIP 09/08/23 22:40:22.849
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
     09/08/23 22:40:22.902
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7397.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7397.svc.cluster.local; sleep 1; done
     09/08/23 22:40:22.902
    STEP: creating a third pod to probe DNS 09/08/23 22:40:22.902
    STEP: submitting the pod to kubernetes 09/08/23 22:40:22.921
    Sep  8 22:40:22.941: INFO: Waiting up to 15m0s for pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e" in namespace "dns-7397" to be "running"
    Sep  8 22:40:22.961: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.780945ms
    Sep  8 22:40:24.969: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02797499s
    Sep  8 22:40:26.972: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e": Phase="Running", Reason="", readiness=true. Elapsed: 4.030675661s
    Sep  8 22:40:26.972: INFO: Pod "dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e" satisfied condition "running"
    STEP: retrieving the pod 09/08/23 22:40:26.972
    STEP: looking for the results for each expected name from probers 09/08/23 22:40:26.978
    Sep  8 22:40:27.013: INFO: DNS probes using dns-test-4c93c625-3f10-401e-a93c-3c342bd11f9e succeeded

    STEP: deleting the pod 09/08/23 22:40:27.013
    STEP: deleting the test externalName service 09/08/23 22:40:27.053
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:27.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7397" for this suite. 09/08/23 22:40:27.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:27.134
Sep  8 22:40:27.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename emptydir 09/08/23 22:40:27.136
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:27.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:27.206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 09/08/23 22:40:27.211
Sep  8 22:40:27.240: INFO: Waiting up to 5m0s for pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690" in namespace "emptydir-8259" to be "Succeeded or Failed"
Sep  8 22:40:27.248: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690": Phase="Pending", Reason="", readiness=false. Elapsed: 7.949159ms
Sep  8 22:40:29.261: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020678459s
Sep  8 22:40:31.260: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020106932s
STEP: Saw pod success 09/08/23 22:40:31.261
Sep  8 22:40:31.261: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690" satisfied condition "Succeeded or Failed"
Sep  8 22:40:31.272: INFO: Trying to get logs from node node-3 pod pod-2cf69805-e2ba-4131-bc17-613a349f3690 container test-container: <nil>
STEP: delete the pod 09/08/23 22:40:31.293
Sep  8 22:40:31.353: INFO: Waiting for pod pod-2cf69805-e2ba-4131-bc17-613a349f3690 to disappear
Sep  8 22:40:31.365: INFO: Pod pod-2cf69805-e2ba-4131-bc17-613a349f3690 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:31.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8259" for this suite. 09/08/23 22:40:31.38
------------------------------
â€¢ [4.265 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:27.134
    Sep  8 22:40:27.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename emptydir 09/08/23 22:40:27.136
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:27.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:27.206
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 09/08/23 22:40:27.211
    Sep  8 22:40:27.240: INFO: Waiting up to 5m0s for pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690" in namespace "emptydir-8259" to be "Succeeded or Failed"
    Sep  8 22:40:27.248: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690": Phase="Pending", Reason="", readiness=false. Elapsed: 7.949159ms
    Sep  8 22:40:29.261: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020678459s
    Sep  8 22:40:31.260: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020106932s
    STEP: Saw pod success 09/08/23 22:40:31.261
    Sep  8 22:40:31.261: INFO: Pod "pod-2cf69805-e2ba-4131-bc17-613a349f3690" satisfied condition "Succeeded or Failed"
    Sep  8 22:40:31.272: INFO: Trying to get logs from node node-3 pod pod-2cf69805-e2ba-4131-bc17-613a349f3690 container test-container: <nil>
    STEP: delete the pod 09/08/23 22:40:31.293
    Sep  8 22:40:31.353: INFO: Waiting for pod pod-2cf69805-e2ba-4131-bc17-613a349f3690 to disappear
    Sep  8 22:40:31.365: INFO: Pod pod-2cf69805-e2ba-4131-bc17-613a349f3690 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:31.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8259" for this suite. 09/08/23 22:40:31.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:31.4
Sep  8 22:40:31.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 22:40:31.401
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:31.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:31.459
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-9b83694a-80e6-4688-9f19-b14990676b65 09/08/23 22:40:31.463
STEP: Creating a pod to test consume configMaps 09/08/23 22:40:31.476
Sep  8 22:40:31.497: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a" in namespace "configmap-4444" to be "Succeeded or Failed"
Sep  8 22:40:31.509: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.974974ms
Sep  8 22:40:33.519: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021856548s
Sep  8 22:40:35.520: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023072889s
STEP: Saw pod success 09/08/23 22:40:35.521
Sep  8 22:40:35.521: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a" satisfied condition "Succeeded or Failed"
Sep  8 22:40:35.528: INFO: Trying to get logs from node node-3 pod pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a container agnhost-container: <nil>
STEP: delete the pod 09/08/23 22:40:35.55
Sep  8 22:40:35.590: INFO: Waiting for pod pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a to disappear
Sep  8 22:40:35.597: INFO: Pod pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:35.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4444" for this suite. 09/08/23 22:40:35.61
------------------------------
â€¢ [4.238 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:31.4
    Sep  8 22:40:31.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 22:40:31.401
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:31.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:31.459
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-9b83694a-80e6-4688-9f19-b14990676b65 09/08/23 22:40:31.463
    STEP: Creating a pod to test consume configMaps 09/08/23 22:40:31.476
    Sep  8 22:40:31.497: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a" in namespace "configmap-4444" to be "Succeeded or Failed"
    Sep  8 22:40:31.509: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.974974ms
    Sep  8 22:40:33.519: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021856548s
    Sep  8 22:40:35.520: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023072889s
    STEP: Saw pod success 09/08/23 22:40:35.521
    Sep  8 22:40:35.521: INFO: Pod "pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a" satisfied condition "Succeeded or Failed"
    Sep  8 22:40:35.528: INFO: Trying to get logs from node node-3 pod pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 22:40:35.55
    Sep  8 22:40:35.590: INFO: Waiting for pod pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a to disappear
    Sep  8 22:40:35.597: INFO: Pod pod-configmaps-c1b4ce30-815e-4ea8-8853-7349c50c0d6a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:35.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4444" for this suite. 09/08/23 22:40:35.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:35.64
Sep  8 22:40:35.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:40:35.641
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:35.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:35.693
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 09/08/23 22:40:35.699
Sep  8 22:40:35.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2293 create -f -'
Sep  8 22:40:36.051: INFO: stderr: ""
Sep  8 22:40:36.051: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 09/08/23 22:40:36.051
Sep  8 22:40:36.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2293 diff -f -'
Sep  8 22:40:36.430: INFO: rc: 1
Sep  8 22:40:36.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2293 delete -f -'
Sep  8 22:40:36.575: INFO: stderr: ""
Sep  8 22:40:36.575: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:36.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2293" for this suite. 09/08/23 22:40:36.606
------------------------------
â€¢ [0.990 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:35.64
    Sep  8 22:40:35.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:40:35.641
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:35.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:35.693
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 09/08/23 22:40:35.699
    Sep  8 22:40:35.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2293 create -f -'
    Sep  8 22:40:36.051: INFO: stderr: ""
    Sep  8 22:40:36.051: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 09/08/23 22:40:36.051
    Sep  8 22:40:36.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2293 diff -f -'
    Sep  8 22:40:36.430: INFO: rc: 1
    Sep  8 22:40:36.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-2293 delete -f -'
    Sep  8 22:40:36.575: INFO: stderr: ""
    Sep  8 22:40:36.575: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:36.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2293" for this suite. 09/08/23 22:40:36.606
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:36.63
Sep  8 22:40:36.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 22:40:36.633
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:36.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:36.701
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-c6f84078-7075-485d-b003-ae940248b37d 09/08/23 22:40:36.712
STEP: Creating a pod to test consume configMaps 09/08/23 22:40:36.73
Sep  8 22:40:36.769: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275" in namespace "configmap-2381" to be "Succeeded or Failed"
Sep  8 22:40:36.780: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00685ms
Sep  8 22:40:38.791: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Running", Reason="", readiness=true. Elapsed: 2.021812005s
Sep  8 22:40:40.789: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Running", Reason="", readiness=false. Elapsed: 4.019653857s
Sep  8 22:40:42.802: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032048571s
STEP: Saw pod success 09/08/23 22:40:42.802
Sep  8 22:40:42.802: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275" satisfied condition "Succeeded or Failed"
Sep  8 22:40:42.811: INFO: Trying to get logs from node node-3 pod pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 22:40:42.833
Sep  8 22:40:42.868: INFO: Waiting for pod pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275 to disappear
Sep  8 22:40:42.878: INFO: Pod pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:42.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2381" for this suite. 09/08/23 22:40:42.901
------------------------------
â€¢ [SLOW TEST] [6.301 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:36.63
    Sep  8 22:40:36.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 22:40:36.633
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:36.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:36.701
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-c6f84078-7075-485d-b003-ae940248b37d 09/08/23 22:40:36.712
    STEP: Creating a pod to test consume configMaps 09/08/23 22:40:36.73
    Sep  8 22:40:36.769: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275" in namespace "configmap-2381" to be "Succeeded or Failed"
    Sep  8 22:40:36.780: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00685ms
    Sep  8 22:40:38.791: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Running", Reason="", readiness=true. Elapsed: 2.021812005s
    Sep  8 22:40:40.789: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Running", Reason="", readiness=false. Elapsed: 4.019653857s
    Sep  8 22:40:42.802: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032048571s
    STEP: Saw pod success 09/08/23 22:40:42.802
    Sep  8 22:40:42.802: INFO: Pod "pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275" satisfied condition "Succeeded or Failed"
    Sep  8 22:40:42.811: INFO: Trying to get logs from node node-3 pod pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 22:40:42.833
    Sep  8 22:40:42.868: INFO: Waiting for pod pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275 to disappear
    Sep  8 22:40:42.878: INFO: Pod pod-configmaps-9f81bee5-c0ad-4beb-98f5-498823a35275 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:42.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2381" for this suite. 09/08/23 22:40:42.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:42.937
Sep  8 22:40:42.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename security-context 09/08/23 22:40:42.939
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:43.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:43.026
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/08/23 22:40:43.037
Sep  8 22:40:43.056: INFO: Waiting up to 5m0s for pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a" in namespace "security-context-4203" to be "Succeeded or Failed"
Sep  8 22:40:43.085: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.230312ms
Sep  8 22:40:45.095: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039234986s
Sep  8 22:40:47.097: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040736278s
Sep  8 22:40:49.096: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039700813s
STEP: Saw pod success 09/08/23 22:40:49.096
Sep  8 22:40:49.096: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a" satisfied condition "Succeeded or Failed"
Sep  8 22:40:49.125: INFO: Trying to get logs from node node-3 pod security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a container test-container: <nil>
STEP: delete the pod 09/08/23 22:40:49.144
Sep  8 22:40:49.188: INFO: Waiting for pod security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a to disappear
Sep  8 22:40:49.198: INFO: Pod security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4203" for this suite. 09/08/23 22:40:49.221
------------------------------
â€¢ [SLOW TEST] [6.301 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:42.937
    Sep  8 22:40:42.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename security-context 09/08/23 22:40:42.939
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:43.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:43.026
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 09/08/23 22:40:43.037
    Sep  8 22:40:43.056: INFO: Waiting up to 5m0s for pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a" in namespace "security-context-4203" to be "Succeeded or Failed"
    Sep  8 22:40:43.085: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.230312ms
    Sep  8 22:40:45.095: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039234986s
    Sep  8 22:40:47.097: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040736278s
    Sep  8 22:40:49.096: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039700813s
    STEP: Saw pod success 09/08/23 22:40:49.096
    Sep  8 22:40:49.096: INFO: Pod "security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a" satisfied condition "Succeeded or Failed"
    Sep  8 22:40:49.125: INFO: Trying to get logs from node node-3 pod security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a container test-container: <nil>
    STEP: delete the pod 09/08/23 22:40:49.144
    Sep  8 22:40:49.188: INFO: Waiting for pod security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a to disappear
    Sep  8 22:40:49.198: INFO: Pod security-context-34a3149f-a6f0-4370-b506-c34e62bd1e5a no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4203" for this suite. 09/08/23 22:40:49.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:49.241
Sep  8 22:40:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename var-expansion 09/08/23 22:40:49.242
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:49.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:49.3
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Sep  8 22:40:49.324: INFO: Waiting up to 2m0s for pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" in namespace "var-expansion-3241" to be "container 0 failed with reason CreateContainerConfigError"
Sep  8 22:40:49.340: INFO: Pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.47688ms
Sep  8 22:40:51.358: INFO: Pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033592525s
Sep  8 22:40:51.358: INFO: Pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Sep  8 22:40:51.358: INFO: Deleting pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" in namespace "var-expansion-3241"
Sep  8 22:40:51.393: INFO: Wait up to 5m0s for pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Sep  8 22:40:53.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3241" for this suite. 09/08/23 22:40:53.473
------------------------------
â€¢ [4.267 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:49.241
    Sep  8 22:40:49.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename var-expansion 09/08/23 22:40:49.242
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:49.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:49.3
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Sep  8 22:40:49.324: INFO: Waiting up to 2m0s for pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" in namespace "var-expansion-3241" to be "container 0 failed with reason CreateContainerConfigError"
    Sep  8 22:40:49.340: INFO: Pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.47688ms
    Sep  8 22:40:51.358: INFO: Pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033592525s
    Sep  8 22:40:51.358: INFO: Pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Sep  8 22:40:51.358: INFO: Deleting pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" in namespace "var-expansion-3241"
    Sep  8 22:40:51.393: INFO: Wait up to 5m0s for pod "var-expansion-f5a18866-b902-4ca1-b8aa-4e6fa0a22d1a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:40:53.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3241" for this suite. 09/08/23 22:40:53.473
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:40:53.508
Sep  8 22:40:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename services 09/08/23 22:40:53.509
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:53.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:53.579
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-7203 09/08/23 22:40:53.584
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[] 09/08/23 22:40:53.628
Sep  8 22:40:53.685: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7203 09/08/23 22:40:53.685
Sep  8 22:40:53.717: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7203" to be "running and ready"
Sep  8 22:40:53.726: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.145962ms
Sep  8 22:40:53.726: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:40:55.736: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018922201s
Sep  8 22:40:55.736: INFO: The phase of Pod pod1 is Running (Ready = true)
Sep  8 22:40:55.736: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[pod1:[80]] 09/08/23 22:40:55.742
Sep  8 22:40:55.766: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 09/08/23 22:40:55.766
Sep  8 22:40:55.766: INFO: Creating new exec pod
Sep  8 22:40:55.787: INFO: Waiting up to 5m0s for pod "execpodh5nq4" in namespace "services-7203" to be "running"
Sep  8 22:40:55.796: INFO: Pod "execpodh5nq4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.453967ms
Sep  8 22:40:57.806: INFO: Pod "execpodh5nq4": Phase="Running", Reason="", readiness=true. Elapsed: 2.019745225s
Sep  8 22:40:57.806: INFO: Pod "execpodh5nq4" satisfied condition "running"
Sep  8 22:40:58.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  8 22:40:59.094: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  8 22:40:59.094: INFO: stdout: ""
Sep  8 22:40:59.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 10.233.29.44 80'
Sep  8 22:40:59.328: INFO: stderr: "+ nc -v -z -w 2 10.233.29.44 80\nConnection to 10.233.29.44 80 port [tcp/http] succeeded!\n"
Sep  8 22:40:59.328: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-7203 09/08/23 22:40:59.328
Sep  8 22:40:59.369: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7203" to be "running and ready"
Sep  8 22:40:59.394: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.091411ms
Sep  8 22:40:59.394: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:41:01.416: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046937957s
Sep  8 22:41:01.417: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:41:03.404: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034788599s
Sep  8 22:41:03.404: INFO: The phase of Pod pod2 is Running (Ready = true)
Sep  8 22:41:03.404: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[pod1:[80] pod2:[80]] 09/08/23 22:41:03.413
Sep  8 22:41:03.462: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 09/08/23 22:41:03.462
Sep  8 22:41:04.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  8 22:41:04.728: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  8 22:41:04.728: INFO: stdout: ""
Sep  8 22:41:04.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 10.233.29.44 80'
Sep  8 22:41:04.945: INFO: stderr: "+ nc -v -z -w 2 10.233.29.44 80\nConnection to 10.233.29.44 80 port [tcp/http] succeeded!\n"
Sep  8 22:41:04.945: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7203 09/08/23 22:41:04.945
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[pod2:[80]] 09/08/23 22:41:04.984
Sep  8 22:41:05.057: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 09/08/23 22:41:05.057
Sep  8 22:41:06.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Sep  8 22:41:06.298: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Sep  8 22:41:06.298: INFO: stdout: ""
Sep  8 22:41:06.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 10.233.29.44 80'
Sep  8 22:41:06.534: INFO: stderr: "+ nc -v -z -w 2 10.233.29.44 80\nConnection to 10.233.29.44 80 port [tcp/http] succeeded!\n"
Sep  8 22:41:06.534: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-7203 09/08/23 22:41:06.534
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[] 09/08/23 22:41:06.599
Sep  8 22:41:06.629: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:06.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7203" for this suite. 09/08/23 22:41:06.701
------------------------------
â€¢ [SLOW TEST] [13.211 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:40:53.508
    Sep  8 22:40:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename services 09/08/23 22:40:53.509
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:40:53.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:40:53.579
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-7203 09/08/23 22:40:53.584
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[] 09/08/23 22:40:53.628
    Sep  8 22:40:53.685: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7203 09/08/23 22:40:53.685
    Sep  8 22:40:53.717: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7203" to be "running and ready"
    Sep  8 22:40:53.726: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.145962ms
    Sep  8 22:40:53.726: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:40:55.736: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018922201s
    Sep  8 22:40:55.736: INFO: The phase of Pod pod1 is Running (Ready = true)
    Sep  8 22:40:55.736: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[pod1:[80]] 09/08/23 22:40:55.742
    Sep  8 22:40:55.766: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 09/08/23 22:40:55.766
    Sep  8 22:40:55.766: INFO: Creating new exec pod
    Sep  8 22:40:55.787: INFO: Waiting up to 5m0s for pod "execpodh5nq4" in namespace "services-7203" to be "running"
    Sep  8 22:40:55.796: INFO: Pod "execpodh5nq4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.453967ms
    Sep  8 22:40:57.806: INFO: Pod "execpodh5nq4": Phase="Running", Reason="", readiness=true. Elapsed: 2.019745225s
    Sep  8 22:40:57.806: INFO: Pod "execpodh5nq4" satisfied condition "running"
    Sep  8 22:40:58.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  8 22:40:59.094: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  8 22:40:59.094: INFO: stdout: ""
    Sep  8 22:40:59.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 10.233.29.44 80'
    Sep  8 22:40:59.328: INFO: stderr: "+ nc -v -z -w 2 10.233.29.44 80\nConnection to 10.233.29.44 80 port [tcp/http] succeeded!\n"
    Sep  8 22:40:59.328: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-7203 09/08/23 22:40:59.328
    Sep  8 22:40:59.369: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7203" to be "running and ready"
    Sep  8 22:40:59.394: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.091411ms
    Sep  8 22:40:59.394: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:41:01.416: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046937957s
    Sep  8 22:41:01.417: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:41:03.404: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.034788599s
    Sep  8 22:41:03.404: INFO: The phase of Pod pod2 is Running (Ready = true)
    Sep  8 22:41:03.404: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[pod1:[80] pod2:[80]] 09/08/23 22:41:03.413
    Sep  8 22:41:03.462: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 09/08/23 22:41:03.462
    Sep  8 22:41:04.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  8 22:41:04.728: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  8 22:41:04.728: INFO: stdout: ""
    Sep  8 22:41:04.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 10.233.29.44 80'
    Sep  8 22:41:04.945: INFO: stderr: "+ nc -v -z -w 2 10.233.29.44 80\nConnection to 10.233.29.44 80 port [tcp/http] succeeded!\n"
    Sep  8 22:41:04.945: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7203 09/08/23 22:41:04.945
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[pod2:[80]] 09/08/23 22:41:04.984
    Sep  8 22:41:05.057: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 09/08/23 22:41:05.057
    Sep  8 22:41:06.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Sep  8 22:41:06.298: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Sep  8 22:41:06.298: INFO: stdout: ""
    Sep  8 22:41:06.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=services-7203 exec execpodh5nq4 -- /bin/sh -x -c nc -v -z -w 2 10.233.29.44 80'
    Sep  8 22:41:06.534: INFO: stderr: "+ nc -v -z -w 2 10.233.29.44 80\nConnection to 10.233.29.44 80 port [tcp/http] succeeded!\n"
    Sep  8 22:41:06.534: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-7203 09/08/23 22:41:06.534
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7203 to expose endpoints map[] 09/08/23 22:41:06.599
    Sep  8 22:41:06.629: INFO: successfully validated that service endpoint-test2 in namespace services-7203 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:06.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7203" for this suite. 09/08/23 22:41:06.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:06.721
Sep  8 22:41:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replicaset 09/08/23 22:41:06.722
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:06.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:06.774
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 09/08/23 22:41:06.778
STEP: Verify that the required pods have come up 09/08/23 22:41:06.797
Sep  8 22:41:06.831: INFO: Pod name sample-pod: Found 2 pods out of 3
Sep  8 22:41:11.848: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 09/08/23 22:41:11.848
Sep  8 22:41:11.857: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 09/08/23 22:41:11.858
STEP: DeleteCollection of the ReplicaSets 09/08/23 22:41:11.882
STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/08/23 22:41:11.926
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:11.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1348" for this suite. 09/08/23 22:41:11.974
------------------------------
â€¢ [SLOW TEST] [5.330 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:06.721
    Sep  8 22:41:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replicaset 09/08/23 22:41:06.722
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:06.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:06.774
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 09/08/23 22:41:06.778
    STEP: Verify that the required pods have come up 09/08/23 22:41:06.797
    Sep  8 22:41:06.831: INFO: Pod name sample-pod: Found 2 pods out of 3
    Sep  8 22:41:11.848: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 09/08/23 22:41:11.848
    Sep  8 22:41:11.857: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 09/08/23 22:41:11.858
    STEP: DeleteCollection of the ReplicaSets 09/08/23 22:41:11.882
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 09/08/23 22:41:11.926
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:11.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1348" for this suite. 09/08/23 22:41:11.974
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:12.051
Sep  8 22:41:12.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:41:12.053
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:12.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:12.109
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 09/08/23 22:41:12.137
Sep  8 22:41:12.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6577 api-versions'
Sep  8 22:41:12.350: INFO: stderr: ""
Sep  8 22:41:12.350: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetallb.io/v1alpha1\nmetallb.io/v1beta1\nmetallb.io/v1beta2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nopenebs.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:12.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6577" for this suite. 09/08/23 22:41:12.39
------------------------------
â€¢ [0.377 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:12.051
    Sep  8 22:41:12.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:41:12.053
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:12.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:12.109
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 09/08/23 22:41:12.137
    Sep  8 22:41:12.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-6577 api-versions'
    Sep  8 22:41:12.350: INFO: stderr: ""
    Sep  8 22:41:12.350: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetallb.io/v1alpha1\nmetallb.io/v1beta1\nmetallb.io/v1beta2\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nopenebs.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:12.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6577" for this suite. 09/08/23 22:41:12.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:12.429
Sep  8 22:41:12.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename podtemplate 09/08/23 22:41:12.43
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:12.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:12.49
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:12.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4080" for this suite. 09/08/23 22:41:12.607
------------------------------
â€¢ [0.198 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:12.429
    Sep  8 22:41:12.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename podtemplate 09/08/23 22:41:12.43
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:12.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:12.49
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:12.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4080" for this suite. 09/08/23 22:41:12.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:12.631
Sep  8 22:41:12.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename job 09/08/23 22:41:12.632
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:12.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:12.704
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 09/08/23 22:41:12.71
STEP: Ensuring job reaches completions 09/08/23 22:41:12.721
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:26.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8444" for this suite. 09/08/23 22:41:26.757
------------------------------
â€¢ [SLOW TEST] [14.149 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:12.631
    Sep  8 22:41:12.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename job 09/08/23 22:41:12.632
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:12.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:12.704
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 09/08/23 22:41:12.71
    STEP: Ensuring job reaches completions 09/08/23 22:41:12.721
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:26.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8444" for this suite. 09/08/23 22:41:26.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:26.794
Sep  8 22:41:26.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubelet-test 09/08/23 22:41:26.796
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:26.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:26.851
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:26.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1686" for this suite. 09/08/23 22:41:26.968
------------------------------
â€¢ [0.195 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:26.794
    Sep  8 22:41:26.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubelet-test 09/08/23 22:41:26.796
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:26.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:26.851
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:26.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1686" for this suite. 09/08/23 22:41:26.968
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:26.996
Sep  8 22:41:26.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename events 09/08/23 22:41:26.998
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:27.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:27.066
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 09/08/23 22:41:27.074
STEP: listing events in all namespaces 09/08/23 22:41:27.11
STEP: listing events in test namespace 09/08/23 22:41:27.133
STEP: listing events with field selection filtering on source 09/08/23 22:41:27.145
STEP: listing events with field selection filtering on reportingController 09/08/23 22:41:27.154
STEP: getting the test event 09/08/23 22:41:27.166
STEP: patching the test event 09/08/23 22:41:27.174
STEP: getting the test event 09/08/23 22:41:27.201
STEP: updating the test event 09/08/23 22:41:27.217
STEP: getting the test event 09/08/23 22:41:27.245
STEP: deleting the test event 09/08/23 22:41:27.257
STEP: listing events in all namespaces 09/08/23 22:41:27.282
STEP: listing events in test namespace 09/08/23 22:41:27.306
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:27.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-642" for this suite. 09/08/23 22:41:27.336
------------------------------
â€¢ [0.364 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:26.996
    Sep  8 22:41:26.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename events 09/08/23 22:41:26.998
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:27.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:27.066
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 09/08/23 22:41:27.074
    STEP: listing events in all namespaces 09/08/23 22:41:27.11
    STEP: listing events in test namespace 09/08/23 22:41:27.133
    STEP: listing events with field selection filtering on source 09/08/23 22:41:27.145
    STEP: listing events with field selection filtering on reportingController 09/08/23 22:41:27.154
    STEP: getting the test event 09/08/23 22:41:27.166
    STEP: patching the test event 09/08/23 22:41:27.174
    STEP: getting the test event 09/08/23 22:41:27.201
    STEP: updating the test event 09/08/23 22:41:27.217
    STEP: getting the test event 09/08/23 22:41:27.245
    STEP: deleting the test event 09/08/23 22:41:27.257
    STEP: listing events in all namespaces 09/08/23 22:41:27.282
    STEP: listing events in test namespace 09/08/23 22:41:27.306
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:27.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-642" for this suite. 09/08/23 22:41:27.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:27.36
Sep  8 22:41:27.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename configmap 09/08/23 22:41:27.361
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:27.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:27.431
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1589/configmap-test-dece4c7f-0e6f-475f-ba43-2353d7b1439b 09/08/23 22:41:27.438
STEP: Creating a pod to test consume configMaps 09/08/23 22:41:27.458
Sep  8 22:41:27.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c" in namespace "configmap-1589" to be "Succeeded or Failed"
Sep  8 22:41:27.510: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.33022ms
Sep  8 22:41:29.520: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01812839s
Sep  8 22:41:31.520: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Running", Reason="", readiness=false. Elapsed: 4.018086391s
Sep  8 22:41:33.523: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021269724s
STEP: Saw pod success 09/08/23 22:41:33.523
Sep  8 22:41:33.523: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c" satisfied condition "Succeeded or Failed"
Sep  8 22:41:33.547: INFO: Trying to get logs from node node-3 pod pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c container env-test: <nil>
STEP: delete the pod 09/08/23 22:41:33.565
Sep  8 22:41:33.594: INFO: Waiting for pod pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c to disappear
Sep  8 22:41:33.602: INFO: Pod pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:33.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1589" for this suite. 09/08/23 22:41:33.613
------------------------------
â€¢ [SLOW TEST] [6.277 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:27.36
    Sep  8 22:41:27.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename configmap 09/08/23 22:41:27.361
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:27.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:27.431
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1589/configmap-test-dece4c7f-0e6f-475f-ba43-2353d7b1439b 09/08/23 22:41:27.438
    STEP: Creating a pod to test consume configMaps 09/08/23 22:41:27.458
    Sep  8 22:41:27.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c" in namespace "configmap-1589" to be "Succeeded or Failed"
    Sep  8 22:41:27.510: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.33022ms
    Sep  8 22:41:29.520: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.01812839s
    Sep  8 22:41:31.520: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Running", Reason="", readiness=false. Elapsed: 4.018086391s
    Sep  8 22:41:33.523: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021269724s
    STEP: Saw pod success 09/08/23 22:41:33.523
    Sep  8 22:41:33.523: INFO: Pod "pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c" satisfied condition "Succeeded or Failed"
    Sep  8 22:41:33.547: INFO: Trying to get logs from node node-3 pod pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c container env-test: <nil>
    STEP: delete the pod 09/08/23 22:41:33.565
    Sep  8 22:41:33.594: INFO: Waiting for pod pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c to disappear
    Sep  8 22:41:33.602: INFO: Pod pod-configmaps-1b880a98-c1fc-4ca3-afc2-2febee992d5c no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:33.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1589" for this suite. 09/08/23 22:41:33.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:33.639
Sep  8 22:41:33.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 22:41:33.642
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:33.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:33.697
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:41:33.703
Sep  8 22:41:33.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb" in namespace "downward-api-7404" to be "Succeeded or Failed"
Sep  8 22:41:33.744: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.370858ms
Sep  8 22:41:35.770: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048560224s
Sep  8 22:41:37.754: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032364913s
Sep  8 22:41:39.776: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055241073s
STEP: Saw pod success 09/08/23 22:41:39.776
Sep  8 22:41:39.777: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb" satisfied condition "Succeeded or Failed"
Sep  8 22:41:39.791: INFO: Trying to get logs from node node-3 pod downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb container client-container: <nil>
STEP: delete the pod 09/08/23 22:41:39.827
Sep  8 22:41:39.905: INFO: Waiting for pod downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb to disappear
Sep  8 22:41:39.911: INFO: Pod downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:39.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7404" for this suite. 09/08/23 22:41:39.924
------------------------------
â€¢ [SLOW TEST] [6.325 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:33.639
    Sep  8 22:41:33.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 22:41:33.642
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:33.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:33.697
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:41:33.703
    Sep  8 22:41:33.721: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb" in namespace "downward-api-7404" to be "Succeeded or Failed"
    Sep  8 22:41:33.744: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.370858ms
    Sep  8 22:41:35.770: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048560224s
    Sep  8 22:41:37.754: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032364913s
    Sep  8 22:41:39.776: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055241073s
    STEP: Saw pod success 09/08/23 22:41:39.776
    Sep  8 22:41:39.777: INFO: Pod "downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb" satisfied condition "Succeeded or Failed"
    Sep  8 22:41:39.791: INFO: Trying to get logs from node node-3 pod downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb container client-container: <nil>
    STEP: delete the pod 09/08/23 22:41:39.827
    Sep  8 22:41:39.905: INFO: Waiting for pod downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb to disappear
    Sep  8 22:41:39.911: INFO: Pod downwardapi-volume-a25126ac-1ce8-4417-a9f2-993ce1b5ebbb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:39.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7404" for this suite. 09/08/23 22:41:39.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:39.966
Sep  8 22:41:39.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename resourcequota 09/08/23 22:41:39.968
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:40.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:40.033
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 09/08/23 22:41:40.039
STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:41:40.085
STEP: Creating a ResourceQuota with not best effort scope 09/08/23 22:41:42.093
STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:41:42.13
STEP: Creating a best-effort pod 09/08/23 22:41:44.147
STEP: Ensuring resource quota with best effort scope captures the pod usage 09/08/23 22:41:44.18
STEP: Ensuring resource quota with not best effort ignored the pod usage 09/08/23 22:41:46.19
STEP: Deleting the pod 09/08/23 22:41:48.209
STEP: Ensuring resource quota status released the pod usage 09/08/23 22:41:48.243
STEP: Creating a not best-effort pod 09/08/23 22:41:50.253
STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/08/23 22:41:50.291
STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/08/23 22:41:52.306
STEP: Deleting the pod 09/08/23 22:41:54.318
STEP: Ensuring resource quota status released the pod usage 09/08/23 22:41:54.348
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Sep  8 22:41:56.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7353" for this suite. 09/08/23 22:41:56.379
------------------------------
â€¢ [SLOW TEST] [16.439 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:39.966
    Sep  8 22:41:39.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename resourcequota 09/08/23 22:41:39.968
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:40.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:40.033
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 09/08/23 22:41:40.039
    STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:41:40.085
    STEP: Creating a ResourceQuota with not best effort scope 09/08/23 22:41:42.093
    STEP: Ensuring ResourceQuota status is calculated 09/08/23 22:41:42.13
    STEP: Creating a best-effort pod 09/08/23 22:41:44.147
    STEP: Ensuring resource quota with best effort scope captures the pod usage 09/08/23 22:41:44.18
    STEP: Ensuring resource quota with not best effort ignored the pod usage 09/08/23 22:41:46.19
    STEP: Deleting the pod 09/08/23 22:41:48.209
    STEP: Ensuring resource quota status released the pod usage 09/08/23 22:41:48.243
    STEP: Creating a not best-effort pod 09/08/23 22:41:50.253
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 09/08/23 22:41:50.291
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 09/08/23 22:41:52.306
    STEP: Deleting the pod 09/08/23 22:41:54.318
    STEP: Ensuring resource quota status released the pod usage 09/08/23 22:41:54.348
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:41:56.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7353" for this suite. 09/08/23 22:41:56.379
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:41:56.412
Sep  8 22:41:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:41:56.414
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:56.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:56.464
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-5b761df4-a167-4974-be1d-4e54f28cb058 09/08/23 22:41:56.487
STEP: Creating secret with name s-test-opt-upd-a0e05000-336d-43eb-9527-72f2e9630632 09/08/23 22:41:56.521
STEP: Creating the pod 09/08/23 22:41:56.541
Sep  8 22:41:56.568: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989" in namespace "projected-5796" to be "running and ready"
Sep  8 22:41:56.584: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989": Phase="Pending", Reason="", readiness=false. Elapsed: 11.647934ms
Sep  8 22:41:56.584: INFO: The phase of Pod pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:41:58.596: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023550368s
Sep  8 22:41:58.596: INFO: The phase of Pod pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:42:00.598: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989": Phase="Running", Reason="", readiness=true. Elapsed: 4.024990635s
Sep  8 22:42:00.598: INFO: The phase of Pod pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989 is Running (Ready = true)
Sep  8 22:42:00.598: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-5b761df4-a167-4974-be1d-4e54f28cb058 09/08/23 22:42:00.656
STEP: Updating secret s-test-opt-upd-a0e05000-336d-43eb-9527-72f2e9630632 09/08/23 22:42:00.676
STEP: Creating secret with name s-test-opt-create-06cd1a1b-81bf-4faf-8952-d3dc47ff8902 09/08/23 22:42:00.692
STEP: waiting to observe update in volume 09/08/23 22:42:00.703
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  8 22:43:23.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5796" for this suite. 09/08/23 22:43:23.806
------------------------------
â€¢ [SLOW TEST] [87.413 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:41:56.412
    Sep  8 22:41:56.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:41:56.414
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:41:56.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:41:56.464
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-5b761df4-a167-4974-be1d-4e54f28cb058 09/08/23 22:41:56.487
    STEP: Creating secret with name s-test-opt-upd-a0e05000-336d-43eb-9527-72f2e9630632 09/08/23 22:41:56.521
    STEP: Creating the pod 09/08/23 22:41:56.541
    Sep  8 22:41:56.568: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989" in namespace "projected-5796" to be "running and ready"
    Sep  8 22:41:56.584: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989": Phase="Pending", Reason="", readiness=false. Elapsed: 11.647934ms
    Sep  8 22:41:56.584: INFO: The phase of Pod pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:41:58.596: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023550368s
    Sep  8 22:41:58.596: INFO: The phase of Pod pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:42:00.598: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989": Phase="Running", Reason="", readiness=true. Elapsed: 4.024990635s
    Sep  8 22:42:00.598: INFO: The phase of Pod pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989 is Running (Ready = true)
    Sep  8 22:42:00.598: INFO: Pod "pod-projected-secrets-bdb98f0b-1077-40fe-a312-ec092e14e989" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-5b761df4-a167-4974-be1d-4e54f28cb058 09/08/23 22:42:00.656
    STEP: Updating secret s-test-opt-upd-a0e05000-336d-43eb-9527-72f2e9630632 09/08/23 22:42:00.676
    STEP: Creating secret with name s-test-opt-create-06cd1a1b-81bf-4faf-8952-d3dc47ff8902 09/08/23 22:42:00.692
    STEP: waiting to observe update in volume 09/08/23 22:42:00.703
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:43:23.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5796" for this suite. 09/08/23 22:43:23.806
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:43:23.826
Sep  8 22:43:23.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-runtime 09/08/23 22:43:23.828
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:23.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:23.896
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 09/08/23 22:43:23.902
STEP: wait for the container to reach Succeeded 09/08/23 22:43:23.955
STEP: get the container status 09/08/23 22:43:29.026
STEP: the container should be terminated 09/08/23 22:43:29.035
STEP: the termination message should be set 09/08/23 22:43:29.035
Sep  8 22:43:29.036: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 09/08/23 22:43:29.036
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  8 22:43:29.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7978" for this suite. 09/08/23 22:43:29.1
------------------------------
â€¢ [SLOW TEST] [5.299 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:43:23.826
    Sep  8 22:43:23.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-runtime 09/08/23 22:43:23.828
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:23.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:23.896
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 09/08/23 22:43:23.902
    STEP: wait for the container to reach Succeeded 09/08/23 22:43:23.955
    STEP: get the container status 09/08/23 22:43:29.026
    STEP: the container should be terminated 09/08/23 22:43:29.035
    STEP: the termination message should be set 09/08/23 22:43:29.035
    Sep  8 22:43:29.036: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 09/08/23 22:43:29.036
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:43:29.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7978" for this suite. 09/08/23 22:43:29.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:43:29.134
Sep  8 22:43:29.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:43:29.135
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:29.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:29.185
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 09/08/23 22:43:29.193
Sep  8 22:43:29.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3675 cluster-info'
Sep  8 22:43:29.315: INFO: stderr: ""
Sep  8 22:43:29.315: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:43:29.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3675" for this suite. 09/08/23 22:43:29.335
------------------------------
â€¢ [0.233 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:43:29.134
    Sep  8 22:43:29.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:43:29.135
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:29.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:29.185
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 09/08/23 22:43:29.193
    Sep  8 22:43:29.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3675 cluster-info'
    Sep  8 22:43:29.315: INFO: stderr: ""
    Sep  8 22:43:29.315: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:43:29.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3675" for this suite. 09/08/23 22:43:29.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:43:29.368
Sep  8 22:43:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replication-controller 09/08/23 22:43:29.369
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:29.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:29.421
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 09/08/23 22:43:29.441
STEP: waiting for RC to be added 09/08/23 22:43:29.454
STEP: waiting for available Replicas 09/08/23 22:43:29.455
STEP: patching ReplicationController 09/08/23 22:43:31.391
STEP: waiting for RC to be modified 09/08/23 22:43:31.408
STEP: patching ReplicationController status 09/08/23 22:43:31.408
STEP: waiting for RC to be modified 09/08/23 22:43:31.435
STEP: waiting for available Replicas 09/08/23 22:43:31.435
STEP: fetching ReplicationController status 09/08/23 22:43:31.439
STEP: patching ReplicationController scale 09/08/23 22:43:31.451
STEP: waiting for RC to be modified 09/08/23 22:43:31.466
STEP: waiting for ReplicationController's scale to be the max amount 09/08/23 22:43:31.467
STEP: fetching ReplicationController; ensuring that it's patched 09/08/23 22:43:33.298
STEP: updating ReplicationController status 09/08/23 22:43:33.312
STEP: waiting for RC to be modified 09/08/23 22:43:33.335
STEP: listing all ReplicationControllers 09/08/23 22:43:33.336
STEP: checking that ReplicationController has expected values 09/08/23 22:43:33.349
STEP: deleting ReplicationControllers by collection 09/08/23 22:43:33.35
STEP: waiting for ReplicationController to have a DELETED watchEvent 09/08/23 22:43:33.382
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Sep  8 22:43:33.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8798" for this suite. 09/08/23 22:43:33.504
------------------------------
â€¢ [4.165 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:43:29.368
    Sep  8 22:43:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replication-controller 09/08/23 22:43:29.369
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:29.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:29.421
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 09/08/23 22:43:29.441
    STEP: waiting for RC to be added 09/08/23 22:43:29.454
    STEP: waiting for available Replicas 09/08/23 22:43:29.455
    STEP: patching ReplicationController 09/08/23 22:43:31.391
    STEP: waiting for RC to be modified 09/08/23 22:43:31.408
    STEP: patching ReplicationController status 09/08/23 22:43:31.408
    STEP: waiting for RC to be modified 09/08/23 22:43:31.435
    STEP: waiting for available Replicas 09/08/23 22:43:31.435
    STEP: fetching ReplicationController status 09/08/23 22:43:31.439
    STEP: patching ReplicationController scale 09/08/23 22:43:31.451
    STEP: waiting for RC to be modified 09/08/23 22:43:31.466
    STEP: waiting for ReplicationController's scale to be the max amount 09/08/23 22:43:31.467
    STEP: fetching ReplicationController; ensuring that it's patched 09/08/23 22:43:33.298
    STEP: updating ReplicationController status 09/08/23 22:43:33.312
    STEP: waiting for RC to be modified 09/08/23 22:43:33.335
    STEP: listing all ReplicationControllers 09/08/23 22:43:33.336
    STEP: checking that ReplicationController has expected values 09/08/23 22:43:33.349
    STEP: deleting ReplicationControllers by collection 09/08/23 22:43:33.35
    STEP: waiting for ReplicationController to have a DELETED watchEvent 09/08/23 22:43:33.382
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:43:33.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8798" for this suite. 09/08/23 22:43:33.504
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:43:33.533
Sep  8 22:43:33.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename gc 09/08/23 22:43:33.535
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:33.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:33.599
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 09/08/23 22:43:33.613
STEP: Wait for the Deployment to create new ReplicaSet 09/08/23 22:43:33.63
STEP: delete the deployment 09/08/23 22:43:34.157
STEP: wait for all rs to be garbage collected 09/08/23 22:43:34.177
STEP: expected 0 rs, got 1 rs 09/08/23 22:43:34.224
STEP: expected 0 pods, got 2 pods 09/08/23 22:43:34.243
STEP: Gathering metrics 09/08/23 22:43:34.79
Sep  8 22:43:34.861: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
Sep  8 22:43:34.873: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.81798ms
Sep  8 22:43:34.873: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
Sep  8 22:43:34.873: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
Sep  8 22:43:35.004: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Sep  8 22:43:35.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4595" for this suite. 09/08/23 22:43:35.013
------------------------------
â€¢ [1.528 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:43:33.533
    Sep  8 22:43:33.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename gc 09/08/23 22:43:33.535
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:33.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:33.599
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 09/08/23 22:43:33.613
    STEP: Wait for the Deployment to create new ReplicaSet 09/08/23 22:43:33.63
    STEP: delete the deployment 09/08/23 22:43:34.157
    STEP: wait for all rs to be garbage collected 09/08/23 22:43:34.177
    STEP: expected 0 rs, got 1 rs 09/08/23 22:43:34.224
    STEP: expected 0 pods, got 2 pods 09/08/23 22:43:34.243
    STEP: Gathering metrics 09/08/23 22:43:34.79
    Sep  8 22:43:34.861: INFO: Waiting up to 5m0s for pod "kube-controller-manager-node-2" in namespace "kube-system" to be "running and ready"
    Sep  8 22:43:34.873: INFO: Pod "kube-controller-manager-node-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.81798ms
    Sep  8 22:43:34.873: INFO: The phase of Pod kube-controller-manager-node-2 is Running (Ready = true)
    Sep  8 22:43:34.873: INFO: Pod "kube-controller-manager-node-2" satisfied condition "running and ready"
    Sep  8 22:43:35.004: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:43:35.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4595" for this suite. 09/08/23 22:43:35.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:43:35.063
Sep  8 22:43:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename controllerrevisions 09/08/23 22:43:35.064
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:35.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:35.125
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-fckhz-daemon-set" 09/08/23 22:43:35.191
STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:43:35.208
Sep  8 22:43:35.219: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:35.219: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:35.219: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:35.228: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
Sep  8 22:43:35.229: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:43:36.246: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:36.246: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:36.246: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:36.259: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
Sep  8 22:43:36.259: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:43:37.241: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:37.241: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:37.242: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:37.254: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
Sep  8 22:43:37.254: INFO: Node node-3 is running 0 daemon pod, expected 1
Sep  8 22:43:38.243: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:38.243: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:38.243: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  8 22:43:38.253: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 2
Sep  8 22:43:38.253: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-fckhz-daemon-set
STEP: Confirm DaemonSet "e2e-fckhz-daemon-set" successfully created with "daemonset-name=e2e-fckhz-daemon-set" label 09/08/23 22:43:38.262
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fckhz-daemon-set" 09/08/23 22:43:38.29
Sep  8 22:43:38.302: INFO: Located ControllerRevision: "e2e-fckhz-daemon-set-d6cbcdb5f"
STEP: Patching ControllerRevision "e2e-fckhz-daemon-set-d6cbcdb5f" 09/08/23 22:43:38.311
Sep  8 22:43:38.329: INFO: e2e-fckhz-daemon-set-d6cbcdb5f has been patched
STEP: Create a new ControllerRevision 09/08/23 22:43:38.329
Sep  8 22:43:38.348: INFO: Created ControllerRevision: e2e-fckhz-daemon-set-776b87dd87
STEP: Confirm that there are two ControllerRevisions 09/08/23 22:43:38.348
Sep  8 22:43:38.349: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  8 22:43:38.358: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-fckhz-daemon-set-d6cbcdb5f" 09/08/23 22:43:38.358
STEP: Confirm that there is only one ControllerRevision 09/08/23 22:43:38.388
Sep  8 22:43:38.389: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  8 22:43:38.399: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-fckhz-daemon-set-776b87dd87" 09/08/23 22:43:38.413
Sep  8 22:43:38.442: INFO: e2e-fckhz-daemon-set-776b87dd87 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 09/08/23 22:43:38.442
W0908 22:43:38.454105      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 09/08/23 22:43:38.454
Sep  8 22:43:38.454: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  8 22:43:39.464: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  8 22:43:39.473: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fckhz-daemon-set-776b87dd87=updated" 09/08/23 22:43:39.473
STEP: Confirm that there is only one ControllerRevision 09/08/23 22:43:39.505
Sep  8 22:43:39.505: INFO: Requesting list of ControllerRevisions to confirm quantity
Sep  8 22:43:39.521: INFO: Found 1 ControllerRevisions
Sep  8 22:43:39.530: INFO: ControllerRevision "e2e-fckhz-daemon-set-6459c45c98" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-fckhz-daemon-set" 09/08/23 22:43:39.541
STEP: deleting DaemonSet.extensions e2e-fckhz-daemon-set in namespace controllerrevisions-129, will wait for the garbage collector to delete the pods 09/08/23 22:43:39.541
Sep  8 22:43:39.625: INFO: Deleting DaemonSet.extensions e2e-fckhz-daemon-set took: 19.646142ms
Sep  8 22:43:39.726: INFO: Terminating DaemonSet.extensions e2e-fckhz-daemon-set pods took: 100.454162ms
Sep  8 22:43:41.640: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
Sep  8 22:43:41.640: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fckhz-daemon-set
Sep  8 22:43:41.651: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56825"},"items":null}

Sep  8 22:43:41.657: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56825"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:43:41.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-129" for this suite. 09/08/23 22:43:41.707
------------------------------
â€¢ [SLOW TEST] [6.674 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:43:35.063
    Sep  8 22:43:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename controllerrevisions 09/08/23 22:43:35.064
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:35.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:35.125
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-fckhz-daemon-set" 09/08/23 22:43:35.191
    STEP: Check that daemon pods launch on every node of the cluster. 09/08/23 22:43:35.208
    Sep  8 22:43:35.219: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:35.219: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:35.219: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:35.228: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
    Sep  8 22:43:35.229: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:43:36.246: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:36.246: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:36.246: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:36.259: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
    Sep  8 22:43:36.259: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:43:37.241: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:37.241: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:37.242: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:37.254: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
    Sep  8 22:43:37.254: INFO: Node node-3 is running 0 daemon pod, expected 1
    Sep  8 22:43:38.243: INFO: DaemonSet pods can't tolerate node node-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:38.243: INFO: DaemonSet pods can't tolerate node node-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:38.243: INFO: DaemonSet pods can't tolerate node node-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Sep  8 22:43:38.253: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 2
    Sep  8 22:43:38.253: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-fckhz-daemon-set
    STEP: Confirm DaemonSet "e2e-fckhz-daemon-set" successfully created with "daemonset-name=e2e-fckhz-daemon-set" label 09/08/23 22:43:38.262
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fckhz-daemon-set" 09/08/23 22:43:38.29
    Sep  8 22:43:38.302: INFO: Located ControllerRevision: "e2e-fckhz-daemon-set-d6cbcdb5f"
    STEP: Patching ControllerRevision "e2e-fckhz-daemon-set-d6cbcdb5f" 09/08/23 22:43:38.311
    Sep  8 22:43:38.329: INFO: e2e-fckhz-daemon-set-d6cbcdb5f has been patched
    STEP: Create a new ControllerRevision 09/08/23 22:43:38.329
    Sep  8 22:43:38.348: INFO: Created ControllerRevision: e2e-fckhz-daemon-set-776b87dd87
    STEP: Confirm that there are two ControllerRevisions 09/08/23 22:43:38.348
    Sep  8 22:43:38.349: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  8 22:43:38.358: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-fckhz-daemon-set-d6cbcdb5f" 09/08/23 22:43:38.358
    STEP: Confirm that there is only one ControllerRevision 09/08/23 22:43:38.388
    Sep  8 22:43:38.389: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  8 22:43:38.399: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-fckhz-daemon-set-776b87dd87" 09/08/23 22:43:38.413
    Sep  8 22:43:38.442: INFO: e2e-fckhz-daemon-set-776b87dd87 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 09/08/23 22:43:38.442
    W0908 22:43:38.454105      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 09/08/23 22:43:38.454
    Sep  8 22:43:38.454: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  8 22:43:39.464: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  8 22:43:39.473: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fckhz-daemon-set-776b87dd87=updated" 09/08/23 22:43:39.473
    STEP: Confirm that there is only one ControllerRevision 09/08/23 22:43:39.505
    Sep  8 22:43:39.505: INFO: Requesting list of ControllerRevisions to confirm quantity
    Sep  8 22:43:39.521: INFO: Found 1 ControllerRevisions
    Sep  8 22:43:39.530: INFO: ControllerRevision "e2e-fckhz-daemon-set-6459c45c98" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-fckhz-daemon-set" 09/08/23 22:43:39.541
    STEP: deleting DaemonSet.extensions e2e-fckhz-daemon-set in namespace controllerrevisions-129, will wait for the garbage collector to delete the pods 09/08/23 22:43:39.541
    Sep  8 22:43:39.625: INFO: Deleting DaemonSet.extensions e2e-fckhz-daemon-set took: 19.646142ms
    Sep  8 22:43:39.726: INFO: Terminating DaemonSet.extensions e2e-fckhz-daemon-set pods took: 100.454162ms
    Sep  8 22:43:41.640: INFO: Number of nodes with available pods controlled by daemonset e2e-fckhz-daemon-set: 0
    Sep  8 22:43:41.640: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fckhz-daemon-set
    Sep  8 22:43:41.651: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56825"},"items":null}

    Sep  8 22:43:41.657: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56825"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:43:41.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-129" for this suite. 09/08/23 22:43:41.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:43:41.739
Sep  8 22:43:41.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:43:41.742
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:41.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:41.797
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-acc2fd82-c6fd-4dd5-9753-647deab11c16 09/08/23 22:43:41.819
STEP: Creating a pod to test consume secrets 09/08/23 22:43:41.836
Sep  8 22:43:41.867: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341" in namespace "projected-4552" to be "Succeeded or Failed"
Sep  8 22:43:41.899: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341": Phase="Pending", Reason="", readiness=false. Elapsed: 32.066086ms
Sep  8 22:43:43.914: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047523161s
Sep  8 22:43:45.909: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042100714s
STEP: Saw pod success 09/08/23 22:43:45.909
Sep  8 22:43:45.909: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341" satisfied condition "Succeeded or Failed"
Sep  8 22:43:45.918: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341 container projected-secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:43:45.935
Sep  8 22:43:45.997: INFO: Waiting for pod pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341 to disappear
Sep  8 22:43:46.013: INFO: Pod pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  8 22:43:46.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4552" for this suite. 09/08/23 22:43:46.034
------------------------------
â€¢ [4.315 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:43:41.739
    Sep  8 22:43:41.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:43:41.742
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:41.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:41.797
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-acc2fd82-c6fd-4dd5-9753-647deab11c16 09/08/23 22:43:41.819
    STEP: Creating a pod to test consume secrets 09/08/23 22:43:41.836
    Sep  8 22:43:41.867: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341" in namespace "projected-4552" to be "Succeeded or Failed"
    Sep  8 22:43:41.899: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341": Phase="Pending", Reason="", readiness=false. Elapsed: 32.066086ms
    Sep  8 22:43:43.914: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047523161s
    Sep  8 22:43:45.909: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042100714s
    STEP: Saw pod success 09/08/23 22:43:45.909
    Sep  8 22:43:45.909: INFO: Pod "pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341" satisfied condition "Succeeded or Failed"
    Sep  8 22:43:45.918: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341 container projected-secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:43:45.935
    Sep  8 22:43:45.997: INFO: Waiting for pod pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341 to disappear
    Sep  8 22:43:46.013: INFO: Pod pod-projected-secrets-53efe8b1-c5e6-488b-834c-467ca5ca5341 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:43:46.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4552" for this suite. 09/08/23 22:43:46.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:43:46.057
Sep  8 22:43:46.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 22:43:46.059
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:46.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:46.113
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a in namespace container-probe-9527 09/08/23 22:43:46.121
Sep  8 22:43:46.149: INFO: Waiting up to 5m0s for pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a" in namespace "container-probe-9527" to be "not pending"
Sep  8 22:43:46.192: INFO: Pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 43.233102ms
Sep  8 22:43:48.202: INFO: Pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.053097253s
Sep  8 22:43:48.203: INFO: Pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a" satisfied condition "not pending"
Sep  8 22:43:48.203: INFO: Started pod busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a in namespace container-probe-9527
STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:43:48.203
Sep  8 22:43:48.217: INFO: Initial restart count of pod busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a is 0
Sep  8 22:44:38.559: INFO: Restart count of pod container-probe-9527/busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a is now 1 (50.341610786s elapsed)
STEP: deleting the pod 09/08/23 22:44:38.559
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 22:44:38.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9527" for this suite. 09/08/23 22:44:38.615
------------------------------
â€¢ [SLOW TEST] [52.573 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:43:46.057
    Sep  8 22:43:46.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 22:43:46.059
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:43:46.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:43:46.113
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a in namespace container-probe-9527 09/08/23 22:43:46.121
    Sep  8 22:43:46.149: INFO: Waiting up to 5m0s for pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a" in namespace "container-probe-9527" to be "not pending"
    Sep  8 22:43:46.192: INFO: Pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 43.233102ms
    Sep  8 22:43:48.202: INFO: Pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.053097253s
    Sep  8 22:43:48.203: INFO: Pod "busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a" satisfied condition "not pending"
    Sep  8 22:43:48.203: INFO: Started pod busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a in namespace container-probe-9527
    STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:43:48.203
    Sep  8 22:43:48.217: INFO: Initial restart count of pod busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a is 0
    Sep  8 22:44:38.559: INFO: Restart count of pod container-probe-9527/busybox-7f6938a8-df4a-4a67-87e0-521c490f2d6a is now 1 (50.341610786s elapsed)
    STEP: deleting the pod 09/08/23 22:44:38.559
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:44:38.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9527" for this suite. 09/08/23 22:44:38.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:44:38.63
Sep  8 22:44:38.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 22:44:38.632
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:44:38.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:44:38.696
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Sep  8 22:44:38.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/08/23 22:44:46.719
Sep  8 22:44:46.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 create -f -'
Sep  8 22:44:47.843: INFO: stderr: ""
Sep  8 22:44:47.843: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  8 22:44:47.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 delete e2e-test-crd-publish-openapi-155-crds test-cr'
Sep  8 22:44:48.055: INFO: stderr: ""
Sep  8 22:44:48.055: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  8 22:44:48.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 apply -f -'
Sep  8 22:44:48.395: INFO: stderr: ""
Sep  8 22:44:48.395: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  8 22:44:48.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 delete e2e-test-crd-publish-openapi-155-crds test-cr'
Sep  8 22:44:48.561: INFO: stderr: ""
Sep  8 22:44:48.561: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 09/08/23 22:44:48.561
Sep  8 22:44:48.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 explain e2e-test-crd-publish-openapi-155-crds'
Sep  8 22:44:49.499: INFO: stderr: ""
Sep  8 22:44:49.499: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-155-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:44:52.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3928" for this suite. 09/08/23 22:44:52.458
------------------------------
â€¢ [SLOW TEST] [13.863 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:44:38.63
    Sep  8 22:44:38.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename crd-publish-openapi 09/08/23 22:44:38.632
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:44:38.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:44:38.696
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Sep  8 22:44:38.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 09/08/23 22:44:46.719
    Sep  8 22:44:46.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 create -f -'
    Sep  8 22:44:47.843: INFO: stderr: ""
    Sep  8 22:44:47.843: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  8 22:44:47.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 delete e2e-test-crd-publish-openapi-155-crds test-cr'
    Sep  8 22:44:48.055: INFO: stderr: ""
    Sep  8 22:44:48.055: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Sep  8 22:44:48.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 apply -f -'
    Sep  8 22:44:48.395: INFO: stderr: ""
    Sep  8 22:44:48.395: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Sep  8 22:44:48.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 --namespace=crd-publish-openapi-3928 delete e2e-test-crd-publish-openapi-155-crds test-cr'
    Sep  8 22:44:48.561: INFO: stderr: ""
    Sep  8 22:44:48.561: INFO: stdout: "e2e-test-crd-publish-openapi-155-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 09/08/23 22:44:48.561
    Sep  8 22:44:48.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=crd-publish-openapi-3928 explain e2e-test-crd-publish-openapi-155-crds'
    Sep  8 22:44:49.499: INFO: stderr: ""
    Sep  8 22:44:49.499: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-155-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:44:52.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3928" for this suite. 09/08/23 22:44:52.458
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:44:52.495
Sep  8 22:44:52.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 22:44:52.496
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:44:52.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:44:52.553
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Sep  8 22:44:52.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:45:54.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-578" for this suite. 09/08/23 22:45:54.553
------------------------------
â€¢ [SLOW TEST] [62.081 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:44:52.495
    Sep  8 22:44:52.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 22:44:52.496
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:44:52.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:44:52.553
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Sep  8 22:44:52.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:45:54.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-578" for this suite. 09/08/23 22:45:54.553
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:45:54.576
Sep  8 22:45:54.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubelet-test 09/08/23 22:45:54.577
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:45:54.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:45:54.622
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Sep  8 22:45:54.672: INFO: Waiting up to 5m0s for pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd" in namespace "kubelet-test-1114" to be "running and ready"
Sep  8 22:45:54.681: INFO: Pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.127979ms
Sep  8 22:45:54.692: INFO: The phase of Pod busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:45:56.714: INFO: Pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.042212897s
Sep  8 22:45:56.714: INFO: The phase of Pod busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd is Running (Ready = true)
Sep  8 22:45:56.714: INFO: Pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:45:56.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1114" for this suite. 09/08/23 22:45:56.782
------------------------------
â€¢ [2.224 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:45:54.576
    Sep  8 22:45:54.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubelet-test 09/08/23 22:45:54.577
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:45:54.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:45:54.622
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Sep  8 22:45:54.672: INFO: Waiting up to 5m0s for pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd" in namespace "kubelet-test-1114" to be "running and ready"
    Sep  8 22:45:54.681: INFO: Pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.127979ms
    Sep  8 22:45:54.692: INFO: The phase of Pod busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:45:56.714: INFO: Pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.042212897s
    Sep  8 22:45:56.714: INFO: The phase of Pod busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd is Running (Ready = true)
    Sep  8 22:45:56.714: INFO: Pod "busybox-scheduling-b6c91051-9dd0-44bd-9c1b-697fb698c0dd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:45:56.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1114" for this suite. 09/08/23 22:45:56.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:45:56.8
Sep  8 22:45:56.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename replicaset 09/08/23 22:45:56.801
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:45:56.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:45:56.845
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Sep  8 22:45:56.884: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  8 22:46:01.897: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 09/08/23 22:46:01.897
STEP: Scaling up "test-rs" replicaset  09/08/23 22:46:01.898
Sep  8 22:46:01.926: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 09/08/23 22:46:01.926
W0908 22:46:01.991592      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Sep  8 22:46:01.996: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
Sep  8 22:46:02.057: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
Sep  8 22:46:02.126: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
Sep  8 22:46:02.157: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
Sep  8 22:46:04.197: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 2, AvailableReplicas 2
Sep  8 22:46:04.236: INFO: observed Replicaset test-rs in namespace replicaset-5875 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:46:04.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5875" for this suite. 09/08/23 22:46:04.253
------------------------------
â€¢ [SLOW TEST] [7.481 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:45:56.8
    Sep  8 22:45:56.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename replicaset 09/08/23 22:45:56.801
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:45:56.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:45:56.845
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Sep  8 22:45:56.884: INFO: Pod name sample-pod: Found 0 pods out of 1
    Sep  8 22:46:01.897: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 09/08/23 22:46:01.897
    STEP: Scaling up "test-rs" replicaset  09/08/23 22:46:01.898
    Sep  8 22:46:01.926: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 09/08/23 22:46:01.926
    W0908 22:46:01.991592      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Sep  8 22:46:01.996: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
    Sep  8 22:46:02.057: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
    Sep  8 22:46:02.126: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
    Sep  8 22:46:02.157: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 1, AvailableReplicas 1
    Sep  8 22:46:04.197: INFO: observed ReplicaSet test-rs in namespace replicaset-5875 with ReadyReplicas 2, AvailableReplicas 2
    Sep  8 22:46:04.236: INFO: observed Replicaset test-rs in namespace replicaset-5875 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:46:04.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5875" for this suite. 09/08/23 22:46:04.253
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:46:04.282
Sep  8 22:46:04.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename sched-preemption 09/08/23 22:46:04.284
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:46:04.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:46:04.334
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Sep  8 22:46:04.415: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  8 22:47:04.534: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 09/08/23 22:47:04.547
Sep  8 22:47:04.645: INFO: Created pod: pod0-0-sched-preemption-low-priority
Sep  8 22:47:04.662: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Sep  8 22:47:04.737: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Sep  8 22:47:04.764: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 09/08/23 22:47:04.765
Sep  8 22:47:04.766: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9426" to be "running"
Sep  8 22:47:04.780: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.292693ms
Sep  8 22:47:06.791: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.025106916s
Sep  8 22:47:06.792: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Sep  8 22:47:06.792: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9426" to be "running"
Sep  8 22:47:06.805: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.239212ms
Sep  8 22:47:06.805: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Sep  8 22:47:06.805: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9426" to be "running"
Sep  8 22:47:06.814: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.185266ms
Sep  8 22:47:06.814: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Sep  8 22:47:06.814: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9426" to be "running"
Sep  8 22:47:06.822: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.753223ms
Sep  8 22:47:06.822: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 09/08/23 22:47:06.822
Sep  8 22:47:06.862: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Sep  8 22:47:06.870: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.425892ms
Sep  8 22:47:08.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018771351s
Sep  8 22:47:10.879: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016764485s
Sep  8 22:47:12.886: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.023536657s
Sep  8 22:47:12.886: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:47:12.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9426" for this suite. 09/08/23 22:47:13.117
------------------------------
â€¢ [SLOW TEST] [68.866 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:46:04.282
    Sep  8 22:46:04.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename sched-preemption 09/08/23 22:46:04.284
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:46:04.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:46:04.334
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Sep  8 22:46:04.415: INFO: Waiting up to 1m0s for all nodes to be ready
    Sep  8 22:47:04.534: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 09/08/23 22:47:04.547
    Sep  8 22:47:04.645: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Sep  8 22:47:04.662: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Sep  8 22:47:04.737: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Sep  8 22:47:04.764: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 09/08/23 22:47:04.765
    Sep  8 22:47:04.766: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9426" to be "running"
    Sep  8 22:47:04.780: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 13.292693ms
    Sep  8 22:47:06.791: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.025106916s
    Sep  8 22:47:06.792: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Sep  8 22:47:06.792: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9426" to be "running"
    Sep  8 22:47:06.805: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 13.239212ms
    Sep  8 22:47:06.805: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Sep  8 22:47:06.805: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9426" to be "running"
    Sep  8 22:47:06.814: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.185266ms
    Sep  8 22:47:06.814: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Sep  8 22:47:06.814: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9426" to be "running"
    Sep  8 22:47:06.822: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.753223ms
    Sep  8 22:47:06.822: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 09/08/23 22:47:06.822
    Sep  8 22:47:06.862: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Sep  8 22:47:06.870: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.425892ms
    Sep  8 22:47:08.881: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018771351s
    Sep  8 22:47:10.879: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016764485s
    Sep  8 22:47:12.886: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.023536657s
    Sep  8 22:47:12.886: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:47:12.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9426" for this suite. 09/08/23 22:47:13.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:47:13.158
Sep  8 22:47:13.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename runtimeclass 09/08/23 22:47:13.16
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:13.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:13.218
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Sep  8 22:47:13.279: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8412 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  8 22:47:13.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8412" for this suite. 09/08/23 22:47:13.377
------------------------------
â€¢ [0.252 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:47:13.158
    Sep  8 22:47:13.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename runtimeclass 09/08/23 22:47:13.16
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:13.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:13.218
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Sep  8 22:47:13.279: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8412 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:47:13.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8412" for this suite. 09/08/23 22:47:13.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:47:13.413
Sep  8 22:47:13.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename subpath 09/08/23 22:47:13.414
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:13.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:13.463
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/08/23 22:47:13.471
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-prcw 09/08/23 22:47:13.513
STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:47:13.513
Sep  8 22:47:13.547: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-prcw" in namespace "subpath-9102" to be "Succeeded or Failed"
Sep  8 22:47:13.567: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Pending", Reason="", readiness=false. Elapsed: 19.811281ms
Sep  8 22:47:15.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.029921231s
Sep  8 22:47:17.586: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 4.039187878s
Sep  8 22:47:19.582: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 6.035120575s
Sep  8 22:47:21.580: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 8.032607303s
Sep  8 22:47:23.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 10.02963814s
Sep  8 22:47:25.578: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 12.030767507s
Sep  8 22:47:27.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 14.03011579s
Sep  8 22:47:29.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 16.029616048s
Sep  8 22:47:31.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 18.030001686s
Sep  8 22:47:33.578: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 20.030696903s
Sep  8 22:47:35.576: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 22.028472061s
Sep  8 22:47:37.575: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=false. Elapsed: 24.028318342s
Sep  8 22:47:39.575: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.028010016s
STEP: Saw pod success 09/08/23 22:47:39.575
Sep  8 22:47:39.575: INFO: Pod "pod-subpath-test-configmap-prcw" satisfied condition "Succeeded or Failed"
Sep  8 22:47:39.591: INFO: Trying to get logs from node node-3 pod pod-subpath-test-configmap-prcw container test-container-subpath-configmap-prcw: <nil>
STEP: delete the pod 09/08/23 22:47:39.631
Sep  8 22:47:39.686: INFO: Waiting for pod pod-subpath-test-configmap-prcw to disappear
Sep  8 22:47:39.703: INFO: Pod pod-subpath-test-configmap-prcw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-prcw 09/08/23 22:47:39.703
Sep  8 22:47:39.704: INFO: Deleting pod "pod-subpath-test-configmap-prcw" in namespace "subpath-9102"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  8 22:47:39.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9102" for this suite. 09/08/23 22:47:39.733
------------------------------
â€¢ [SLOW TEST] [26.344 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:47:13.413
    Sep  8 22:47:13.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename subpath 09/08/23 22:47:13.414
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:13.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:13.463
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/08/23 22:47:13.471
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-prcw 09/08/23 22:47:13.513
    STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:47:13.513
    Sep  8 22:47:13.547: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-prcw" in namespace "subpath-9102" to be "Succeeded or Failed"
    Sep  8 22:47:13.567: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Pending", Reason="", readiness=false. Elapsed: 19.811281ms
    Sep  8 22:47:15.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 2.029921231s
    Sep  8 22:47:17.586: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 4.039187878s
    Sep  8 22:47:19.582: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 6.035120575s
    Sep  8 22:47:21.580: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 8.032607303s
    Sep  8 22:47:23.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 10.02963814s
    Sep  8 22:47:25.578: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 12.030767507s
    Sep  8 22:47:27.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 14.03011579s
    Sep  8 22:47:29.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 16.029616048s
    Sep  8 22:47:31.577: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 18.030001686s
    Sep  8 22:47:33.578: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 20.030696903s
    Sep  8 22:47:35.576: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=true. Elapsed: 22.028472061s
    Sep  8 22:47:37.575: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Running", Reason="", readiness=false. Elapsed: 24.028318342s
    Sep  8 22:47:39.575: INFO: Pod "pod-subpath-test-configmap-prcw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.028010016s
    STEP: Saw pod success 09/08/23 22:47:39.575
    Sep  8 22:47:39.575: INFO: Pod "pod-subpath-test-configmap-prcw" satisfied condition "Succeeded or Failed"
    Sep  8 22:47:39.591: INFO: Trying to get logs from node node-3 pod pod-subpath-test-configmap-prcw container test-container-subpath-configmap-prcw: <nil>
    STEP: delete the pod 09/08/23 22:47:39.631
    Sep  8 22:47:39.686: INFO: Waiting for pod pod-subpath-test-configmap-prcw to disappear
    Sep  8 22:47:39.703: INFO: Pod pod-subpath-test-configmap-prcw no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-prcw 09/08/23 22:47:39.703
    Sep  8 22:47:39.704: INFO: Deleting pod "pod-subpath-test-configmap-prcw" in namespace "subpath-9102"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:47:39.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9102" for this suite. 09/08/23 22:47:39.733
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:47:39.757
Sep  8 22:47:39.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:47:39.763
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:39.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:39.854
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-d06d3dff-e3dd-49e6-a1e6-dc03eaf46756 09/08/23 22:47:39.86
STEP: Creating a pod to test consume configMaps 09/08/23 22:47:39.872
Sep  8 22:47:39.910: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279" in namespace "projected-2739" to be "Succeeded or Failed"
Sep  8 22:47:39.928: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279": Phase="Pending", Reason="", readiness=false. Elapsed: 18.701709ms
Sep  8 22:47:41.942: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032462936s
Sep  8 22:47:43.937: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027590693s
STEP: Saw pod success 09/08/23 22:47:43.937
Sep  8 22:47:43.937: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279" satisfied condition "Succeeded or Failed"
Sep  8 22:47:43.947: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279 container agnhost-container: <nil>
STEP: delete the pod 09/08/23 22:47:43.967
Sep  8 22:47:44.007: INFO: Waiting for pod pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279 to disappear
Sep  8 22:47:44.019: INFO: Pod pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Sep  8 22:47:44.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2739" for this suite. 09/08/23 22:47:44.029
------------------------------
â€¢ [4.298 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:47:39.757
    Sep  8 22:47:39.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:47:39.763
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:39.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:39.854
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-d06d3dff-e3dd-49e6-a1e6-dc03eaf46756 09/08/23 22:47:39.86
    STEP: Creating a pod to test consume configMaps 09/08/23 22:47:39.872
    Sep  8 22:47:39.910: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279" in namespace "projected-2739" to be "Succeeded or Failed"
    Sep  8 22:47:39.928: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279": Phase="Pending", Reason="", readiness=false. Elapsed: 18.701709ms
    Sep  8 22:47:41.942: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032462936s
    Sep  8 22:47:43.937: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027590693s
    STEP: Saw pod success 09/08/23 22:47:43.937
    Sep  8 22:47:43.937: INFO: Pod "pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279" satisfied condition "Succeeded or Failed"
    Sep  8 22:47:43.947: INFO: Trying to get logs from node node-3 pod pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279 container agnhost-container: <nil>
    STEP: delete the pod 09/08/23 22:47:43.967
    Sep  8 22:47:44.007: INFO: Waiting for pod pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279 to disappear
    Sep  8 22:47:44.019: INFO: Pod pod-projected-configmaps-a6f86ad7-98e7-45f7-a501-19ca9a7bb279 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:47:44.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2739" for this suite. 09/08/23 22:47:44.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:47:44.058
Sep  8 22:47:44.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:47:44.059
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:44.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:44.119
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:47:44.123
Sep  8 22:47:44.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829" in namespace "projected-7970" to be "Succeeded or Failed"
Sep  8 22:47:44.154: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829": Phase="Pending", Reason="", readiness=false. Elapsed: 8.258165ms
Sep  8 22:47:46.169: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023718833s
Sep  8 22:47:48.163: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017579724s
STEP: Saw pod success 09/08/23 22:47:48.163
Sep  8 22:47:48.164: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829" satisfied condition "Succeeded or Failed"
Sep  8 22:47:48.175: INFO: Trying to get logs from node node-3 pod downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829 container client-container: <nil>
STEP: delete the pod 09/08/23 22:47:48.191
Sep  8 22:47:48.217: INFO: Waiting for pod downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829 to disappear
Sep  8 22:47:48.222: INFO: Pod downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Sep  8 22:47:48.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7970" for this suite. 09/08/23 22:47:48.231
------------------------------
â€¢ [4.186 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:47:44.058
    Sep  8 22:47:44.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:47:44.059
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:44.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:44.119
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:47:44.123
    Sep  8 22:47:44.146: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829" in namespace "projected-7970" to be "Succeeded or Failed"
    Sep  8 22:47:44.154: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829": Phase="Pending", Reason="", readiness=false. Elapsed: 8.258165ms
    Sep  8 22:47:46.169: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023718833s
    Sep  8 22:47:48.163: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017579724s
    STEP: Saw pod success 09/08/23 22:47:48.163
    Sep  8 22:47:48.164: INFO: Pod "downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829" satisfied condition "Succeeded or Failed"
    Sep  8 22:47:48.175: INFO: Trying to get logs from node node-3 pod downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829 container client-container: <nil>
    STEP: delete the pod 09/08/23 22:47:48.191
    Sep  8 22:47:48.217: INFO: Waiting for pod downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829 to disappear
    Sep  8 22:47:48.222: INFO: Pod downwardapi-volume-01b62716-943d-47ae-9796-10e8b00e5829 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:47:48.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7970" for this suite. 09/08/23 22:47:48.231
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:47:48.246
Sep  8 22:47:48.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-runtime 09/08/23 22:47:48.247
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:48.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:48.299
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 09/08/23 22:47:48.311
STEP: wait for the container to reach Succeeded 09/08/23 22:47:48.333
STEP: get the container status 09/08/23 22:47:52.411
STEP: the container should be terminated 09/08/23 22:47:52.423
STEP: the termination message should be set 09/08/23 22:47:52.423
Sep  8 22:47:52.423: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 09/08/23 22:47:52.423
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Sep  8 22:47:52.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9634" for this suite. 09/08/23 22:47:52.498
------------------------------
â€¢ [4.291 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:47:48.246
    Sep  8 22:47:48.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-runtime 09/08/23 22:47:48.247
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:48.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:48.299
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 09/08/23 22:47:48.311
    STEP: wait for the container to reach Succeeded 09/08/23 22:47:48.333
    STEP: get the container status 09/08/23 22:47:52.411
    STEP: the container should be terminated 09/08/23 22:47:52.423
    STEP: the termination message should be set 09/08/23 22:47:52.423
    Sep  8 22:47:52.423: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 09/08/23 22:47:52.423
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:47:52.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9634" for this suite. 09/08/23 22:47:52.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:47:52.537
Sep  8 22:47:52.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 22:47:52.538
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:52.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:52.586
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 09/08/23 22:47:52.591
Sep  8 22:47:52.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352" in namespace "downward-api-4735" to be "Succeeded or Failed"
Sep  8 22:47:52.635: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Pending", Reason="", readiness=false. Elapsed: 19.486444ms
Sep  8 22:47:54.652: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036463115s
Sep  8 22:47:56.646: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030783933s
Sep  8 22:47:58.646: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030283578s
STEP: Saw pod success 09/08/23 22:47:58.646
Sep  8 22:47:58.646: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352" satisfied condition "Succeeded or Failed"
Sep  8 22:47:58.657: INFO: Trying to get logs from node node-3 pod downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352 container client-container: <nil>
STEP: delete the pod 09/08/23 22:47:58.676
Sep  8 22:47:58.726: INFO: Waiting for pod downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352 to disappear
Sep  8 22:47:58.738: INFO: Pod downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Sep  8 22:47:58.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4735" for this suite. 09/08/23 22:47:58.746
------------------------------
â€¢ [SLOW TEST] [6.240 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:47:52.537
    Sep  8 22:47:52.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 22:47:52.538
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:52.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:52.586
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 09/08/23 22:47:52.591
    Sep  8 22:47:52.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352" in namespace "downward-api-4735" to be "Succeeded or Failed"
    Sep  8 22:47:52.635: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Pending", Reason="", readiness=false. Elapsed: 19.486444ms
    Sep  8 22:47:54.652: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036463115s
    Sep  8 22:47:56.646: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030783933s
    Sep  8 22:47:58.646: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030283578s
    STEP: Saw pod success 09/08/23 22:47:58.646
    Sep  8 22:47:58.646: INFO: Pod "downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352" satisfied condition "Succeeded or Failed"
    Sep  8 22:47:58.657: INFO: Trying to get logs from node node-3 pod downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352 container client-container: <nil>
    STEP: delete the pod 09/08/23 22:47:58.676
    Sep  8 22:47:58.726: INFO: Waiting for pod downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352 to disappear
    Sep  8 22:47:58.738: INFO: Pod downwardapi-volume-2d2b9402-3f63-4a88-9fc5-a49f20fe0352 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:47:58.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4735" for this suite. 09/08/23 22:47:58.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:47:58.778
Sep  8 22:47:58.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:47:58.779
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:58.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:58.821
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Sep  8 22:47:58.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 create -f -'
Sep  8 22:48:00.397: INFO: stderr: ""
Sep  8 22:48:00.397: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  8 22:48:00.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 create -f -'
Sep  8 22:48:02.097: INFO: stderr: ""
Sep  8 22:48:02.097: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 09/08/23 22:48:02.097
Sep  8 22:48:03.107: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:48:03.107: INFO: Found 1 / 1
Sep  8 22:48:03.107: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  8 22:48:03.115: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  8 22:48:03.115: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  8 22:48:03.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe pod agnhost-primary-76dss'
Sep  8 22:48:03.258: INFO: stderr: ""
Sep  8 22:48:03.258: INFO: stdout: "Name:             agnhost-primary-76dss\nNamespace:        kubectl-5815\nPriority:         0\nService Account:  default\nNode:             node-3/10.100.19.129\nStart Time:       Fri, 08 Sep 2023 22:48:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8d19a8eac598d3e7b2a01059afae69cf87ce475f78218ae3031d6c7081fd8749\n                  cni.projectcalico.org/podIP: 10.233.75.83/32\n                  cni.projectcalico.org/podIPs: 10.233.75.83/32\nStatus:           Running\nIP:               10.233.75.83\nIPs:\n  IP:           10.233.75.83\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b1aa38f1c2a24a8587983270e09a4b9db48f2aa0b8f5f08b1debf0c6f66b15cc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 08 Sep 2023 22:48:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bt47d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bt47d:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5815/agnhost-primary-76dss to node-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Sep  8 22:48:03.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe rc agnhost-primary'
Sep  8 22:48:03.431: INFO: stderr: ""
Sep  8 22:48:03.431: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5815\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-76dss\n"
Sep  8 22:48:03.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe service agnhost-primary'
Sep  8 22:48:03.597: INFO: stderr: ""
Sep  8 22:48:03.597: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5815\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.30.104\nIPs:               10.233.30.104\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.75.83:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep  8 22:48:03.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe node node-0'
Sep  8 22:48:03.885: INFO: stderr: ""
Sep  8 22:48:03.885: INFO: stdout: "Name:               node-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    whitemist-version=jetstream-PR456\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.100.20.57/21\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.88.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 08 Sep 2023 20:48:02 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  node-0\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 08 Sep 2023 22:48:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 08 Sep 2023 20:50:59 +0000   Fri, 08 Sep 2023 20:50:59 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:47:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:47:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:47:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:54:20 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.100.20.57\n  Hostname:    node-0\nCapacity:\n  cpu:                4\n  ephemeral-storage:  81120644Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8152772Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  74760785387\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8050372Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c6fb1437a99e489db6a510140445df30\n  System UUID:                c6fb1437-a99e-489d-b6a5-10140445df30\n  Boot ID:                    2d49ebcc-baa4-4ece-89a1-c322bbfe823b\n  Kernel Version:             5.4.0-65-generic\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.1\n  Kubelet Version:            v1.26.5\n  Kube-Proxy Version:         v1.26.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                     netchecker-agent-cp2hp                                     1m (0%)       30m (0%)    10M (0%)         100M (1%)      115m\n  default                     netchecker-agent-hostnet-d696c                             1m (0%)       30m (0%)    10M (0%)         100M (1%)      115m\n  kube-system                 calico-node-wgtqk                                          150m (3%)     300m (7%)   64M (0%)         500M (6%)      117m\n  kube-system                 coredns-6c86d97486-gnmjr                                   100m (2%)     0 (0%)      70Mi (0%)        300Mi (3%)     64m\n  kube-system                 kube-apiserver-node-0                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-controller-manager-node-0                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         119m\n  kube-system                 kube-proxy-tbx8q                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 kube-scheduler-node-0                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         119m\n  kube-system                 kube-vip-ds-lvcgw                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         110m\n  kube-system                 kubernetes-dashboard-admin-79b7cc94c8-spcsn                50m (1%)      100m (2%)   64M (0%)         256M (3%)      64m\n  kube-system                 nodelocaldns-srnsw                                         100m (2%)     0 (0%)      70Mi (0%)        200Mi (2%)     115m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-ssk49    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                952m (23%)      460m (11%)\n  memory             294800640 (3%)  1480288k (17%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
Sep  8 22:48:03.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe namespace kubectl-5815'
Sep  8 22:48:04.097: INFO: stderr: ""
Sep  8 22:48:04.097: INFO: stdout: "Name:         kubectl-5815\nLabels:       e2e-framework=kubectl\n              e2e-run=3af7af1c-08fb-48ad-95c3-714ce808d119\n              kubernetes.io/metadata.name=kubectl-5815\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:04.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5815" for this suite. 09/08/23 22:48:04.108
------------------------------
â€¢ [SLOW TEST] [5.362 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:47:58.778
    Sep  8 22:47:58.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:47:58.779
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:47:58.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:47:58.821
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Sep  8 22:47:58.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 create -f -'
    Sep  8 22:48:00.397: INFO: stderr: ""
    Sep  8 22:48:00.397: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Sep  8 22:48:00.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 create -f -'
    Sep  8 22:48:02.097: INFO: stderr: ""
    Sep  8 22:48:02.097: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 09/08/23 22:48:02.097
    Sep  8 22:48:03.107: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:48:03.107: INFO: Found 1 / 1
    Sep  8 22:48:03.107: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Sep  8 22:48:03.115: INFO: Selector matched 1 pods for map[app:agnhost]
    Sep  8 22:48:03.115: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Sep  8 22:48:03.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe pod agnhost-primary-76dss'
    Sep  8 22:48:03.258: INFO: stderr: ""
    Sep  8 22:48:03.258: INFO: stdout: "Name:             agnhost-primary-76dss\nNamespace:        kubectl-5815\nPriority:         0\nService Account:  default\nNode:             node-3/10.100.19.129\nStart Time:       Fri, 08 Sep 2023 22:48:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 8d19a8eac598d3e7b2a01059afae69cf87ce475f78218ae3031d6c7081fd8749\n                  cni.projectcalico.org/podIP: 10.233.75.83/32\n                  cni.projectcalico.org/podIPs: 10.233.75.83/32\nStatus:           Running\nIP:               10.233.75.83\nIPs:\n  IP:           10.233.75.83\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b1aa38f1c2a24a8587983270e09a4b9db48f2aa0b8f5f08b1debf0c6f66b15cc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 08 Sep 2023 22:48:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bt47d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bt47d:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5815/agnhost-primary-76dss to node-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Sep  8 22:48:03.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe rc agnhost-primary'
    Sep  8 22:48:03.431: INFO: stderr: ""
    Sep  8 22:48:03.431: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5815\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-76dss\n"
    Sep  8 22:48:03.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe service agnhost-primary'
    Sep  8 22:48:03.597: INFO: stderr: ""
    Sep  8 22:48:03.597: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5815\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.30.104\nIPs:               10.233.30.104\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.75.83:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Sep  8 22:48:03.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe node node-0'
    Sep  8 22:48:03.885: INFO: stderr: ""
    Sep  8 22:48:03.885: INFO: stdout: "Name:               node-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    whitemist-version=jetstream-PR456\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.100.20.57/21\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.233.88.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 08 Sep 2023 20:48:02 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\n                    node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  node-0\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 08 Sep 2023 22:48:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 08 Sep 2023 20:50:59 +0000   Fri, 08 Sep 2023 20:50:59 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:47:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:47:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:47:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 08 Sep 2023 22:48:01 +0000   Fri, 08 Sep 2023 20:54:20 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.100.20.57\n  Hostname:    node-0\nCapacity:\n  cpu:                4\n  ephemeral-storage:  81120644Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8152772Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  74760785387\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8050372Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c6fb1437a99e489db6a510140445df30\n  System UUID:                c6fb1437-a99e-489d-b6a5-10140445df30\n  Boot ID:                    2d49ebcc-baa4-4ece-89a1-c322bbfe823b\n  Kernel Version:             5.4.0-65-generic\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.1\n  Kubelet Version:            v1.26.5\n  Kube-Proxy Version:         v1.26.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                     netchecker-agent-cp2hp                                     1m (0%)       30m (0%)    10M (0%)         100M (1%)      115m\n  default                     netchecker-agent-hostnet-d696c                             1m (0%)       30m (0%)    10M (0%)         100M (1%)      115m\n  kube-system                 calico-node-wgtqk                                          150m (3%)     300m (7%)   64M (0%)         500M (6%)      117m\n  kube-system                 coredns-6c86d97486-gnmjr                                   100m (2%)     0 (0%)      70Mi (0%)        300Mi (3%)     64m\n  kube-system                 kube-apiserver-node-0                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         120m\n  kube-system                 kube-controller-manager-node-0                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         119m\n  kube-system                 kube-proxy-tbx8q                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 kube-scheduler-node-0                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         119m\n  kube-system                 kube-vip-ds-lvcgw                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         110m\n  kube-system                 kubernetes-dashboard-admin-79b7cc94c8-spcsn                50m (1%)      100m (2%)   64M (0%)         256M (3%)      64m\n  kube-system                 nodelocaldns-srnsw                                         100m (2%)     0 (0%)      70Mi (0%)        200Mi (2%)     115m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5c405620ecbe4654-ssk49    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                952m (23%)      460m (11%)\n  memory             294800640 (3%)  1480288k (17%)\n  ephemeral-storage  0 (0%)          0 (0%)\n  hugepages-1Gi      0 (0%)          0 (0%)\n  hugepages-2Mi      0 (0%)          0 (0%)\nEvents:              <none>\n"
    Sep  8 22:48:03.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-5815 describe namespace kubectl-5815'
    Sep  8 22:48:04.097: INFO: stderr: ""
    Sep  8 22:48:04.097: INFO: stdout: "Name:         kubectl-5815\nLabels:       e2e-framework=kubectl\n              e2e-run=3af7af1c-08fb-48ad-95c3-714ce808d119\n              kubernetes.io/metadata.name=kubectl-5815\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:04.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5815" for this suite. 09/08/23 22:48:04.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:04.141
Sep  8 22:48:04.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename svc-latency 09/08/23 22:48:04.142
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:04.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:04.18
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Sep  8 22:48:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7276 09/08/23 22:48:04.189
I0908 22:48:04.200994      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7276, replica count: 1
I0908 22:48:05.252249      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0908 22:48:06.253300      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep  8 22:48:06.388: INFO: Created: latency-svc-nj9zl
Sep  8 22:48:06.403: INFO: Got endpoints: latency-svc-nj9zl [49.070177ms]
Sep  8 22:48:06.450: INFO: Created: latency-svc-88qs5
Sep  8 22:48:06.468: INFO: Got endpoints: latency-svc-88qs5 [63.693614ms]
Sep  8 22:48:06.497: INFO: Created: latency-svc-rffvp
Sep  8 22:48:06.508: INFO: Got endpoints: latency-svc-rffvp [103.566573ms]
Sep  8 22:48:06.557: INFO: Created: latency-svc-pt6tw
Sep  8 22:48:06.564: INFO: Got endpoints: latency-svc-pt6tw [159.568618ms]
Sep  8 22:48:06.591: INFO: Created: latency-svc-dj2ph
Sep  8 22:48:06.609: INFO: Got endpoints: latency-svc-dj2ph [204.550364ms]
Sep  8 22:48:06.626: INFO: Created: latency-svc-w2mbj
Sep  8 22:48:06.630: INFO: Got endpoints: latency-svc-w2mbj [226.100331ms]
Sep  8 22:48:06.676: INFO: Created: latency-svc-cf8kr
Sep  8 22:48:06.695: INFO: Got endpoints: latency-svc-cf8kr [290.1776ms]
Sep  8 22:48:06.721: INFO: Created: latency-svc-mpsr4
Sep  8 22:48:06.732: INFO: Got endpoints: latency-svc-mpsr4 [327.626514ms]
Sep  8 22:48:06.760: INFO: Created: latency-svc-vwzdm
Sep  8 22:48:06.762: INFO: Got endpoints: latency-svc-vwzdm [357.914801ms]
Sep  8 22:48:06.778: INFO: Created: latency-svc-n8q78
Sep  8 22:48:06.808: INFO: Got endpoints: latency-svc-n8q78 [404.190041ms]
Sep  8 22:48:06.838: INFO: Created: latency-svc-98kw4
Sep  8 22:48:06.856: INFO: Got endpoints: latency-svc-98kw4 [451.512516ms]
Sep  8 22:48:06.873: INFO: Created: latency-svc-8v9wl
Sep  8 22:48:06.894: INFO: Got endpoints: latency-svc-8v9wl [490.793159ms]
Sep  8 22:48:06.914: INFO: Created: latency-svc-h5kbt
Sep  8 22:48:06.928: INFO: Got endpoints: latency-svc-h5kbt [523.798118ms]
Sep  8 22:48:06.928: INFO: Created: latency-svc-4f5sb
Sep  8 22:48:06.947: INFO: Got endpoints: latency-svc-4f5sb [543.611092ms]
Sep  8 22:48:06.981: INFO: Created: latency-svc-j7xld
Sep  8 22:48:06.981: INFO: Got endpoints: latency-svc-j7xld [576.139601ms]
Sep  8 22:48:07.026: INFO: Created: latency-svc-2llmm
Sep  8 22:48:07.036: INFO: Got endpoints: latency-svc-2llmm [632.728293ms]
Sep  8 22:48:07.057: INFO: Created: latency-svc-4qwxn
Sep  8 22:48:07.089: INFO: Got endpoints: latency-svc-4qwxn [620.714036ms]
Sep  8 22:48:07.143: INFO: Created: latency-svc-h476g
Sep  8 22:48:07.146: INFO: Got endpoints: latency-svc-h476g [637.698129ms]
Sep  8 22:48:07.159: INFO: Created: latency-svc-hjgdt
Sep  8 22:48:07.171: INFO: Got endpoints: latency-svc-hjgdt [607.069215ms]
Sep  8 22:48:07.202: INFO: Created: latency-svc-7lpdb
Sep  8 22:48:07.214: INFO: Got endpoints: latency-svc-7lpdb [604.865542ms]
Sep  8 22:48:07.252: INFO: Created: latency-svc-42pj2
Sep  8 22:48:07.267: INFO: Got endpoints: latency-svc-42pj2 [636.612874ms]
Sep  8 22:48:07.272: INFO: Created: latency-svc-pqgwg
Sep  8 22:48:07.296: INFO: Created: latency-svc-klldn
Sep  8 22:48:07.297: INFO: Got endpoints: latency-svc-pqgwg [601.895309ms]
Sep  8 22:48:07.333: INFO: Got endpoints: latency-svc-klldn [601.153723ms]
Sep  8 22:48:07.335: INFO: Created: latency-svc-rctgr
Sep  8 22:48:07.343: INFO: Got endpoints: latency-svc-rctgr [580.99831ms]
Sep  8 22:48:07.363: INFO: Created: latency-svc-hfgjz
Sep  8 22:48:07.367: INFO: Got endpoints: latency-svc-hfgjz [559.024012ms]
Sep  8 22:48:07.386: INFO: Created: latency-svc-c25dw
Sep  8 22:48:07.400: INFO: Got endpoints: latency-svc-c25dw [544.322951ms]
Sep  8 22:48:07.422: INFO: Created: latency-svc-pcbj2
Sep  8 22:48:07.440: INFO: Got endpoints: latency-svc-pcbj2 [545.673763ms]
Sep  8 22:48:07.453: INFO: Created: latency-svc-g9w28
Sep  8 22:48:07.462: INFO: Got endpoints: latency-svc-g9w28 [533.465444ms]
Sep  8 22:48:07.481: INFO: Created: latency-svc-tl8mt
Sep  8 22:48:07.497: INFO: Got endpoints: latency-svc-tl8mt [550.041737ms]
Sep  8 22:48:07.513: INFO: Created: latency-svc-dj8bl
Sep  8 22:48:07.529: INFO: Got endpoints: latency-svc-dj8bl [548.317978ms]
Sep  8 22:48:07.550: INFO: Created: latency-svc-rl9ms
Sep  8 22:48:07.555: INFO: Got endpoints: latency-svc-rl9ms [519.012917ms]
Sep  8 22:48:07.575: INFO: Created: latency-svc-tr8mv
Sep  8 22:48:07.594: INFO: Got endpoints: latency-svc-tr8mv [505.359149ms]
Sep  8 22:48:07.622: INFO: Created: latency-svc-rnb4c
Sep  8 22:48:07.627: INFO: Got endpoints: latency-svc-rnb4c [481.415825ms]
Sep  8 22:48:07.649: INFO: Created: latency-svc-74tlf
Sep  8 22:48:07.659: INFO: Got endpoints: latency-svc-74tlf [488.530225ms]
Sep  8 22:48:07.683: INFO: Created: latency-svc-p5z4l
Sep  8 22:48:07.703: INFO: Got endpoints: latency-svc-p5z4l [488.911748ms]
Sep  8 22:48:07.738: INFO: Created: latency-svc-2s6d5
Sep  8 22:48:07.744: INFO: Got endpoints: latency-svc-2s6d5 [476.99257ms]
Sep  8 22:48:07.757: INFO: Created: latency-svc-26n55
Sep  8 22:48:07.764: INFO: Got endpoints: latency-svc-26n55 [467.592583ms]
Sep  8 22:48:07.799: INFO: Created: latency-svc-45dgm
Sep  8 22:48:07.814: INFO: Got endpoints: latency-svc-45dgm [480.601676ms]
Sep  8 22:48:07.820: INFO: Created: latency-svc-9lllf
Sep  8 22:48:07.842: INFO: Got endpoints: latency-svc-9lllf [498.781631ms]
Sep  8 22:48:07.853: INFO: Created: latency-svc-62rfr
Sep  8 22:48:07.869: INFO: Got endpoints: latency-svc-62rfr [500.636112ms]
Sep  8 22:48:07.894: INFO: Created: latency-svc-bqqjp
Sep  8 22:48:07.912: INFO: Got endpoints: latency-svc-bqqjp [511.259633ms]
Sep  8 22:48:07.938: INFO: Created: latency-svc-ksm9p
Sep  8 22:48:07.973: INFO: Got endpoints: latency-svc-ksm9p [533.032677ms]
Sep  8 22:48:07.999: INFO: Created: latency-svc-w4gjw
Sep  8 22:48:08.013: INFO: Got endpoints: latency-svc-w4gjw [551.561122ms]
Sep  8 22:48:08.031: INFO: Created: latency-svc-p2djd
Sep  8 22:48:08.054: INFO: Got endpoints: latency-svc-p2djd [556.454365ms]
Sep  8 22:48:08.066: INFO: Created: latency-svc-n87lb
Sep  8 22:48:08.086: INFO: Got endpoints: latency-svc-n87lb [557.536385ms]
Sep  8 22:48:08.104: INFO: Created: latency-svc-ldtvn
Sep  8 22:48:08.120: INFO: Got endpoints: latency-svc-ldtvn [564.741925ms]
Sep  8 22:48:08.133: INFO: Created: latency-svc-jqw5m
Sep  8 22:48:08.150: INFO: Got endpoints: latency-svc-jqw5m [555.521624ms]
Sep  8 22:48:08.167: INFO: Created: latency-svc-88m5d
Sep  8 22:48:08.170: INFO: Got endpoints: latency-svc-88m5d [541.985276ms]
Sep  8 22:48:08.190: INFO: Created: latency-svc-5zcmx
Sep  8 22:48:08.198: INFO: Got endpoints: latency-svc-5zcmx [538.989214ms]
Sep  8 22:48:08.214: INFO: Created: latency-svc-2h9cp
Sep  8 22:48:08.238: INFO: Got endpoints: latency-svc-2h9cp [535.354319ms]
Sep  8 22:48:08.251: INFO: Created: latency-svc-669g8
Sep  8 22:48:08.263: INFO: Got endpoints: latency-svc-669g8 [518.204346ms]
Sep  8 22:48:08.272: INFO: Created: latency-svc-dpsrw
Sep  8 22:48:08.295: INFO: Created: latency-svc-nd5pf
Sep  8 22:48:08.296: INFO: Got endpoints: latency-svc-dpsrw [532.179752ms]
Sep  8 22:48:08.319: INFO: Got endpoints: latency-svc-nd5pf [504.799354ms]
Sep  8 22:48:08.333: INFO: Created: latency-svc-4pzj2
Sep  8 22:48:08.349: INFO: Got endpoints: latency-svc-4pzj2 [506.574417ms]
Sep  8 22:48:08.363: INFO: Created: latency-svc-6lrnh
Sep  8 22:48:08.388: INFO: Got endpoints: latency-svc-6lrnh [518.830965ms]
Sep  8 22:48:08.389: INFO: Created: latency-svc-rzhvg
Sep  8 22:48:08.403: INFO: Got endpoints: latency-svc-rzhvg [491.711113ms]
Sep  8 22:48:08.414: INFO: Created: latency-svc-vgclp
Sep  8 22:48:08.433: INFO: Created: latency-svc-jn22s
Sep  8 22:48:08.455: INFO: Got endpoints: latency-svc-vgclp [481.782358ms]
Sep  8 22:48:08.459: INFO: Got endpoints: latency-svc-jn22s [445.994305ms]
Sep  8 22:48:08.475: INFO: Created: latency-svc-9qqrh
Sep  8 22:48:08.484: INFO: Got endpoints: latency-svc-9qqrh [429.92528ms]
Sep  8 22:48:08.509: INFO: Created: latency-svc-8x7xm
Sep  8 22:48:08.517: INFO: Got endpoints: latency-svc-8x7xm [430.34364ms]
Sep  8 22:48:08.539: INFO: Created: latency-svc-lh9p8
Sep  8 22:48:08.567: INFO: Got endpoints: latency-svc-lh9p8 [447.104918ms]
Sep  8 22:48:08.585: INFO: Created: latency-svc-wxl4t
Sep  8 22:48:08.624: INFO: Got endpoints: latency-svc-wxl4t [474.334205ms]
Sep  8 22:48:08.630: INFO: Created: latency-svc-bmq5p
Sep  8 22:48:08.641: INFO: Got endpoints: latency-svc-bmq5p [470.653614ms]
Sep  8 22:48:08.660: INFO: Created: latency-svc-lcrtp
Sep  8 22:48:08.677: INFO: Got endpoints: latency-svc-lcrtp [478.453194ms]
Sep  8 22:48:08.697: INFO: Created: latency-svc-6rrxp
Sep  8 22:48:08.708: INFO: Got endpoints: latency-svc-6rrxp [469.813807ms]
Sep  8 22:48:08.732: INFO: Created: latency-svc-2snwz
Sep  8 22:48:08.745: INFO: Got endpoints: latency-svc-2snwz [481.826431ms]
Sep  8 22:48:08.766: INFO: Created: latency-svc-w8v62
Sep  8 22:48:08.770: INFO: Got endpoints: latency-svc-w8v62 [472.641396ms]
Sep  8 22:48:08.783: INFO: Created: latency-svc-lb4vc
Sep  8 22:48:08.795: INFO: Got endpoints: latency-svc-lb4vc [475.725482ms]
Sep  8 22:48:08.822: INFO: Created: latency-svc-xmxk7
Sep  8 22:48:08.863: INFO: Got endpoints: latency-svc-xmxk7 [514.029432ms]
Sep  8 22:48:08.864: INFO: Created: latency-svc-ncv9f
Sep  8 22:48:08.875: INFO: Created: latency-svc-nxr26
Sep  8 22:48:08.879: INFO: Got endpoints: latency-svc-ncv9f [491.278274ms]
Sep  8 22:48:08.899: INFO: Got endpoints: latency-svc-nxr26 [495.563784ms]
Sep  8 22:48:08.911: INFO: Created: latency-svc-nngrc
Sep  8 22:48:08.931: INFO: Got endpoints: latency-svc-nngrc [475.620457ms]
Sep  8 22:48:08.950: INFO: Created: latency-svc-7wtws
Sep  8 22:48:08.956: INFO: Got endpoints: latency-svc-7wtws [496.913112ms]
Sep  8 22:48:08.966: INFO: Created: latency-svc-ckfrp
Sep  8 22:48:08.981: INFO: Got endpoints: latency-svc-ckfrp [496.770857ms]
Sep  8 22:48:08.997: INFO: Created: latency-svc-hkfln
Sep  8 22:48:09.015: INFO: Got endpoints: latency-svc-hkfln [498.054693ms]
Sep  8 22:48:09.026: INFO: Created: latency-svc-v8fvf
Sep  8 22:48:09.043: INFO: Got endpoints: latency-svc-v8fvf [475.791816ms]
Sep  8 22:48:09.070: INFO: Created: latency-svc-chqx9
Sep  8 22:48:09.074: INFO: Got endpoints: latency-svc-chqx9 [449.616466ms]
Sep  8 22:48:09.118: INFO: Created: latency-svc-x57vs
Sep  8 22:48:09.139: INFO: Got endpoints: latency-svc-x57vs [498.253504ms]
Sep  8 22:48:09.149: INFO: Created: latency-svc-z4mzv
Sep  8 22:48:09.172: INFO: Got endpoints: latency-svc-z4mzv [494.767738ms]
Sep  8 22:48:09.186: INFO: Created: latency-svc-7xmxh
Sep  8 22:48:09.202: INFO: Got endpoints: latency-svc-7xmxh [494.044869ms]
Sep  8 22:48:09.231: INFO: Created: latency-svc-swnkd
Sep  8 22:48:09.256: INFO: Got endpoints: latency-svc-swnkd [511.422406ms]
Sep  8 22:48:09.258: INFO: Created: latency-svc-fcqjp
Sep  8 22:48:09.278: INFO: Got endpoints: latency-svc-fcqjp [508.175005ms]
Sep  8 22:48:09.293: INFO: Created: latency-svc-rscwv
Sep  8 22:48:09.307: INFO: Got endpoints: latency-svc-rscwv [512.262924ms]
Sep  8 22:48:09.329: INFO: Created: latency-svc-jlgzg
Sep  8 22:48:09.357: INFO: Got endpoints: latency-svc-jlgzg [493.494732ms]
Sep  8 22:48:09.360: INFO: Created: latency-svc-8gt7t
Sep  8 22:48:09.381: INFO: Got endpoints: latency-svc-8gt7t [501.356325ms]
Sep  8 22:48:09.410: INFO: Created: latency-svc-n7tvr
Sep  8 22:48:09.423: INFO: Got endpoints: latency-svc-n7tvr [524.244861ms]
Sep  8 22:48:09.437: INFO: Created: latency-svc-qg55z
Sep  8 22:48:09.455: INFO: Got endpoints: latency-svc-qg55z [524.100041ms]
Sep  8 22:48:09.485: INFO: Created: latency-svc-zmtqx
Sep  8 22:48:09.496: INFO: Created: latency-svc-4hg97
Sep  8 22:48:09.509: INFO: Got endpoints: latency-svc-zmtqx [552.216374ms]
Sep  8 22:48:09.529: INFO: Got endpoints: latency-svc-4hg97 [548.100472ms]
Sep  8 22:48:09.536: INFO: Created: latency-svc-zxgmc
Sep  8 22:48:09.564: INFO: Got endpoints: latency-svc-zxgmc [549.213389ms]
Sep  8 22:48:09.593: INFO: Created: latency-svc-5wx9z
Sep  8 22:48:09.628: INFO: Got endpoints: latency-svc-5wx9z [584.661188ms]
Sep  8 22:48:09.640: INFO: Created: latency-svc-gwh6t
Sep  8 22:48:09.652: INFO: Got endpoints: latency-svc-gwh6t [578.118495ms]
Sep  8 22:48:09.678: INFO: Created: latency-svc-cqlls
Sep  8 22:48:09.688: INFO: Got endpoints: latency-svc-cqlls [549.346758ms]
Sep  8 22:48:09.717: INFO: Created: latency-svc-xhwqz
Sep  8 22:48:09.728: INFO: Got endpoints: latency-svc-xhwqz [556.427135ms]
Sep  8 22:48:09.741: INFO: Created: latency-svc-spgv9
Sep  8 22:48:09.766: INFO: Created: latency-svc-fdj2k
Sep  8 22:48:09.781: INFO: Got endpoints: latency-svc-spgv9 [578.358132ms]
Sep  8 22:48:09.818: INFO: Created: latency-svc-d5rn4
Sep  8 22:48:09.825: INFO: Got endpoints: latency-svc-fdj2k [568.720552ms]
Sep  8 22:48:09.849: INFO: Created: latency-svc-p8c7j
Sep  8 22:48:09.862: INFO: Got endpoints: latency-svc-d5rn4 [584.172957ms]
Sep  8 22:48:09.891: INFO: Created: latency-svc-84pfq
Sep  8 22:48:09.925: INFO: Got endpoints: latency-svc-p8c7j [617.759952ms]
Sep  8 22:48:09.983: INFO: Got endpoints: latency-svc-84pfq [626.289985ms]
Sep  8 22:48:09.989: INFO: Created: latency-svc-rqnwz
Sep  8 22:48:10.009: INFO: Created: latency-svc-c94rk
Sep  8 22:48:10.020: INFO: Got endpoints: latency-svc-rqnwz [596.40005ms]
Sep  8 22:48:10.096: INFO: Got endpoints: latency-svc-c94rk [641.194496ms]
Sep  8 22:48:10.097: INFO: Created: latency-svc-4rg6q
Sep  8 22:48:10.123: INFO: Got endpoints: latency-svc-4rg6q [613.959568ms]
Sep  8 22:48:10.141: INFO: Created: latency-svc-jmcsv
Sep  8 22:48:10.176: INFO: Got endpoints: latency-svc-jmcsv [646.275171ms]
Sep  8 22:48:10.207: INFO: Created: latency-svc-cb8wd
Sep  8 22:48:10.239: INFO: Got endpoints: latency-svc-cb8wd [674.367068ms]
Sep  8 22:48:10.263: INFO: Created: latency-svc-g8xz8
Sep  8 22:48:10.292: INFO: Got endpoints: latency-svc-g8xz8 [663.862749ms]
Sep  8 22:48:10.348: INFO: Created: latency-svc-7qcbp
Sep  8 22:48:10.379: INFO: Got endpoints: latency-svc-7qcbp [726.664518ms]
Sep  8 22:48:10.412: INFO: Created: latency-svc-9jp69
Sep  8 22:48:10.436: INFO: Got endpoints: latency-svc-9jp69 [747.61772ms]
Sep  8 22:48:10.489: INFO: Created: latency-svc-2j7z2
Sep  8 22:48:10.527: INFO: Got endpoints: latency-svc-2j7z2 [798.38179ms]
Sep  8 22:48:10.600: INFO: Created: latency-svc-qqgs6
Sep  8 22:48:10.630: INFO: Got endpoints: latency-svc-qqgs6 [848.832784ms]
Sep  8 22:48:10.689: INFO: Created: latency-svc-zwnj7
Sep  8 22:48:10.700: INFO: Created: latency-svc-m9wvf
Sep  8 22:48:10.709: INFO: Got endpoints: latency-svc-zwnj7 [884.050994ms]
Sep  8 22:48:10.710: INFO: Created: latency-svc-nk4kz
Sep  8 22:48:10.739: INFO: Got endpoints: latency-svc-m9wvf [876.574068ms]
Sep  8 22:48:10.740: INFO: Got endpoints: latency-svc-nk4kz [1.359103482s]
Sep  8 22:48:10.764: INFO: Created: latency-svc-8j67f
Sep  8 22:48:10.766: INFO: Got endpoints: latency-svc-8j67f [841.191752ms]
Sep  8 22:48:10.796: INFO: Created: latency-svc-25hpn
Sep  8 22:48:10.815: INFO: Got endpoints: latency-svc-25hpn [831.461538ms]
Sep  8 22:48:10.839: INFO: Created: latency-svc-4fw9g
Sep  8 22:48:10.854: INFO: Got endpoints: latency-svc-4fw9g [834.41455ms]
Sep  8 22:48:10.870: INFO: Created: latency-svc-rprmh
Sep  8 22:48:10.924: INFO: Created: latency-svc-x4wtx
Sep  8 22:48:10.925: INFO: Got endpoints: latency-svc-rprmh [828.672571ms]
Sep  8 22:48:10.955: INFO: Got endpoints: latency-svc-x4wtx [832.454491ms]
Sep  8 22:48:10.960: INFO: Created: latency-svc-7nsn8
Sep  8 22:48:10.973: INFO: Got endpoints: latency-svc-7nsn8 [796.878703ms]
Sep  8 22:48:10.999: INFO: Created: latency-svc-7z2nz
Sep  8 22:48:11.048: INFO: Created: latency-svc-szdsl
Sep  8 22:48:11.048: INFO: Got endpoints: latency-svc-7z2nz [808.349656ms]
Sep  8 22:48:11.062: INFO: Got endpoints: latency-svc-szdsl [770.459328ms]
Sep  8 22:48:11.081: INFO: Created: latency-svc-txb79
Sep  8 22:48:11.118: INFO: Got endpoints: latency-svc-txb79 [738.468813ms]
Sep  8 22:48:11.157: INFO: Created: latency-svc-7gdxn
Sep  8 22:48:11.160: INFO: Got endpoints: latency-svc-7gdxn [723.438977ms]
Sep  8 22:48:11.217: INFO: Created: latency-svc-z89xl
Sep  8 22:48:11.217: INFO: Got endpoints: latency-svc-z89xl [690.134831ms]
Sep  8 22:48:11.273: INFO: Created: latency-svc-qqkfl
Sep  8 22:48:11.291: INFO: Got endpoints: latency-svc-qqkfl [661.751277ms]
Sep  8 22:48:11.308: INFO: Created: latency-svc-l6z6c
Sep  8 22:48:11.321: INFO: Got endpoints: latency-svc-l6z6c [611.873073ms]
Sep  8 22:48:11.327: INFO: Created: latency-svc-sjtf8
Sep  8 22:48:11.343: INFO: Got endpoints: latency-svc-sjtf8 [603.52542ms]
Sep  8 22:48:11.373: INFO: Created: latency-svc-j8h67
Sep  8 22:48:11.396: INFO: Got endpoints: latency-svc-j8h67 [655.968173ms]
Sep  8 22:48:11.407: INFO: Created: latency-svc-lvs66
Sep  8 22:48:11.429: INFO: Got endpoints: latency-svc-lvs66 [663.03789ms]
Sep  8 22:48:11.451: INFO: Created: latency-svc-ck65f
Sep  8 22:48:11.472: INFO: Got endpoints: latency-svc-ck65f [657.646587ms]
Sep  8 22:48:11.506: INFO: Created: latency-svc-5ctqc
Sep  8 22:48:11.527: INFO: Got endpoints: latency-svc-5ctqc [672.891152ms]
Sep  8 22:48:11.529: INFO: Created: latency-svc-qlh5r
Sep  8 22:48:11.565: INFO: Created: latency-svc-w28ss
Sep  8 22:48:11.582: INFO: Got endpoints: latency-svc-qlh5r [657.394164ms]
Sep  8 22:48:11.610: INFO: Created: latency-svc-8vc78
Sep  8 22:48:11.642: INFO: Got endpoints: latency-svc-w28ss [686.675624ms]
Sep  8 22:48:11.657: INFO: Created: latency-svc-ksnft
Sep  8 22:48:11.684: INFO: Got endpoints: latency-svc-8vc78 [711.103903ms]
Sep  8 22:48:11.686: INFO: Created: latency-svc-jhmtk
Sep  8 22:48:11.721: INFO: Got endpoints: latency-svc-ksnft [672.325065ms]
Sep  8 22:48:11.772: INFO: Got endpoints: latency-svc-jhmtk [710.151795ms]
Sep  8 22:48:12.219: INFO: Created: latency-svc-mjfjh
Sep  8 22:48:12.219: INFO: Created: latency-svc-6grn4
Sep  8 22:48:12.242: INFO: Created: latency-svc-94d24
Sep  8 22:48:12.246: INFO: Created: latency-svc-mknqc
Sep  8 22:48:12.247: INFO: Created: latency-svc-pkqbz
Sep  8 22:48:12.248: INFO: Created: latency-svc-njfnd
Sep  8 22:48:12.250: INFO: Created: latency-svc-8cchs
Sep  8 22:48:12.253: INFO: Created: latency-svc-9bfjr
Sep  8 22:48:12.253: INFO: Created: latency-svc-hdkx7
Sep  8 22:48:12.272: INFO: Created: latency-svc-w4wfh
Sep  8 22:48:12.272: INFO: Created: latency-svc-m7mwp
Sep  8 22:48:12.273: INFO: Created: latency-svc-njx7t
Sep  8 22:48:12.273: INFO: Created: latency-svc-gnkhn
Sep  8 22:48:12.272: INFO: Created: latency-svc-bb58p
Sep  8 22:48:12.273: INFO: Created: latency-svc-dlwv8
Sep  8 22:48:12.298: INFO: Got endpoints: latency-svc-mjfjh [576.906271ms]
Sep  8 22:48:12.307: INFO: Got endpoints: latency-svc-6grn4 [1.089471782s]
Sep  8 22:48:12.312: INFO: Got endpoints: latency-svc-hdkx7 [882.196302ms]
Sep  8 22:48:12.338: INFO: Got endpoints: latency-svc-njx7t [565.663296ms]
Sep  8 22:48:12.339: INFO: Got endpoints: latency-svc-9bfjr [1.047339478s]
Sep  8 22:48:12.339: INFO: Got endpoints: latency-svc-w4wfh [756.786886ms]
Sep  8 22:48:12.355: INFO: Got endpoints: latency-svc-dlwv8 [828.048797ms]
Sep  8 22:48:12.356: INFO: Got endpoints: latency-svc-m7mwp [713.298217ms]
Sep  8 22:48:12.367: INFO: Got endpoints: latency-svc-pkqbz [1.249407185s]
Sep  8 22:48:12.374: INFO: Got endpoints: latency-svc-bb58p [1.03145211s]
Sep  8 22:48:12.379: INFO: Got endpoints: latency-svc-8cchs [1.219023932s]
Sep  8 22:48:12.387: INFO: Got endpoints: latency-svc-gnkhn [991.115856ms]
Sep  8 22:48:12.406: INFO: Created: latency-svc-f27lb
Sep  8 22:48:12.425: INFO: Got endpoints: latency-svc-94d24 [952.631389ms]
Sep  8 22:48:12.441: INFO: Created: latency-svc-l7clx
Sep  8 22:48:12.477: INFO: Got endpoints: latency-svc-njfnd [1.155858073s]
Sep  8 22:48:12.479: INFO: Created: latency-svc-8v84p
Sep  8 22:48:12.516: INFO: Created: latency-svc-w4nqg
Sep  8 22:48:12.530: INFO: Got endpoints: latency-svc-mknqc [845.318014ms]
Sep  8 22:48:12.566: INFO: Created: latency-svc-ztkcm
Sep  8 22:48:12.566: INFO: Got endpoints: latency-svc-f27lb [268.773426ms]
Sep  8 22:48:12.609: INFO: Created: latency-svc-kz7qs
Sep  8 22:48:12.654: INFO: Got endpoints: latency-svc-l7clx [342.170824ms]
Sep  8 22:48:12.681: INFO: Got endpoints: latency-svc-8v84p [373.868701ms]
Sep  8 22:48:12.683: INFO: Created: latency-svc-7rlbs
Sep  8 22:48:12.717: INFO: Created: latency-svc-k95f5
Sep  8 22:48:12.720: INFO: Got endpoints: latency-svc-w4nqg [382.057288ms]
Sep  8 22:48:12.766: INFO: Got endpoints: latency-svc-ztkcm [426.862313ms]
Sep  8 22:48:12.776: INFO: Created: latency-svc-76gpc
Sep  8 22:48:12.797: INFO: Created: latency-svc-jwjxt
Sep  8 22:48:12.832: INFO: Got endpoints: latency-svc-kz7qs [493.378475ms]
Sep  8 22:48:12.835: INFO: Created: latency-svc-46w5d
Sep  8 22:48:12.879: INFO: Got endpoints: latency-svc-7rlbs [523.676711ms]
Sep  8 22:48:12.881: INFO: Created: latency-svc-fg2cp
Sep  8 22:48:12.903: INFO: Created: latency-svc-6mq2x
Sep  8 22:48:12.932: INFO: Created: latency-svc-9sc94
Sep  8 22:48:12.934: INFO: Got endpoints: latency-svc-k95f5 [578.794376ms]
Sep  8 22:48:12.953: INFO: Created: latency-svc-qhw98
Sep  8 22:48:12.992: INFO: Got endpoints: latency-svc-76gpc [618.238695ms]
Sep  8 22:48:13.012: INFO: Created: latency-svc-x658t
Sep  8 22:48:13.035: INFO: Got endpoints: latency-svc-jwjxt [656.23947ms]
Sep  8 22:48:13.036: INFO: Created: latency-svc-mrh4q
Sep  8 22:48:13.056: INFO: Created: latency-svc-2cxrm
Sep  8 22:48:13.059: INFO: Created: latency-svc-dhztr
Sep  8 22:48:13.071: INFO: Got endpoints: latency-svc-46w5d [683.825663ms]
Sep  8 22:48:13.097: INFO: Created: latency-svc-jq4gk
Sep  8 22:48:13.120: INFO: Got endpoints: latency-svc-fg2cp [694.401635ms]
Sep  8 22:48:13.127: INFO: Created: latency-svc-pc5lm
Sep  8 22:48:13.137: INFO: Created: latency-svc-n7mns
Sep  8 22:48:13.161: INFO: Created: latency-svc-zjchs
Sep  8 22:48:13.186: INFO: Got endpoints: latency-svc-6mq2x [709.295998ms]
Sep  8 22:48:13.214: INFO: Created: latency-svc-hkbjv
Sep  8 22:48:13.248: INFO: Got endpoints: latency-svc-9sc94 [718.210157ms]
Sep  8 22:48:13.271: INFO: Created: latency-svc-c7xjh
Sep  8 22:48:13.274: INFO: Got endpoints: latency-svc-qhw98 [707.556812ms]
Sep  8 22:48:13.317: INFO: Created: latency-svc-cjpzl
Sep  8 22:48:13.320: INFO: Got endpoints: latency-svc-x658t [665.756774ms]
Sep  8 22:48:13.340: INFO: Created: latency-svc-vnwq5
Sep  8 22:48:13.376: INFO: Got endpoints: latency-svc-mrh4q [695.102704ms]
Sep  8 22:48:13.387: INFO: Created: latency-svc-cxqpp
Sep  8 22:48:13.412: INFO: Created: latency-svc-mgvlc
Sep  8 22:48:13.427: INFO: Got endpoints: latency-svc-2cxrm [1.059715489s]
Sep  8 22:48:13.448: INFO: Created: latency-svc-4b79r
Sep  8 22:48:13.474: INFO: Got endpoints: latency-svc-dhztr [753.919262ms]
Sep  8 22:48:13.486: INFO: Created: latency-svc-9bpr5
Sep  8 22:48:13.516: INFO: Got endpoints: latency-svc-jq4gk [750.177919ms]
Sep  8 22:48:13.521: INFO: Created: latency-svc-ctzjt
Sep  8 22:48:13.568: INFO: Created: latency-svc-v8ppf
Sep  8 22:48:13.593: INFO: Got endpoints: latency-svc-pc5lm [760.367801ms]
Sep  8 22:48:13.631: INFO: Created: latency-svc-8ztnl
Sep  8 22:48:13.636: INFO: Got endpoints: latency-svc-n7mns [756.29014ms]
Sep  8 22:48:13.673: INFO: Got endpoints: latency-svc-zjchs [738.771878ms]
Sep  8 22:48:13.706: INFO: Created: latency-svc-498t5
Sep  8 22:48:13.726: INFO: Created: latency-svc-6tqzr
Sep  8 22:48:13.736: INFO: Got endpoints: latency-svc-hkbjv [743.1227ms]
Sep  8 22:48:13.764: INFO: Created: latency-svc-vkt79
Sep  8 22:48:13.769: INFO: Got endpoints: latency-svc-c7xjh [733.958935ms]
Sep  8 22:48:13.798: INFO: Created: latency-svc-t9flj
Sep  8 22:48:13.808: INFO: Created: latency-svc-tfrqz
Sep  8 22:48:13.821: INFO: Got endpoints: latency-svc-cjpzl [750.151301ms]
Sep  8 22:48:13.853: INFO: Created: latency-svc-jj4cb
Sep  8 22:48:13.867: INFO: Got endpoints: latency-svc-vnwq5 [747.641565ms]
Sep  8 22:48:13.886: INFO: Created: latency-svc-mhnr2
Sep  8 22:48:13.925: INFO: Got endpoints: latency-svc-cxqpp [738.11891ms]
Sep  8 22:48:13.926: INFO: Created: latency-svc-87mlq
Sep  8 22:48:13.967: INFO: Got endpoints: latency-svc-mgvlc [718.356782ms]
Sep  8 22:48:13.997: INFO: Created: latency-svc-bg52v
Sep  8 22:48:14.013: INFO: Got endpoints: latency-svc-4b79r [738.686189ms]
Sep  8 22:48:14.037: INFO: Created: latency-svc-pjc5j
Sep  8 22:48:14.066: INFO: Created: latency-svc-7h2md
Sep  8 22:48:14.079: INFO: Got endpoints: latency-svc-9bpr5 [757.556573ms]
Sep  8 22:48:14.131: INFO: Created: latency-svc-l2gc7
Sep  8 22:48:14.132: INFO: Got endpoints: latency-svc-ctzjt [754.600756ms]
Sep  8 22:48:14.165: INFO: Got endpoints: latency-svc-v8ppf [738.114031ms]
Sep  8 22:48:14.186: INFO: Created: latency-svc-f8ghd
Sep  8 22:48:14.219: INFO: Created: latency-svc-dzr8n
Sep  8 22:48:14.226: INFO: Got endpoints: latency-svc-8ztnl [751.490891ms]
Sep  8 22:48:14.260: INFO: Created: latency-svc-bv9ts
Sep  8 22:48:14.273: INFO: Got endpoints: latency-svc-498t5 [756.309266ms]
Sep  8 22:48:14.328: INFO: Got endpoints: latency-svc-6tqzr [735.755398ms]
Sep  8 22:48:14.334: INFO: Created: latency-svc-pdmbb
Sep  8 22:48:14.371: INFO: Got endpoints: latency-svc-vkt79 [735.40845ms]
Sep  8 22:48:14.382: INFO: Created: latency-svc-4zggt
Sep  8 22:48:14.426: INFO: Got endpoints: latency-svc-t9flj [752.545929ms]
Sep  8 22:48:14.484: INFO: Got endpoints: latency-svc-tfrqz [748.091494ms]
Sep  8 22:48:14.529: INFO: Got endpoints: latency-svc-jj4cb [759.525117ms]
Sep  8 22:48:14.562: INFO: Got endpoints: latency-svc-mhnr2 [741.147503ms]
Sep  8 22:48:14.633: INFO: Got endpoints: latency-svc-87mlq [765.595369ms]
Sep  8 22:48:14.679: INFO: Got endpoints: latency-svc-bg52v [753.948448ms]
Sep  8 22:48:14.719: INFO: Got endpoints: latency-svc-pjc5j [752.489084ms]
Sep  8 22:48:14.762: INFO: Got endpoints: latency-svc-7h2md [749.462383ms]
Sep  8 22:48:14.823: INFO: Got endpoints: latency-svc-l2gc7 [744.076449ms]
Sep  8 22:48:14.881: INFO: Got endpoints: latency-svc-f8ghd [748.846052ms]
Sep  8 22:48:14.922: INFO: Got endpoints: latency-svc-dzr8n [756.535055ms]
Sep  8 22:48:14.983: INFO: Got endpoints: latency-svc-bv9ts [756.729788ms]
Sep  8 22:48:15.020: INFO: Got endpoints: latency-svc-pdmbb [747.358114ms]
Sep  8 22:48:15.071: INFO: Got endpoints: latency-svc-4zggt [742.489107ms]
Sep  8 22:48:15.071: INFO: Latencies: [63.693614ms 103.566573ms 159.568618ms 204.550364ms 226.100331ms 268.773426ms 290.1776ms 327.626514ms 342.170824ms 357.914801ms 373.868701ms 382.057288ms 404.190041ms 426.862313ms 429.92528ms 430.34364ms 445.994305ms 447.104918ms 449.616466ms 451.512516ms 467.592583ms 469.813807ms 470.653614ms 472.641396ms 474.334205ms 475.620457ms 475.725482ms 475.791816ms 476.99257ms 478.453194ms 480.601676ms 481.415825ms 481.782358ms 481.826431ms 488.530225ms 488.911748ms 490.793159ms 491.278274ms 491.711113ms 493.378475ms 493.494732ms 494.044869ms 494.767738ms 495.563784ms 496.770857ms 496.913112ms 498.054693ms 498.253504ms 498.781631ms 500.636112ms 501.356325ms 504.799354ms 505.359149ms 506.574417ms 508.175005ms 511.259633ms 511.422406ms 512.262924ms 514.029432ms 518.204346ms 518.830965ms 519.012917ms 523.676711ms 523.798118ms 524.100041ms 524.244861ms 532.179752ms 533.032677ms 533.465444ms 535.354319ms 538.989214ms 541.985276ms 543.611092ms 544.322951ms 545.673763ms 548.100472ms 548.317978ms 549.213389ms 549.346758ms 550.041737ms 551.561122ms 552.216374ms 555.521624ms 556.427135ms 556.454365ms 557.536385ms 559.024012ms 564.741925ms 565.663296ms 568.720552ms 576.139601ms 576.906271ms 578.118495ms 578.358132ms 578.794376ms 580.99831ms 584.172957ms 584.661188ms 596.40005ms 601.153723ms 601.895309ms 603.52542ms 604.865542ms 607.069215ms 611.873073ms 613.959568ms 617.759952ms 618.238695ms 620.714036ms 626.289985ms 632.728293ms 636.612874ms 637.698129ms 641.194496ms 646.275171ms 655.968173ms 656.23947ms 657.394164ms 657.646587ms 661.751277ms 663.03789ms 663.862749ms 665.756774ms 672.325065ms 672.891152ms 674.367068ms 683.825663ms 686.675624ms 690.134831ms 694.401635ms 695.102704ms 707.556812ms 709.295998ms 710.151795ms 711.103903ms 713.298217ms 718.210157ms 718.356782ms 723.438977ms 726.664518ms 733.958935ms 735.40845ms 735.755398ms 738.114031ms 738.11891ms 738.468813ms 738.686189ms 738.771878ms 741.147503ms 742.489107ms 743.1227ms 744.076449ms 747.358114ms 747.61772ms 747.641565ms 748.091494ms 748.846052ms 749.462383ms 750.151301ms 750.177919ms 751.490891ms 752.489084ms 752.545929ms 753.919262ms 753.948448ms 754.600756ms 756.29014ms 756.309266ms 756.535055ms 756.729788ms 756.786886ms 757.556573ms 759.525117ms 760.367801ms 765.595369ms 770.459328ms 796.878703ms 798.38179ms 808.349656ms 828.048797ms 828.672571ms 831.461538ms 832.454491ms 834.41455ms 841.191752ms 845.318014ms 848.832784ms 876.574068ms 882.196302ms 884.050994ms 952.631389ms 991.115856ms 1.03145211s 1.047339478s 1.059715489s 1.089471782s 1.155858073s 1.219023932s 1.249407185s 1.359103482s]
Sep  8 22:48:15.071: INFO: 50 %ile: 601.895309ms
Sep  8 22:48:15.071: INFO: 90 %ile: 828.672571ms
Sep  8 22:48:15.071: INFO: 99 %ile: 1.249407185s
Sep  8 22:48:15.071: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:15.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7276" for this suite. 09/08/23 22:48:15.087
------------------------------
â€¢ [SLOW TEST] [10.965 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:04.141
    Sep  8 22:48:04.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename svc-latency 09/08/23 22:48:04.142
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:04.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:04.18
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Sep  8 22:48:04.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7276 09/08/23 22:48:04.189
    I0908 22:48:04.200994      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7276, replica count: 1
    I0908 22:48:05.252249      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0908 22:48:06.253300      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Sep  8 22:48:06.388: INFO: Created: latency-svc-nj9zl
    Sep  8 22:48:06.403: INFO: Got endpoints: latency-svc-nj9zl [49.070177ms]
    Sep  8 22:48:06.450: INFO: Created: latency-svc-88qs5
    Sep  8 22:48:06.468: INFO: Got endpoints: latency-svc-88qs5 [63.693614ms]
    Sep  8 22:48:06.497: INFO: Created: latency-svc-rffvp
    Sep  8 22:48:06.508: INFO: Got endpoints: latency-svc-rffvp [103.566573ms]
    Sep  8 22:48:06.557: INFO: Created: latency-svc-pt6tw
    Sep  8 22:48:06.564: INFO: Got endpoints: latency-svc-pt6tw [159.568618ms]
    Sep  8 22:48:06.591: INFO: Created: latency-svc-dj2ph
    Sep  8 22:48:06.609: INFO: Got endpoints: latency-svc-dj2ph [204.550364ms]
    Sep  8 22:48:06.626: INFO: Created: latency-svc-w2mbj
    Sep  8 22:48:06.630: INFO: Got endpoints: latency-svc-w2mbj [226.100331ms]
    Sep  8 22:48:06.676: INFO: Created: latency-svc-cf8kr
    Sep  8 22:48:06.695: INFO: Got endpoints: latency-svc-cf8kr [290.1776ms]
    Sep  8 22:48:06.721: INFO: Created: latency-svc-mpsr4
    Sep  8 22:48:06.732: INFO: Got endpoints: latency-svc-mpsr4 [327.626514ms]
    Sep  8 22:48:06.760: INFO: Created: latency-svc-vwzdm
    Sep  8 22:48:06.762: INFO: Got endpoints: latency-svc-vwzdm [357.914801ms]
    Sep  8 22:48:06.778: INFO: Created: latency-svc-n8q78
    Sep  8 22:48:06.808: INFO: Got endpoints: latency-svc-n8q78 [404.190041ms]
    Sep  8 22:48:06.838: INFO: Created: latency-svc-98kw4
    Sep  8 22:48:06.856: INFO: Got endpoints: latency-svc-98kw4 [451.512516ms]
    Sep  8 22:48:06.873: INFO: Created: latency-svc-8v9wl
    Sep  8 22:48:06.894: INFO: Got endpoints: latency-svc-8v9wl [490.793159ms]
    Sep  8 22:48:06.914: INFO: Created: latency-svc-h5kbt
    Sep  8 22:48:06.928: INFO: Got endpoints: latency-svc-h5kbt [523.798118ms]
    Sep  8 22:48:06.928: INFO: Created: latency-svc-4f5sb
    Sep  8 22:48:06.947: INFO: Got endpoints: latency-svc-4f5sb [543.611092ms]
    Sep  8 22:48:06.981: INFO: Created: latency-svc-j7xld
    Sep  8 22:48:06.981: INFO: Got endpoints: latency-svc-j7xld [576.139601ms]
    Sep  8 22:48:07.026: INFO: Created: latency-svc-2llmm
    Sep  8 22:48:07.036: INFO: Got endpoints: latency-svc-2llmm [632.728293ms]
    Sep  8 22:48:07.057: INFO: Created: latency-svc-4qwxn
    Sep  8 22:48:07.089: INFO: Got endpoints: latency-svc-4qwxn [620.714036ms]
    Sep  8 22:48:07.143: INFO: Created: latency-svc-h476g
    Sep  8 22:48:07.146: INFO: Got endpoints: latency-svc-h476g [637.698129ms]
    Sep  8 22:48:07.159: INFO: Created: latency-svc-hjgdt
    Sep  8 22:48:07.171: INFO: Got endpoints: latency-svc-hjgdt [607.069215ms]
    Sep  8 22:48:07.202: INFO: Created: latency-svc-7lpdb
    Sep  8 22:48:07.214: INFO: Got endpoints: latency-svc-7lpdb [604.865542ms]
    Sep  8 22:48:07.252: INFO: Created: latency-svc-42pj2
    Sep  8 22:48:07.267: INFO: Got endpoints: latency-svc-42pj2 [636.612874ms]
    Sep  8 22:48:07.272: INFO: Created: latency-svc-pqgwg
    Sep  8 22:48:07.296: INFO: Created: latency-svc-klldn
    Sep  8 22:48:07.297: INFO: Got endpoints: latency-svc-pqgwg [601.895309ms]
    Sep  8 22:48:07.333: INFO: Got endpoints: latency-svc-klldn [601.153723ms]
    Sep  8 22:48:07.335: INFO: Created: latency-svc-rctgr
    Sep  8 22:48:07.343: INFO: Got endpoints: latency-svc-rctgr [580.99831ms]
    Sep  8 22:48:07.363: INFO: Created: latency-svc-hfgjz
    Sep  8 22:48:07.367: INFO: Got endpoints: latency-svc-hfgjz [559.024012ms]
    Sep  8 22:48:07.386: INFO: Created: latency-svc-c25dw
    Sep  8 22:48:07.400: INFO: Got endpoints: latency-svc-c25dw [544.322951ms]
    Sep  8 22:48:07.422: INFO: Created: latency-svc-pcbj2
    Sep  8 22:48:07.440: INFO: Got endpoints: latency-svc-pcbj2 [545.673763ms]
    Sep  8 22:48:07.453: INFO: Created: latency-svc-g9w28
    Sep  8 22:48:07.462: INFO: Got endpoints: latency-svc-g9w28 [533.465444ms]
    Sep  8 22:48:07.481: INFO: Created: latency-svc-tl8mt
    Sep  8 22:48:07.497: INFO: Got endpoints: latency-svc-tl8mt [550.041737ms]
    Sep  8 22:48:07.513: INFO: Created: latency-svc-dj8bl
    Sep  8 22:48:07.529: INFO: Got endpoints: latency-svc-dj8bl [548.317978ms]
    Sep  8 22:48:07.550: INFO: Created: latency-svc-rl9ms
    Sep  8 22:48:07.555: INFO: Got endpoints: latency-svc-rl9ms [519.012917ms]
    Sep  8 22:48:07.575: INFO: Created: latency-svc-tr8mv
    Sep  8 22:48:07.594: INFO: Got endpoints: latency-svc-tr8mv [505.359149ms]
    Sep  8 22:48:07.622: INFO: Created: latency-svc-rnb4c
    Sep  8 22:48:07.627: INFO: Got endpoints: latency-svc-rnb4c [481.415825ms]
    Sep  8 22:48:07.649: INFO: Created: latency-svc-74tlf
    Sep  8 22:48:07.659: INFO: Got endpoints: latency-svc-74tlf [488.530225ms]
    Sep  8 22:48:07.683: INFO: Created: latency-svc-p5z4l
    Sep  8 22:48:07.703: INFO: Got endpoints: latency-svc-p5z4l [488.911748ms]
    Sep  8 22:48:07.738: INFO: Created: latency-svc-2s6d5
    Sep  8 22:48:07.744: INFO: Got endpoints: latency-svc-2s6d5 [476.99257ms]
    Sep  8 22:48:07.757: INFO: Created: latency-svc-26n55
    Sep  8 22:48:07.764: INFO: Got endpoints: latency-svc-26n55 [467.592583ms]
    Sep  8 22:48:07.799: INFO: Created: latency-svc-45dgm
    Sep  8 22:48:07.814: INFO: Got endpoints: latency-svc-45dgm [480.601676ms]
    Sep  8 22:48:07.820: INFO: Created: latency-svc-9lllf
    Sep  8 22:48:07.842: INFO: Got endpoints: latency-svc-9lllf [498.781631ms]
    Sep  8 22:48:07.853: INFO: Created: latency-svc-62rfr
    Sep  8 22:48:07.869: INFO: Got endpoints: latency-svc-62rfr [500.636112ms]
    Sep  8 22:48:07.894: INFO: Created: latency-svc-bqqjp
    Sep  8 22:48:07.912: INFO: Got endpoints: latency-svc-bqqjp [511.259633ms]
    Sep  8 22:48:07.938: INFO: Created: latency-svc-ksm9p
    Sep  8 22:48:07.973: INFO: Got endpoints: latency-svc-ksm9p [533.032677ms]
    Sep  8 22:48:07.999: INFO: Created: latency-svc-w4gjw
    Sep  8 22:48:08.013: INFO: Got endpoints: latency-svc-w4gjw [551.561122ms]
    Sep  8 22:48:08.031: INFO: Created: latency-svc-p2djd
    Sep  8 22:48:08.054: INFO: Got endpoints: latency-svc-p2djd [556.454365ms]
    Sep  8 22:48:08.066: INFO: Created: latency-svc-n87lb
    Sep  8 22:48:08.086: INFO: Got endpoints: latency-svc-n87lb [557.536385ms]
    Sep  8 22:48:08.104: INFO: Created: latency-svc-ldtvn
    Sep  8 22:48:08.120: INFO: Got endpoints: latency-svc-ldtvn [564.741925ms]
    Sep  8 22:48:08.133: INFO: Created: latency-svc-jqw5m
    Sep  8 22:48:08.150: INFO: Got endpoints: latency-svc-jqw5m [555.521624ms]
    Sep  8 22:48:08.167: INFO: Created: latency-svc-88m5d
    Sep  8 22:48:08.170: INFO: Got endpoints: latency-svc-88m5d [541.985276ms]
    Sep  8 22:48:08.190: INFO: Created: latency-svc-5zcmx
    Sep  8 22:48:08.198: INFO: Got endpoints: latency-svc-5zcmx [538.989214ms]
    Sep  8 22:48:08.214: INFO: Created: latency-svc-2h9cp
    Sep  8 22:48:08.238: INFO: Got endpoints: latency-svc-2h9cp [535.354319ms]
    Sep  8 22:48:08.251: INFO: Created: latency-svc-669g8
    Sep  8 22:48:08.263: INFO: Got endpoints: latency-svc-669g8 [518.204346ms]
    Sep  8 22:48:08.272: INFO: Created: latency-svc-dpsrw
    Sep  8 22:48:08.295: INFO: Created: latency-svc-nd5pf
    Sep  8 22:48:08.296: INFO: Got endpoints: latency-svc-dpsrw [532.179752ms]
    Sep  8 22:48:08.319: INFO: Got endpoints: latency-svc-nd5pf [504.799354ms]
    Sep  8 22:48:08.333: INFO: Created: latency-svc-4pzj2
    Sep  8 22:48:08.349: INFO: Got endpoints: latency-svc-4pzj2 [506.574417ms]
    Sep  8 22:48:08.363: INFO: Created: latency-svc-6lrnh
    Sep  8 22:48:08.388: INFO: Got endpoints: latency-svc-6lrnh [518.830965ms]
    Sep  8 22:48:08.389: INFO: Created: latency-svc-rzhvg
    Sep  8 22:48:08.403: INFO: Got endpoints: latency-svc-rzhvg [491.711113ms]
    Sep  8 22:48:08.414: INFO: Created: latency-svc-vgclp
    Sep  8 22:48:08.433: INFO: Created: latency-svc-jn22s
    Sep  8 22:48:08.455: INFO: Got endpoints: latency-svc-vgclp [481.782358ms]
    Sep  8 22:48:08.459: INFO: Got endpoints: latency-svc-jn22s [445.994305ms]
    Sep  8 22:48:08.475: INFO: Created: latency-svc-9qqrh
    Sep  8 22:48:08.484: INFO: Got endpoints: latency-svc-9qqrh [429.92528ms]
    Sep  8 22:48:08.509: INFO: Created: latency-svc-8x7xm
    Sep  8 22:48:08.517: INFO: Got endpoints: latency-svc-8x7xm [430.34364ms]
    Sep  8 22:48:08.539: INFO: Created: latency-svc-lh9p8
    Sep  8 22:48:08.567: INFO: Got endpoints: latency-svc-lh9p8 [447.104918ms]
    Sep  8 22:48:08.585: INFO: Created: latency-svc-wxl4t
    Sep  8 22:48:08.624: INFO: Got endpoints: latency-svc-wxl4t [474.334205ms]
    Sep  8 22:48:08.630: INFO: Created: latency-svc-bmq5p
    Sep  8 22:48:08.641: INFO: Got endpoints: latency-svc-bmq5p [470.653614ms]
    Sep  8 22:48:08.660: INFO: Created: latency-svc-lcrtp
    Sep  8 22:48:08.677: INFO: Got endpoints: latency-svc-lcrtp [478.453194ms]
    Sep  8 22:48:08.697: INFO: Created: latency-svc-6rrxp
    Sep  8 22:48:08.708: INFO: Got endpoints: latency-svc-6rrxp [469.813807ms]
    Sep  8 22:48:08.732: INFO: Created: latency-svc-2snwz
    Sep  8 22:48:08.745: INFO: Got endpoints: latency-svc-2snwz [481.826431ms]
    Sep  8 22:48:08.766: INFO: Created: latency-svc-w8v62
    Sep  8 22:48:08.770: INFO: Got endpoints: latency-svc-w8v62 [472.641396ms]
    Sep  8 22:48:08.783: INFO: Created: latency-svc-lb4vc
    Sep  8 22:48:08.795: INFO: Got endpoints: latency-svc-lb4vc [475.725482ms]
    Sep  8 22:48:08.822: INFO: Created: latency-svc-xmxk7
    Sep  8 22:48:08.863: INFO: Got endpoints: latency-svc-xmxk7 [514.029432ms]
    Sep  8 22:48:08.864: INFO: Created: latency-svc-ncv9f
    Sep  8 22:48:08.875: INFO: Created: latency-svc-nxr26
    Sep  8 22:48:08.879: INFO: Got endpoints: latency-svc-ncv9f [491.278274ms]
    Sep  8 22:48:08.899: INFO: Got endpoints: latency-svc-nxr26 [495.563784ms]
    Sep  8 22:48:08.911: INFO: Created: latency-svc-nngrc
    Sep  8 22:48:08.931: INFO: Got endpoints: latency-svc-nngrc [475.620457ms]
    Sep  8 22:48:08.950: INFO: Created: latency-svc-7wtws
    Sep  8 22:48:08.956: INFO: Got endpoints: latency-svc-7wtws [496.913112ms]
    Sep  8 22:48:08.966: INFO: Created: latency-svc-ckfrp
    Sep  8 22:48:08.981: INFO: Got endpoints: latency-svc-ckfrp [496.770857ms]
    Sep  8 22:48:08.997: INFO: Created: latency-svc-hkfln
    Sep  8 22:48:09.015: INFO: Got endpoints: latency-svc-hkfln [498.054693ms]
    Sep  8 22:48:09.026: INFO: Created: latency-svc-v8fvf
    Sep  8 22:48:09.043: INFO: Got endpoints: latency-svc-v8fvf [475.791816ms]
    Sep  8 22:48:09.070: INFO: Created: latency-svc-chqx9
    Sep  8 22:48:09.074: INFO: Got endpoints: latency-svc-chqx9 [449.616466ms]
    Sep  8 22:48:09.118: INFO: Created: latency-svc-x57vs
    Sep  8 22:48:09.139: INFO: Got endpoints: latency-svc-x57vs [498.253504ms]
    Sep  8 22:48:09.149: INFO: Created: latency-svc-z4mzv
    Sep  8 22:48:09.172: INFO: Got endpoints: latency-svc-z4mzv [494.767738ms]
    Sep  8 22:48:09.186: INFO: Created: latency-svc-7xmxh
    Sep  8 22:48:09.202: INFO: Got endpoints: latency-svc-7xmxh [494.044869ms]
    Sep  8 22:48:09.231: INFO: Created: latency-svc-swnkd
    Sep  8 22:48:09.256: INFO: Got endpoints: latency-svc-swnkd [511.422406ms]
    Sep  8 22:48:09.258: INFO: Created: latency-svc-fcqjp
    Sep  8 22:48:09.278: INFO: Got endpoints: latency-svc-fcqjp [508.175005ms]
    Sep  8 22:48:09.293: INFO: Created: latency-svc-rscwv
    Sep  8 22:48:09.307: INFO: Got endpoints: latency-svc-rscwv [512.262924ms]
    Sep  8 22:48:09.329: INFO: Created: latency-svc-jlgzg
    Sep  8 22:48:09.357: INFO: Got endpoints: latency-svc-jlgzg [493.494732ms]
    Sep  8 22:48:09.360: INFO: Created: latency-svc-8gt7t
    Sep  8 22:48:09.381: INFO: Got endpoints: latency-svc-8gt7t [501.356325ms]
    Sep  8 22:48:09.410: INFO: Created: latency-svc-n7tvr
    Sep  8 22:48:09.423: INFO: Got endpoints: latency-svc-n7tvr [524.244861ms]
    Sep  8 22:48:09.437: INFO: Created: latency-svc-qg55z
    Sep  8 22:48:09.455: INFO: Got endpoints: latency-svc-qg55z [524.100041ms]
    Sep  8 22:48:09.485: INFO: Created: latency-svc-zmtqx
    Sep  8 22:48:09.496: INFO: Created: latency-svc-4hg97
    Sep  8 22:48:09.509: INFO: Got endpoints: latency-svc-zmtqx [552.216374ms]
    Sep  8 22:48:09.529: INFO: Got endpoints: latency-svc-4hg97 [548.100472ms]
    Sep  8 22:48:09.536: INFO: Created: latency-svc-zxgmc
    Sep  8 22:48:09.564: INFO: Got endpoints: latency-svc-zxgmc [549.213389ms]
    Sep  8 22:48:09.593: INFO: Created: latency-svc-5wx9z
    Sep  8 22:48:09.628: INFO: Got endpoints: latency-svc-5wx9z [584.661188ms]
    Sep  8 22:48:09.640: INFO: Created: latency-svc-gwh6t
    Sep  8 22:48:09.652: INFO: Got endpoints: latency-svc-gwh6t [578.118495ms]
    Sep  8 22:48:09.678: INFO: Created: latency-svc-cqlls
    Sep  8 22:48:09.688: INFO: Got endpoints: latency-svc-cqlls [549.346758ms]
    Sep  8 22:48:09.717: INFO: Created: latency-svc-xhwqz
    Sep  8 22:48:09.728: INFO: Got endpoints: latency-svc-xhwqz [556.427135ms]
    Sep  8 22:48:09.741: INFO: Created: latency-svc-spgv9
    Sep  8 22:48:09.766: INFO: Created: latency-svc-fdj2k
    Sep  8 22:48:09.781: INFO: Got endpoints: latency-svc-spgv9 [578.358132ms]
    Sep  8 22:48:09.818: INFO: Created: latency-svc-d5rn4
    Sep  8 22:48:09.825: INFO: Got endpoints: latency-svc-fdj2k [568.720552ms]
    Sep  8 22:48:09.849: INFO: Created: latency-svc-p8c7j
    Sep  8 22:48:09.862: INFO: Got endpoints: latency-svc-d5rn4 [584.172957ms]
    Sep  8 22:48:09.891: INFO: Created: latency-svc-84pfq
    Sep  8 22:48:09.925: INFO: Got endpoints: latency-svc-p8c7j [617.759952ms]
    Sep  8 22:48:09.983: INFO: Got endpoints: latency-svc-84pfq [626.289985ms]
    Sep  8 22:48:09.989: INFO: Created: latency-svc-rqnwz
    Sep  8 22:48:10.009: INFO: Created: latency-svc-c94rk
    Sep  8 22:48:10.020: INFO: Got endpoints: latency-svc-rqnwz [596.40005ms]
    Sep  8 22:48:10.096: INFO: Got endpoints: latency-svc-c94rk [641.194496ms]
    Sep  8 22:48:10.097: INFO: Created: latency-svc-4rg6q
    Sep  8 22:48:10.123: INFO: Got endpoints: latency-svc-4rg6q [613.959568ms]
    Sep  8 22:48:10.141: INFO: Created: latency-svc-jmcsv
    Sep  8 22:48:10.176: INFO: Got endpoints: latency-svc-jmcsv [646.275171ms]
    Sep  8 22:48:10.207: INFO: Created: latency-svc-cb8wd
    Sep  8 22:48:10.239: INFO: Got endpoints: latency-svc-cb8wd [674.367068ms]
    Sep  8 22:48:10.263: INFO: Created: latency-svc-g8xz8
    Sep  8 22:48:10.292: INFO: Got endpoints: latency-svc-g8xz8 [663.862749ms]
    Sep  8 22:48:10.348: INFO: Created: latency-svc-7qcbp
    Sep  8 22:48:10.379: INFO: Got endpoints: latency-svc-7qcbp [726.664518ms]
    Sep  8 22:48:10.412: INFO: Created: latency-svc-9jp69
    Sep  8 22:48:10.436: INFO: Got endpoints: latency-svc-9jp69 [747.61772ms]
    Sep  8 22:48:10.489: INFO: Created: latency-svc-2j7z2
    Sep  8 22:48:10.527: INFO: Got endpoints: latency-svc-2j7z2 [798.38179ms]
    Sep  8 22:48:10.600: INFO: Created: latency-svc-qqgs6
    Sep  8 22:48:10.630: INFO: Got endpoints: latency-svc-qqgs6 [848.832784ms]
    Sep  8 22:48:10.689: INFO: Created: latency-svc-zwnj7
    Sep  8 22:48:10.700: INFO: Created: latency-svc-m9wvf
    Sep  8 22:48:10.709: INFO: Got endpoints: latency-svc-zwnj7 [884.050994ms]
    Sep  8 22:48:10.710: INFO: Created: latency-svc-nk4kz
    Sep  8 22:48:10.739: INFO: Got endpoints: latency-svc-m9wvf [876.574068ms]
    Sep  8 22:48:10.740: INFO: Got endpoints: latency-svc-nk4kz [1.359103482s]
    Sep  8 22:48:10.764: INFO: Created: latency-svc-8j67f
    Sep  8 22:48:10.766: INFO: Got endpoints: latency-svc-8j67f [841.191752ms]
    Sep  8 22:48:10.796: INFO: Created: latency-svc-25hpn
    Sep  8 22:48:10.815: INFO: Got endpoints: latency-svc-25hpn [831.461538ms]
    Sep  8 22:48:10.839: INFO: Created: latency-svc-4fw9g
    Sep  8 22:48:10.854: INFO: Got endpoints: latency-svc-4fw9g [834.41455ms]
    Sep  8 22:48:10.870: INFO: Created: latency-svc-rprmh
    Sep  8 22:48:10.924: INFO: Created: latency-svc-x4wtx
    Sep  8 22:48:10.925: INFO: Got endpoints: latency-svc-rprmh [828.672571ms]
    Sep  8 22:48:10.955: INFO: Got endpoints: latency-svc-x4wtx [832.454491ms]
    Sep  8 22:48:10.960: INFO: Created: latency-svc-7nsn8
    Sep  8 22:48:10.973: INFO: Got endpoints: latency-svc-7nsn8 [796.878703ms]
    Sep  8 22:48:10.999: INFO: Created: latency-svc-7z2nz
    Sep  8 22:48:11.048: INFO: Created: latency-svc-szdsl
    Sep  8 22:48:11.048: INFO: Got endpoints: latency-svc-7z2nz [808.349656ms]
    Sep  8 22:48:11.062: INFO: Got endpoints: latency-svc-szdsl [770.459328ms]
    Sep  8 22:48:11.081: INFO: Created: latency-svc-txb79
    Sep  8 22:48:11.118: INFO: Got endpoints: latency-svc-txb79 [738.468813ms]
    Sep  8 22:48:11.157: INFO: Created: latency-svc-7gdxn
    Sep  8 22:48:11.160: INFO: Got endpoints: latency-svc-7gdxn [723.438977ms]
    Sep  8 22:48:11.217: INFO: Created: latency-svc-z89xl
    Sep  8 22:48:11.217: INFO: Got endpoints: latency-svc-z89xl [690.134831ms]
    Sep  8 22:48:11.273: INFO: Created: latency-svc-qqkfl
    Sep  8 22:48:11.291: INFO: Got endpoints: latency-svc-qqkfl [661.751277ms]
    Sep  8 22:48:11.308: INFO: Created: latency-svc-l6z6c
    Sep  8 22:48:11.321: INFO: Got endpoints: latency-svc-l6z6c [611.873073ms]
    Sep  8 22:48:11.327: INFO: Created: latency-svc-sjtf8
    Sep  8 22:48:11.343: INFO: Got endpoints: latency-svc-sjtf8 [603.52542ms]
    Sep  8 22:48:11.373: INFO: Created: latency-svc-j8h67
    Sep  8 22:48:11.396: INFO: Got endpoints: latency-svc-j8h67 [655.968173ms]
    Sep  8 22:48:11.407: INFO: Created: latency-svc-lvs66
    Sep  8 22:48:11.429: INFO: Got endpoints: latency-svc-lvs66 [663.03789ms]
    Sep  8 22:48:11.451: INFO: Created: latency-svc-ck65f
    Sep  8 22:48:11.472: INFO: Got endpoints: latency-svc-ck65f [657.646587ms]
    Sep  8 22:48:11.506: INFO: Created: latency-svc-5ctqc
    Sep  8 22:48:11.527: INFO: Got endpoints: latency-svc-5ctqc [672.891152ms]
    Sep  8 22:48:11.529: INFO: Created: latency-svc-qlh5r
    Sep  8 22:48:11.565: INFO: Created: latency-svc-w28ss
    Sep  8 22:48:11.582: INFO: Got endpoints: latency-svc-qlh5r [657.394164ms]
    Sep  8 22:48:11.610: INFO: Created: latency-svc-8vc78
    Sep  8 22:48:11.642: INFO: Got endpoints: latency-svc-w28ss [686.675624ms]
    Sep  8 22:48:11.657: INFO: Created: latency-svc-ksnft
    Sep  8 22:48:11.684: INFO: Got endpoints: latency-svc-8vc78 [711.103903ms]
    Sep  8 22:48:11.686: INFO: Created: latency-svc-jhmtk
    Sep  8 22:48:11.721: INFO: Got endpoints: latency-svc-ksnft [672.325065ms]
    Sep  8 22:48:11.772: INFO: Got endpoints: latency-svc-jhmtk [710.151795ms]
    Sep  8 22:48:12.219: INFO: Created: latency-svc-mjfjh
    Sep  8 22:48:12.219: INFO: Created: latency-svc-6grn4
    Sep  8 22:48:12.242: INFO: Created: latency-svc-94d24
    Sep  8 22:48:12.246: INFO: Created: latency-svc-mknqc
    Sep  8 22:48:12.247: INFO: Created: latency-svc-pkqbz
    Sep  8 22:48:12.248: INFO: Created: latency-svc-njfnd
    Sep  8 22:48:12.250: INFO: Created: latency-svc-8cchs
    Sep  8 22:48:12.253: INFO: Created: latency-svc-9bfjr
    Sep  8 22:48:12.253: INFO: Created: latency-svc-hdkx7
    Sep  8 22:48:12.272: INFO: Created: latency-svc-w4wfh
    Sep  8 22:48:12.272: INFO: Created: latency-svc-m7mwp
    Sep  8 22:48:12.273: INFO: Created: latency-svc-njx7t
    Sep  8 22:48:12.273: INFO: Created: latency-svc-gnkhn
    Sep  8 22:48:12.272: INFO: Created: latency-svc-bb58p
    Sep  8 22:48:12.273: INFO: Created: latency-svc-dlwv8
    Sep  8 22:48:12.298: INFO: Got endpoints: latency-svc-mjfjh [576.906271ms]
    Sep  8 22:48:12.307: INFO: Got endpoints: latency-svc-6grn4 [1.089471782s]
    Sep  8 22:48:12.312: INFO: Got endpoints: latency-svc-hdkx7 [882.196302ms]
    Sep  8 22:48:12.338: INFO: Got endpoints: latency-svc-njx7t [565.663296ms]
    Sep  8 22:48:12.339: INFO: Got endpoints: latency-svc-9bfjr [1.047339478s]
    Sep  8 22:48:12.339: INFO: Got endpoints: latency-svc-w4wfh [756.786886ms]
    Sep  8 22:48:12.355: INFO: Got endpoints: latency-svc-dlwv8 [828.048797ms]
    Sep  8 22:48:12.356: INFO: Got endpoints: latency-svc-m7mwp [713.298217ms]
    Sep  8 22:48:12.367: INFO: Got endpoints: latency-svc-pkqbz [1.249407185s]
    Sep  8 22:48:12.374: INFO: Got endpoints: latency-svc-bb58p [1.03145211s]
    Sep  8 22:48:12.379: INFO: Got endpoints: latency-svc-8cchs [1.219023932s]
    Sep  8 22:48:12.387: INFO: Got endpoints: latency-svc-gnkhn [991.115856ms]
    Sep  8 22:48:12.406: INFO: Created: latency-svc-f27lb
    Sep  8 22:48:12.425: INFO: Got endpoints: latency-svc-94d24 [952.631389ms]
    Sep  8 22:48:12.441: INFO: Created: latency-svc-l7clx
    Sep  8 22:48:12.477: INFO: Got endpoints: latency-svc-njfnd [1.155858073s]
    Sep  8 22:48:12.479: INFO: Created: latency-svc-8v84p
    Sep  8 22:48:12.516: INFO: Created: latency-svc-w4nqg
    Sep  8 22:48:12.530: INFO: Got endpoints: latency-svc-mknqc [845.318014ms]
    Sep  8 22:48:12.566: INFO: Created: latency-svc-ztkcm
    Sep  8 22:48:12.566: INFO: Got endpoints: latency-svc-f27lb [268.773426ms]
    Sep  8 22:48:12.609: INFO: Created: latency-svc-kz7qs
    Sep  8 22:48:12.654: INFO: Got endpoints: latency-svc-l7clx [342.170824ms]
    Sep  8 22:48:12.681: INFO: Got endpoints: latency-svc-8v84p [373.868701ms]
    Sep  8 22:48:12.683: INFO: Created: latency-svc-7rlbs
    Sep  8 22:48:12.717: INFO: Created: latency-svc-k95f5
    Sep  8 22:48:12.720: INFO: Got endpoints: latency-svc-w4nqg [382.057288ms]
    Sep  8 22:48:12.766: INFO: Got endpoints: latency-svc-ztkcm [426.862313ms]
    Sep  8 22:48:12.776: INFO: Created: latency-svc-76gpc
    Sep  8 22:48:12.797: INFO: Created: latency-svc-jwjxt
    Sep  8 22:48:12.832: INFO: Got endpoints: latency-svc-kz7qs [493.378475ms]
    Sep  8 22:48:12.835: INFO: Created: latency-svc-46w5d
    Sep  8 22:48:12.879: INFO: Got endpoints: latency-svc-7rlbs [523.676711ms]
    Sep  8 22:48:12.881: INFO: Created: latency-svc-fg2cp
    Sep  8 22:48:12.903: INFO: Created: latency-svc-6mq2x
    Sep  8 22:48:12.932: INFO: Created: latency-svc-9sc94
    Sep  8 22:48:12.934: INFO: Got endpoints: latency-svc-k95f5 [578.794376ms]
    Sep  8 22:48:12.953: INFO: Created: latency-svc-qhw98
    Sep  8 22:48:12.992: INFO: Got endpoints: latency-svc-76gpc [618.238695ms]
    Sep  8 22:48:13.012: INFO: Created: latency-svc-x658t
    Sep  8 22:48:13.035: INFO: Got endpoints: latency-svc-jwjxt [656.23947ms]
    Sep  8 22:48:13.036: INFO: Created: latency-svc-mrh4q
    Sep  8 22:48:13.056: INFO: Created: latency-svc-2cxrm
    Sep  8 22:48:13.059: INFO: Created: latency-svc-dhztr
    Sep  8 22:48:13.071: INFO: Got endpoints: latency-svc-46w5d [683.825663ms]
    Sep  8 22:48:13.097: INFO: Created: latency-svc-jq4gk
    Sep  8 22:48:13.120: INFO: Got endpoints: latency-svc-fg2cp [694.401635ms]
    Sep  8 22:48:13.127: INFO: Created: latency-svc-pc5lm
    Sep  8 22:48:13.137: INFO: Created: latency-svc-n7mns
    Sep  8 22:48:13.161: INFO: Created: latency-svc-zjchs
    Sep  8 22:48:13.186: INFO: Got endpoints: latency-svc-6mq2x [709.295998ms]
    Sep  8 22:48:13.214: INFO: Created: latency-svc-hkbjv
    Sep  8 22:48:13.248: INFO: Got endpoints: latency-svc-9sc94 [718.210157ms]
    Sep  8 22:48:13.271: INFO: Created: latency-svc-c7xjh
    Sep  8 22:48:13.274: INFO: Got endpoints: latency-svc-qhw98 [707.556812ms]
    Sep  8 22:48:13.317: INFO: Created: latency-svc-cjpzl
    Sep  8 22:48:13.320: INFO: Got endpoints: latency-svc-x658t [665.756774ms]
    Sep  8 22:48:13.340: INFO: Created: latency-svc-vnwq5
    Sep  8 22:48:13.376: INFO: Got endpoints: latency-svc-mrh4q [695.102704ms]
    Sep  8 22:48:13.387: INFO: Created: latency-svc-cxqpp
    Sep  8 22:48:13.412: INFO: Created: latency-svc-mgvlc
    Sep  8 22:48:13.427: INFO: Got endpoints: latency-svc-2cxrm [1.059715489s]
    Sep  8 22:48:13.448: INFO: Created: latency-svc-4b79r
    Sep  8 22:48:13.474: INFO: Got endpoints: latency-svc-dhztr [753.919262ms]
    Sep  8 22:48:13.486: INFO: Created: latency-svc-9bpr5
    Sep  8 22:48:13.516: INFO: Got endpoints: latency-svc-jq4gk [750.177919ms]
    Sep  8 22:48:13.521: INFO: Created: latency-svc-ctzjt
    Sep  8 22:48:13.568: INFO: Created: latency-svc-v8ppf
    Sep  8 22:48:13.593: INFO: Got endpoints: latency-svc-pc5lm [760.367801ms]
    Sep  8 22:48:13.631: INFO: Created: latency-svc-8ztnl
    Sep  8 22:48:13.636: INFO: Got endpoints: latency-svc-n7mns [756.29014ms]
    Sep  8 22:48:13.673: INFO: Got endpoints: latency-svc-zjchs [738.771878ms]
    Sep  8 22:48:13.706: INFO: Created: latency-svc-498t5
    Sep  8 22:48:13.726: INFO: Created: latency-svc-6tqzr
    Sep  8 22:48:13.736: INFO: Got endpoints: latency-svc-hkbjv [743.1227ms]
    Sep  8 22:48:13.764: INFO: Created: latency-svc-vkt79
    Sep  8 22:48:13.769: INFO: Got endpoints: latency-svc-c7xjh [733.958935ms]
    Sep  8 22:48:13.798: INFO: Created: latency-svc-t9flj
    Sep  8 22:48:13.808: INFO: Created: latency-svc-tfrqz
    Sep  8 22:48:13.821: INFO: Got endpoints: latency-svc-cjpzl [750.151301ms]
    Sep  8 22:48:13.853: INFO: Created: latency-svc-jj4cb
    Sep  8 22:48:13.867: INFO: Got endpoints: latency-svc-vnwq5 [747.641565ms]
    Sep  8 22:48:13.886: INFO: Created: latency-svc-mhnr2
    Sep  8 22:48:13.925: INFO: Got endpoints: latency-svc-cxqpp [738.11891ms]
    Sep  8 22:48:13.926: INFO: Created: latency-svc-87mlq
    Sep  8 22:48:13.967: INFO: Got endpoints: latency-svc-mgvlc [718.356782ms]
    Sep  8 22:48:13.997: INFO: Created: latency-svc-bg52v
    Sep  8 22:48:14.013: INFO: Got endpoints: latency-svc-4b79r [738.686189ms]
    Sep  8 22:48:14.037: INFO: Created: latency-svc-pjc5j
    Sep  8 22:48:14.066: INFO: Created: latency-svc-7h2md
    Sep  8 22:48:14.079: INFO: Got endpoints: latency-svc-9bpr5 [757.556573ms]
    Sep  8 22:48:14.131: INFO: Created: latency-svc-l2gc7
    Sep  8 22:48:14.132: INFO: Got endpoints: latency-svc-ctzjt [754.600756ms]
    Sep  8 22:48:14.165: INFO: Got endpoints: latency-svc-v8ppf [738.114031ms]
    Sep  8 22:48:14.186: INFO: Created: latency-svc-f8ghd
    Sep  8 22:48:14.219: INFO: Created: latency-svc-dzr8n
    Sep  8 22:48:14.226: INFO: Got endpoints: latency-svc-8ztnl [751.490891ms]
    Sep  8 22:48:14.260: INFO: Created: latency-svc-bv9ts
    Sep  8 22:48:14.273: INFO: Got endpoints: latency-svc-498t5 [756.309266ms]
    Sep  8 22:48:14.328: INFO: Got endpoints: latency-svc-6tqzr [735.755398ms]
    Sep  8 22:48:14.334: INFO: Created: latency-svc-pdmbb
    Sep  8 22:48:14.371: INFO: Got endpoints: latency-svc-vkt79 [735.40845ms]
    Sep  8 22:48:14.382: INFO: Created: latency-svc-4zggt
    Sep  8 22:48:14.426: INFO: Got endpoints: latency-svc-t9flj [752.545929ms]
    Sep  8 22:48:14.484: INFO: Got endpoints: latency-svc-tfrqz [748.091494ms]
    Sep  8 22:48:14.529: INFO: Got endpoints: latency-svc-jj4cb [759.525117ms]
    Sep  8 22:48:14.562: INFO: Got endpoints: latency-svc-mhnr2 [741.147503ms]
    Sep  8 22:48:14.633: INFO: Got endpoints: latency-svc-87mlq [765.595369ms]
    Sep  8 22:48:14.679: INFO: Got endpoints: latency-svc-bg52v [753.948448ms]
    Sep  8 22:48:14.719: INFO: Got endpoints: latency-svc-pjc5j [752.489084ms]
    Sep  8 22:48:14.762: INFO: Got endpoints: latency-svc-7h2md [749.462383ms]
    Sep  8 22:48:14.823: INFO: Got endpoints: latency-svc-l2gc7 [744.076449ms]
    Sep  8 22:48:14.881: INFO: Got endpoints: latency-svc-f8ghd [748.846052ms]
    Sep  8 22:48:14.922: INFO: Got endpoints: latency-svc-dzr8n [756.535055ms]
    Sep  8 22:48:14.983: INFO: Got endpoints: latency-svc-bv9ts [756.729788ms]
    Sep  8 22:48:15.020: INFO: Got endpoints: latency-svc-pdmbb [747.358114ms]
    Sep  8 22:48:15.071: INFO: Got endpoints: latency-svc-4zggt [742.489107ms]
    Sep  8 22:48:15.071: INFO: Latencies: [63.693614ms 103.566573ms 159.568618ms 204.550364ms 226.100331ms 268.773426ms 290.1776ms 327.626514ms 342.170824ms 357.914801ms 373.868701ms 382.057288ms 404.190041ms 426.862313ms 429.92528ms 430.34364ms 445.994305ms 447.104918ms 449.616466ms 451.512516ms 467.592583ms 469.813807ms 470.653614ms 472.641396ms 474.334205ms 475.620457ms 475.725482ms 475.791816ms 476.99257ms 478.453194ms 480.601676ms 481.415825ms 481.782358ms 481.826431ms 488.530225ms 488.911748ms 490.793159ms 491.278274ms 491.711113ms 493.378475ms 493.494732ms 494.044869ms 494.767738ms 495.563784ms 496.770857ms 496.913112ms 498.054693ms 498.253504ms 498.781631ms 500.636112ms 501.356325ms 504.799354ms 505.359149ms 506.574417ms 508.175005ms 511.259633ms 511.422406ms 512.262924ms 514.029432ms 518.204346ms 518.830965ms 519.012917ms 523.676711ms 523.798118ms 524.100041ms 524.244861ms 532.179752ms 533.032677ms 533.465444ms 535.354319ms 538.989214ms 541.985276ms 543.611092ms 544.322951ms 545.673763ms 548.100472ms 548.317978ms 549.213389ms 549.346758ms 550.041737ms 551.561122ms 552.216374ms 555.521624ms 556.427135ms 556.454365ms 557.536385ms 559.024012ms 564.741925ms 565.663296ms 568.720552ms 576.139601ms 576.906271ms 578.118495ms 578.358132ms 578.794376ms 580.99831ms 584.172957ms 584.661188ms 596.40005ms 601.153723ms 601.895309ms 603.52542ms 604.865542ms 607.069215ms 611.873073ms 613.959568ms 617.759952ms 618.238695ms 620.714036ms 626.289985ms 632.728293ms 636.612874ms 637.698129ms 641.194496ms 646.275171ms 655.968173ms 656.23947ms 657.394164ms 657.646587ms 661.751277ms 663.03789ms 663.862749ms 665.756774ms 672.325065ms 672.891152ms 674.367068ms 683.825663ms 686.675624ms 690.134831ms 694.401635ms 695.102704ms 707.556812ms 709.295998ms 710.151795ms 711.103903ms 713.298217ms 718.210157ms 718.356782ms 723.438977ms 726.664518ms 733.958935ms 735.40845ms 735.755398ms 738.114031ms 738.11891ms 738.468813ms 738.686189ms 738.771878ms 741.147503ms 742.489107ms 743.1227ms 744.076449ms 747.358114ms 747.61772ms 747.641565ms 748.091494ms 748.846052ms 749.462383ms 750.151301ms 750.177919ms 751.490891ms 752.489084ms 752.545929ms 753.919262ms 753.948448ms 754.600756ms 756.29014ms 756.309266ms 756.535055ms 756.729788ms 756.786886ms 757.556573ms 759.525117ms 760.367801ms 765.595369ms 770.459328ms 796.878703ms 798.38179ms 808.349656ms 828.048797ms 828.672571ms 831.461538ms 832.454491ms 834.41455ms 841.191752ms 845.318014ms 848.832784ms 876.574068ms 882.196302ms 884.050994ms 952.631389ms 991.115856ms 1.03145211s 1.047339478s 1.059715489s 1.089471782s 1.155858073s 1.219023932s 1.249407185s 1.359103482s]
    Sep  8 22:48:15.071: INFO: 50 %ile: 601.895309ms
    Sep  8 22:48:15.071: INFO: 90 %ile: 828.672571ms
    Sep  8 22:48:15.071: INFO: 99 %ile: 1.249407185s
    Sep  8 22:48:15.071: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:15.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7276" for this suite. 09/08/23 22:48:15.087
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:15.106
Sep  8 22:48:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename limitrange 09/08/23 22:48:15.107
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:15.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:15.163
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 09/08/23 22:48:15.174
STEP: Setting up watch 09/08/23 22:48:15.174
STEP: Submitting a LimitRange 09/08/23 22:48:15.288
STEP: Verifying LimitRange creation was observed 09/08/23 22:48:15.3
STEP: Fetching the LimitRange to ensure it has proper values 09/08/23 22:48:15.3
Sep  8 22:48:15.308: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  8 22:48:15.308: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 09/08/23 22:48:15.308
STEP: Ensuring Pod has resource requirements applied from LimitRange 09/08/23 22:48:15.323
Sep  8 22:48:15.331: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  8 22:48:15.331: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 09/08/23 22:48:15.331
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/08/23 22:48:15.356
Sep  8 22:48:15.366: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  8 22:48:15.367: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 09/08/23 22:48:15.367
STEP: Failing to create a Pod with more than max resources 09/08/23 22:48:15.371
STEP: Updating a LimitRange 09/08/23 22:48:15.376
STEP: Verifying LimitRange updating is effective 09/08/23 22:48:15.39
STEP: Creating a Pod with less than former min resources 09/08/23 22:48:17.4
STEP: Failing to create a Pod with more than max resources 09/08/23 22:48:17.416
STEP: Deleting a LimitRange 09/08/23 22:48:17.428
STEP: Verifying the LimitRange was deleted 09/08/23 22:48:17.464
Sep  8 22:48:22.482: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 09/08/23 22:48:22.482
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:22.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-4937" for this suite. 09/08/23 22:48:22.555
------------------------------
â€¢ [SLOW TEST] [7.488 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:15.106
    Sep  8 22:48:15.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename limitrange 09/08/23 22:48:15.107
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:15.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:15.163
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 09/08/23 22:48:15.174
    STEP: Setting up watch 09/08/23 22:48:15.174
    STEP: Submitting a LimitRange 09/08/23 22:48:15.288
    STEP: Verifying LimitRange creation was observed 09/08/23 22:48:15.3
    STEP: Fetching the LimitRange to ensure it has proper values 09/08/23 22:48:15.3
    Sep  8 22:48:15.308: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  8 22:48:15.308: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 09/08/23 22:48:15.308
    STEP: Ensuring Pod has resource requirements applied from LimitRange 09/08/23 22:48:15.323
    Sep  8 22:48:15.331: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Sep  8 22:48:15.331: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 09/08/23 22:48:15.331
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 09/08/23 22:48:15.356
    Sep  8 22:48:15.366: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Sep  8 22:48:15.367: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 09/08/23 22:48:15.367
    STEP: Failing to create a Pod with more than max resources 09/08/23 22:48:15.371
    STEP: Updating a LimitRange 09/08/23 22:48:15.376
    STEP: Verifying LimitRange updating is effective 09/08/23 22:48:15.39
    STEP: Creating a Pod with less than former min resources 09/08/23 22:48:17.4
    STEP: Failing to create a Pod with more than max resources 09/08/23 22:48:17.416
    STEP: Deleting a LimitRange 09/08/23 22:48:17.428
    STEP: Verifying the LimitRange was deleted 09/08/23 22:48:17.464
    Sep  8 22:48:22.482: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 09/08/23 22:48:22.482
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:22.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-4937" for this suite. 09/08/23 22:48:22.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:22.605
Sep  8 22:48:22.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 22:48:22.608
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:22.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:22.689
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 09/08/23 22:48:22.755
Sep  8 22:48:22.794: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5505" to be "running and ready"
Sep  8 22:48:22.824: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 30.433919ms
Sep  8 22:48:22.824: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:48:24.841: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047373518s
Sep  8 22:48:24.841: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:48:26.858: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.063758725s
Sep  8 22:48:26.858: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Sep  8 22:48:26.858: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 09/08/23 22:48:26.872
Sep  8 22:48:26.913: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5505" to be "running and ready"
Sep  8 22:48:26.946: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 32.999518ms
Sep  8 22:48:26.946: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:48:28.984: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071223509s
Sep  8 22:48:28.984: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:48:30.968: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.055364655s
Sep  8 22:48:30.968: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Sep  8 22:48:30.968: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 09/08/23 22:48:30.994
Sep  8 22:48:31.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  8 22:48:31.047: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  8 22:48:33.047: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  8 22:48:33.064: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 09/08/23 22:48:33.064
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:33.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5505" for this suite. 09/08/23 22:48:33.114
------------------------------
â€¢ [SLOW TEST] [10.539 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:22.605
    Sep  8 22:48:22.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-lifecycle-hook 09/08/23 22:48:22.608
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:22.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:22.689
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 09/08/23 22:48:22.755
    Sep  8 22:48:22.794: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5505" to be "running and ready"
    Sep  8 22:48:22.824: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 30.433919ms
    Sep  8 22:48:22.824: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:48:24.841: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047373518s
    Sep  8 22:48:24.841: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:48:26.858: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.063758725s
    Sep  8 22:48:26.858: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Sep  8 22:48:26.858: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 09/08/23 22:48:26.872
    Sep  8 22:48:26.913: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5505" to be "running and ready"
    Sep  8 22:48:26.946: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 32.999518ms
    Sep  8 22:48:26.946: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:48:28.984: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071223509s
    Sep  8 22:48:28.984: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:48:30.968: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.055364655s
    Sep  8 22:48:30.968: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Sep  8 22:48:30.968: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 09/08/23 22:48:30.994
    Sep  8 22:48:31.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  8 22:48:31.047: INFO: Pod pod-with-prestop-exec-hook still exists
    Sep  8 22:48:33.047: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Sep  8 22:48:33.064: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 09/08/23 22:48:33.064
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:33.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5505" for this suite. 09/08/23 22:48:33.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:33.147
Sep  8 22:48:33.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 22:48:33.149
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:33.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:33.221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 22:48:33.265
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:48:33.998
STEP: Deploying the webhook pod 09/08/23 22:48:34.032
STEP: Wait for the deployment to be ready 09/08/23 22:48:34.073
Sep  8 22:48:34.121: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  8 22:48:36.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 22:48:38.193
STEP: Verifying the service has paired with the endpoint 09/08/23 22:48:38.253
Sep  8 22:48:39.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep  8 22:48:40.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep  8 22:48:41.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Sep  8 22:48:42.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/08/23 22:48:42.276
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/08/23 22:48:42.311
STEP: Creating a dummy validating-webhook-configuration object 09/08/23 22:48:42.357
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/08/23 22:48:42.391
STEP: Creating a dummy mutating-webhook-configuration object 09/08/23 22:48:42.409
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/08/23 22:48:42.432
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9905" for this suite. 09/08/23 22:48:42.662
STEP: Destroying namespace "webhook-9905-markers" for this suite. 09/08/23 22:48:42.697
------------------------------
â€¢ [SLOW TEST] [9.609 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:33.147
    Sep  8 22:48:33.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 22:48:33.149
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:33.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:33.221
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 22:48:33.265
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:48:33.998
    STEP: Deploying the webhook pod 09/08/23 22:48:34.032
    STEP: Wait for the deployment to be ready 09/08/23 22:48:34.073
    Sep  8 22:48:34.121: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Sep  8 22:48:36.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 48, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 22:48:38.193
    STEP: Verifying the service has paired with the endpoint 09/08/23 22:48:38.253
    Sep  8 22:48:39.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Sep  8 22:48:40.254: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Sep  8 22:48:41.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Sep  8 22:48:42.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/08/23 22:48:42.276
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 09/08/23 22:48:42.311
    STEP: Creating a dummy validating-webhook-configuration object 09/08/23 22:48:42.357
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 09/08/23 22:48:42.391
    STEP: Creating a dummy mutating-webhook-configuration object 09/08/23 22:48:42.409
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 09/08/23 22:48:42.432
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:42.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9905" for this suite. 09/08/23 22:48:42.662
    STEP: Destroying namespace "webhook-9905-markers" for this suite. 09/08/23 22:48:42.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:42.757
Sep  8 22:48:42.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename kubectl 09/08/23 22:48:42.759
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:42.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:42.816
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 09/08/23 22:48:42.842
Sep  8 22:48:42.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  8 22:48:42.975: INFO: stderr: ""
Sep  8 22:48:42.975: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 09/08/23 22:48:42.975
Sep  8 22:48:42.975: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  8 22:48:42.975: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3466" to be "running and ready, or succeeded"
Sep  8 22:48:42.986: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.5647ms
Sep  8 22:48:42.986: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'node-3' to be 'Running' but was 'Pending'
Sep  8 22:48:44.997: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022125423s
Sep  8 22:48:44.998: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'node-3' to be 'Running' but was 'Pending'
Sep  8 22:48:46.997: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.021212467s
Sep  8 22:48:46.997: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  8 22:48:46.997: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 09/08/23 22:48:46.997
Sep  8 22:48:46.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator'
Sep  8 22:48:47.160: INFO: stderr: ""
Sep  8 22:48:47.160: INFO: stdout: "I0908 22:48:44.185089       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/dg27 375\nI0908 22:48:44.385463       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ms99 462\nI0908 22:48:44.586122       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/h9p6 308\nI0908 22:48:44.785743       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jlq 297\nI0908 22:48:44.985146       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/lk4 284\nI0908 22:48:45.185613       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/hth 435\nI0908 22:48:45.386153       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/htww 203\nI0908 22:48:45.585421       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/n6fk 233\nI0908 22:48:45.785886       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/vr5 299\nI0908 22:48:45.985236       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/cqq 588\nI0908 22:48:46.185443       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/sxj 254\nI0908 22:48:46.386065       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/26s 507\nI0908 22:48:46.585338       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/8c6c 592\nI0908 22:48:46.785881       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/scf 455\nI0908 22:48:46.985231       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/vt64 324\n"
STEP: limiting log lines 09/08/23 22:48:47.16
Sep  8 22:48:47.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --tail=1'
Sep  8 22:48:47.309: INFO: stderr: ""
Sep  8 22:48:47.309: INFO: stdout: "I0908 22:48:47.185234       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kx2 416\n"
Sep  8 22:48:47.309: INFO: got output "I0908 22:48:47.185234       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kx2 416\n"
STEP: limiting log bytes 09/08/23 22:48:47.309
Sep  8 22:48:47.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --limit-bytes=1'
Sep  8 22:48:47.485: INFO: stderr: ""
Sep  8 22:48:47.485: INFO: stdout: "I"
Sep  8 22:48:47.485: INFO: got output "I"
STEP: exposing timestamps 09/08/23 22:48:47.485
Sep  8 22:48:47.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  8 22:48:47.622: INFO: stderr: ""
Sep  8 22:48:47.622: INFO: stdout: "2023-09-08T22:48:47.586425095Z I0908 22:48:47.586176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/n7z 482\n"
Sep  8 22:48:47.622: INFO: got output "2023-09-08T22:48:47.586425095Z I0908 22:48:47.586176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/n7z 482\n"
STEP: restricting to a time range 09/08/23 22:48:47.622
Sep  8 22:48:50.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --since=1s'
Sep  8 22:48:50.264: INFO: stderr: ""
Sep  8 22:48:50.264: INFO: stdout: "I0908 22:48:49.385596       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/2fr 586\nI0908 22:48:49.586029       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/92x 433\nI0908 22:48:49.785384       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/cnfd 268\nI0908 22:48:49.985810       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/bfz8 336\nI0908 22:48:50.185814       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/rwg 323\n"
Sep  8 22:48:50.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --since=24h'
Sep  8 22:48:50.418: INFO: stderr: ""
Sep  8 22:48:50.418: INFO: stdout: "I0908 22:48:44.185089       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/dg27 375\nI0908 22:48:44.385463       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ms99 462\nI0908 22:48:44.586122       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/h9p6 308\nI0908 22:48:44.785743       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jlq 297\nI0908 22:48:44.985146       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/lk4 284\nI0908 22:48:45.185613       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/hth 435\nI0908 22:48:45.386153       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/htww 203\nI0908 22:48:45.585421       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/n6fk 233\nI0908 22:48:45.785886       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/vr5 299\nI0908 22:48:45.985236       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/cqq 588\nI0908 22:48:46.185443       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/sxj 254\nI0908 22:48:46.386065       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/26s 507\nI0908 22:48:46.585338       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/8c6c 592\nI0908 22:48:46.785881       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/scf 455\nI0908 22:48:46.985231       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/vt64 324\nI0908 22:48:47.185234       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kx2 416\nI0908 22:48:47.385727       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/g6s8 436\nI0908 22:48:47.586176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/n7z 482\nI0908 22:48:47.785957       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/lg2x 313\nI0908 22:48:47.985321       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/2t2 497\nI0908 22:48:48.185890       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/g2jz 227\nI0908 22:48:48.385246       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/ggs4 288\nI0908 22:48:48.585759       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/v4dq 507\nI0908 22:48:48.786152       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/p5g 224\nI0908 22:48:48.985580       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/rfh 408\nI0908 22:48:49.186000       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/ndl 472\nI0908 22:48:49.385596       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/2fr 586\nI0908 22:48:49.586029       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/92x 433\nI0908 22:48:49.785384       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/cnfd 268\nI0908 22:48:49.985810       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/bfz8 336\nI0908 22:48:50.185814       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/rwg 323\nI0908 22:48:50.385281       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/9cz 576\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Sep  8 22:48:50.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 delete pod logs-generator'
Sep  8 22:48:52.227: INFO: stderr: ""
Sep  8 22:48:52.227: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:52.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3466" for this suite. 09/08/23 22:48:52.26
------------------------------
â€¢ [SLOW TEST] [9.524 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:42.757
    Sep  8 22:48:42.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename kubectl 09/08/23 22:48:42.759
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:42.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:42.816
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 09/08/23 22:48:42.842
    Sep  8 22:48:42.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Sep  8 22:48:42.975: INFO: stderr: ""
    Sep  8 22:48:42.975: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 09/08/23 22:48:42.975
    Sep  8 22:48:42.975: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Sep  8 22:48:42.975: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3466" to be "running and ready, or succeeded"
    Sep  8 22:48:42.986: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.5647ms
    Sep  8 22:48:42.986: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'node-3' to be 'Running' but was 'Pending'
    Sep  8 22:48:44.997: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022125423s
    Sep  8 22:48:44.998: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'node-3' to be 'Running' but was 'Pending'
    Sep  8 22:48:46.997: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.021212467s
    Sep  8 22:48:46.997: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Sep  8 22:48:46.997: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 09/08/23 22:48:46.997
    Sep  8 22:48:46.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator'
    Sep  8 22:48:47.160: INFO: stderr: ""
    Sep  8 22:48:47.160: INFO: stdout: "I0908 22:48:44.185089       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/dg27 375\nI0908 22:48:44.385463       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ms99 462\nI0908 22:48:44.586122       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/h9p6 308\nI0908 22:48:44.785743       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jlq 297\nI0908 22:48:44.985146       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/lk4 284\nI0908 22:48:45.185613       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/hth 435\nI0908 22:48:45.386153       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/htww 203\nI0908 22:48:45.585421       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/n6fk 233\nI0908 22:48:45.785886       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/vr5 299\nI0908 22:48:45.985236       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/cqq 588\nI0908 22:48:46.185443       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/sxj 254\nI0908 22:48:46.386065       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/26s 507\nI0908 22:48:46.585338       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/8c6c 592\nI0908 22:48:46.785881       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/scf 455\nI0908 22:48:46.985231       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/vt64 324\n"
    STEP: limiting log lines 09/08/23 22:48:47.16
    Sep  8 22:48:47.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --tail=1'
    Sep  8 22:48:47.309: INFO: stderr: ""
    Sep  8 22:48:47.309: INFO: stdout: "I0908 22:48:47.185234       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kx2 416\n"
    Sep  8 22:48:47.309: INFO: got output "I0908 22:48:47.185234       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kx2 416\n"
    STEP: limiting log bytes 09/08/23 22:48:47.309
    Sep  8 22:48:47.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --limit-bytes=1'
    Sep  8 22:48:47.485: INFO: stderr: ""
    Sep  8 22:48:47.485: INFO: stdout: "I"
    Sep  8 22:48:47.485: INFO: got output "I"
    STEP: exposing timestamps 09/08/23 22:48:47.485
    Sep  8 22:48:47.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --tail=1 --timestamps'
    Sep  8 22:48:47.622: INFO: stderr: ""
    Sep  8 22:48:47.622: INFO: stdout: "2023-09-08T22:48:47.586425095Z I0908 22:48:47.586176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/n7z 482\n"
    Sep  8 22:48:47.622: INFO: got output "2023-09-08T22:48:47.586425095Z I0908 22:48:47.586176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/n7z 482\n"
    STEP: restricting to a time range 09/08/23 22:48:47.622
    Sep  8 22:48:50.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --since=1s'
    Sep  8 22:48:50.264: INFO: stderr: ""
    Sep  8 22:48:50.264: INFO: stdout: "I0908 22:48:49.385596       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/2fr 586\nI0908 22:48:49.586029       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/92x 433\nI0908 22:48:49.785384       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/cnfd 268\nI0908 22:48:49.985810       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/bfz8 336\nI0908 22:48:50.185814       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/rwg 323\n"
    Sep  8 22:48:50.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 logs logs-generator logs-generator --since=24h'
    Sep  8 22:48:50.418: INFO: stderr: ""
    Sep  8 22:48:50.418: INFO: stdout: "I0908 22:48:44.185089       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/dg27 375\nI0908 22:48:44.385463       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/ms99 462\nI0908 22:48:44.586122       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/h9p6 308\nI0908 22:48:44.785743       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/jlq 297\nI0908 22:48:44.985146       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/lk4 284\nI0908 22:48:45.185613       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/hth 435\nI0908 22:48:45.386153       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/htww 203\nI0908 22:48:45.585421       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/n6fk 233\nI0908 22:48:45.785886       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/vr5 299\nI0908 22:48:45.985236       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/cqq 588\nI0908 22:48:46.185443       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/sxj 254\nI0908 22:48:46.386065       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/26s 507\nI0908 22:48:46.585338       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/8c6c 592\nI0908 22:48:46.785881       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/scf 455\nI0908 22:48:46.985231       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/vt64 324\nI0908 22:48:47.185234       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/kx2 416\nI0908 22:48:47.385727       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/g6s8 436\nI0908 22:48:47.586176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/n7z 482\nI0908 22:48:47.785957       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/lg2x 313\nI0908 22:48:47.985321       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/2t2 497\nI0908 22:48:48.185890       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/g2jz 227\nI0908 22:48:48.385246       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/ggs4 288\nI0908 22:48:48.585759       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/v4dq 507\nI0908 22:48:48.786152       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/p5g 224\nI0908 22:48:48.985580       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/rfh 408\nI0908 22:48:49.186000       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/ndl 472\nI0908 22:48:49.385596       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/2fr 586\nI0908 22:48:49.586029       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/92x 433\nI0908 22:48:49.785384       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/cnfd 268\nI0908 22:48:49.985810       1 logs_generator.go:76] 29 POST /api/v1/namespaces/default/pods/bfz8 336\nI0908 22:48:50.185814       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/rwg 323\nI0908 22:48:50.385281       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/9cz 576\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Sep  8 22:48:50.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2096145757 --namespace=kubectl-3466 delete pod logs-generator'
    Sep  8 22:48:52.227: INFO: stderr: ""
    Sep  8 22:48:52.227: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:52.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3466" for this suite. 09/08/23 22:48:52.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:52.286
Sep  8 22:48:52.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename projected 09/08/23 22:48:52.289
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:52.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:52.357
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-e8f18e38-cd0c-4931-aff6-d59b5c0c2e56 09/08/23 22:48:52.363
STEP: Creating a pod to test consume secrets 09/08/23 22:48:52.379
Sep  8 22:48:52.404: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89" in namespace "projected-7779" to be "Succeeded or Failed"
Sep  8 22:48:52.429: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89": Phase="Pending", Reason="", readiness=false. Elapsed: 24.55138ms
Sep  8 22:48:54.439: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034408819s
Sep  8 22:48:56.439: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034146929s
STEP: Saw pod success 09/08/23 22:48:56.439
Sep  8 22:48:56.439: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89" satisfied condition "Succeeded or Failed"
Sep  8 22:48:56.449: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89 container secret-volume-test: <nil>
STEP: delete the pod 09/08/23 22:48:56.465
Sep  8 22:48:56.498: INFO: Waiting for pod pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89 to disappear
Sep  8 22:48:56.505: INFO: Pod pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:56.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7779" for this suite. 09/08/23 22:48:56.518
------------------------------
â€¢ [4.248 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:52.286
    Sep  8 22:48:52.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename projected 09/08/23 22:48:52.289
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:52.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:52.357
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-e8f18e38-cd0c-4931-aff6-d59b5c0c2e56 09/08/23 22:48:52.363
    STEP: Creating a pod to test consume secrets 09/08/23 22:48:52.379
    Sep  8 22:48:52.404: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89" in namespace "projected-7779" to be "Succeeded or Failed"
    Sep  8 22:48:52.429: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89": Phase="Pending", Reason="", readiness=false. Elapsed: 24.55138ms
    Sep  8 22:48:54.439: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034408819s
    Sep  8 22:48:56.439: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034146929s
    STEP: Saw pod success 09/08/23 22:48:56.439
    Sep  8 22:48:56.439: INFO: Pod "pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89" satisfied condition "Succeeded or Failed"
    Sep  8 22:48:56.449: INFO: Trying to get logs from node node-3 pod pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89 container secret-volume-test: <nil>
    STEP: delete the pod 09/08/23 22:48:56.465
    Sep  8 22:48:56.498: INFO: Waiting for pod pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89 to disappear
    Sep  8 22:48:56.505: INFO: Pod pod-projected-secrets-0f2160c3-6ba2-42f1-9525-419224491f89 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:56.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7779" for this suite. 09/08/23 22:48:56.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:56.546
Sep  8 22:48:56.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename pods 09/08/23 22:48:56.547
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:56.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:56.579
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 09/08/23 22:48:56.588
STEP: submitting the pod to kubernetes 09/08/23 22:48:56.589
Sep  8 22:48:56.617: INFO: Waiting up to 5m0s for pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" in namespace "pods-3637" to be "running and ready"
Sep  8 22:48:56.632: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.299238ms
Sep  8 22:48:56.632: INFO: The phase of Pod pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4 is Pending, waiting for it to be Running (with Ready = true)
Sep  8 22:48:58.643: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4": Phase="Running", Reason="", readiness=true. Elapsed: 2.026226185s
Sep  8 22:48:58.643: INFO: The phase of Pod pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4 is Running (Ready = true)
Sep  8 22:48:58.643: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 09/08/23 22:48:58.656
STEP: updating the pod 09/08/23 22:48:58.665
Sep  8 22:48:59.197: INFO: Successfully updated pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4"
Sep  8 22:48:59.197: INFO: Waiting up to 5m0s for pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" in namespace "pods-3637" to be "running"
Sep  8 22:48:59.206: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4": Phase="Running", Reason="", readiness=true. Elapsed: 8.749322ms
Sep  8 22:48:59.206: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 09/08/23 22:48:59.206
Sep  8 22:48:59.216: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Sep  8 22:48:59.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3637" for this suite. 09/08/23 22:48:59.231
------------------------------
â€¢ [2.704 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:56.546
    Sep  8 22:48:56.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename pods 09/08/23 22:48:56.547
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:56.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:56.579
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 09/08/23 22:48:56.588
    STEP: submitting the pod to kubernetes 09/08/23 22:48:56.589
    Sep  8 22:48:56.617: INFO: Waiting up to 5m0s for pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" in namespace "pods-3637" to be "running and ready"
    Sep  8 22:48:56.632: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.299238ms
    Sep  8 22:48:56.632: INFO: The phase of Pod pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4 is Pending, waiting for it to be Running (with Ready = true)
    Sep  8 22:48:58.643: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4": Phase="Running", Reason="", readiness=true. Elapsed: 2.026226185s
    Sep  8 22:48:58.643: INFO: The phase of Pod pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4 is Running (Ready = true)
    Sep  8 22:48:58.643: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 09/08/23 22:48:58.656
    STEP: updating the pod 09/08/23 22:48:58.665
    Sep  8 22:48:59.197: INFO: Successfully updated pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4"
    Sep  8 22:48:59.197: INFO: Waiting up to 5m0s for pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" in namespace "pods-3637" to be "running"
    Sep  8 22:48:59.206: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4": Phase="Running", Reason="", readiness=true. Elapsed: 8.749322ms
    Sep  8 22:48:59.206: INFO: Pod "pod-update-9f53364f-af81-4d7f-9381-0931a3415ad4" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 09/08/23 22:48:59.206
    Sep  8 22:48:59.216: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:48:59.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3637" for this suite. 09/08/23 22:48:59.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:48:59.25
Sep  8 22:48:59.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 22:48:59.252
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:59.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:59.313
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 22:48:59.358
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:48:59.745
STEP: Deploying the webhook pod 09/08/23 22:48:59.757
STEP: Wait for the deployment to be ready 09/08/23 22:48:59.799
Sep  8 22:48:59.834: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 09/08/23 22:49:01.866
STEP: Verifying the service has paired with the endpoint 09/08/23 22:49:01.909
Sep  8 22:49:02.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Sep  8 22:49:02.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/08/23 22:49:08.462
STEP: Creating a custom resource that should be denied by the webhook 09/08/23 22:49:08.499
STEP: Creating a custom resource whose deletion would be denied by the webhook 09/08/23 22:49:10.552
STEP: Updating the custom resource with disallowed data should be denied 09/08/23 22:49:10.568
STEP: Deleting the custom resource should be denied 09/08/23 22:49:10.59
STEP: Remove the offending key and value from the custom resource data 09/08/23 22:49:10.608
STEP: Deleting the updated custom resource should be successful 09/08/23 22:49:10.634
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:49:11.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9873" for this suite. 09/08/23 22:49:11.39
STEP: Destroying namespace "webhook-9873-markers" for this suite. 09/08/23 22:49:11.422
------------------------------
â€¢ [SLOW TEST] [12.229 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:48:59.25
    Sep  8 22:48:59.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 22:48:59.252
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:48:59.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:48:59.313
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 22:48:59.358
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:48:59.745
    STEP: Deploying the webhook pod 09/08/23 22:48:59.757
    STEP: Wait for the deployment to be ready 09/08/23 22:48:59.799
    Sep  8 22:48:59.834: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 09/08/23 22:49:01.866
    STEP: Verifying the service has paired with the endpoint 09/08/23 22:49:01.909
    Sep  8 22:49:02.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Sep  8 22:49:02.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 09/08/23 22:49:08.462
    STEP: Creating a custom resource that should be denied by the webhook 09/08/23 22:49:08.499
    STEP: Creating a custom resource whose deletion would be denied by the webhook 09/08/23 22:49:10.552
    STEP: Updating the custom resource with disallowed data should be denied 09/08/23 22:49:10.568
    STEP: Deleting the custom resource should be denied 09/08/23 22:49:10.59
    STEP: Remove the offending key and value from the custom resource data 09/08/23 22:49:10.608
    STEP: Deleting the updated custom resource should be successful 09/08/23 22:49:10.634
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:49:11.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9873" for this suite. 09/08/23 22:49:11.39
    STEP: Destroying namespace "webhook-9873-markers" for this suite. 09/08/23 22:49:11.422
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:49:11.486
Sep  8 22:49:11.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename downward-api 09/08/23 22:49:11.487
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:11.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:11.58
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 09/08/23 22:49:11.59
Sep  8 22:49:11.654: INFO: Waiting up to 5m0s for pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2" in namespace "downward-api-2873" to be "Succeeded or Failed"
Sep  8 22:49:11.667: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.115141ms
Sep  8 22:49:13.685: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030853121s
Sep  8 22:49:15.681: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026801458s
STEP: Saw pod success 09/08/23 22:49:15.681
Sep  8 22:49:15.681: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2" satisfied condition "Succeeded or Failed"
Sep  8 22:49:15.690: INFO: Trying to get logs from node node-3 pod downward-api-12ae05dc-8377-4825-a28b-e401942110a2 container dapi-container: <nil>
STEP: delete the pod 09/08/23 22:49:15.707
Sep  8 22:49:15.750: INFO: Waiting for pod downward-api-12ae05dc-8377-4825-a28b-e401942110a2 to disappear
Sep  8 22:49:15.760: INFO: Pod downward-api-12ae05dc-8377-4825-a28b-e401942110a2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Sep  8 22:49:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2873" for this suite. 09/08/23 22:49:15.771
------------------------------
â€¢ [4.306 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:49:11.486
    Sep  8 22:49:11.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename downward-api 09/08/23 22:49:11.487
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:11.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:11.58
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 09/08/23 22:49:11.59
    Sep  8 22:49:11.654: INFO: Waiting up to 5m0s for pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2" in namespace "downward-api-2873" to be "Succeeded or Failed"
    Sep  8 22:49:11.667: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.115141ms
    Sep  8 22:49:13.685: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030853121s
    Sep  8 22:49:15.681: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026801458s
    STEP: Saw pod success 09/08/23 22:49:15.681
    Sep  8 22:49:15.681: INFO: Pod "downward-api-12ae05dc-8377-4825-a28b-e401942110a2" satisfied condition "Succeeded or Failed"
    Sep  8 22:49:15.690: INFO: Trying to get logs from node node-3 pod downward-api-12ae05dc-8377-4825-a28b-e401942110a2 container dapi-container: <nil>
    STEP: delete the pod 09/08/23 22:49:15.707
    Sep  8 22:49:15.750: INFO: Waiting for pod downward-api-12ae05dc-8377-4825-a28b-e401942110a2 to disappear
    Sep  8 22:49:15.760: INFO: Pod downward-api-12ae05dc-8377-4825-a28b-e401942110a2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:49:15.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2873" for this suite. 09/08/23 22:49:15.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:49:15.793
Sep  8 22:49:15.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 22:49:15.794
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:15.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:15.848
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 09/08/23 22:49:15.856
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/08/23 22:49:15.859
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/08/23 22:49:15.859
STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/08/23 22:49:15.859
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/08/23 22:49:15.862
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/08/23 22:49:15.862
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/08/23 22:49:15.865
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:49:15.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1617" for this suite. 09/08/23 22:49:15.887
------------------------------
â€¢ [0.119 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:49:15.793
    Sep  8 22:49:15.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename custom-resource-definition 09/08/23 22:49:15.794
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:15.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:15.848
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 09/08/23 22:49:15.856
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 09/08/23 22:49:15.859
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 09/08/23 22:49:15.859
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 09/08/23 22:49:15.859
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 09/08/23 22:49:15.862
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 09/08/23 22:49:15.862
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 09/08/23 22:49:15.865
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:49:15.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1617" for this suite. 09/08/23 22:49:15.887
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:49:15.913
Sep  8 22:49:15.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename webhook 09/08/23 22:49:15.914
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:15.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:15.97
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 09/08/23 22:49:16.014
STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:49:16.639
STEP: Deploying the webhook pod 09/08/23 22:49:16.671
STEP: Wait for the deployment to be ready 09/08/23 22:49:16.707
Sep  8 22:49:16.722: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  8 22:49:18.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 09/08/23 22:49:20.76
STEP: Verifying the service has paired with the endpoint 09/08/23 22:49:20.802
Sep  8 22:49:21.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Sep  8 22:49:21.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1070-crds.webhook.example.com via the AdmissionRegistration API 09/08/23 22:49:27.336
STEP: Creating a custom resource while v1 is storage version 09/08/23 22:49:27.37
STEP: Patching Custom Resource Definition to set v2 as storage 09/08/23 22:49:29.466
STEP: Patching the custom resource while v2 is storage version 09/08/23 22:49:29.496
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Sep  8 22:49:30.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6904" for this suite. 09/08/23 22:49:30.292
STEP: Destroying namespace "webhook-6904-markers" for this suite. 09/08/23 22:49:30.316
------------------------------
â€¢ [SLOW TEST] [14.419 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:49:15.913
    Sep  8 22:49:15.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename webhook 09/08/23 22:49:15.914
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:15.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:15.97
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 09/08/23 22:49:16.014
    STEP: Create role binding to let webhook read extension-apiserver-authentication 09/08/23 22:49:16.639
    STEP: Deploying the webhook pod 09/08/23 22:49:16.671
    STEP: Wait for the deployment to be ready 09/08/23 22:49:16.707
    Sep  8 22:49:16.722: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Sep  8 22:49:18.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.September, 8, 22, 49, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 09/08/23 22:49:20.76
    STEP: Verifying the service has paired with the endpoint 09/08/23 22:49:20.802
    Sep  8 22:49:21.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Sep  8 22:49:21.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1070-crds.webhook.example.com via the AdmissionRegistration API 09/08/23 22:49:27.336
    STEP: Creating a custom resource while v1 is storage version 09/08/23 22:49:27.37
    STEP: Patching Custom Resource Definition to set v2 as storage 09/08/23 22:49:29.466
    STEP: Patching the custom resource while v2 is storage version 09/08/23 22:49:29.496
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:49:30.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6904" for this suite. 09/08/23 22:49:30.292
    STEP: Destroying namespace "webhook-6904-markers" for this suite. 09/08/23 22:49:30.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:49:30.333
Sep  8 22:49:30.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename subpath 09/08/23 22:49:30.335
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:30.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:30.386
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 09/08/23 22:49:30.403
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-sm72 09/08/23 22:49:30.428
STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:49:30.428
Sep  8 22:49:30.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-sm72" in namespace "subpath-316" to be "Succeeded or Failed"
Sep  8 22:49:30.460: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Pending", Reason="", readiness=false. Elapsed: 11.476553ms
Sep  8 22:49:32.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 2.022333338s
Sep  8 22:49:34.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 4.021956615s
Sep  8 22:49:36.476: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 6.027793808s
Sep  8 22:49:38.472: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 8.023929006s
Sep  8 22:49:40.469: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 10.020747737s
Sep  8 22:49:42.469: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 12.020744738s
Sep  8 22:49:44.470: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 14.021161466s
Sep  8 22:49:46.473: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 16.024669314s
Sep  8 22:49:48.470: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 18.02146969s
Sep  8 22:49:50.473: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 20.024020561s
Sep  8 22:49:52.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 22.022353897s
Sep  8 22:49:54.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=false. Elapsed: 24.022013291s
Sep  8 22:49:56.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.022212643s
STEP: Saw pod success 09/08/23 22:49:56.471
Sep  8 22:49:56.471: INFO: Pod "pod-subpath-test-downwardapi-sm72" satisfied condition "Succeeded or Failed"
Sep  8 22:49:56.479: INFO: Trying to get logs from node node-3 pod pod-subpath-test-downwardapi-sm72 container test-container-subpath-downwardapi-sm72: <nil>
STEP: delete the pod 09/08/23 22:49:56.495
Sep  8 22:49:56.532: INFO: Waiting for pod pod-subpath-test-downwardapi-sm72 to disappear
Sep  8 22:49:56.539: INFO: Pod pod-subpath-test-downwardapi-sm72 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-sm72 09/08/23 22:49:56.539
Sep  8 22:49:56.539: INFO: Deleting pod "pod-subpath-test-downwardapi-sm72" in namespace "subpath-316"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Sep  8 22:49:56.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-316" for this suite. 09/08/23 22:49:56.557
------------------------------
â€¢ [SLOW TEST] [26.239 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:49:30.333
    Sep  8 22:49:30.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename subpath 09/08/23 22:49:30.335
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:30.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:30.386
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 09/08/23 22:49:30.403
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-sm72 09/08/23 22:49:30.428
    STEP: Creating a pod to test atomic-volume-subpath 09/08/23 22:49:30.428
    Sep  8 22:49:30.449: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-sm72" in namespace "subpath-316" to be "Succeeded or Failed"
    Sep  8 22:49:30.460: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Pending", Reason="", readiness=false. Elapsed: 11.476553ms
    Sep  8 22:49:32.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 2.022333338s
    Sep  8 22:49:34.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 4.021956615s
    Sep  8 22:49:36.476: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 6.027793808s
    Sep  8 22:49:38.472: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 8.023929006s
    Sep  8 22:49:40.469: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 10.020747737s
    Sep  8 22:49:42.469: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 12.020744738s
    Sep  8 22:49:44.470: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 14.021161466s
    Sep  8 22:49:46.473: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 16.024669314s
    Sep  8 22:49:48.470: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 18.02146969s
    Sep  8 22:49:50.473: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 20.024020561s
    Sep  8 22:49:52.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=true. Elapsed: 22.022353897s
    Sep  8 22:49:54.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Running", Reason="", readiness=false. Elapsed: 24.022013291s
    Sep  8 22:49:56.471: INFO: Pod "pod-subpath-test-downwardapi-sm72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.022212643s
    STEP: Saw pod success 09/08/23 22:49:56.471
    Sep  8 22:49:56.471: INFO: Pod "pod-subpath-test-downwardapi-sm72" satisfied condition "Succeeded or Failed"
    Sep  8 22:49:56.479: INFO: Trying to get logs from node node-3 pod pod-subpath-test-downwardapi-sm72 container test-container-subpath-downwardapi-sm72: <nil>
    STEP: delete the pod 09/08/23 22:49:56.495
    Sep  8 22:49:56.532: INFO: Waiting for pod pod-subpath-test-downwardapi-sm72 to disappear
    Sep  8 22:49:56.539: INFO: Pod pod-subpath-test-downwardapi-sm72 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-sm72 09/08/23 22:49:56.539
    Sep  8 22:49:56.539: INFO: Deleting pod "pod-subpath-test-downwardapi-sm72" in namespace "subpath-316"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:49:56.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-316" for this suite. 09/08/23 22:49:56.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:49:56.574
Sep  8 22:49:56.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename statefulset 09/08/23 22:49:56.575
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:56.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:56.621
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3193 09/08/23 22:49:56.628
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-3193 09/08/23 22:49:56.65
Sep  8 22:49:56.686: INFO: Found 0 stateful pods, waiting for 1
Sep  8 22:50:06.699: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 09/08/23 22:50:06.734
STEP: updating a scale subresource 09/08/23 22:50:06.741
STEP: verifying the statefulset Spec.Replicas was modified 09/08/23 22:50:06.761
STEP: Patch a scale subresource 09/08/23 22:50:06.775
STEP: verifying the statefulset Spec.Replicas was modified 09/08/23 22:50:06.81
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Sep  8 22:50:06.820: INFO: Deleting all statefulset in ns statefulset-3193
Sep  8 22:50:06.824: INFO: Scaling statefulset ss to 0
Sep  8 22:50:16.870: INFO: Waiting for statefulset status.replicas updated to 0
Sep  8 22:50:16.892: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Sep  8 22:50:16.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3193" for this suite. 09/08/23 22:50:16.96
------------------------------
â€¢ [SLOW TEST] [20.401 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:49:56.574
    Sep  8 22:49:56.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename statefulset 09/08/23 22:49:56.575
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:49:56.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:49:56.621
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3193 09/08/23 22:49:56.628
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-3193 09/08/23 22:49:56.65
    Sep  8 22:49:56.686: INFO: Found 0 stateful pods, waiting for 1
    Sep  8 22:50:06.699: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 09/08/23 22:50:06.734
    STEP: updating a scale subresource 09/08/23 22:50:06.741
    STEP: verifying the statefulset Spec.Replicas was modified 09/08/23 22:50:06.761
    STEP: Patch a scale subresource 09/08/23 22:50:06.775
    STEP: verifying the statefulset Spec.Replicas was modified 09/08/23 22:50:06.81
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Sep  8 22:50:06.820: INFO: Deleting all statefulset in ns statefulset-3193
    Sep  8 22:50:06.824: INFO: Scaling statefulset ss to 0
    Sep  8 22:50:16.870: INFO: Waiting for statefulset status.replicas updated to 0
    Sep  8 22:50:16.892: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:50:16.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3193" for this suite. 09/08/23 22:50:16.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:50:16.976
Sep  8 22:50:16.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename cronjob 09/08/23 22:50:16.977
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:50:17.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:50:17.019
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 09/08/23 22:50:17.033
STEP: Ensuring no jobs are scheduled 09/08/23 22:50:17.051
STEP: Ensuring no job exists by listing jobs explicitly 09/08/23 22:55:17.071
STEP: Removing cronjob 09/08/23 22:55:17.078
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Sep  8 22:55:17.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5999" for this suite. 09/08/23 22:55:17.113
------------------------------
â€¢ [SLOW TEST] [300.155 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:50:16.976
    Sep  8 22:50:16.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename cronjob 09/08/23 22:50:16.977
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:50:17.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:50:17.019
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 09/08/23 22:50:17.033
    STEP: Ensuring no jobs are scheduled 09/08/23 22:50:17.051
    STEP: Ensuring no job exists by listing jobs explicitly 09/08/23 22:55:17.071
    STEP: Removing cronjob 09/08/23 22:55:17.078
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:55:17.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5999" for this suite. 09/08/23 22:55:17.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:55:17.139
Sep  8 22:55:17.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename runtimeclass 09/08/23 22:55:17.14
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:17.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:17.189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 09/08/23 22:55:17.197
STEP: getting /apis/node.k8s.io 09/08/23 22:55:17.204
STEP: getting /apis/node.k8s.io/v1 09/08/23 22:55:17.206
STEP: creating 09/08/23 22:55:17.207
STEP: watching 09/08/23 22:55:17.252
Sep  8 22:55:17.252: INFO: starting watch
STEP: getting 09/08/23 22:55:17.267
STEP: listing 09/08/23 22:55:17.277
STEP: patching 09/08/23 22:55:17.284
STEP: updating 09/08/23 22:55:17.297
Sep  8 22:55:17.307: INFO: waiting for watch events with expected annotations
STEP: deleting 09/08/23 22:55:17.307
STEP: deleting a collection 09/08/23 22:55:17.337
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Sep  8 22:55:17.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5211" for this suite. 09/08/23 22:55:17.4
------------------------------
â€¢ [0.278 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:55:17.139
    Sep  8 22:55:17.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename runtimeclass 09/08/23 22:55:17.14
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:17.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:17.189
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 09/08/23 22:55:17.197
    STEP: getting /apis/node.k8s.io 09/08/23 22:55:17.204
    STEP: getting /apis/node.k8s.io/v1 09/08/23 22:55:17.206
    STEP: creating 09/08/23 22:55:17.207
    STEP: watching 09/08/23 22:55:17.252
    Sep  8 22:55:17.252: INFO: starting watch
    STEP: getting 09/08/23 22:55:17.267
    STEP: listing 09/08/23 22:55:17.277
    STEP: patching 09/08/23 22:55:17.284
    STEP: updating 09/08/23 22:55:17.297
    Sep  8 22:55:17.307: INFO: waiting for watch events with expected annotations
    STEP: deleting 09/08/23 22:55:17.307
    STEP: deleting a collection 09/08/23 22:55:17.337
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:55:17.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5211" for this suite. 09/08/23 22:55:17.4
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:55:17.417
Sep  8 22:55:17.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename disruption 09/08/23 22:55:17.419
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:17.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:17.479
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 09/08/23 22:55:17.485
STEP: Waiting for the pdb to be processed 09/08/23 22:55:17.498
STEP: updating the pdb 09/08/23 22:55:19.519
STEP: Waiting for the pdb to be processed 09/08/23 22:55:19.546
STEP: patching the pdb 09/08/23 22:55:19.556
STEP: Waiting for the pdb to be processed 09/08/23 22:55:19.589
STEP: Waiting for the pdb to be deleted 09/08/23 22:55:19.625
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Sep  8 22:55:19.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1321" for this suite. 09/08/23 22:55:19.648
------------------------------
â€¢ [2.255 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:55:17.417
    Sep  8 22:55:17.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename disruption 09/08/23 22:55:17.419
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:17.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:17.479
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 09/08/23 22:55:17.485
    STEP: Waiting for the pdb to be processed 09/08/23 22:55:17.498
    STEP: updating the pdb 09/08/23 22:55:19.519
    STEP: Waiting for the pdb to be processed 09/08/23 22:55:19.546
    STEP: patching the pdb 09/08/23 22:55:19.556
    STEP: Waiting for the pdb to be processed 09/08/23 22:55:19.589
    STEP: Waiting for the pdb to be deleted 09/08/23 22:55:19.625
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:55:19.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1321" for this suite. 09/08/23 22:55:19.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:55:19.678
Sep  8 22:55:19.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename secrets 09/08/23 22:55:19.68
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:19.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:19.751
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-585b4eb8-16eb-4796-903e-699ca3668360 09/08/23 22:55:19.761
STEP: Creating a pod to test consume secrets 09/08/23 22:55:19.773
Sep  8 22:55:19.795: INFO: Waiting up to 5m0s for pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd" in namespace "secrets-4005" to be "Succeeded or Failed"
Sep  8 22:55:19.813: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.062515ms
Sep  8 22:55:21.824: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0288894s
Sep  8 22:55:23.827: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032260438s
Sep  8 22:55:25.824: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028878059s
STEP: Saw pod success 09/08/23 22:55:25.824
Sep  8 22:55:25.824: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd" satisfied condition "Succeeded or Failed"
Sep  8 22:55:25.836: INFO: Trying to get logs from node node-3 pod pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd container secret-env-test: <nil>
STEP: delete the pod 09/08/23 22:55:25.877
Sep  8 22:55:25.908: INFO: Waiting for pod pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd to disappear
Sep  8 22:55:25.920: INFO: Pod pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Sep  8 22:55:25.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4005" for this suite. 09/08/23 22:55:25.929
------------------------------
â€¢ [SLOW TEST] [6.266 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:55:19.678
    Sep  8 22:55:19.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename secrets 09/08/23 22:55:19.68
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:19.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:19.751
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-585b4eb8-16eb-4796-903e-699ca3668360 09/08/23 22:55:19.761
    STEP: Creating a pod to test consume secrets 09/08/23 22:55:19.773
    Sep  8 22:55:19.795: INFO: Waiting up to 5m0s for pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd" in namespace "secrets-4005" to be "Succeeded or Failed"
    Sep  8 22:55:19.813: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.062515ms
    Sep  8 22:55:21.824: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0288894s
    Sep  8 22:55:23.827: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032260438s
    Sep  8 22:55:25.824: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028878059s
    STEP: Saw pod success 09/08/23 22:55:25.824
    Sep  8 22:55:25.824: INFO: Pod "pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd" satisfied condition "Succeeded or Failed"
    Sep  8 22:55:25.836: INFO: Trying to get logs from node node-3 pod pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd container secret-env-test: <nil>
    STEP: delete the pod 09/08/23 22:55:25.877
    Sep  8 22:55:25.908: INFO: Waiting for pod pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd to disappear
    Sep  8 22:55:25.920: INFO: Pod pod-secrets-02afd436-2177-432f-bcea-c0b4a2f6b8fd no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:55:25.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4005" for this suite. 09/08/23 22:55:25.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 09/08/23 22:55:25.946
Sep  8 22:55:25.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
STEP: Building a namespace api object, basename container-probe 09/08/23 22:55:25.947
STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:25.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:25.993
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-b190af23-a17f-4551-a3ee-c59178340cc6 in namespace container-probe-2224 09/08/23 22:55:26
Sep  8 22:55:26.027: INFO: Waiting up to 5m0s for pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6" in namespace "container-probe-2224" to be "not pending"
Sep  8 22:55:26.045: INFO: Pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.076041ms
Sep  8 22:55:28.059: INFO: Pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.032099923s
Sep  8 22:55:28.059: INFO: Pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6" satisfied condition "not pending"
Sep  8 22:55:28.059: INFO: Started pod liveness-b190af23-a17f-4551-a3ee-c59178340cc6 in namespace container-probe-2224
STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:55:28.059
Sep  8 22:55:28.070: INFO: Initial restart count of pod liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is 0
Sep  8 22:55:48.202: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 1 (20.132175595s elapsed)
Sep  8 22:56:08.339: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 2 (40.269291642s elapsed)
Sep  8 22:56:28.464: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 3 (1m0.393763864s elapsed)
Sep  8 22:56:48.582: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 4 (1m20.511957778s elapsed)
Sep  8 22:58:01.071: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 5 (2m33.001271345s elapsed)
STEP: deleting the pod 09/08/23 22:58:01.071
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Sep  8 22:58:01.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2224" for this suite. 09/08/23 22:58:01.133
------------------------------
â€¢ [SLOW TEST] [155.203 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 09/08/23 22:55:25.946
    Sep  8 22:55:25.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2096145757
    STEP: Building a namespace api object, basename container-probe 09/08/23 22:55:25.947
    STEP: Waiting for a default service account to be provisioned in namespace 09/08/23 22:55:25.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 09/08/23 22:55:25.993
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-b190af23-a17f-4551-a3ee-c59178340cc6 in namespace container-probe-2224 09/08/23 22:55:26
    Sep  8 22:55:26.027: INFO: Waiting up to 5m0s for pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6" in namespace "container-probe-2224" to be "not pending"
    Sep  8 22:55:26.045: INFO: Pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.076041ms
    Sep  8 22:55:28.059: INFO: Pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.032099923s
    Sep  8 22:55:28.059: INFO: Pod "liveness-b190af23-a17f-4551-a3ee-c59178340cc6" satisfied condition "not pending"
    Sep  8 22:55:28.059: INFO: Started pod liveness-b190af23-a17f-4551-a3ee-c59178340cc6 in namespace container-probe-2224
    STEP: checking the pod's current state and verifying that restartCount is present 09/08/23 22:55:28.059
    Sep  8 22:55:28.070: INFO: Initial restart count of pod liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is 0
    Sep  8 22:55:48.202: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 1 (20.132175595s elapsed)
    Sep  8 22:56:08.339: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 2 (40.269291642s elapsed)
    Sep  8 22:56:28.464: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 3 (1m0.393763864s elapsed)
    Sep  8 22:56:48.582: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 4 (1m20.511957778s elapsed)
    Sep  8 22:58:01.071: INFO: Restart count of pod container-probe-2224/liveness-b190af23-a17f-4551-a3ee-c59178340cc6 is now 5 (2m33.001271345s elapsed)
    STEP: deleting the pod 09/08/23 22:58:01.071
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Sep  8 22:58:01.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2224" for this suite. 09/08/23 22:58:01.133
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Sep  8 22:58:01.151: INFO: Running AfterSuite actions on node 1
Sep  8 22:58:01.151: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Sep  8 22:58:01.151: INFO: Running AfterSuite actions on node 1
    Sep  8 22:58:01.151: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.107 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6442.305 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h47m22.723565065s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m


I0824 10:40:42.261220      22 e2e.go:126] Starting e2e run "b80dffc4-45ea-4898-a190-ca68b5b82212" on Ginkgo node 1
Aug 24 10:40:42.288: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1692873642 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 24 10:40:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:40:42.470: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0824 10:40:42.471686      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Aug 24 10:40:42.510: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 24 10:40:42.589: INFO: 29 / 29 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 24 10:40:42.589: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Aug 24 10:40:42.589: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 24 10:40:42.602: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 24 10:40:42.602: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Aug 24 10:40:42.602: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Aug 24 10:40:42.602: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'magnum-prometheus-node-exporter' (0 seconds elapsed)
Aug 24 10:40:42.602: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Aug 24 10:40:42.602: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Aug 24 10:40:42.602: INFO: e2e test version: v1.26.7
Aug 24 10:40:42.607: INFO: kube-apiserver version: v1.26.7
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 24 10:40:42.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:40:42.613: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.145 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 24 10:40:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:40:42.470: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0824 10:40:42.471686      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Aug 24 10:40:42.510: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 24 10:40:42.589: INFO: 29 / 29 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 24 10:40:42.589: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
    Aug 24 10:40:42.589: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 24 10:40:42.602: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Aug 24 10:40:42.602: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
    Aug 24 10:40:42.602: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
    Aug 24 10:40:42.602: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'magnum-prometheus-node-exporter' (0 seconds elapsed)
    Aug 24 10:40:42.602: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
    Aug 24 10:40:42.602: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
    Aug 24 10:40:42.602: INFO: e2e test version: v1.26.7
    Aug 24 10:40:42.607: INFO: kube-apiserver version: v1.26.7
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 24 10:40:42.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:40:42.613: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:40:42.651
Aug 24 10:40:42.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 10:40:42.652
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:40:42.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:40:42.719
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/24/23 10:40:42.729
STEP: Getting a ResourceQuota 08/24/23 10:40:42.746
STEP: Updating a ResourceQuota 08/24/23 10:40:42.783
STEP: Verifying a ResourceQuota was modified 08/24/23 10:40:42.807
STEP: Deleting a ResourceQuota 08/24/23 10:40:42.813
STEP: Verifying the deleted ResourceQuota 08/24/23 10:40:42.833
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 10:40:42.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-696" for this suite. 08/24/23 10:40:42.842
------------------------------
â€¢ [0.202 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:40:42.651
    Aug 24 10:40:42.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 10:40:42.652
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:40:42.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:40:42.719
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/24/23 10:40:42.729
    STEP: Getting a ResourceQuota 08/24/23 10:40:42.746
    STEP: Updating a ResourceQuota 08/24/23 10:40:42.783
    STEP: Verifying a ResourceQuota was modified 08/24/23 10:40:42.807
    STEP: Deleting a ResourceQuota 08/24/23 10:40:42.813
    STEP: Verifying the deleted ResourceQuota 08/24/23 10:40:42.833
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:40:42.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-696" for this suite. 08/24/23 10:40:42.842
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:40:42.853
Aug 24 10:40:42.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
E0824 10:40:42.854771      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename job 08/24/23 10:40:42.855
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:40:42.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:40:42.89
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/24/23 10:40:42.896
STEP: Ensuring active pods == parallelism 08/24/23 10:40:42.917
STEP: delete a job 08/24/23 10:40:56.923
STEP: deleting Job.batch foo in namespace job-9159, will wait for the garbage collector to delete the pods 08/24/23 10:40:56.923
Aug 24 10:40:56.988: INFO: Deleting Job.batch foo took: 9.271292ms
Aug 24 10:40:57.189: INFO: Terminating Job.batch foo pods took: 200.941834ms
STEP: Ensuring job was deleted 08/24/23 10:41:29.389
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 10:41:29.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9159" for this suite. 08/24/23 10:41:29.418
------------------------------
â€¢ [SLOW TEST] [46.583 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:40:42.853
    Aug 24 10:40:42.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    E0824 10:40:42.854771      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Building a namespace api object, basename job 08/24/23 10:40:42.855
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:40:42.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:40:42.89
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/24/23 10:40:42.896
    STEP: Ensuring active pods == parallelism 08/24/23 10:40:42.917
    STEP: delete a job 08/24/23 10:40:56.923
    STEP: deleting Job.batch foo in namespace job-9159, will wait for the garbage collector to delete the pods 08/24/23 10:40:56.923
    Aug 24 10:40:56.988: INFO: Deleting Job.batch foo took: 9.271292ms
    Aug 24 10:40:57.189: INFO: Terminating Job.batch foo pods took: 200.941834ms
    STEP: Ensuring job was deleted 08/24/23 10:41:29.389
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:41:29.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9159" for this suite. 08/24/23 10:41:29.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:41:29.441
Aug 24 10:41:29.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 10:41:29.443
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:41:29.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:41:29.48
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 in namespace container-probe-2394 08/24/23 10:41:29.486
Aug 24 10:41:29.498: INFO: Waiting up to 5m0s for pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70" in namespace "container-probe-2394" to be "not pending"
Aug 24 10:41:29.514: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 15.528398ms
Aug 24 10:41:31.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020788092s
Aug 24 10:41:33.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019387325s
Aug 24 10:41:35.520: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021099426s
Aug 24 10:41:37.517: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018809993s
Aug 24 10:41:39.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020295607s
Aug 24 10:41:41.517: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018990087s
Aug 24 10:41:43.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019620471s
Aug 24 10:41:45.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020153289s
Aug 24 10:41:47.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 18.019551637s
Aug 24 10:41:49.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020757402s
Aug 24 10:41:51.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020571183s
Aug 24 10:41:53.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019228825s
Aug 24 10:41:55.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 26.02086187s
Aug 24 10:41:57.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 28.019390337s
Aug 24 10:41:59.523: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 30.024910968s
Aug 24 10:42:01.517: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018967967s
Aug 24 10:42:03.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 34.019170817s
Aug 24 10:42:05.525: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026839335s
Aug 24 10:42:07.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Running", Reason="", readiness=true. Elapsed: 38.019260353s
Aug 24 10:42:07.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70" satisfied condition "not pending"
Aug 24 10:42:07.518: INFO: Started pod liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 in namespace container-probe-2394
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 10:42:07.518
Aug 24 10:42:07.522: INFO: Initial restart count of pod liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 is 0
Aug 24 10:42:31.633: INFO: Restart count of pod container-probe-2394/liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 is now 1 (24.111670005s elapsed)
STEP: deleting the pod 08/24/23 10:42:31.633
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 10:42:31.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2394" for this suite. 08/24/23 10:42:31.668
------------------------------
â€¢ [SLOW TEST] [62.238 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:41:29.441
    Aug 24 10:41:29.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 10:41:29.443
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:41:29.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:41:29.48
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 in namespace container-probe-2394 08/24/23 10:41:29.486
    Aug 24 10:41:29.498: INFO: Waiting up to 5m0s for pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70" in namespace "container-probe-2394" to be "not pending"
    Aug 24 10:41:29.514: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 15.528398ms
    Aug 24 10:41:31.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020788092s
    Aug 24 10:41:33.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019387325s
    Aug 24 10:41:35.520: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021099426s
    Aug 24 10:41:37.517: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018809993s
    Aug 24 10:41:39.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 10.020295607s
    Aug 24 10:41:41.517: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 12.018990087s
    Aug 24 10:41:43.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019620471s
    Aug 24 10:41:45.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 16.020153289s
    Aug 24 10:41:47.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 18.019551637s
    Aug 24 10:41:49.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020757402s
    Aug 24 10:41:51.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 22.020571183s
    Aug 24 10:41:53.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019228825s
    Aug 24 10:41:55.519: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 26.02086187s
    Aug 24 10:41:57.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 28.019390337s
    Aug 24 10:41:59.523: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 30.024910968s
    Aug 24 10:42:01.517: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018967967s
    Aug 24 10:42:03.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 34.019170817s
    Aug 24 10:42:05.525: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026839335s
    Aug 24 10:42:07.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70": Phase="Running", Reason="", readiness=true. Elapsed: 38.019260353s
    Aug 24 10:42:07.518: INFO: Pod "liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70" satisfied condition "not pending"
    Aug 24 10:42:07.518: INFO: Started pod liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 in namespace container-probe-2394
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 10:42:07.518
    Aug 24 10:42:07.522: INFO: Initial restart count of pod liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 is 0
    Aug 24 10:42:31.633: INFO: Restart count of pod container-probe-2394/liveness-98c7adb9-5fb1-43c7-8e01-110bccee4c70 is now 1 (24.111670005s elapsed)
    STEP: deleting the pod 08/24/23 10:42:31.633
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:42:31.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2394" for this suite. 08/24/23 10:42:31.668
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:42:31.684
Aug 24 10:42:31.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 10:42:31.686
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:31.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:31.714
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 10:42:31.757
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:42:32.929
STEP: Deploying the webhook pod 08/24/23 10:42:32.947
STEP: Wait for the deployment to be ready 08/24/23 10:42:32.986
Aug 24 10:42:32.997: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 10:42:35.009
STEP: Verifying the service has paired with the endpoint 08/24/23 10:42:35.036
Aug 24 10:42:36.036: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/24/23 10:42:36.16
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 10:42:36.233
STEP: Deleting the collection of validation webhooks 08/24/23 10:42:36.386
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 10:42:36.472
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:42:36.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9601" for this suite. 08/24/23 10:42:36.648
STEP: Destroying namespace "webhook-9601-markers" for this suite. 08/24/23 10:42:36.671
------------------------------
â€¢ [SLOW TEST] [5.014 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:42:31.684
    Aug 24 10:42:31.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 10:42:31.686
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:31.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:31.714
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 10:42:31.757
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:42:32.929
    STEP: Deploying the webhook pod 08/24/23 10:42:32.947
    STEP: Wait for the deployment to be ready 08/24/23 10:42:32.986
    Aug 24 10:42:32.997: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 10:42:35.009
    STEP: Verifying the service has paired with the endpoint 08/24/23 10:42:35.036
    Aug 24 10:42:36.036: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/24/23 10:42:36.16
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 10:42:36.233
    STEP: Deleting the collection of validation webhooks 08/24/23 10:42:36.386
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 10:42:36.472
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:42:36.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9601" for this suite. 08/24/23 10:42:36.648
    STEP: Destroying namespace "webhook-9601-markers" for this suite. 08/24/23 10:42:36.671
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:42:36.698
Aug 24 10:42:36.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 10:42:36.699
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:36.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:36.81
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 24 10:42:36.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 create -f -'
Aug 24 10:42:37.456: INFO: stderr: ""
Aug 24 10:42:37.456: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 24 10:42:37.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 create -f -'
Aug 24 10:42:37.922: INFO: stderr: ""
Aug 24 10:42:37.922: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/24/23 10:42:37.922
Aug 24 10:42:38.928: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 10:42:38.928: INFO: Found 1 / 1
Aug 24 10:42:38.928: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 24 10:42:38.931: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 10:42:38.931: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 24 10:42:38.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe pod agnhost-primary-k957j'
Aug 24 10:42:39.064: INFO: stderr: ""
Aug 24 10:42:39.064: INFO: stdout: "Name:             agnhost-primary-k957j\nNamespace:        kubectl-7873\nPriority:         0\nService Account:  default\nNode:             gitlab-1-26-36460-guscsyka22xa-node-2/10.0.0.17\nStart Time:       Thu, 24 Aug 2023 10:42:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 263b3526d6b8cf8e402b9ad882fc5e4342c8b357337495d4b5927599af85288d\n                  cni.projectcalico.org/podIP: 10.100.45.133/32\n                  cni.projectcalico.org/podIPs: 10.100.45.133/32\nStatus:           Running\nIP:               10.100.45.133\nIPs:\n  IP:           10.100.45.133\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://87bb82013ec3236d7320a11c015eb13047562eefa62f759b2778c2c017485d64\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Aug 2023 10:42:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c2gxr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-c2gxr:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7873/agnhost-primary-k957j to gitlab-1-26-36460-guscsyka22xa-node-2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Aug 24 10:42:39.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe rc agnhost-primary'
Aug 24 10:42:39.226: INFO: stderr: ""
Aug 24 10:42:39.226: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7873\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-k957j\n"
Aug 24 10:42:39.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe service agnhost-primary'
Aug 24 10:42:39.377: INFO: stderr: ""
Aug 24 10:42:39.377: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7873\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.51.251\nIPs:               10.254.51.251\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.45.133:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 24 10:42:39.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe node gitlab-1-26-36460-guscsyka22xa-master-0'
Aug 24 10:42:39.737: INFO: stderr: ""
Aug 24 10:42:39.737: INFO: stdout: "Name:               gitlab-1-26-36460-guscsyka22xa-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.c2r4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz-ppd-1\n                    failure-domain.beta.kubernetes.io/zone=nz-ppd-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=gitlab-1-26-36460-guscsyka22xa-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=c1.c2r4\n                    topology.kubernetes.io/region=nz-ppd-1\n                    topology.kubernetes.io/zone=nz-ppd-1a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.15/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Aug 2023 10:08:58 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  gitlab-1-26-36460-guscsyka22xa-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 24 Aug 2023 10:42:32 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 24 Aug 2023 10:09:53 +0000   Thu, 24 Aug 2023 10:09:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:08:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:08:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:08:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:09:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.15\n  ExternalIP:  10.10.8.50\n  Hostname:    gitlab-1-26-36460-guscsyka22xa-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20380652Ki\n  hugepages-2Mi:      0\n  memory:             3995524Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  20380652Ki\n  hugepages-2Mi:      0\n  memory:             3995524Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 743cbefffd354a3899bd489a4b386db8\n  System UUID:                743cbeff-fd35-4a38-99bd-489a4b386db8\n  Boot ID:                    ecc6752b-e5d5-4739-a7ff-90547f63fa7b\n  Kernel Version:             6.3.8-200.fc38.x86_64\n  OS Image:                   Fedora CoreOS 38.20230625.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.13\n  Kubelet Version:            v1.26.7\n  Kube-Proxy Version:         v1.26.7\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nProviderID:                   openstack:///743cbeff-fd35-4a38-99bd-489a4b386db8\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-cf46849c5-fsknr                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 calico-node-6wj7w                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 coredns-75cbffd999-l6zrt                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     33m\n  kube-system                 coredns-75cbffd999-n9rfn                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     33m\n  kube-system                 csi-cinder-controllerplugin-0                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 dashboard-metrics-scraper-65d58d557c-4jrzv                 50m (2%)      0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 k8s-keystone-auth-fjn7c                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 kubernetes-dashboard-968d4b48b-r8q5p                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         33m\n  kube-system                 magnum-prometheus-node-exporter-h8zjh                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 octavia-ingress-controller-0                               50m (2%)      0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 openstack-cloud-controller-manager-75qb7                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         33m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-mgf6k    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m25s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1150m (57%)  0 (0%)\n  memory             240Mi (6%)   340Mi (8%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 33m                kube-proxy             \n  Normal   Starting                 33m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      33m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  33m                kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  33m (x3 over 33m)  kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    33m (x3 over 33m)  kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     33m (x3 over 33m)  kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           33m                node-controller        Node gitlab-1-26-36460-guscsyka22xa-master-0 event: Registered Node gitlab-1-26-36460-guscsyka22xa-master-0 in Controller\n  Normal   NodeReady                32m                kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeReady\n  Normal   Synced                   32m                cloud-node-controller  Node synced successfully\n"
Aug 24 10:42:39.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe namespace kubectl-7873'
Aug 24 10:42:39.865: INFO: stderr: ""
Aug 24 10:42:39.865: INFO: stdout: "Name:         kubectl-7873\nLabels:       e2e-framework=kubectl\n              e2e-run=b80dffc4-45ea-4898-a190-ca68b5b82212\n              kubernetes.io/metadata.name=kubectl-7873\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 10:42:39.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7873" for this suite. 08/24/23 10:42:39.871
------------------------------
â€¢ [3.185 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:42:36.698
    Aug 24 10:42:36.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 10:42:36.699
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:36.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:36.81
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 24 10:42:36.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 create -f -'
    Aug 24 10:42:37.456: INFO: stderr: ""
    Aug 24 10:42:37.456: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 24 10:42:37.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 create -f -'
    Aug 24 10:42:37.922: INFO: stderr: ""
    Aug 24 10:42:37.922: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/24/23 10:42:37.922
    Aug 24 10:42:38.928: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 10:42:38.928: INFO: Found 1 / 1
    Aug 24 10:42:38.928: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 24 10:42:38.931: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 10:42:38.931: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 24 10:42:38.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe pod agnhost-primary-k957j'
    Aug 24 10:42:39.064: INFO: stderr: ""
    Aug 24 10:42:39.064: INFO: stdout: "Name:             agnhost-primary-k957j\nNamespace:        kubectl-7873\nPriority:         0\nService Account:  default\nNode:             gitlab-1-26-36460-guscsyka22xa-node-2/10.0.0.17\nStart Time:       Thu, 24 Aug 2023 10:42:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 263b3526d6b8cf8e402b9ad882fc5e4342c8b357337495d4b5927599af85288d\n                  cni.projectcalico.org/podIP: 10.100.45.133/32\n                  cni.projectcalico.org/podIPs: 10.100.45.133/32\nStatus:           Running\nIP:               10.100.45.133\nIPs:\n  IP:           10.100.45.133\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://87bb82013ec3236d7320a11c015eb13047562eefa62f759b2778c2c017485d64\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Aug 2023 10:42:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c2gxr (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-c2gxr:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7873/agnhost-primary-k957j to gitlab-1-26-36460-guscsyka22xa-node-2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Aug 24 10:42:39.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe rc agnhost-primary'
    Aug 24 10:42:39.226: INFO: stderr: ""
    Aug 24 10:42:39.226: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7873\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-k957j\n"
    Aug 24 10:42:39.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe service agnhost-primary'
    Aug 24 10:42:39.377: INFO: stderr: ""
    Aug 24 10:42:39.377: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7873\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.51.251\nIPs:               10.254.51.251\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.45.133:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 24 10:42:39.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe node gitlab-1-26-36460-guscsyka22xa-master-0'
    Aug 24 10:42:39.737: INFO: stderr: ""
    Aug 24 10:42:39.737: INFO: stdout: "Name:               gitlab-1-26-36460-guscsyka22xa-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.c2r4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz-ppd-1\n                    failure-domain.beta.kubernetes.io/zone=nz-ppd-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=gitlab-1-26-36460-guscsyka22xa-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=c1.c2r4\n                    topology.kubernetes.io/region=nz-ppd-1\n                    topology.kubernetes.io/zone=nz-ppd-1a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.15/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Aug 2023 10:08:58 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  gitlab-1-26-36460-guscsyka22xa-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 24 Aug 2023 10:42:32 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 24 Aug 2023 10:09:53 +0000   Thu, 24 Aug 2023 10:09:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:08:57 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:08:57 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:08:57 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 24 Aug 2023 10:39:56 +0000   Thu, 24 Aug 2023 10:09:40 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.15\n  ExternalIP:  10.10.8.50\n  Hostname:    gitlab-1-26-36460-guscsyka22xa-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20380652Ki\n  hugepages-2Mi:      0\n  memory:             3995524Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  20380652Ki\n  hugepages-2Mi:      0\n  memory:             3995524Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 743cbefffd354a3899bd489a4b386db8\n  System UUID:                743cbeff-fd35-4a38-99bd-489a4b386db8\n  Boot ID:                    ecc6752b-e5d5-4739-a7ff-90547f63fa7b\n  Kernel Version:             6.3.8-200.fc38.x86_64\n  OS Image:                   Fedora CoreOS 38.20230625.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.13\n  Kubelet Version:            v1.26.7\n  Kube-Proxy Version:         v1.26.7\nPodCIDR:                      10.100.0.0/24\nPodCIDRs:                     10.100.0.0/24\nProviderID:                   openstack:///743cbeff-fd35-4a38-99bd-489a4b386db8\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-cf46849c5-fsknr                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 calico-node-6wj7w                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 coredns-75cbffd999-l6zrt                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     33m\n  kube-system                 coredns-75cbffd999-n9rfn                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     33m\n  kube-system                 csi-cinder-controllerplugin-0                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 dashboard-metrics-scraper-65d58d557c-4jrzv                 50m (2%)      0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 k8s-keystone-auth-fjn7c                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 kubernetes-dashboard-968d4b48b-r8q5p                       100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         33m\n  kube-system                 magnum-prometheus-node-exporter-h8zjh                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 octavia-ingress-controller-0                               50m (2%)      0 (0%)      0 (0%)           0 (0%)         33m\n  kube-system                 openstack-cloud-controller-manager-75qb7                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         33m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-mgf6k    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m25s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1150m (57%)  0 (0%)\n  memory             240Mi (6%)   340Mi (8%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 33m                kube-proxy             \n  Normal   Starting                 33m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      33m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeAllocatableEnforced  33m                kubelet                Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory  33m (x3 over 33m)  kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    33m (x3 over 33m)  kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     33m (x3 over 33m)  kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           33m                node-controller        Node gitlab-1-26-36460-guscsyka22xa-master-0 event: Registered Node gitlab-1-26-36460-guscsyka22xa-master-0 in Controller\n  Normal   NodeReady                32m                kubelet                Node gitlab-1-26-36460-guscsyka22xa-master-0 status is now: NodeReady\n  Normal   Synced                   32m                cloud-node-controller  Node synced successfully\n"
    Aug 24 10:42:39.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-7873 describe namespace kubectl-7873'
    Aug 24 10:42:39.865: INFO: stderr: ""
    Aug 24 10:42:39.865: INFO: stdout: "Name:         kubectl-7873\nLabels:       e2e-framework=kubectl\n              e2e-run=b80dffc4-45ea-4898-a190-ca68b5b82212\n              kubernetes.io/metadata.name=kubectl-7873\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:42:39.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7873" for this suite. 08/24/23 10:42:39.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:42:39.885
Aug 24 10:42:39.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename watch 08/24/23 10:42:39.887
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:39.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:39.917
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/24/23 10:42:39.925
STEP: starting a background goroutine to produce watch events 08/24/23 10:42:39.929
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/24/23 10:42:39.929
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 10:42:42.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4637" for this suite. 08/24/23 10:42:42.744
------------------------------
â€¢ [2.918 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:42:39.885
    Aug 24 10:42:39.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename watch 08/24/23 10:42:39.887
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:39.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:39.917
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/24/23 10:42:39.925
    STEP: starting a background goroutine to produce watch events 08/24/23 10:42:39.929
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/24/23 10:42:39.929
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:42:42.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4637" for this suite. 08/24/23 10:42:42.744
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:42:42.804
Aug 24 10:42:42.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 10:42:42.805
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:42.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:42.845
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-af7d1902-22c6-416c-bfa5-fddf757358ff 08/24/23 10:42:42.851
STEP: Creating a pod to test consume configMaps 08/24/23 10:42:42.859
Aug 24 10:42:42.874: INFO: Waiting up to 5m0s for pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3" in namespace "configmap-7480" to be "Succeeded or Failed"
Aug 24 10:42:42.881: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.131948ms
Aug 24 10:42:44.885: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010070367s
Aug 24 10:42:46.887: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012700667s
Aug 24 10:42:48.885: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010328964s
Aug 24 10:42:50.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011136854s
Aug 24 10:42:52.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011322308s
Aug 24 10:42:54.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011906203s
Aug 24 10:42:56.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011928539s
Aug 24 10:42:58.884: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009973404s
Aug 24 10:43:00.887: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012186495s
Aug 24 10:43:02.885: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010647783s
Aug 24 10:43:04.889: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014455912s
Aug 24 10:43:06.887: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012468878s
STEP: Saw pod success 08/24/23 10:43:06.887
Aug 24 10:43:06.888: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3" satisfied condition "Succeeded or Failed"
Aug 24 10:43:06.891: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 10:43:06.976
Aug 24 10:43:06.998: INFO: Waiting for pod pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3 to disappear
Aug 24 10:43:07.004: INFO: Pod pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 10:43:07.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7480" for this suite. 08/24/23 10:43:07.012
------------------------------
â€¢ [SLOW TEST] [24.222 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:42:42.804
    Aug 24 10:42:42.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 10:42:42.805
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:42:42.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:42:42.845
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-af7d1902-22c6-416c-bfa5-fddf757358ff 08/24/23 10:42:42.851
    STEP: Creating a pod to test consume configMaps 08/24/23 10:42:42.859
    Aug 24 10:42:42.874: INFO: Waiting up to 5m0s for pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3" in namespace "configmap-7480" to be "Succeeded or Failed"
    Aug 24 10:42:42.881: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.131948ms
    Aug 24 10:42:44.885: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010070367s
    Aug 24 10:42:46.887: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012700667s
    Aug 24 10:42:48.885: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010328964s
    Aug 24 10:42:50.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011136854s
    Aug 24 10:42:52.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011322308s
    Aug 24 10:42:54.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011906203s
    Aug 24 10:42:56.886: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011928539s
    Aug 24 10:42:58.884: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009973404s
    Aug 24 10:43:00.887: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012186495s
    Aug 24 10:43:02.885: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010647783s
    Aug 24 10:43:04.889: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014455912s
    Aug 24 10:43:06.887: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012468878s
    STEP: Saw pod success 08/24/23 10:43:06.887
    Aug 24 10:43:06.888: INFO: Pod "pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3" satisfied condition "Succeeded or Failed"
    Aug 24 10:43:06.891: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 10:43:06.976
    Aug 24 10:43:06.998: INFO: Waiting for pod pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3 to disappear
    Aug 24 10:43:07.004: INFO: Pod pod-configmaps-a70295bd-0dce-4d4d-bff7-86abc61fe2c3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:43:07.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7480" for this suite. 08/24/23 10:43:07.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:43:07.031
Aug 24 10:43:07.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename endpointslice 08/24/23 10:43:07.033
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:07.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:07.063
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 08/24/23 10:43:22.256
STEP: referencing matching pods with named port 08/24/23 10:43:27.269
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/24/23 10:43:32.28
STEP: recreating EndpointSlices after they've been deleted 08/24/23 10:43:37.294
Aug 24 10:43:37.339: INFO: EndpointSlice for Service endpointslice-3641/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 10:43:47.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3641" for this suite. 08/24/23 10:43:47.357
------------------------------
â€¢ [SLOW TEST] [40.339 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:43:07.031
    Aug 24 10:43:07.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename endpointslice 08/24/23 10:43:07.033
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:07.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:07.063
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 08/24/23 10:43:22.256
    STEP: referencing matching pods with named port 08/24/23 10:43:27.269
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/24/23 10:43:32.28
    STEP: recreating EndpointSlices after they've been deleted 08/24/23 10:43:37.294
    Aug 24 10:43:37.339: INFO: EndpointSlice for Service endpointslice-3641/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:43:47.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3641" for this suite. 08/24/23 10:43:47.357
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:43:47.372
Aug 24 10:43:47.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 10:43:47.374
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:47.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:47.416
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/24/23 10:43:47.423
Aug 24 10:43:47.440: INFO: Waiting up to 5m0s for pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6" in namespace "var-expansion-8112" to be "Succeeded or Failed"
Aug 24 10:43:47.461: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.143908ms
Aug 24 10:43:49.464: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023858054s
Aug 24 10:43:51.466: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025564221s
STEP: Saw pod success 08/24/23 10:43:51.466
Aug 24 10:43:51.466: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6" satisfied condition "Succeeded or Failed"
Aug 24 10:43:51.471: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6 container dapi-container: <nil>
STEP: delete the pod 08/24/23 10:43:51.53
Aug 24 10:43:51.558: INFO: Waiting for pod var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6 to disappear
Aug 24 10:43:51.566: INFO: Pod var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 10:43:51.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8112" for this suite. 08/24/23 10:43:51.57
------------------------------
â€¢ [4.210 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:43:47.372
    Aug 24 10:43:47.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 10:43:47.374
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:47.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:47.416
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/24/23 10:43:47.423
    Aug 24 10:43:47.440: INFO: Waiting up to 5m0s for pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6" in namespace "var-expansion-8112" to be "Succeeded or Failed"
    Aug 24 10:43:47.461: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.143908ms
    Aug 24 10:43:49.464: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023858054s
    Aug 24 10:43:51.466: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025564221s
    STEP: Saw pod success 08/24/23 10:43:51.466
    Aug 24 10:43:51.466: INFO: Pod "var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6" satisfied condition "Succeeded or Failed"
    Aug 24 10:43:51.471: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 10:43:51.53
    Aug 24 10:43:51.558: INFO: Waiting for pod var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6 to disappear
    Aug 24 10:43:51.566: INFO: Pod var-expansion-dc87ec1c-a1a9-488d-b8b5-9e8e8538d5e6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:43:51.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8112" for this suite. 08/24/23 10:43:51.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:43:51.583
Aug 24 10:43:51.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 10:43:51.585
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:51.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:51.62
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-f80e9840-6579-4ce6-937d-71a717afa604 08/24/23 10:43:51.627
STEP: Creating a pod to test consume secrets 08/24/23 10:43:51.636
Aug 24 10:43:51.656: INFO: Waiting up to 5m0s for pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef" in namespace "secrets-3349" to be "Succeeded or Failed"
Aug 24 10:43:51.668: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.218627ms
Aug 24 10:43:53.673: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017273265s
Aug 24 10:43:55.674: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017910979s
STEP: Saw pod success 08/24/23 10:43:55.674
Aug 24 10:43:55.674: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef" satisfied condition "Succeeded or Failed"
Aug 24 10:43:55.677: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 10:43:55.685
Aug 24 10:43:55.705: INFO: Waiting for pod pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef to disappear
Aug 24 10:43:55.708: INFO: Pod pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 10:43:55.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3349" for this suite. 08/24/23 10:43:55.712
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:43:51.583
    Aug 24 10:43:51.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 10:43:51.585
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:51.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:51.62
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-f80e9840-6579-4ce6-937d-71a717afa604 08/24/23 10:43:51.627
    STEP: Creating a pod to test consume secrets 08/24/23 10:43:51.636
    Aug 24 10:43:51.656: INFO: Waiting up to 5m0s for pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef" in namespace "secrets-3349" to be "Succeeded or Failed"
    Aug 24 10:43:51.668: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.218627ms
    Aug 24 10:43:53.673: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017273265s
    Aug 24 10:43:55.674: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017910979s
    STEP: Saw pod success 08/24/23 10:43:55.674
    Aug 24 10:43:55.674: INFO: Pod "pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef" satisfied condition "Succeeded or Failed"
    Aug 24 10:43:55.677: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 10:43:55.685
    Aug 24 10:43:55.705: INFO: Waiting for pod pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef to disappear
    Aug 24 10:43:55.708: INFO: Pod pod-secrets-f3f7d676-f8cb-45f1-9421-14616355feef no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:43:55.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3349" for this suite. 08/24/23 10:43:55.712
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:43:55.724
Aug 24 10:43:55.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 10:43:55.726
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:55.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:55.751
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/24/23 10:43:55.765
STEP: watching for Pod to be ready 08/24/23 10:43:55.778
Aug 24 10:43:55.782: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 24 10:43:55.794: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
Aug 24 10:43:55.816: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
Aug 24 10:43:56.373: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
Aug 24 10:43:57.180: INFO: Found Pod pod-test in namespace pods-3985 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/24/23 10:43:57.192
STEP: getting the Pod and ensuring that it's patched 08/24/23 10:43:57.225
STEP: replacing the Pod's status Ready condition to False 08/24/23 10:43:57.229
STEP: check the Pod again to ensure its Ready conditions are False 08/24/23 10:43:57.258
STEP: deleting the Pod via a Collection with a LabelSelector 08/24/23 10:43:57.258
STEP: watching for the Pod to be deleted 08/24/23 10:43:57.277
Aug 24 10:43:57.281: INFO: observed event type MODIFIED
Aug 24 10:43:58.916: INFO: observed event type MODIFIED
Aug 24 10:43:59.109: INFO: observed event type MODIFIED
Aug 24 10:43:59.914: INFO: observed event type MODIFIED
Aug 24 10:43:59.926: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 10:43:59.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3985" for this suite. 08/24/23 10:43:59.949
------------------------------
â€¢ [4.239 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:43:55.724
    Aug 24 10:43:55.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 10:43:55.726
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:43:55.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:43:55.751
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/24/23 10:43:55.765
    STEP: watching for Pod to be ready 08/24/23 10:43:55.778
    Aug 24 10:43:55.782: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 24 10:43:55.794: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
    Aug 24 10:43:55.816: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
    Aug 24 10:43:56.373: INFO: observed Pod pod-test in namespace pods-3985 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
    Aug 24 10:43:57.180: INFO: Found Pod pod-test in namespace pods-3985 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 10:43:55 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/24/23 10:43:57.192
    STEP: getting the Pod and ensuring that it's patched 08/24/23 10:43:57.225
    STEP: replacing the Pod's status Ready condition to False 08/24/23 10:43:57.229
    STEP: check the Pod again to ensure its Ready conditions are False 08/24/23 10:43:57.258
    STEP: deleting the Pod via a Collection with a LabelSelector 08/24/23 10:43:57.258
    STEP: watching for the Pod to be deleted 08/24/23 10:43:57.277
    Aug 24 10:43:57.281: INFO: observed event type MODIFIED
    Aug 24 10:43:58.916: INFO: observed event type MODIFIED
    Aug 24 10:43:59.109: INFO: observed event type MODIFIED
    Aug 24 10:43:59.914: INFO: observed event type MODIFIED
    Aug 24 10:43:59.926: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:43:59.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3985" for this suite. 08/24/23 10:43:59.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:43:59.966
Aug 24 10:43:59.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 10:43:59.968
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:44:00.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:44:00.012
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1423 08/24/23 10:44:00.019
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/24/23 10:44:00.035
STEP: Creating pod with conflicting port in namespace statefulset-1423 08/24/23 10:44:00.044
STEP: Waiting until pod test-pod will start running in namespace statefulset-1423 08/24/23 10:44:00.063
Aug 24 10:44:00.063: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1423" to be "running"
Aug 24 10:44:00.068: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514419ms
Aug 24 10:44:02.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010031618s
Aug 24 10:44:04.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011964041s
Aug 24 10:44:06.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009560529s
Aug 24 10:44:08.088: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025474936s
Aug 24 10:44:10.082: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019362891s
Aug 24 10:44:12.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012571957s
Aug 24 10:44:14.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010027745s
Aug 24 10:44:16.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010000008s
Aug 24 10:44:18.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009499834s
Aug 24 10:44:20.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012354226s
Aug 24 10:44:22.074: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01075205s
Aug 24 10:44:24.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0118225s
Aug 24 10:44:26.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009974434s
Aug 24 10:44:28.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009225041s
Aug 24 10:44:30.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012673145s
Aug 24 10:44:32.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011726734s
Aug 24 10:44:34.076: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012758591s
Aug 24 10:44:36.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009227097s
Aug 24 10:44:38.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010317735s
Aug 24 10:44:40.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009895919s
Aug 24 10:44:42.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010327467s
Aug 24 10:44:44.074: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 44.010963446s
Aug 24 10:44:44.074: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-1423 08/24/23 10:44:44.074
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1423 08/24/23 10:44:44.086
Aug 24 10:44:44.116: INFO: Observed stateful pod in namespace: statefulset-1423, name: ss-0, uid: 81997fa5-5681-458d-a071-fd6497765ac6, status phase: Pending. Waiting for statefulset controller to delete.
Aug 24 10:44:44.140: INFO: Observed stateful pod in namespace: statefulset-1423, name: ss-0, uid: 81997fa5-5681-458d-a071-fd6497765ac6, status phase: Failed. Waiting for statefulset controller to delete.
Aug 24 10:44:44.167: INFO: Observed stateful pod in namespace: statefulset-1423, name: ss-0, uid: 81997fa5-5681-458d-a071-fd6497765ac6, status phase: Failed. Waiting for statefulset controller to delete.
Aug 24 10:44:44.176: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1423
STEP: Removing pod with conflicting port in namespace statefulset-1423 08/24/23 10:44:44.176
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1423 and will be in running state 08/24/23 10:44:44.216
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 10:44:46.244: INFO: Deleting all statefulset in ns statefulset-1423
Aug 24 10:44:46.248: INFO: Scaling statefulset ss to 0
Aug 24 10:44:56.279: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 10:44:56.282: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 10:44:56.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1423" for this suite. 08/24/23 10:44:56.341
------------------------------
â€¢ [SLOW TEST] [56.384 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:43:59.966
    Aug 24 10:43:59.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 10:43:59.968
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:44:00.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:44:00.012
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1423 08/24/23 10:44:00.019
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/24/23 10:44:00.035
    STEP: Creating pod with conflicting port in namespace statefulset-1423 08/24/23 10:44:00.044
    STEP: Waiting until pod test-pod will start running in namespace statefulset-1423 08/24/23 10:44:00.063
    Aug 24 10:44:00.063: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-1423" to be "running"
    Aug 24 10:44:00.068: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514419ms
    Aug 24 10:44:02.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010031618s
    Aug 24 10:44:04.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011964041s
    Aug 24 10:44:06.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009560529s
    Aug 24 10:44:08.088: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025474936s
    Aug 24 10:44:10.082: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.019362891s
    Aug 24 10:44:12.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012571957s
    Aug 24 10:44:14.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010027745s
    Aug 24 10:44:16.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010000008s
    Aug 24 10:44:18.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009499834s
    Aug 24 10:44:20.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 20.012354226s
    Aug 24 10:44:22.074: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01075205s
    Aug 24 10:44:24.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.0118225s
    Aug 24 10:44:26.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009974434s
    Aug 24 10:44:28.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009225041s
    Aug 24 10:44:30.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012673145s
    Aug 24 10:44:32.075: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 32.011726734s
    Aug 24 10:44:34.076: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012758591s
    Aug 24 10:44:36.072: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009227097s
    Aug 24 10:44:38.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010317735s
    Aug 24 10:44:40.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009895919s
    Aug 24 10:44:42.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010327467s
    Aug 24 10:44:44.074: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 44.010963446s
    Aug 24 10:44:44.074: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-1423 08/24/23 10:44:44.074
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1423 08/24/23 10:44:44.086
    Aug 24 10:44:44.116: INFO: Observed stateful pod in namespace: statefulset-1423, name: ss-0, uid: 81997fa5-5681-458d-a071-fd6497765ac6, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 24 10:44:44.140: INFO: Observed stateful pod in namespace: statefulset-1423, name: ss-0, uid: 81997fa5-5681-458d-a071-fd6497765ac6, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 24 10:44:44.167: INFO: Observed stateful pod in namespace: statefulset-1423, name: ss-0, uid: 81997fa5-5681-458d-a071-fd6497765ac6, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 24 10:44:44.176: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1423
    STEP: Removing pod with conflicting port in namespace statefulset-1423 08/24/23 10:44:44.176
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1423 and will be in running state 08/24/23 10:44:44.216
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 10:44:46.244: INFO: Deleting all statefulset in ns statefulset-1423
    Aug 24 10:44:46.248: INFO: Scaling statefulset ss to 0
    Aug 24 10:44:56.279: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 10:44:56.282: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:44:56.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1423" for this suite. 08/24/23 10:44:56.341
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:44:56.355
Aug 24 10:44:56.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 10:44:56.357
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:44:56.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:44:56.396
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/24/23 10:44:56.403
Aug 24 10:44:56.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912" in namespace "projected-4382" to be "Succeeded or Failed"
Aug 24 10:44:56.425: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090649ms
Aug 24 10:44:58.429: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010755516s
Aug 24 10:45:00.429: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010235231s
STEP: Saw pod success 08/24/23 10:45:00.429
Aug 24 10:45:00.429: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912" satisfied condition "Succeeded or Failed"
Aug 24 10:45:00.432: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912 container client-container: <nil>
STEP: delete the pod 08/24/23 10:45:00.438
Aug 24 10:45:00.469: INFO: Waiting for pod downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912 to disappear
Aug 24 10:45:00.473: INFO: Pod downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 10:45:00.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4382" for this suite. 08/24/23 10:45:00.477
------------------------------
â€¢ [4.133 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:44:56.355
    Aug 24 10:44:56.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 10:44:56.357
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:44:56.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:44:56.396
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/24/23 10:44:56.403
    Aug 24 10:44:56.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912" in namespace "projected-4382" to be "Succeeded or Failed"
    Aug 24 10:44:56.425: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090649ms
    Aug 24 10:44:58.429: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010755516s
    Aug 24 10:45:00.429: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010235231s
    STEP: Saw pod success 08/24/23 10:45:00.429
    Aug 24 10:45:00.429: INFO: Pod "downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912" satisfied condition "Succeeded or Failed"
    Aug 24 10:45:00.432: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912 container client-container: <nil>
    STEP: delete the pod 08/24/23 10:45:00.438
    Aug 24 10:45:00.469: INFO: Waiting for pod downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912 to disappear
    Aug 24 10:45:00.473: INFO: Pod downwardapi-volume-f97dd594-dc80-4033-8cc2-0c946b783912 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:45:00.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4382" for this suite. 08/24/23 10:45:00.477
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:45:00.489
Aug 24 10:45:00.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 10:45:00.491
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:00.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:00.533
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-f25a8e5c-8641-4f6c-9959-973c987094a0 08/24/23 10:45:00.539
STEP: Creating a pod to test consume secrets 08/24/23 10:45:00.546
Aug 24 10:45:00.558: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956" in namespace "projected-4265" to be "Succeeded or Failed"
Aug 24 10:45:00.574: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956": Phase="Pending", Reason="", readiness=false. Elapsed: 15.095136ms
Aug 24 10:45:02.578: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019329782s
Aug 24 10:45:04.578: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019497446s
STEP: Saw pod success 08/24/23 10:45:04.578
Aug 24 10:45:04.578: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956" satisfied condition "Succeeded or Failed"
Aug 24 10:45:04.581: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 10:45:04.589
Aug 24 10:45:04.608: INFO: Waiting for pod pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956 to disappear
Aug 24 10:45:04.614: INFO: Pod pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 10:45:04.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4265" for this suite. 08/24/23 10:45:04.619
------------------------------
â€¢ [4.139 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:45:00.489
    Aug 24 10:45:00.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 10:45:00.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:00.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:00.533
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-f25a8e5c-8641-4f6c-9959-973c987094a0 08/24/23 10:45:00.539
    STEP: Creating a pod to test consume secrets 08/24/23 10:45:00.546
    Aug 24 10:45:00.558: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956" in namespace "projected-4265" to be "Succeeded or Failed"
    Aug 24 10:45:00.574: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956": Phase="Pending", Reason="", readiness=false. Elapsed: 15.095136ms
    Aug 24 10:45:02.578: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019329782s
    Aug 24 10:45:04.578: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019497446s
    STEP: Saw pod success 08/24/23 10:45:04.578
    Aug 24 10:45:04.578: INFO: Pod "pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956" satisfied condition "Succeeded or Failed"
    Aug 24 10:45:04.581: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 10:45:04.589
    Aug 24 10:45:04.608: INFO: Waiting for pod pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956 to disappear
    Aug 24 10:45:04.614: INFO: Pod pod-projected-secrets-3938852c-7c55-4ced-988e-ecb934732956 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:45:04.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4265" for this suite. 08/24/23 10:45:04.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:45:04.636
Aug 24 10:45:04.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename runtimeclass 08/24/23 10:45:04.637
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:04.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:04.662
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 24 10:45:04.697: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9618 to be scheduled
Aug 24 10:45:04.702: INFO: 1 pods are not scheduled: [runtimeclass-9618/test-runtimeclass-runtimeclass-9618-preconfigured-handler-tdz24(e96ef42f-09f4-4b97-b055-f0783059def1)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 10:45:06.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9618" for this suite. 08/24/23 10:45:06.723
------------------------------
â€¢ [2.104 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:45:04.636
    Aug 24 10:45:04.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 10:45:04.637
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:04.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:04.662
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 24 10:45:04.697: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9618 to be scheduled
    Aug 24 10:45:04.702: INFO: 1 pods are not scheduled: [runtimeclass-9618/test-runtimeclass-runtimeclass-9618-preconfigured-handler-tdz24(e96ef42f-09f4-4b97-b055-f0783059def1)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:45:06.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9618" for this suite. 08/24/23 10:45:06.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:45:06.742
Aug 24 10:45:06.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 10:45:06.743
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:06.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:06.78
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/24/23 10:45:06.79
Aug 24 10:45:06.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: rename a version 08/24/23 10:45:12.568
STEP: check the new version name is served 08/24/23 10:45:12.608
STEP: check the old version name is removed 08/24/23 10:45:14.856
STEP: check the other version is not changed 08/24/23 10:45:15.936
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:45:20.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5279" for this suite. 08/24/23 10:45:20.386
------------------------------
â€¢ [SLOW TEST] [13.652 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:45:06.742
    Aug 24 10:45:06.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 10:45:06.743
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:06.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:06.78
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/24/23 10:45:06.79
    Aug 24 10:45:06.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: rename a version 08/24/23 10:45:12.568
    STEP: check the new version name is served 08/24/23 10:45:12.608
    STEP: check the old version name is removed 08/24/23 10:45:14.856
    STEP: check the other version is not changed 08/24/23 10:45:15.936
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:45:20.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5279" for this suite. 08/24/23 10:45:20.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:45:20.394
Aug 24 10:45:20.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 10:45:20.395
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:20.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:20.423
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 10:45:20.465
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:45:21.158
STEP: Deploying the webhook pod 08/24/23 10:45:21.17
STEP: Wait for the deployment to be ready 08/24/23 10:45:21.192
Aug 24 10:45:21.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 10:45:23.235
STEP: Verifying the service has paired with the endpoint 08/24/23 10:45:23.261
Aug 24 10:45:24.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/24/23 10:45:24.267
STEP: create a configmap that should be updated by the webhook 08/24/23 10:45:24.297
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:45:24.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2474" for this suite. 08/24/23 10:45:24.417
STEP: Destroying namespace "webhook-2474-markers" for this suite. 08/24/23 10:45:24.432
------------------------------
â€¢ [4.064 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:45:20.394
    Aug 24 10:45:20.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 10:45:20.395
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:20.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:20.423
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 10:45:20.465
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:45:21.158
    STEP: Deploying the webhook pod 08/24/23 10:45:21.17
    STEP: Wait for the deployment to be ready 08/24/23 10:45:21.192
    Aug 24 10:45:21.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 10:45:23.235
    STEP: Verifying the service has paired with the endpoint 08/24/23 10:45:23.261
    Aug 24 10:45:24.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/24/23 10:45:24.267
    STEP: create a configmap that should be updated by the webhook 08/24/23 10:45:24.297
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:45:24.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2474" for this suite. 08/24/23 10:45:24.417
    STEP: Destroying namespace "webhook-2474-markers" for this suite. 08/24/23 10:45:24.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:45:24.461
Aug 24 10:45:24.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 10:45:24.462
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:24.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:24.5
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-e59c9a73-11f6-4b58-b3d9-46d357738288 08/24/23 10:45:24.508
STEP: Creating a pod to test consume configMaps 08/24/23 10:45:24.528
Aug 24 10:45:24.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5" in namespace "projected-3466" to be "Succeeded or Failed"
Aug 24 10:45:24.567: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.564728ms
Aug 24 10:45:26.572: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02849683s
Aug 24 10:45:28.571: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027607034s
STEP: Saw pod success 08/24/23 10:45:28.571
Aug 24 10:45:28.571: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5" satisfied condition "Succeeded or Failed"
Aug 24 10:45:28.575: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 10:45:28.582
Aug 24 10:45:28.607: INFO: Waiting for pod pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5 to disappear
Aug 24 10:45:28.613: INFO: Pod pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 10:45:28.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3466" for this suite. 08/24/23 10:45:28.619
------------------------------
â€¢ [4.166 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:45:24.461
    Aug 24 10:45:24.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 10:45:24.462
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:24.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:24.5
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-e59c9a73-11f6-4b58-b3d9-46d357738288 08/24/23 10:45:24.508
    STEP: Creating a pod to test consume configMaps 08/24/23 10:45:24.528
    Aug 24 10:45:24.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5" in namespace "projected-3466" to be "Succeeded or Failed"
    Aug 24 10:45:24.567: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.564728ms
    Aug 24 10:45:26.572: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02849683s
    Aug 24 10:45:28.571: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027607034s
    STEP: Saw pod success 08/24/23 10:45:28.571
    Aug 24 10:45:28.571: INFO: Pod "pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5" satisfied condition "Succeeded or Failed"
    Aug 24 10:45:28.575: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 10:45:28.582
    Aug 24 10:45:28.607: INFO: Waiting for pod pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5 to disappear
    Aug 24 10:45:28.613: INFO: Pod pod-projected-configmaps-6a3e8542-fff0-4a7b-8f3c-53619a9ed7b5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:45:28.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3466" for this suite. 08/24/23 10:45:28.619
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:45:28.627
Aug 24 10:45:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 10:45:28.629
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:28.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:28.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 10:45:28.692
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:45:29.403
STEP: Deploying the webhook pod 08/24/23 10:45:29.412
STEP: Wait for the deployment to be ready 08/24/23 10:45:29.439
Aug 24 10:45:29.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 24 10:45:31.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/24/23 10:45:33.521
STEP: Verifying the service has paired with the endpoint 08/24/23 10:45:33.551
Aug 24 10:45:34.552: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/24/23 10:45:34.558
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/24/23 10:45:34.561
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 10:45:34.562
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/24/23 10:45:34.562
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/24/23 10:45:34.565
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 10:45:34.565
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 10:45:34.569
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:45:34.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5816" for this suite. 08/24/23 10:45:34.685
STEP: Destroying namespace "webhook-5816-markers" for this suite. 08/24/23 10:45:34.699
------------------------------
â€¢ [SLOW TEST] [6.096 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:45:28.627
    Aug 24 10:45:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 10:45:28.629
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:28.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:28.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 10:45:28.692
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:45:29.403
    STEP: Deploying the webhook pod 08/24/23 10:45:29.412
    STEP: Wait for the deployment to be ready 08/24/23 10:45:29.439
    Aug 24 10:45:29.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 24 10:45:31.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 10, 45, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/24/23 10:45:33.521
    STEP: Verifying the service has paired with the endpoint 08/24/23 10:45:33.551
    Aug 24 10:45:34.552: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/24/23 10:45:34.558
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/24/23 10:45:34.561
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 10:45:34.562
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/24/23 10:45:34.562
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/24/23 10:45:34.565
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 10:45:34.565
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/24/23 10:45:34.569
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:45:34.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5816" for this suite. 08/24/23 10:45:34.685
    STEP: Destroying namespace "webhook-5816-markers" for this suite. 08/24/23 10:45:34.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:45:34.724
Aug 24 10:45:34.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-preemption 08/24/23 10:45:34.727
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:34.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:34.771
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 10:45:34.812: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 10:46:34.855: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:46:34.859
Aug 24 10:46:34.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 10:46:34.86
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:46:34.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:46:34.891
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/24/23 10:46:34.896
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 10:46:34.897
Aug 24 10:46:34.909: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1613" to be "running"
Aug 24 10:46:34.914: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.281863ms
Aug 24 10:46:36.919: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010073177s
Aug 24 10:46:38.918: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009022043s
Aug 24 10:46:40.919: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009425635s
Aug 24 10:46:42.918: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 8.008955524s
Aug 24 10:46:42.918: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 10:46:42.922
Aug 24 10:46:42.940: INFO: found a healthy node: gitlab-1-26-36460-guscsyka22xa-node-2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 24 10:46:49.058: INFO: pods created so far: [1 1 1]
Aug 24 10:46:49.058: INFO: length of pods created so far: 3
Aug 24 10:46:55.086: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 24 10:47:02.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:47:02.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1613" for this suite. 08/24/23 10:47:02.229
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6954" for this suite. 08/24/23 10:47:02.239
------------------------------
â€¢ [SLOW TEST] [87.524 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:45:34.724
    Aug 24 10:45:34.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 10:45:34.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:45:34.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:45:34.771
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 10:45:34.812: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 10:46:34.855: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:46:34.859
    Aug 24 10:46:34.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 10:46:34.86
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:46:34.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:46:34.891
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/24/23 10:46:34.896
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 10:46:34.897
    Aug 24 10:46:34.909: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1613" to be "running"
    Aug 24 10:46:34.914: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.281863ms
    Aug 24 10:46:36.919: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010073177s
    Aug 24 10:46:38.918: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009022043s
    Aug 24 10:46:40.919: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009425635s
    Aug 24 10:46:42.918: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 8.008955524s
    Aug 24 10:46:42.918: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 10:46:42.922
    Aug 24 10:46:42.940: INFO: found a healthy node: gitlab-1-26-36460-guscsyka22xa-node-2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 24 10:46:49.058: INFO: pods created so far: [1 1 1]
    Aug 24 10:46:49.058: INFO: length of pods created so far: 3
    Aug 24 10:46:55.086: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:47:02.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:47:02.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1613" for this suite. 08/24/23 10:47:02.229
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6954" for this suite. 08/24/23 10:47:02.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:47:02.25
Aug 24 10:47:02.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename events 08/24/23 10:47:02.252
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:02.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:02.278
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/24/23 10:47:02.285
STEP: get a list of Events with a label in the current namespace 08/24/23 10:47:02.313
STEP: delete a list of events 08/24/23 10:47:02.318
Aug 24 10:47:02.318: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/24/23 10:47:02.336
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 24 10:47:02.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1887" for this suite. 08/24/23 10:47:02.343
------------------------------
â€¢ [0.101 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:47:02.25
    Aug 24 10:47:02.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename events 08/24/23 10:47:02.252
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:02.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:02.278
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/24/23 10:47:02.285
    STEP: get a list of Events with a label in the current namespace 08/24/23 10:47:02.313
    STEP: delete a list of events 08/24/23 10:47:02.318
    Aug 24 10:47:02.318: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/24/23 10:47:02.336
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:47:02.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1887" for this suite. 08/24/23 10:47:02.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:47:02.352
Aug 24 10:47:02.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 10:47:02.353
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:02.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:02.385
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 10:47:02.414
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:47:02.84
STEP: Deploying the webhook pod 08/24/23 10:47:02.85
STEP: Wait for the deployment to be ready 08/24/23 10:47:02.874
Aug 24 10:47:02.912: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 10:47:04.924
STEP: Verifying the service has paired with the endpoint 08/24/23 10:47:04.94
Aug 24 10:47:05.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 24 10:47:05.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7154-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 10:47:06.468
STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 10:47:06.496
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:47:09.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-459" for this suite. 08/24/23 10:47:09.646
STEP: Destroying namespace "webhook-459-markers" for this suite. 08/24/23 10:47:09.698
------------------------------
â€¢ [SLOW TEST] [7.444 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:47:02.352
    Aug 24 10:47:02.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 10:47:02.353
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:02.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:02.385
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 10:47:02.414
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:47:02.84
    STEP: Deploying the webhook pod 08/24/23 10:47:02.85
    STEP: Wait for the deployment to be ready 08/24/23 10:47:02.874
    Aug 24 10:47:02.912: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 10:47:04.924
    STEP: Verifying the service has paired with the endpoint 08/24/23 10:47:04.94
    Aug 24 10:47:05.941: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 24 10:47:05.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7154-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 10:47:06.468
    STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 10:47:06.496
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:47:09.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-459" for this suite. 08/24/23 10:47:09.646
    STEP: Destroying namespace "webhook-459-markers" for this suite. 08/24/23 10:47:09.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:47:09.801
Aug 24 10:47:09.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 10:47:09.802
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:09.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:09.889
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/24/23 10:47:09.941
Aug 24 10:47:09.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670" in namespace "downward-api-7975" to be "Succeeded or Failed"
Aug 24 10:47:09.990: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670": Phase="Pending", Reason="", readiness=false. Elapsed: 16.471666ms
Aug 24 10:47:11.996: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022056279s
Aug 24 10:47:13.995: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020941277s
STEP: Saw pod success 08/24/23 10:47:13.995
Aug 24 10:47:13.995: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670" satisfied condition "Succeeded or Failed"
Aug 24 10:47:13.998: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670 container client-container: <nil>
STEP: delete the pod 08/24/23 10:47:14.052
Aug 24 10:47:14.069: INFO: Waiting for pod downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670 to disappear
Aug 24 10:47:14.074: INFO: Pod downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 10:47:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7975" for this suite. 08/24/23 10:47:14.079
------------------------------
â€¢ [4.288 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:47:09.801
    Aug 24 10:47:09.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 10:47:09.802
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:09.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:09.889
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/24/23 10:47:09.941
    Aug 24 10:47:09.974: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670" in namespace "downward-api-7975" to be "Succeeded or Failed"
    Aug 24 10:47:09.990: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670": Phase="Pending", Reason="", readiness=false. Elapsed: 16.471666ms
    Aug 24 10:47:11.996: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022056279s
    Aug 24 10:47:13.995: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020941277s
    STEP: Saw pod success 08/24/23 10:47:13.995
    Aug 24 10:47:13.995: INFO: Pod "downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670" satisfied condition "Succeeded or Failed"
    Aug 24 10:47:13.998: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670 container client-container: <nil>
    STEP: delete the pod 08/24/23 10:47:14.052
    Aug 24 10:47:14.069: INFO: Waiting for pod downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670 to disappear
    Aug 24 10:47:14.074: INFO: Pod downwardapi-volume-d16a15bd-9331-4e04-bd3e-8296faa0b670 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:47:14.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7975" for this suite. 08/24/23 10:47:14.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:47:14.091
Aug 24 10:47:14.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replication-controller 08/24/23 10:47:14.093
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:14.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:14.118
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/24/23 10:47:14.125
STEP: When the matched label of one of its pods change 08/24/23 10:47:14.137
Aug 24 10:47:14.141: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 24 10:47:19.151: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/24/23 10:47:19.175
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 10:47:20.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4483" for this suite. 08/24/23 10:47:20.193
------------------------------
â€¢ [SLOW TEST] [6.112 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:47:14.091
    Aug 24 10:47:14.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replication-controller 08/24/23 10:47:14.093
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:14.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:14.118
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/24/23 10:47:14.125
    STEP: When the matched label of one of its pods change 08/24/23 10:47:14.137
    Aug 24 10:47:14.141: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 24 10:47:19.151: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/24/23 10:47:19.175
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:47:20.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4483" for this suite. 08/24/23 10:47:20.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:47:20.204
Aug 24 10:47:20.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 10:47:20.206
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:20.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:20.238
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 in namespace container-probe-5420 08/24/23 10:47:20.245
Aug 24 10:47:20.260: INFO: Waiting up to 5m0s for pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4" in namespace "container-probe-5420" to be "not pending"
Aug 24 10:47:20.264: INFO: Pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.200824ms
Aug 24 10:47:22.270: INFO: Pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010205978s
Aug 24 10:47:22.270: INFO: Pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4" satisfied condition "not pending"
Aug 24 10:47:22.270: INFO: Started pod busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 in namespace container-probe-5420
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 10:47:22.27
Aug 24 10:47:22.274: INFO: Initial restart count of pod busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 is 0
Aug 24 10:48:12.404: INFO: Restart count of pod container-probe-5420/busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 is now 1 (50.129822827s elapsed)
STEP: deleting the pod 08/24/23 10:48:12.404
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 10:48:12.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5420" for this suite. 08/24/23 10:48:12.431
------------------------------
â€¢ [SLOW TEST] [52.238 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:47:20.204
    Aug 24 10:47:20.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 10:47:20.206
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:47:20.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:47:20.238
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 in namespace container-probe-5420 08/24/23 10:47:20.245
    Aug 24 10:47:20.260: INFO: Waiting up to 5m0s for pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4" in namespace "container-probe-5420" to be "not pending"
    Aug 24 10:47:20.264: INFO: Pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.200824ms
    Aug 24 10:47:22.270: INFO: Pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010205978s
    Aug 24 10:47:22.270: INFO: Pod "busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4" satisfied condition "not pending"
    Aug 24 10:47:22.270: INFO: Started pod busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 in namespace container-probe-5420
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 10:47:22.27
    Aug 24 10:47:22.274: INFO: Initial restart count of pod busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 is 0
    Aug 24 10:48:12.404: INFO: Restart count of pod container-probe-5420/busybox-de99540f-90f6-42d2-9ce1-d4c4477821c4 is now 1 (50.129822827s elapsed)
    STEP: deleting the pod 08/24/23 10:48:12.404
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:48:12.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5420" for this suite. 08/24/23 10:48:12.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:48:12.446
Aug 24 10:48:12.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replicaset 08/24/23 10:48:12.447
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:48:12.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:48:12.475
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 24 10:48:12.505: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 24 10:48:17.513: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 10:48:17.513
STEP: Scaling up "test-rs" replicaset  08/24/23 10:48:17.513
Aug 24 10:48:17.544: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/24/23 10:48:17.544
W0824 10:48:17.570412      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 24 10:48:17.576: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 10:48:17.599: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 10:48:17.640: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 10:48:17.656: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
Aug 24 10:48:19.478: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 2, AvailableReplicas 2
Aug 24 10:48:54.292: INFO: observed Replicaset test-rs in namespace replicaset-6046 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 10:48:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6046" for this suite. 08/24/23 10:48:54.3
------------------------------
â€¢ [SLOW TEST] [41.866 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:48:12.446
    Aug 24 10:48:12.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replicaset 08/24/23 10:48:12.447
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:48:12.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:48:12.475
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 24 10:48:12.505: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 24 10:48:17.513: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 10:48:17.513
    STEP: Scaling up "test-rs" replicaset  08/24/23 10:48:17.513
    Aug 24 10:48:17.544: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/24/23 10:48:17.544
    W0824 10:48:17.570412      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 24 10:48:17.576: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 10:48:17.599: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 10:48:17.640: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 10:48:17.656: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 1, AvailableReplicas 1
    Aug 24 10:48:19.478: INFO: observed ReplicaSet test-rs in namespace replicaset-6046 with ReadyReplicas 2, AvailableReplicas 2
    Aug 24 10:48:54.292: INFO: observed Replicaset test-rs in namespace replicaset-6046 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:48:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6046" for this suite. 08/24/23 10:48:54.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:48:54.312
Aug 24 10:48:54.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 10:48:54.314
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:48:54.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:48:54.341
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/24/23 10:48:54.348
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
 08/24/23 10:48:54.36
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
 08/24/23 10:48:54.36
STEP: creating a pod to probe DNS 08/24/23 10:48:54.361
STEP: submitting the pod to kubernetes 08/24/23 10:48:54.361
Aug 24 10:48:54.385: INFO: Waiting up to 15m0s for pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7" in namespace "dns-8542" to be "running"
Aug 24 10:48:54.396: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.291536ms
Aug 24 10:48:56.403: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018107579s
Aug 24 10:48:58.403: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018455637s
Aug 24 10:49:00.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017116415s
Aug 24 10:49:02.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01601475s
Aug 24 10:49:04.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016398556s
Aug 24 10:49:06.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017564166s
Aug 24 10:49:08.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015010315s
Aug 24 10:49:10.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01544144s
Aug 24 10:49:12.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.015118213s
Aug 24 10:49:14.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015204553s
Aug 24 10:49:16.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016189702s
Aug 24 10:49:18.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015872635s
Aug 24 10:49:20.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.017637577s
Aug 24 10:49:22.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 28.015073503s
Aug 24 10:49:24.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017215303s
Aug 24 10:49:26.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.017243591s
Aug 24 10:49:28.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016285438s
Aug 24 10:49:30.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016486267s
Aug 24 10:49:32.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01586114s
Aug 24 10:49:34.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016676529s
Aug 24 10:49:36.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017677473s
Aug 24 10:49:38.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016840105s
Aug 24 10:49:40.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016428198s
Aug 24 10:49:42.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016306013s
Aug 24 10:49:44.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016442984s
Aug 24 10:49:46.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017255119s
Aug 24 10:49:48.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 54.016020002s
Aug 24 10:49:50.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015538386s
Aug 24 10:49:52.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 58.01511018s
Aug 24 10:49:54.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015884007s
Aug 24 10:49:56.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.01679194s
Aug 24 10:49:58.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015869066s
Aug 24 10:50:00.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015859148s
Aug 24 10:50:02.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016431062s
Aug 24 10:50:04.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Running", Reason="", readiness=false. Elapsed: 1m10.01642971s
Aug 24 10:50:04.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7" satisfied condition "running"
STEP: retrieving the pod 08/24/23 10:50:04.401
STEP: looking for the results for each expected name from probers 08/24/23 10:50:04.404
Aug 24 10:50:04.414: INFO: DNS probes using dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7 succeeded

STEP: deleting the pod 08/24/23 10:50:04.414
STEP: changing the externalName to bar.example.com 08/24/23 10:50:04.44
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
 08/24/23 10:50:04.461
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
 08/24/23 10:50:04.461
STEP: creating a second pod to probe DNS 08/24/23 10:50:04.461
STEP: submitting the pod to kubernetes 08/24/23 10:50:04.461
Aug 24 10:50:04.472: INFO: Waiting up to 15m0s for pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2" in namespace "dns-8542" to be "running"
Aug 24 10:50:04.492: INFO: Pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.749077ms
Aug 24 10:50:06.497: INFO: Pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025146543s
Aug 24 10:50:06.498: INFO: Pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2" satisfied condition "running"
STEP: retrieving the pod 08/24/23 10:50:06.498
STEP: looking for the results for each expected name from probers 08/24/23 10:50:06.502
Aug 24 10:50:06.509: INFO: File wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local from pod  dns-8542/dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 24 10:50:06.513: INFO: File jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local from pod  dns-8542/dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 24 10:50:06.513: INFO: Lookups using dns-8542/dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 failed for: [wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local]

Aug 24 10:50:11.524: INFO: DNS probes using dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 succeeded

STEP: deleting the pod 08/24/23 10:50:11.524
STEP: changing the service to type=ClusterIP 08/24/23 10:50:11.558
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
 08/24/23 10:50:11.586
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
 08/24/23 10:50:11.586
STEP: creating a third pod to probe DNS 08/24/23 10:50:11.586
STEP: submitting the pod to kubernetes 08/24/23 10:50:11.592
Aug 24 10:50:11.604: INFO: Waiting up to 15m0s for pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f" in namespace "dns-8542" to be "running"
Aug 24 10:50:11.615: INFO: Pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.522079ms
Aug 24 10:50:13.620: INFO: Pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016640485s
Aug 24 10:50:13.620: INFO: Pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f" satisfied condition "running"
STEP: retrieving the pod 08/24/23 10:50:13.62
STEP: looking for the results for each expected name from probers 08/24/23 10:50:13.624
Aug 24 10:50:13.633: INFO: DNS probes using dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f succeeded

STEP: deleting the pod 08/24/23 10:50:13.634
STEP: deleting the test externalName service 08/24/23 10:50:13.657
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 10:50:13.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8542" for this suite. 08/24/23 10:50:13.702
------------------------------
â€¢ [SLOW TEST] [79.401 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:48:54.312
    Aug 24 10:48:54.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 10:48:54.314
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:48:54.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:48:54.341
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/24/23 10:48:54.348
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
     08/24/23 10:48:54.36
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
     08/24/23 10:48:54.36
    STEP: creating a pod to probe DNS 08/24/23 10:48:54.361
    STEP: submitting the pod to kubernetes 08/24/23 10:48:54.361
    Aug 24 10:48:54.385: INFO: Waiting up to 15m0s for pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7" in namespace "dns-8542" to be "running"
    Aug 24 10:48:54.396: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.291536ms
    Aug 24 10:48:56.403: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018107579s
    Aug 24 10:48:58.403: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018455637s
    Aug 24 10:49:00.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017116415s
    Aug 24 10:49:02.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01601475s
    Aug 24 10:49:04.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016398556s
    Aug 24 10:49:06.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017564166s
    Aug 24 10:49:08.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015010315s
    Aug 24 10:49:10.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.01544144s
    Aug 24 10:49:12.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.015118213s
    Aug 24 10:49:14.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015204553s
    Aug 24 10:49:16.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016189702s
    Aug 24 10:49:18.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015872635s
    Aug 24 10:49:20.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.017637577s
    Aug 24 10:49:22.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 28.015073503s
    Aug 24 10:49:24.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017215303s
    Aug 24 10:49:26.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.017243591s
    Aug 24 10:49:28.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016285438s
    Aug 24 10:49:30.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 36.016486267s
    Aug 24 10:49:32.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01586114s
    Aug 24 10:49:34.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 40.016676529s
    Aug 24 10:49:36.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017677473s
    Aug 24 10:49:38.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016840105s
    Aug 24 10:49:40.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016428198s
    Aug 24 10:49:42.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 48.016306013s
    Aug 24 10:49:44.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016442984s
    Aug 24 10:49:46.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 52.017255119s
    Aug 24 10:49:48.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 54.016020002s
    Aug 24 10:49:50.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 56.015538386s
    Aug 24 10:49:52.400: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 58.01511018s
    Aug 24 10:49:54.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.015884007s
    Aug 24 10:49:56.402: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.01679194s
    Aug 24 10:49:58.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015869066s
    Aug 24 10:50:00.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015859148s
    Aug 24 10:50:02.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.016431062s
    Aug 24 10:50:04.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7": Phase="Running", Reason="", readiness=false. Elapsed: 1m10.01642971s
    Aug 24 10:50:04.401: INFO: Pod "dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 10:50:04.401
    STEP: looking for the results for each expected name from probers 08/24/23 10:50:04.404
    Aug 24 10:50:04.414: INFO: DNS probes using dns-test-53b20b4d-8627-4965-a18a-c67fda2867c7 succeeded

    STEP: deleting the pod 08/24/23 10:50:04.414
    STEP: changing the externalName to bar.example.com 08/24/23 10:50:04.44
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
     08/24/23 10:50:04.461
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
     08/24/23 10:50:04.461
    STEP: creating a second pod to probe DNS 08/24/23 10:50:04.461
    STEP: submitting the pod to kubernetes 08/24/23 10:50:04.461
    Aug 24 10:50:04.472: INFO: Waiting up to 15m0s for pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2" in namespace "dns-8542" to be "running"
    Aug 24 10:50:04.492: INFO: Pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2": Phase="Pending", Reason="", readiness=false. Elapsed: 19.749077ms
    Aug 24 10:50:06.497: INFO: Pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.025146543s
    Aug 24 10:50:06.498: INFO: Pod "dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 10:50:06.498
    STEP: looking for the results for each expected name from probers 08/24/23 10:50:06.502
    Aug 24 10:50:06.509: INFO: File wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local from pod  dns-8542/dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 24 10:50:06.513: INFO: File jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local from pod  dns-8542/dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 24 10:50:06.513: INFO: Lookups using dns-8542/dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 failed for: [wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local]

    Aug 24 10:50:11.524: INFO: DNS probes using dns-test-fd3bf766-723c-410e-ab96-b2884a6c14e2 succeeded

    STEP: deleting the pod 08/24/23 10:50:11.524
    STEP: changing the service to type=ClusterIP 08/24/23 10:50:11.558
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
     08/24/23 10:50:11.586
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8542.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8542.svc.cluster.local; sleep 1; done
     08/24/23 10:50:11.586
    STEP: creating a third pod to probe DNS 08/24/23 10:50:11.586
    STEP: submitting the pod to kubernetes 08/24/23 10:50:11.592
    Aug 24 10:50:11.604: INFO: Waiting up to 15m0s for pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f" in namespace "dns-8542" to be "running"
    Aug 24 10:50:11.615: INFO: Pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.522079ms
    Aug 24 10:50:13.620: INFO: Pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016640485s
    Aug 24 10:50:13.620: INFO: Pod "dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 10:50:13.62
    STEP: looking for the results for each expected name from probers 08/24/23 10:50:13.624
    Aug 24 10:50:13.633: INFO: DNS probes using dns-test-764c42d8-fef1-4a55-9c49-d07fb1c4f45f succeeded

    STEP: deleting the pod 08/24/23 10:50:13.634
    STEP: deleting the test externalName service 08/24/23 10:50:13.657
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:50:13.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8542" for this suite. 08/24/23 10:50:13.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:50:13.715
Aug 24 10:50:13.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubelet-test 08/24/23 10:50:13.718
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:13.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:13.751
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 24 10:50:13.773: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc" in namespace "kubelet-test-120" to be "running and ready"
Aug 24 10:50:13.802: INFO: Pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc": Phase="Pending", Reason="", readiness=false. Elapsed: 28.466092ms
Aug 24 10:50:13.802: INFO: The phase of Pod busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:50:15.807: INFO: Pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.033588292s
Aug 24 10:50:15.807: INFO: The phase of Pod busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc is Running (Ready = true)
Aug 24 10:50:15.807: INFO: Pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 10:50:15.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-120" for this suite. 08/24/23 10:50:15.873
------------------------------
â€¢ [2.169 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:50:13.715
    Aug 24 10:50:13.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 10:50:13.718
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:13.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:13.751
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 24 10:50:13.773: INFO: Waiting up to 5m0s for pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc" in namespace "kubelet-test-120" to be "running and ready"
    Aug 24 10:50:13.802: INFO: Pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc": Phase="Pending", Reason="", readiness=false. Elapsed: 28.466092ms
    Aug 24 10:50:13.802: INFO: The phase of Pod busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:50:15.807: INFO: Pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.033588292s
    Aug 24 10:50:15.807: INFO: The phase of Pod busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc is Running (Ready = true)
    Aug 24 10:50:15.807: INFO: Pod "busybox-scheduling-fbb9f769-443c-480a-9830-e18853a2c3bc" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:50:15.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-120" for this suite. 08/24/23 10:50:15.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:50:15.885
Aug 24 10:50:15.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 10:50:15.886
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:15.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:15.961
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/24/23 10:50:32.973
STEP: Creating a ResourceQuota 08/24/23 10:50:37.98
STEP: Ensuring resource quota status is calculated 08/24/23 10:50:37.99
STEP: Creating a ConfigMap 08/24/23 10:50:39.995
STEP: Ensuring resource quota status captures configMap creation 08/24/23 10:50:40.019
STEP: Deleting a ConfigMap 08/24/23 10:50:42.028
STEP: Ensuring resource quota status released usage 08/24/23 10:50:42.039
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 10:50:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6435" for this suite. 08/24/23 10:50:44.053
------------------------------
â€¢ [SLOW TEST] [28.183 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:50:15.885
    Aug 24 10:50:15.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 10:50:15.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:15.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:15.961
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/24/23 10:50:32.973
    STEP: Creating a ResourceQuota 08/24/23 10:50:37.98
    STEP: Ensuring resource quota status is calculated 08/24/23 10:50:37.99
    STEP: Creating a ConfigMap 08/24/23 10:50:39.995
    STEP: Ensuring resource quota status captures configMap creation 08/24/23 10:50:40.019
    STEP: Deleting a ConfigMap 08/24/23 10:50:42.028
    STEP: Ensuring resource quota status released usage 08/24/23 10:50:42.039
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:50:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6435" for this suite. 08/24/23 10:50:44.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:50:44.072
Aug 24 10:50:44.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 10:50:44.074
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:44.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:44.118
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 10:50:44.162
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:50:45.477
STEP: Deploying the webhook pod 08/24/23 10:50:45.49
STEP: Wait for the deployment to be ready 08/24/23 10:50:45.512
Aug 24 10:50:45.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 10:50:47.543
STEP: Verifying the service has paired with the endpoint 08/24/23 10:50:47.572
Aug 24 10:50:48.573: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 10:50:48.578
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 10:50:48.605
STEP: Creating a dummy validating-webhook-configuration object 08/24/23 10:50:48.627
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/24/23 10:50:48.638
STEP: Creating a dummy mutating-webhook-configuration object 08/24/23 10:50:48.65
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/24/23 10:50:48.663
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:50:48.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1524" for this suite. 08/24/23 10:50:48.788
STEP: Destroying namespace "webhook-1524-markers" for this suite. 08/24/23 10:50:48.815
------------------------------
â€¢ [4.763 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:50:44.072
    Aug 24 10:50:44.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 10:50:44.074
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:44.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:44.118
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 10:50:44.162
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:50:45.477
    STEP: Deploying the webhook pod 08/24/23 10:50:45.49
    STEP: Wait for the deployment to be ready 08/24/23 10:50:45.512
    Aug 24 10:50:45.532: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 10:50:47.543
    STEP: Verifying the service has paired with the endpoint 08/24/23 10:50:47.572
    Aug 24 10:50:48.573: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 10:50:48.578
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/24/23 10:50:48.605
    STEP: Creating a dummy validating-webhook-configuration object 08/24/23 10:50:48.627
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/24/23 10:50:48.638
    STEP: Creating a dummy mutating-webhook-configuration object 08/24/23 10:50:48.65
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/24/23 10:50:48.663
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:50:48.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1524" for this suite. 08/24/23 10:50:48.788
    STEP: Destroying namespace "webhook-1524-markers" for this suite. 08/24/23 10:50:48.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:50:48.837
Aug 24 10:50:48.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename daemonsets 08/24/23 10:50:48.839
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:48.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:48.893
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Aug 24 10:50:48.937: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/24/23 10:50:48.95
Aug 24 10:50:48.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:48.956: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/24/23 10:50:48.956
Aug 24 10:50:49.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:49.016: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
Aug 24 10:50:50.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:50.021: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
Aug 24 10:50:51.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 10:50:51.020: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/24/23 10:50:51.025
Aug 24 10:50:51.059: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 10:50:51.059: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 24 10:50:52.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:52.065: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/24/23 10:50:52.065
Aug 24 10:50:52.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:52.097: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
Aug 24 10:50:53.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:53.103: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
Aug 24 10:50:54.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:54.109: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
Aug 24 10:50:55.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:55.103: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
Aug 24 10:50:56.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 10:50:56.102: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 10:50:56.109
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-157, will wait for the garbage collector to delete the pods 08/24/23 10:50:56.11
Aug 24 10:50:56.173: INFO: Deleting DaemonSet.extensions daemon-set took: 9.51959ms
Aug 24 10:50:56.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.008558ms
Aug 24 10:50:59.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:59.080: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 10:50:59.090: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8757"},"items":null}

Aug 24 10:50:59.100: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8757"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:50:59.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-157" for this suite. 08/24/23 10:50:59.297
------------------------------
â€¢ [SLOW TEST] [10.473 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:50:48.837
    Aug 24 10:50:48.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename daemonsets 08/24/23 10:50:48.839
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:48.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:48.893
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Aug 24 10:50:48.937: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/24/23 10:50:48.95
    Aug 24 10:50:48.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:48.956: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/24/23 10:50:48.956
    Aug 24 10:50:49.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:49.016: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
    Aug 24 10:50:50.021: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:50.021: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
    Aug 24 10:50:51.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 10:50:51.020: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/24/23 10:50:51.025
    Aug 24 10:50:51.059: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 10:50:51.059: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 24 10:50:52.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:52.065: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/24/23 10:50:52.065
    Aug 24 10:50:52.097: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:52.097: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
    Aug 24 10:50:53.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:53.103: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
    Aug 24 10:50:54.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:54.109: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
    Aug 24 10:50:55.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:55.103: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
    Aug 24 10:50:56.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 10:50:56.102: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 10:50:56.109
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-157, will wait for the garbage collector to delete the pods 08/24/23 10:50:56.11
    Aug 24 10:50:56.173: INFO: Deleting DaemonSet.extensions daemon-set took: 9.51959ms
    Aug 24 10:50:56.274: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.008558ms
    Aug 24 10:50:59.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:59.080: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 10:50:59.090: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8757"},"items":null}

    Aug 24 10:50:59.100: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8757"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:50:59.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-157" for this suite. 08/24/23 10:50:59.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:50:59.311
Aug 24 10:50:59.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename daemonsets 08/24/23 10:50:59.312
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:59.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:59.396
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 08/24/23 10:50:59.478
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 10:50:59.492
Aug 24 10:50:59.495: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 10:50:59.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:50:59.510: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 10:51:00.519: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 10:51:00.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:51:00.522: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 10:51:01.522: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 10:51:01.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 10:51:01.528: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/24/23 10:51:01.533
Aug 24 10:51:01.587: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 10:51:01.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 10:51:01.593: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 10:51:02.600: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 10:51:02.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 10:51:02.604: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 10:51:03.599: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 10:51:03.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 10:51:03.603: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/24/23 10:51:03.603
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 10:51:03.612
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7177, will wait for the garbage collector to delete the pods 08/24/23 10:51:03.612
Aug 24 10:51:03.676: INFO: Deleting DaemonSet.extensions daemon-set took: 9.875061ms
Aug 24 10:51:03.776: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.647679ms
Aug 24 10:51:05.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 10:51:05.896: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 10:51:05.900: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8899"},"items":null}

Aug 24 10:51:05.904: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8899"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:51:05.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7177" for this suite. 08/24/23 10:51:05.938
------------------------------
â€¢ [SLOW TEST] [6.637 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:50:59.311
    Aug 24 10:50:59.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename daemonsets 08/24/23 10:50:59.312
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:50:59.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:50:59.396
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 08/24/23 10:50:59.478
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 10:50:59.492
    Aug 24 10:50:59.495: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 10:50:59.510: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:50:59.510: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 10:51:00.519: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 10:51:00.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:51:00.522: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 10:51:01.522: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 10:51:01.528: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 10:51:01.528: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/24/23 10:51:01.533
    Aug 24 10:51:01.587: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 10:51:01.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 10:51:01.593: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 10:51:02.600: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 10:51:02.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 10:51:02.604: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 10:51:03.599: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 10:51:03.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 10:51:03.603: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/24/23 10:51:03.603
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 10:51:03.612
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7177, will wait for the garbage collector to delete the pods 08/24/23 10:51:03.612
    Aug 24 10:51:03.676: INFO: Deleting DaemonSet.extensions daemon-set took: 9.875061ms
    Aug 24 10:51:03.776: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.647679ms
    Aug 24 10:51:05.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 10:51:05.896: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 10:51:05.900: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8899"},"items":null}

    Aug 24 10:51:05.904: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8899"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:51:05.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7177" for this suite. 08/24/23 10:51:05.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:51:05.953
Aug 24 10:51:05.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename gc 08/24/23 10:51:05.954
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:51:05.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:51:05.981
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/24/23 10:51:05.99
STEP: delete the rc 08/24/23 10:51:11.268
STEP: wait for the rc to be deleted 08/24/23 10:51:11.337
Aug 24 10:51:12.437: INFO: 80 pods remaining
Aug 24 10:51:12.437: INFO: 80 pods has nil DeletionTimestamp
Aug 24 10:51:12.437: INFO: 
Aug 24 10:51:13.387: INFO: 70 pods remaining
Aug 24 10:51:13.387: INFO: 70 pods has nil DeletionTimestamp
Aug 24 10:51:13.387: INFO: 
Aug 24 10:51:14.398: INFO: 60 pods remaining
Aug 24 10:51:14.398: INFO: 60 pods has nil DeletionTimestamp
Aug 24 10:51:14.398: INFO: 
Aug 24 10:51:15.368: INFO: 41 pods remaining
Aug 24 10:51:15.369: INFO: 41 pods has nil DeletionTimestamp
Aug 24 10:51:15.369: INFO: 
Aug 24 10:51:16.416: INFO: 30 pods remaining
Aug 24 10:51:16.416: INFO: 29 pods has nil DeletionTimestamp
Aug 24 10:51:16.416: INFO: 
Aug 24 10:51:17.398: INFO: 19 pods remaining
Aug 24 10:51:17.398: INFO: 19 pods has nil DeletionTimestamp
Aug 24 10:51:17.398: INFO: 
STEP: Gathering metrics 08/24/23 10:51:18.358
W0824 10:51:18.384496      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 24 10:51:18.384: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 10:51:18.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8535" for this suite. 08/24/23 10:51:18.419
------------------------------
â€¢ [SLOW TEST] [12.491 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:51:05.953
    Aug 24 10:51:05.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename gc 08/24/23 10:51:05.954
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:51:05.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:51:05.981
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/24/23 10:51:05.99
    STEP: delete the rc 08/24/23 10:51:11.268
    STEP: wait for the rc to be deleted 08/24/23 10:51:11.337
    Aug 24 10:51:12.437: INFO: 80 pods remaining
    Aug 24 10:51:12.437: INFO: 80 pods has nil DeletionTimestamp
    Aug 24 10:51:12.437: INFO: 
    Aug 24 10:51:13.387: INFO: 70 pods remaining
    Aug 24 10:51:13.387: INFO: 70 pods has nil DeletionTimestamp
    Aug 24 10:51:13.387: INFO: 
    Aug 24 10:51:14.398: INFO: 60 pods remaining
    Aug 24 10:51:14.398: INFO: 60 pods has nil DeletionTimestamp
    Aug 24 10:51:14.398: INFO: 
    Aug 24 10:51:15.368: INFO: 41 pods remaining
    Aug 24 10:51:15.369: INFO: 41 pods has nil DeletionTimestamp
    Aug 24 10:51:15.369: INFO: 
    Aug 24 10:51:16.416: INFO: 30 pods remaining
    Aug 24 10:51:16.416: INFO: 29 pods has nil DeletionTimestamp
    Aug 24 10:51:16.416: INFO: 
    Aug 24 10:51:17.398: INFO: 19 pods remaining
    Aug 24 10:51:17.398: INFO: 19 pods has nil DeletionTimestamp
    Aug 24 10:51:17.398: INFO: 
    STEP: Gathering metrics 08/24/23 10:51:18.358
    W0824 10:51:18.384496      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 24 10:51:18.384: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:51:18.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8535" for this suite. 08/24/23 10:51:18.419
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:51:18.445
Aug 24 10:51:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pod-network-test 08/24/23 10:51:18.448
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:51:18.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:51:18.568
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5430 08/24/23 10:51:18.615
STEP: creating a selector 08/24/23 10:51:18.615
STEP: Creating the service pods in kubernetes 08/24/23 10:51:18.616
Aug 24 10:51:18.616: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 10:51:18.732: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5430" to be "running and ready"
Aug 24 10:51:18.748: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.427492ms
Aug 24 10:51:18.748: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:20.756: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024011465s
Aug 24 10:51:20.756: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:22.764: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032032981s
Aug 24 10:51:22.764: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:24.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022002056s
Aug 24 10:51:24.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:26.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022720948s
Aug 24 10:51:26.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:28.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021695009s
Aug 24 10:51:28.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:30.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023023662s
Aug 24 10:51:30.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:32.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021279131s
Aug 24 10:51:32.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:34.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.02242217s
Aug 24 10:51:34.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:36.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.02262695s
Aug 24 10:51:36.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:38.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.02171995s
Aug 24 10:51:38.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:40.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022788114s
Aug 24 10:51:40.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:42.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020280304s
Aug 24 10:51:42.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:44.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021252818s
Aug 24 10:51:44.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:46.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023071306s
Aug 24 10:51:46.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:48.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022256452s
Aug 24 10:51:48.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:50.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023181869s
Aug 24 10:51:50.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:52.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022228181s
Aug 24 10:51:52.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:54.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.022334649s
Aug 24 10:51:54.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:56.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022658197s
Aug 24 10:51:56.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:51:58.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022892368s
Aug 24 10:51:58.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:00.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021323038s
Aug 24 10:52:00.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:02.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 44.021057377s
Aug 24 10:52:02.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:04.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 46.022161011s
Aug 24 10:52:04.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:06.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0217759s
Aug 24 10:52:06.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:08.763: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031266067s
Aug 24 10:52:08.763: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:10.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022852966s
Aug 24 10:52:10.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:12.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 54.021084928s
Aug 24 10:52:12.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:14.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020480492s
Aug 24 10:52:14.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:16.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020128321s
Aug 24 10:52:16.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:18.751: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01965913s
Aug 24 10:52:18.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:20.770: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.038609398s
Aug 24 10:52:20.770: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:52:22.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m4.020377254s
Aug 24 10:52:22.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:24.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m6.022429856s
Aug 24 10:52:24.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:26.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m8.022340901s
Aug 24 10:52:26.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:28.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m10.021287341s
Aug 24 10:52:28.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:30.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m12.021102501s
Aug 24 10:52:30.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:32.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m14.02265779s
Aug 24 10:52:32.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:34.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m16.022473947s
Aug 24 10:52:34.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:36.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m18.023443612s
Aug 24 10:52:36.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:38.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m20.020727315s
Aug 24 10:52:38.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:52:40.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 1m22.021688458s
Aug 24 10:52:40.753: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 10:52:40.753: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 10:52:40.756: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5430" to be "running and ready"
Aug 24 10:52:40.760: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.903984ms
Aug 24 10:52:40.760: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 10:52:40.760: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 10:52:40.763: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5430" to be "running and ready"
Aug 24 10:52:40.767: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.698729ms
Aug 24 10:52:40.767: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 10:52:40.767: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 10:52:40.77
Aug 24 10:52:40.807: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5430" to be "running"
Aug 24 10:52:40.821: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.354497ms
Aug 24 10:52:42.827: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019683016s
Aug 24 10:52:42.827: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 10:52:42.831: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5430" to be "running"
Aug 24 10:52:42.835: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.889038ms
Aug 24 10:52:42.835: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 24 10:52:42.838: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 10:52:42.838: INFO: Going to poll 10.100.148.240 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 24 10:52:42.841: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.148.240 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 10:52:42.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:52:42.842: INFO: ExecWithOptions: Clientset creation
Aug 24 10:52:42.843: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.148.240+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 10:52:44.005: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 24 10:52:44.005: INFO: Going to poll 10.100.181.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 24 10:52:44.009: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.181.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 10:52:44.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:52:44.009: INFO: ExecWithOptions: Clientset creation
Aug 24 10:52:44.009: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.181.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 10:52:45.149: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 24 10:52:45.149: INFO: Going to poll 10.100.45.189 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 24 10:52:45.152: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.45.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 10:52:45.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:52:45.153: INFO: ExecWithOptions: Clientset creation
Aug 24 10:52:45.153: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.45.189+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 10:52:46.297: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 10:52:46.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5430" for this suite. 08/24/23 10:52:46.303
------------------------------
â€¢ [SLOW TEST] [87.865 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:51:18.445
    Aug 24 10:51:18.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 10:51:18.448
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:51:18.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:51:18.568
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5430 08/24/23 10:51:18.615
    STEP: creating a selector 08/24/23 10:51:18.615
    STEP: Creating the service pods in kubernetes 08/24/23 10:51:18.616
    Aug 24 10:51:18.616: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 10:51:18.732: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5430" to be "running and ready"
    Aug 24 10:51:18.748: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.427492ms
    Aug 24 10:51:18.748: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:20.756: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024011465s
    Aug 24 10:51:20.756: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:22.764: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032032981s
    Aug 24 10:51:22.764: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:24.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022002056s
    Aug 24 10:51:24.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:26.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022720948s
    Aug 24 10:51:26.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:28.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021695009s
    Aug 24 10:51:28.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:30.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023023662s
    Aug 24 10:51:30.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:32.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.021279131s
    Aug 24 10:51:32.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:34.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.02242217s
    Aug 24 10:51:34.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:36.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.02262695s
    Aug 24 10:51:36.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:38.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.02171995s
    Aug 24 10:51:38.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:40.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022788114s
    Aug 24 10:51:40.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:42.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.020280304s
    Aug 24 10:51:42.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:44.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021252818s
    Aug 24 10:51:44.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:46.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.023071306s
    Aug 24 10:51:46.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:48.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.022256452s
    Aug 24 10:51:48.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:50.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.023181869s
    Aug 24 10:51:50.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:52.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022228181s
    Aug 24 10:51:52.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:54.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.022334649s
    Aug 24 10:51:54.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:56.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022658197s
    Aug 24 10:51:56.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:51:58.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022892368s
    Aug 24 10:51:58.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:00.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.021323038s
    Aug 24 10:52:00.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:02.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 44.021057377s
    Aug 24 10:52:02.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:04.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 46.022161011s
    Aug 24 10:52:04.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:06.754: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0217759s
    Aug 24 10:52:06.754: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:08.763: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031266067s
    Aug 24 10:52:08.763: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:10.755: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.022852966s
    Aug 24 10:52:10.755: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:12.753: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 54.021084928s
    Aug 24 10:52:12.753: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:14.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020480492s
    Aug 24 10:52:14.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:16.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020128321s
    Aug 24 10:52:16.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:18.751: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01965913s
    Aug 24 10:52:18.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:20.770: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.038609398s
    Aug 24 10:52:20.770: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:52:22.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m4.020377254s
    Aug 24 10:52:22.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:24.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m6.022429856s
    Aug 24 10:52:24.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:26.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m8.022340901s
    Aug 24 10:52:26.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:28.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m10.021287341s
    Aug 24 10:52:28.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:30.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m12.021102501s
    Aug 24 10:52:30.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:32.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m14.02265779s
    Aug 24 10:52:32.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:34.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m16.022473947s
    Aug 24 10:52:34.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:36.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m18.023443612s
    Aug 24 10:52:36.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:38.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 1m20.020727315s
    Aug 24 10:52:38.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:52:40.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 1m22.021688458s
    Aug 24 10:52:40.753: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 10:52:40.753: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 10:52:40.756: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5430" to be "running and ready"
    Aug 24 10:52:40.760: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.903984ms
    Aug 24 10:52:40.760: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 10:52:40.760: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 10:52:40.763: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5430" to be "running and ready"
    Aug 24 10:52:40.767: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.698729ms
    Aug 24 10:52:40.767: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 10:52:40.767: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 10:52:40.77
    Aug 24 10:52:40.807: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5430" to be "running"
    Aug 24 10:52:40.821: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.354497ms
    Aug 24 10:52:42.827: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019683016s
    Aug 24 10:52:42.827: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 10:52:42.831: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5430" to be "running"
    Aug 24 10:52:42.835: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.889038ms
    Aug 24 10:52:42.835: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 24 10:52:42.838: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 10:52:42.838: INFO: Going to poll 10.100.148.240 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 10:52:42.841: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.148.240 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 10:52:42.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:52:42.842: INFO: ExecWithOptions: Clientset creation
    Aug 24 10:52:42.843: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.148.240+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 10:52:44.005: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 24 10:52:44.005: INFO: Going to poll 10.100.181.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 10:52:44.009: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.181.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 10:52:44.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:52:44.009: INFO: ExecWithOptions: Clientset creation
    Aug 24 10:52:44.009: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.181.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 10:52:45.149: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 24 10:52:45.149: INFO: Going to poll 10.100.45.189 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 10:52:45.152: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.45.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5430 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 10:52:45.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:52:45.153: INFO: ExecWithOptions: Clientset creation
    Aug 24 10:52:45.153: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-5430/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.45.189+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 10:52:46.297: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:52:46.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5430" for this suite. 08/24/23 10:52:46.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:52:46.311
Aug 24 10:52:46.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename disruption 08/24/23 10:52:46.312
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:52:46.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:52:46.344
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/24/23 10:52:46.359
STEP: Waiting for all pods to be running 08/24/23 10:52:48.452
Aug 24 10:52:48.478: INFO: running pods: 0 < 3
Aug 24 10:52:50.482: INFO: running pods: 2 < 3
Aug 24 10:52:52.485: INFO: running pods: 2 < 3
Aug 24 10:52:54.482: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 10:52:56.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8436" for this suite. 08/24/23 10:52:56.498
------------------------------
â€¢ [SLOW TEST] [10.196 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:52:46.311
    Aug 24 10:52:46.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename disruption 08/24/23 10:52:46.312
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:52:46.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:52:46.344
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/24/23 10:52:46.359
    STEP: Waiting for all pods to be running 08/24/23 10:52:48.452
    Aug 24 10:52:48.478: INFO: running pods: 0 < 3
    Aug 24 10:52:50.482: INFO: running pods: 2 < 3
    Aug 24 10:52:52.485: INFO: running pods: 2 < 3
    Aug 24 10:52:54.482: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:52:56.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8436" for this suite. 08/24/23 10:52:56.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:52:56.512
Aug 24 10:52:56.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename subpath 08/24/23 10:52:56.514
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:52:56.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:52:56.561
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 10:52:56.567
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-bn8m 08/24/23 10:52:56.583
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 10:52:56.583
Aug 24 10:52:56.599: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bn8m" in namespace "subpath-7315" to be "Succeeded or Failed"
Aug 24 10:52:56.621: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Pending", Reason="", readiness=false. Elapsed: 21.57451ms
Aug 24 10:52:58.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 2.025267845s
Aug 24 10:53:00.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.025683884s
Aug 24 10:53:02.628: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 6.0287188s
Aug 24 10:53:04.627: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 8.027410428s
Aug 24 10:53:06.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 10.025825585s
Aug 24 10:53:08.626: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 12.026687803s
Aug 24 10:53:10.626: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 14.027077599s
Aug 24 10:53:12.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 16.026215225s
Aug 24 10:53:14.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 18.025270779s
Aug 24 10:53:16.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 20.025967209s
Aug 24 10:53:18.626: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=false. Elapsed: 22.026350384s
Aug 24 10:53:20.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02551376s
STEP: Saw pod success 08/24/23 10:53:20.625
Aug 24 10:53:20.625: INFO: Pod "pod-subpath-test-projected-bn8m" satisfied condition "Succeeded or Failed"
Aug 24 10:53:20.628: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-subpath-test-projected-bn8m container test-container-subpath-projected-bn8m: <nil>
STEP: delete the pod 08/24/23 10:53:20.695
Aug 24 10:53:20.716: INFO: Waiting for pod pod-subpath-test-projected-bn8m to disappear
Aug 24 10:53:20.722: INFO: Pod pod-subpath-test-projected-bn8m no longer exists
STEP: Deleting pod pod-subpath-test-projected-bn8m 08/24/23 10:53:20.722
Aug 24 10:53:20.723: INFO: Deleting pod "pod-subpath-test-projected-bn8m" in namespace "subpath-7315"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 10:53:20.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7315" for this suite. 08/24/23 10:53:20.73
------------------------------
â€¢ [SLOW TEST] [24.227 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:52:56.512
    Aug 24 10:52:56.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename subpath 08/24/23 10:52:56.514
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:52:56.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:52:56.561
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 10:52:56.567
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-bn8m 08/24/23 10:52:56.583
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 10:52:56.583
    Aug 24 10:52:56.599: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bn8m" in namespace "subpath-7315" to be "Succeeded or Failed"
    Aug 24 10:52:56.621: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Pending", Reason="", readiness=false. Elapsed: 21.57451ms
    Aug 24 10:52:58.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 2.025267845s
    Aug 24 10:53:00.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.025683884s
    Aug 24 10:53:02.628: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 6.0287188s
    Aug 24 10:53:04.627: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 8.027410428s
    Aug 24 10:53:06.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 10.025825585s
    Aug 24 10:53:08.626: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 12.026687803s
    Aug 24 10:53:10.626: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 14.027077599s
    Aug 24 10:53:12.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 16.026215225s
    Aug 24 10:53:14.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 18.025270779s
    Aug 24 10:53:16.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=true. Elapsed: 20.025967209s
    Aug 24 10:53:18.626: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Running", Reason="", readiness=false. Elapsed: 22.026350384s
    Aug 24 10:53:20.625: INFO: Pod "pod-subpath-test-projected-bn8m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.02551376s
    STEP: Saw pod success 08/24/23 10:53:20.625
    Aug 24 10:53:20.625: INFO: Pod "pod-subpath-test-projected-bn8m" satisfied condition "Succeeded or Failed"
    Aug 24 10:53:20.628: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-subpath-test-projected-bn8m container test-container-subpath-projected-bn8m: <nil>
    STEP: delete the pod 08/24/23 10:53:20.695
    Aug 24 10:53:20.716: INFO: Waiting for pod pod-subpath-test-projected-bn8m to disappear
    Aug 24 10:53:20.722: INFO: Pod pod-subpath-test-projected-bn8m no longer exists
    STEP: Deleting pod pod-subpath-test-projected-bn8m 08/24/23 10:53:20.722
    Aug 24 10:53:20.723: INFO: Deleting pod "pod-subpath-test-projected-bn8m" in namespace "subpath-7315"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:53:20.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7315" for this suite. 08/24/23 10:53:20.73
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:53:20.74
Aug 24 10:53:20.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename podtemplate 08/24/23 10:53:20.742
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:20.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:20.777
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/24/23 10:53:20.783
STEP: Replace a pod template 08/24/23 10:53:20.8
Aug 24 10:53:20.813: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 24 10:53:20.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5440" for this suite. 08/24/23 10:53:20.818
------------------------------
â€¢ [0.086 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:53:20.74
    Aug 24 10:53:20.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename podtemplate 08/24/23 10:53:20.742
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:20.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:20.777
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/24/23 10:53:20.783
    STEP: Replace a pod template 08/24/23 10:53:20.8
    Aug 24 10:53:20.813: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:53:20.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5440" for this suite. 08/24/23 10:53:20.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:53:20.828
Aug 24 10:53:20.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 10:53:20.829
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:20.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:20.855
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/24/23 10:53:20.861
Aug 24 10:53:20.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d" in namespace "projected-6587" to be "Succeeded or Failed"
Aug 24 10:53:20.887: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.552604ms
Aug 24 10:53:22.890: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009152404s
Aug 24 10:53:24.892: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010585009s
STEP: Saw pod success 08/24/23 10:53:24.892
Aug 24 10:53:24.892: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d" satisfied condition "Succeeded or Failed"
Aug 24 10:53:24.894: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d container client-container: <nil>
STEP: delete the pod 08/24/23 10:53:24.942
Aug 24 10:53:24.963: INFO: Waiting for pod downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d to disappear
Aug 24 10:53:24.976: INFO: Pod downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 10:53:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6587" for this suite. 08/24/23 10:53:24.983
------------------------------
â€¢ [4.165 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:53:20.828
    Aug 24 10:53:20.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 10:53:20.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:20.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:20.855
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/24/23 10:53:20.861
    Aug 24 10:53:20.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d" in namespace "projected-6587" to be "Succeeded or Failed"
    Aug 24 10:53:20.887: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.552604ms
    Aug 24 10:53:22.890: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009152404s
    Aug 24 10:53:24.892: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010585009s
    STEP: Saw pod success 08/24/23 10:53:24.892
    Aug 24 10:53:24.892: INFO: Pod "downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d" satisfied condition "Succeeded or Failed"
    Aug 24 10:53:24.894: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d container client-container: <nil>
    STEP: delete the pod 08/24/23 10:53:24.942
    Aug 24 10:53:24.963: INFO: Waiting for pod downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d to disappear
    Aug 24 10:53:24.976: INFO: Pod downwardapi-volume-bcfff709-f279-463d-b3c5-027ce1ad116d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:53:24.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6587" for this suite. 08/24/23 10:53:24.983
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:53:24.993
Aug 24 10:53:24.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 10:53:24.995
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:25.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:25.021
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 10:53:25.051
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:53:26.421
STEP: Deploying the webhook pod 08/24/23 10:53:26.43
STEP: Wait for the deployment to be ready 08/24/23 10:53:26.455
Aug 24 10:53:26.477: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 10:53:28.488
STEP: Verifying the service has paired with the endpoint 08/24/23 10:53:28.511
Aug 24 10:53:29.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 24 10:53:29.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/24/23 10:53:30.042
STEP: Creating a custom resource that should be denied by the webhook 08/24/23 10:53:30.072
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/24/23 10:53:32.118
STEP: Updating the custom resource with disallowed data should be denied 08/24/23 10:53:32.127
STEP: Deleting the custom resource should be denied 08/24/23 10:53:32.14
STEP: Remove the offending key and value from the custom resource data 08/24/23 10:53:32.156
STEP: Deleting the updated custom resource should be successful 08/24/23 10:53:32.168
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:53:32.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-565" for this suite. 08/24/23 10:53:32.857
STEP: Destroying namespace "webhook-565-markers" for this suite. 08/24/23 10:53:32.865
------------------------------
â€¢ [SLOW TEST] [7.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:53:24.993
    Aug 24 10:53:24.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 10:53:24.995
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:25.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:25.021
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 10:53:25.051
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 10:53:26.421
    STEP: Deploying the webhook pod 08/24/23 10:53:26.43
    STEP: Wait for the deployment to be ready 08/24/23 10:53:26.455
    Aug 24 10:53:26.477: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 10:53:28.488
    STEP: Verifying the service has paired with the endpoint 08/24/23 10:53:28.511
    Aug 24 10:53:29.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 24 10:53:29.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/24/23 10:53:30.042
    STEP: Creating a custom resource that should be denied by the webhook 08/24/23 10:53:30.072
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/24/23 10:53:32.118
    STEP: Updating the custom resource with disallowed data should be denied 08/24/23 10:53:32.127
    STEP: Deleting the custom resource should be denied 08/24/23 10:53:32.14
    STEP: Remove the offending key and value from the custom resource data 08/24/23 10:53:32.156
    STEP: Deleting the updated custom resource should be successful 08/24/23 10:53:32.168
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:53:32.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-565" for this suite. 08/24/23 10:53:32.857
    STEP: Destroying namespace "webhook-565-markers" for this suite. 08/24/23 10:53:32.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:53:32.892
Aug 24 10:53:32.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svcaccounts 08/24/23 10:53:32.893
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:32.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:32.984
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/24/23 10:53:33.014
Aug 24 10:53:33.060: INFO: Waiting up to 5m0s for pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746" in namespace "svcaccounts-3101" to be "Succeeded or Failed"
Aug 24 10:53:33.077: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746": Phase="Pending", Reason="", readiness=false. Elapsed: 17.188887ms
Aug 24 10:53:35.083: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022777161s
Aug 24 10:53:37.083: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022876844s
STEP: Saw pod success 08/24/23 10:53:37.083
Aug 24 10:53:37.083: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746" satisfied condition "Succeeded or Failed"
Aug 24 10:53:37.086: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 10:53:37.094
Aug 24 10:53:37.118: INFO: Waiting for pod test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746 to disappear
Aug 24 10:53:37.123: INFO: Pod test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 10:53:37.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3101" for this suite. 08/24/23 10:53:37.128
------------------------------
â€¢ [4.244 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:53:32.892
    Aug 24 10:53:32.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 10:53:32.893
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:32.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:32.984
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/24/23 10:53:33.014
    Aug 24 10:53:33.060: INFO: Waiting up to 5m0s for pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746" in namespace "svcaccounts-3101" to be "Succeeded or Failed"
    Aug 24 10:53:33.077: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746": Phase="Pending", Reason="", readiness=false. Elapsed: 17.188887ms
    Aug 24 10:53:35.083: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022777161s
    Aug 24 10:53:37.083: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022876844s
    STEP: Saw pod success 08/24/23 10:53:37.083
    Aug 24 10:53:37.083: INFO: Pod "test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746" satisfied condition "Succeeded or Failed"
    Aug 24 10:53:37.086: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 10:53:37.094
    Aug 24 10:53:37.118: INFO: Waiting for pod test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746 to disappear
    Aug 24 10:53:37.123: INFO: Pod test-pod-7f9bd85f-8d0c-4c74-a31e-c65b0eacc746 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:53:37.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3101" for this suite. 08/24/23 10:53:37.128
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:53:37.139
Aug 24 10:53:37.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 10:53:37.14
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:37.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:37.165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/24/23 10:53:37.174
Aug 24 10:53:37.189: INFO: Waiting up to 5m0s for pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a" in namespace "downward-api-1819" to be "Succeeded or Failed"
Aug 24 10:53:37.195: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222321ms
Aug 24 10:53:39.201: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012058106s
Aug 24 10:53:41.201: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01170772s
STEP: Saw pod success 08/24/23 10:53:41.201
Aug 24 10:53:41.201: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a" satisfied condition "Succeeded or Failed"
Aug 24 10:53:41.204: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a container dapi-container: <nil>
STEP: delete the pod 08/24/23 10:53:41.213
Aug 24 10:53:41.241: INFO: Waiting for pod downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a to disappear
Aug 24 10:53:41.249: INFO: Pod downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 10:53:41.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1819" for this suite. 08/24/23 10:53:41.254
------------------------------
â€¢ [4.129 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:53:37.139
    Aug 24 10:53:37.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 10:53:37.14
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:37.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:37.165
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/24/23 10:53:37.174
    Aug 24 10:53:37.189: INFO: Waiting up to 5m0s for pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a" in namespace "downward-api-1819" to be "Succeeded or Failed"
    Aug 24 10:53:37.195: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222321ms
    Aug 24 10:53:39.201: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012058106s
    Aug 24 10:53:41.201: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01170772s
    STEP: Saw pod success 08/24/23 10:53:41.201
    Aug 24 10:53:41.201: INFO: Pod "downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a" satisfied condition "Succeeded or Failed"
    Aug 24 10:53:41.204: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a container dapi-container: <nil>
    STEP: delete the pod 08/24/23 10:53:41.213
    Aug 24 10:53:41.241: INFO: Waiting for pod downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a to disappear
    Aug 24 10:53:41.249: INFO: Pod downward-api-0a3d9abc-7aa8-4baf-93f0-27101657672a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:53:41.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1819" for this suite. 08/24/23 10:53:41.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:53:41.269
Aug 24 10:53:41.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pod-network-test 08/24/23 10:53:41.27
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:41.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:41.303
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7580 08/24/23 10:53:41.311
STEP: creating a selector 08/24/23 10:53:41.311
STEP: Creating the service pods in kubernetes 08/24/23 10:53:41.312
Aug 24 10:53:41.312: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 10:53:41.379: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7580" to be "running and ready"
Aug 24 10:53:41.399: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.036369ms
Aug 24 10:53:41.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:53:43.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023764721s
Aug 24 10:53:43.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:45.418: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.038435766s
Aug 24 10:53:45.418: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:47.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023771597s
Aug 24 10:53:47.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:49.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025198938s
Aug 24 10:53:49.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:51.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024571575s
Aug 24 10:53:51.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:53.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.026293079s
Aug 24 10:53:53.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:55.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.023944064s
Aug 24 10:53:55.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:57.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025528022s
Aug 24 10:53:57.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:53:59.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.02484388s
Aug 24 10:53:59.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:54:01.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.026101175s
Aug 24 10:54:01.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 10:54:03.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025110199s
Aug 24 10:54:03.405: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 10:54:03.405: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 10:54:03.408: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7580" to be "running and ready"
Aug 24 10:54:03.412: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.118199ms
Aug 24 10:54:03.412: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 10:54:03.412: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 10:54:03.428: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7580" to be "running and ready"
Aug 24 10:54:03.432: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.318281ms
Aug 24 10:54:03.432: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 10:54:03.432: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 10:54:03.435
Aug 24 10:54:03.454: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7580" to be "running"
Aug 24 10:54:03.465: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.338248ms
Aug 24 10:54:05.469: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015704611s
Aug 24 10:54:05.470: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 10:54:05.472: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7580" to be "running"
Aug 24 10:54:05.476: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.630527ms
Aug 24 10:54:05.476: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 24 10:54:05.478: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 10:54:05.478: INFO: Going to poll 10.100.148.241 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 24 10:54:05.481: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.148.241:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7580 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 10:54:05.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:54:05.482: INFO: ExecWithOptions: Clientset creation
Aug 24 10:54:05.482: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7580/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.148.241%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 10:54:05.642: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 24 10:54:05.642: INFO: Going to poll 10.100.181.174 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 24 10:54:05.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.181.174:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7580 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 10:54:05.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:54:05.646: INFO: ExecWithOptions: Clientset creation
Aug 24 10:54:05.646: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7580/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.181.174%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 10:54:05.768: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 24 10:54:05.768: INFO: Going to poll 10.100.45.134 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 24 10:54:05.772: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.45.134:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7580 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 10:54:05.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 10:54:05.773: INFO: ExecWithOptions: Clientset creation
Aug 24 10:54:05.773: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7580/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.45.134%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 10:54:05.897: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 10:54:05.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7580" for this suite. 08/24/23 10:54:05.903
------------------------------
â€¢ [SLOW TEST] [24.645 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:53:41.269
    Aug 24 10:53:41.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 10:53:41.27
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:53:41.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:53:41.303
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7580 08/24/23 10:53:41.311
    STEP: creating a selector 08/24/23 10:53:41.311
    STEP: Creating the service pods in kubernetes 08/24/23 10:53:41.312
    Aug 24 10:53:41.312: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 10:53:41.379: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7580" to be "running and ready"
    Aug 24 10:53:41.399: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.036369ms
    Aug 24 10:53:41.400: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:53:43.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.023764721s
    Aug 24 10:53:43.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:45.418: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.038435766s
    Aug 24 10:53:45.418: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:47.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.023771597s
    Aug 24 10:53:47.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:49.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.025198938s
    Aug 24 10:53:49.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:51.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.024571575s
    Aug 24 10:53:51.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:53.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.026293079s
    Aug 24 10:53:53.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:55.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.023944064s
    Aug 24 10:53:55.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:57.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.025528022s
    Aug 24 10:53:57.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:53:59.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.02484388s
    Aug 24 10:53:59.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:54:01.406: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.026101175s
    Aug 24 10:54:01.406: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 10:54:03.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.025110199s
    Aug 24 10:54:03.405: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 10:54:03.405: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 10:54:03.408: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7580" to be "running and ready"
    Aug 24 10:54:03.412: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.118199ms
    Aug 24 10:54:03.412: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 10:54:03.412: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 10:54:03.428: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7580" to be "running and ready"
    Aug 24 10:54:03.432: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.318281ms
    Aug 24 10:54:03.432: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 10:54:03.432: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 10:54:03.435
    Aug 24 10:54:03.454: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7580" to be "running"
    Aug 24 10:54:03.465: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.338248ms
    Aug 24 10:54:05.469: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015704611s
    Aug 24 10:54:05.470: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 10:54:05.472: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7580" to be "running"
    Aug 24 10:54:05.476: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.630527ms
    Aug 24 10:54:05.476: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 24 10:54:05.478: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 10:54:05.478: INFO: Going to poll 10.100.148.241 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 10:54:05.481: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.148.241:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7580 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 10:54:05.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:54:05.482: INFO: ExecWithOptions: Clientset creation
    Aug 24 10:54:05.482: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7580/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.148.241%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 10:54:05.642: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 24 10:54:05.642: INFO: Going to poll 10.100.181.174 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 10:54:05.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.181.174:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7580 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 10:54:05.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:54:05.646: INFO: ExecWithOptions: Clientset creation
    Aug 24 10:54:05.646: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7580/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.181.174%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 10:54:05.768: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 24 10:54:05.768: INFO: Going to poll 10.100.45.134 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 24 10:54:05.772: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.45.134:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7580 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 10:54:05.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 10:54:05.773: INFO: ExecWithOptions: Clientset creation
    Aug 24 10:54:05.773: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-7580/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.45.134%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 10:54:05.897: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:54:05.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7580" for this suite. 08/24/23 10:54:05.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:54:05.915
Aug 24 10:54:05.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replication-controller 08/24/23 10:54:05.916
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:05.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:05.954
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-rs5rk" 08/24/23 10:54:05.959
Aug 24 10:54:05.966: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
Aug 24 10:54:06.975: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
Aug 24 10:54:06.979: INFO: Found 1 replicas for "e2e-rc-rs5rk" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-rs5rk" 08/24/23 10:54:06.979
STEP: Updating a scale subresource 08/24/23 10:54:06.984
STEP: Verifying replicas where modified for replication controller "e2e-rc-rs5rk" 08/24/23 10:54:06.995
Aug 24 10:54:06.995: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
Aug 24 10:54:07.998: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
Aug 24 10:54:08.002: INFO: Found 2 replicas for "e2e-rc-rs5rk" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 10:54:08.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1392" for this suite. 08/24/23 10:54:08.006
------------------------------
â€¢ [2.104 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:54:05.915
    Aug 24 10:54:05.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replication-controller 08/24/23 10:54:05.916
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:05.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:05.954
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-rs5rk" 08/24/23 10:54:05.959
    Aug 24 10:54:05.966: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
    Aug 24 10:54:06.975: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
    Aug 24 10:54:06.979: INFO: Found 1 replicas for "e2e-rc-rs5rk" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-rs5rk" 08/24/23 10:54:06.979
    STEP: Updating a scale subresource 08/24/23 10:54:06.984
    STEP: Verifying replicas where modified for replication controller "e2e-rc-rs5rk" 08/24/23 10:54:06.995
    Aug 24 10:54:06.995: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
    Aug 24 10:54:07.998: INFO: Get Replication Controller "e2e-rc-rs5rk" to confirm replicas
    Aug 24 10:54:08.002: INFO: Found 2 replicas for "e2e-rc-rs5rk" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:54:08.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1392" for this suite. 08/24/23 10:54:08.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:54:08.02
Aug 24 10:54:08.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename disruption 08/24/23 10:54:08.022
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:08.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:08.049
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:54:08.059
Aug 24 10:54:08.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename disruption-2 08/24/23 10:54:08.06
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:08.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:08.108
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/24/23 10:54:08.12
STEP: Waiting for the pdb to be processed 08/24/23 10:54:10.136
STEP: Waiting for the pdb to be processed 08/24/23 10:54:10.158
STEP: listing a collection of PDBs across all namespaces 08/24/23 10:54:12.167
STEP: listing a collection of PDBs in namespace disruption-6723 08/24/23 10:54:12.173
STEP: deleting a collection of PDBs 08/24/23 10:54:12.177
STEP: Waiting for the PDB collection to be deleted 08/24/23 10:54:12.192
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 24 10:54:12.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 10:54:12.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-6282" for this suite. 08/24/23 10:54:12.202
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6723" for this suite. 08/24/23 10:54:12.219
------------------------------
â€¢ [4.211 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:54:08.02
    Aug 24 10:54:08.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename disruption 08/24/23 10:54:08.022
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:08.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:08.049
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:54:08.059
    Aug 24 10:54:08.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename disruption-2 08/24/23 10:54:08.06
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:08.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:08.108
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/24/23 10:54:08.12
    STEP: Waiting for the pdb to be processed 08/24/23 10:54:10.136
    STEP: Waiting for the pdb to be processed 08/24/23 10:54:10.158
    STEP: listing a collection of PDBs across all namespaces 08/24/23 10:54:12.167
    STEP: listing a collection of PDBs in namespace disruption-6723 08/24/23 10:54:12.173
    STEP: deleting a collection of PDBs 08/24/23 10:54:12.177
    STEP: Waiting for the PDB collection to be deleted 08/24/23 10:54:12.192
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:54:12.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:54:12.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-6282" for this suite. 08/24/23 10:54:12.202
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6723" for this suite. 08/24/23 10:54:12.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:54:12.233
Aug 24 10:54:12.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 10:54:12.234
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:12.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:12.262
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/24/23 10:54:12.268
Aug 24 10:54:12.278: INFO: Waiting up to 5m0s for pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b" in namespace "var-expansion-4342" to be "Succeeded or Failed"
Aug 24 10:54:12.285: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.134346ms
Aug 24 10:54:14.290: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011575606s
Aug 24 10:54:16.291: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012374904s
STEP: Saw pod success 08/24/23 10:54:16.291
Aug 24 10:54:16.291: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b" satisfied condition "Succeeded or Failed"
Aug 24 10:54:16.295: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod var-expansion-df922a69-2731-4337-9858-5e58e29d996b container dapi-container: <nil>
STEP: delete the pod 08/24/23 10:54:16.305
Aug 24 10:54:16.325: INFO: Waiting for pod var-expansion-df922a69-2731-4337-9858-5e58e29d996b to disappear
Aug 24 10:54:16.330: INFO: Pod var-expansion-df922a69-2731-4337-9858-5e58e29d996b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 10:54:16.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4342" for this suite. 08/24/23 10:54:16.335
------------------------------
â€¢ [4.134 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:54:12.233
    Aug 24 10:54:12.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 10:54:12.234
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:12.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:12.262
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/24/23 10:54:12.268
    Aug 24 10:54:12.278: INFO: Waiting up to 5m0s for pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b" in namespace "var-expansion-4342" to be "Succeeded or Failed"
    Aug 24 10:54:12.285: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.134346ms
    Aug 24 10:54:14.290: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011575606s
    Aug 24 10:54:16.291: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012374904s
    STEP: Saw pod success 08/24/23 10:54:16.291
    Aug 24 10:54:16.291: INFO: Pod "var-expansion-df922a69-2731-4337-9858-5e58e29d996b" satisfied condition "Succeeded or Failed"
    Aug 24 10:54:16.295: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod var-expansion-df922a69-2731-4337-9858-5e58e29d996b container dapi-container: <nil>
    STEP: delete the pod 08/24/23 10:54:16.305
    Aug 24 10:54:16.325: INFO: Waiting for pod var-expansion-df922a69-2731-4337-9858-5e58e29d996b to disappear
    Aug 24 10:54:16.330: INFO: Pod var-expansion-df922a69-2731-4337-9858-5e58e29d996b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:54:16.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4342" for this suite. 08/24/23 10:54:16.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:54:16.37
Aug 24 10:54:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 10:54:16.372
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:16.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:16.406
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8476 08/24/23 10:54:16.413
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-8476 08/24/23 10:54:16.436
Aug 24 10:54:16.449: INFO: Found 0 stateful pods, waiting for 1
Aug 24 10:54:26.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/24/23 10:54:26.461
STEP: Getting /status 08/24/23 10:54:26.475
Aug 24 10:54:26.484: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/24/23 10:54:26.484
Aug 24 10:54:26.503: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/24/23 10:54:26.503
Aug 24 10:54:26.509: INFO: Observed &StatefulSet event: ADDED
Aug 24 10:54:26.509: INFO: Found Statefulset ss in namespace statefulset-8476 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 10:54:26.509: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/24/23 10:54:26.509
Aug 24 10:54:26.509: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 24 10:54:26.521: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/24/23 10:54:26.521
Aug 24 10:54:26.525: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 10:54:26.526: INFO: Deleting all statefulset in ns statefulset-8476
Aug 24 10:54:26.530: INFO: Scaling statefulset ss to 0
Aug 24 10:54:36.554: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 10:54:36.558: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 10:54:36.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8476" for this suite. 08/24/23 10:54:36.616
------------------------------
â€¢ [SLOW TEST] [20.259 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:54:16.37
    Aug 24 10:54:16.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 10:54:16.372
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:16.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:16.406
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8476 08/24/23 10:54:16.413
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-8476 08/24/23 10:54:16.436
    Aug 24 10:54:16.449: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 10:54:26.454: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/24/23 10:54:26.461
    STEP: Getting /status 08/24/23 10:54:26.475
    Aug 24 10:54:26.484: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/24/23 10:54:26.484
    Aug 24 10:54:26.503: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/24/23 10:54:26.503
    Aug 24 10:54:26.509: INFO: Observed &StatefulSet event: ADDED
    Aug 24 10:54:26.509: INFO: Found Statefulset ss in namespace statefulset-8476 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 10:54:26.509: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/24/23 10:54:26.509
    Aug 24 10:54:26.509: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 24 10:54:26.521: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/24/23 10:54:26.521
    Aug 24 10:54:26.525: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 10:54:26.526: INFO: Deleting all statefulset in ns statefulset-8476
    Aug 24 10:54:26.530: INFO: Scaling statefulset ss to 0
    Aug 24 10:54:36.554: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 10:54:36.558: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:54:36.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8476" for this suite. 08/24/23 10:54:36.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:54:36.629
Aug 24 10:54:36.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename taint-multiple-pods 08/24/23 10:54:36.631
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:36.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:36.658
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 24 10:54:36.668: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 10:55:36.715: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 24 10:55:36.720: INFO: Starting informer...
STEP: Starting pods... 08/24/23 10:55:36.72
Aug 24 10:55:36.862: INFO: Pod1 is running on gitlab-1-26-36460-guscsyka22xa-node-2. Tainting Node
Aug 24 10:55:37.089: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7316" to be "running"
Aug 24 10:55:37.091: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821184ms
Aug 24 10:55:39.096: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00715299s
Aug 24 10:55:39.096: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 24 10:55:39.096: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7316" to be "running"
Aug 24 10:55:39.099: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.756732ms
Aug 24 10:55:39.099: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 24 10:55:39.099: INFO: Pod2 is running on gitlab-1-26-36460-guscsyka22xa-node-2. Tainting Node
STEP: Trying to apply a taint on the Node 08/24/23 10:55:39.099
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 10:55:39.118
STEP: Waiting for Pod1 and Pod2 to be deleted 08/24/23 10:55:39.123
Aug 24 10:55:45.044: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 24 10:56:05.095: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 10:56:05.115
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:56:05.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7316" for this suite. 08/24/23 10:56:05.134
------------------------------
â€¢ [SLOW TEST] [88.536 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:54:36.629
    Aug 24 10:54:36.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename taint-multiple-pods 08/24/23 10:54:36.631
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:54:36.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:54:36.658
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 24 10:54:36.668: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 10:55:36.715: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 24 10:55:36.720: INFO: Starting informer...
    STEP: Starting pods... 08/24/23 10:55:36.72
    Aug 24 10:55:36.862: INFO: Pod1 is running on gitlab-1-26-36460-guscsyka22xa-node-2. Tainting Node
    Aug 24 10:55:37.089: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7316" to be "running"
    Aug 24 10:55:37.091: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.821184ms
    Aug 24 10:55:39.096: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00715299s
    Aug 24 10:55:39.096: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 24 10:55:39.096: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7316" to be "running"
    Aug 24 10:55:39.099: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.756732ms
    Aug 24 10:55:39.099: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 24 10:55:39.099: INFO: Pod2 is running on gitlab-1-26-36460-guscsyka22xa-node-2. Tainting Node
    STEP: Trying to apply a taint on the Node 08/24/23 10:55:39.099
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 10:55:39.118
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/24/23 10:55:39.123
    Aug 24 10:55:45.044: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 24 10:56:05.095: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 10:56:05.115
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:56:05.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7316" for this suite. 08/24/23 10:56:05.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:56:05.168
Aug 24 10:56:05.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svcaccounts 08/24/23 10:56:05.169
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:05.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:05.261
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Aug 24 10:56:05.313: INFO: created pod
Aug 24 10:56:05.313: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8080" to be "Succeeded or Failed"
Aug 24 10:56:05.321: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.88698ms
Aug 24 10:56:07.326: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012638893s
Aug 24 10:56:09.327: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013384283s
STEP: Saw pod success 08/24/23 10:56:09.327
Aug 24 10:56:09.327: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 24 10:56:39.330: INFO: polling logs
Aug 24 10:56:39.404: INFO: Pod logs: 
I0824 10:56:06.773950       1 log.go:198] OK: Got token
I0824 10:56:06.774003       1 log.go:198] validating with in-cluster discovery
I0824 10:56:06.774532       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0824 10:56:06.774565       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8080:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692875165, NotBefore:1692874565, IssuedAt:1692874565, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8080", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"731d8ed1-751e-4b1f-bc51-27490b4c4e45"}}}
I0824 10:56:06.861215       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0824 10:56:06.918262       1 log.go:198] OK: Validated signature on JWT
I0824 10:56:06.918470       1 log.go:198] OK: Got valid claims from token!
I0824 10:56:06.918514       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8080:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692875165, NotBefore:1692874565, IssuedAt:1692874565, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8080", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"731d8ed1-751e-4b1f-bc51-27490b4c4e45"}}}

Aug 24 10:56:39.404: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 10:56:39.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8080" for this suite. 08/24/23 10:56:39.427
------------------------------
â€¢ [SLOW TEST] [34.269 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:56:05.168
    Aug 24 10:56:05.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 10:56:05.169
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:05.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:05.261
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Aug 24 10:56:05.313: INFO: created pod
    Aug 24 10:56:05.313: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8080" to be "Succeeded or Failed"
    Aug 24 10:56:05.321: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.88698ms
    Aug 24 10:56:07.326: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012638893s
    Aug 24 10:56:09.327: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013384283s
    STEP: Saw pod success 08/24/23 10:56:09.327
    Aug 24 10:56:09.327: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 24 10:56:39.330: INFO: polling logs
    Aug 24 10:56:39.404: INFO: Pod logs: 
    I0824 10:56:06.773950       1 log.go:198] OK: Got token
    I0824 10:56:06.774003       1 log.go:198] validating with in-cluster discovery
    I0824 10:56:06.774532       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0824 10:56:06.774565       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8080:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692875165, NotBefore:1692874565, IssuedAt:1692874565, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8080", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"731d8ed1-751e-4b1f-bc51-27490b4c4e45"}}}
    I0824 10:56:06.861215       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0824 10:56:06.918262       1 log.go:198] OK: Validated signature on JWT
    I0824 10:56:06.918470       1 log.go:198] OK: Got valid claims from token!
    I0824 10:56:06.918514       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8080:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692875165, NotBefore:1692874565, IssuedAt:1692874565, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8080", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"731d8ed1-751e-4b1f-bc51-27490b4c4e45"}}}

    Aug 24 10:56:39.404: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:56:39.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8080" for this suite. 08/24/23 10:56:39.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:56:39.44
Aug 24 10:56:39.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 10:56:39.442
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:39.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:39.467
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/24/23 10:56:39.475
Aug 24 10:56:39.493: INFO: Waiting up to 5m0s for pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53" in namespace "projected-2898" to be "running and ready"
Aug 24 10:56:39.499: INFO: Pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.141885ms
Aug 24 10:56:39.499: INFO: The phase of Pod annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 10:56:41.504: INFO: Pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53": Phase="Running", Reason="", readiness=true. Elapsed: 2.011047007s
Aug 24 10:56:41.504: INFO: The phase of Pod annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53 is Running (Ready = true)
Aug 24 10:56:41.504: INFO: Pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53" satisfied condition "running and ready"
Aug 24 10:56:42.037: INFO: Successfully updated pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 10:56:44.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2898" for this suite. 08/24/23 10:56:44.06
------------------------------
â€¢ [4.630 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:56:39.44
    Aug 24 10:56:39.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 10:56:39.442
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:39.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:39.467
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/24/23 10:56:39.475
    Aug 24 10:56:39.493: INFO: Waiting up to 5m0s for pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53" in namespace "projected-2898" to be "running and ready"
    Aug 24 10:56:39.499: INFO: Pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.141885ms
    Aug 24 10:56:39.499: INFO: The phase of Pod annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 10:56:41.504: INFO: Pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53": Phase="Running", Reason="", readiness=true. Elapsed: 2.011047007s
    Aug 24 10:56:41.504: INFO: The phase of Pod annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53 is Running (Ready = true)
    Aug 24 10:56:41.504: INFO: Pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53" satisfied condition "running and ready"
    Aug 24 10:56:42.037: INFO: Successfully updated pod "annotationupdate6fc5d3eb-de33-4fb0-a959-9dd91b84ad53"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:56:44.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2898" for this suite. 08/24/23 10:56:44.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:56:44.071
Aug 24 10:56:44.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename gc 08/24/23 10:56:44.072
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:44.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:44.096
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/24/23 10:56:44.103
STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 10:56:44.111
STEP: delete the deployment 08/24/23 10:56:44.624
STEP: wait for all rs to be garbage collected 08/24/23 10:56:44.636
STEP: expected 0 rs, got 1 rs 08/24/23 10:56:44.67
STEP: expected 0 pods, got 2 pods 08/24/23 10:56:44.681
STEP: Gathering metrics 08/24/23 10:56:45.188
W0824 10:56:45.215059      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 24 10:56:45.215: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 10:56:45.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4452" for this suite. 08/24/23 10:56:45.219
------------------------------
â€¢ [1.157 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:56:44.071
    Aug 24 10:56:44.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename gc 08/24/23 10:56:44.072
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:44.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:44.096
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/24/23 10:56:44.103
    STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 10:56:44.111
    STEP: delete the deployment 08/24/23 10:56:44.624
    STEP: wait for all rs to be garbage collected 08/24/23 10:56:44.636
    STEP: expected 0 rs, got 1 rs 08/24/23 10:56:44.67
    STEP: expected 0 pods, got 2 pods 08/24/23 10:56:44.681
    STEP: Gathering metrics 08/24/23 10:56:45.188
    W0824 10:56:45.215059      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 24 10:56:45.215: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:56:45.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4452" for this suite. 08/24/23 10:56:45.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:56:45.23
Aug 24 10:56:45.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename subpath 08/24/23 10:56:45.232
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:45.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:45.264
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 10:56:45.276
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-nkxj 08/24/23 10:56:45.321
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 10:56:45.321
Aug 24 10:56:45.337: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nkxj" in namespace "subpath-6599" to be "Succeeded or Failed"
Aug 24 10:56:45.367: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Pending", Reason="", readiness=false. Elapsed: 30.289053ms
Aug 24 10:56:47.373: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.036343343s
Aug 24 10:56:49.374: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 4.037193282s
Aug 24 10:56:51.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.034894402s
Aug 24 10:56:53.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 8.035341565s
Aug 24 10:56:55.371: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 10.034308237s
Aug 24 10:56:57.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 12.035615492s
Aug 24 10:56:59.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 14.03500826s
Aug 24 10:57:01.373: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 16.036425226s
Aug 24 10:57:03.371: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.034513256s
Aug 24 10:57:05.373: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 20.03610688s
Aug 24 10:57:07.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=false. Elapsed: 22.035454956s
Aug 24 10:57:09.376: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03882607s
STEP: Saw pod success 08/24/23 10:57:09.376
Aug 24 10:57:09.376: INFO: Pod "pod-subpath-test-configmap-nkxj" satisfied condition "Succeeded or Failed"
Aug 24 10:57:09.379: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-subpath-test-configmap-nkxj container test-container-subpath-configmap-nkxj: <nil>
STEP: delete the pod 08/24/23 10:57:09.447
Aug 24 10:57:09.468: INFO: Waiting for pod pod-subpath-test-configmap-nkxj to disappear
Aug 24 10:57:09.471: INFO: Pod pod-subpath-test-configmap-nkxj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-nkxj 08/24/23 10:57:09.474
Aug 24 10:57:09.474: INFO: Deleting pod "pod-subpath-test-configmap-nkxj" in namespace "subpath-6599"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 10:57:09.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6599" for this suite. 08/24/23 10:57:09.484
------------------------------
â€¢ [SLOW TEST] [24.264 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:56:45.23
    Aug 24 10:56:45.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename subpath 08/24/23 10:56:45.232
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:56:45.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:56:45.264
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 10:56:45.276
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-nkxj 08/24/23 10:56:45.321
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 10:56:45.321
    Aug 24 10:56:45.337: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-nkxj" in namespace "subpath-6599" to be "Succeeded or Failed"
    Aug 24 10:56:45.367: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Pending", Reason="", readiness=false. Elapsed: 30.289053ms
    Aug 24 10:56:47.373: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.036343343s
    Aug 24 10:56:49.374: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 4.037193282s
    Aug 24 10:56:51.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.034894402s
    Aug 24 10:56:53.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 8.035341565s
    Aug 24 10:56:55.371: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 10.034308237s
    Aug 24 10:56:57.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 12.035615492s
    Aug 24 10:56:59.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 14.03500826s
    Aug 24 10:57:01.373: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 16.036425226s
    Aug 24 10:57:03.371: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.034513256s
    Aug 24 10:57:05.373: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=true. Elapsed: 20.03610688s
    Aug 24 10:57:07.372: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Running", Reason="", readiness=false. Elapsed: 22.035454956s
    Aug 24 10:57:09.376: INFO: Pod "pod-subpath-test-configmap-nkxj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.03882607s
    STEP: Saw pod success 08/24/23 10:57:09.376
    Aug 24 10:57:09.376: INFO: Pod "pod-subpath-test-configmap-nkxj" satisfied condition "Succeeded or Failed"
    Aug 24 10:57:09.379: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-subpath-test-configmap-nkxj container test-container-subpath-configmap-nkxj: <nil>
    STEP: delete the pod 08/24/23 10:57:09.447
    Aug 24 10:57:09.468: INFO: Waiting for pod pod-subpath-test-configmap-nkxj to disappear
    Aug 24 10:57:09.471: INFO: Pod pod-subpath-test-configmap-nkxj no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-nkxj 08/24/23 10:57:09.474
    Aug 24 10:57:09.474: INFO: Deleting pod "pod-subpath-test-configmap-nkxj" in namespace "subpath-6599"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:57:09.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6599" for this suite. 08/24/23 10:57:09.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:57:09.495
Aug 24 10:57:09.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sysctl 08/24/23 10:57:09.497
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:57:09.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:57:09.525
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/24/23 10:57:09.532
STEP: Watching for error events or started pod 08/24/23 10:57:09.546
STEP: Waiting for pod completion 08/24/23 10:57:11.552
Aug 24 10:57:11.552: INFO: Waiting up to 3m0s for pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8" in namespace "sysctl-4219" to be "completed"
Aug 24 10:57:11.556: INFO: Pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138635ms
Aug 24 10:57:13.561: INFO: Pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008767838s
Aug 24 10:57:13.561: INFO: Pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/24/23 10:57:13.564
STEP: Getting logs from the pod 08/24/23 10:57:13.564
STEP: Checking that the sysctl is actually updated 08/24/23 10:57:13.57
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 10:57:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4219" for this suite. 08/24/23 10:57:13.574
------------------------------
â€¢ [4.089 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:57:09.495
    Aug 24 10:57:09.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sysctl 08/24/23 10:57:09.497
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:57:09.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:57:09.525
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/24/23 10:57:09.532
    STEP: Watching for error events or started pod 08/24/23 10:57:09.546
    STEP: Waiting for pod completion 08/24/23 10:57:11.552
    Aug 24 10:57:11.552: INFO: Waiting up to 3m0s for pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8" in namespace "sysctl-4219" to be "completed"
    Aug 24 10:57:11.556: INFO: Pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.138635ms
    Aug 24 10:57:13.561: INFO: Pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008767838s
    Aug 24 10:57:13.561: INFO: Pod "sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/24/23 10:57:13.564
    STEP: Getting logs from the pod 08/24/23 10:57:13.564
    STEP: Checking that the sysctl is actually updated 08/24/23 10:57:13.57
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 10:57:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4219" for this suite. 08/24/23 10:57:13.574
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 10:57:13.588
Aug 24 10:57:13.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-pred 08/24/23 10:57:13.59
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:57:13.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:57:13.624
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 10:57:13.630: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 10:57:13.638: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 10:57:13.642: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
Aug 24 10:57:13.652: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 10:57:13.652: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 10:57:13.652: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 10:57:13.652: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container autoscaler ready: true, restart count 0
Aug 24 10:57:13.652: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container grafana ready: true, restart count 0
Aug 24 10:57:13.652: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Aug 24 10:57:13.652: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Aug 24 10:57:13.652: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Aug 24 10:57:13.652: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 24 10:57:13.652: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container metrics-server ready: true, restart count 0
Aug 24 10:57:13.652: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 10:57:13.652: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.652: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 10:57:13.652: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.653: INFO: 	Container config-reloader ready: true, restart count 0
Aug 24 10:57:13.653: INFO: 	Container prometheus ready: true, restart count 0
Aug 24 10:57:13.653: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.653: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 10:57:13.653: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 10:57:13.653: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.653: INFO: 	Container busybox ready: true, restart count 0
Aug 24 10:57:13.653: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.653: INFO: 	Container nginx ready: true, restart count 0
Aug 24 10:57:13.653: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.653: INFO: 	Container nginx ready: true, restart count 0
Aug 24 10:57:13.653: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
Aug 24 10:57:13.661: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.661: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 10:57:13.661: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.661: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 10:57:13.661: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 10:57:13.661: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.661: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 10:57:13.661: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.661: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 10:57:13.661: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.661: INFO: 	Container e2e ready: true, restart count 0
Aug 24 10:57:13.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 10:57:13.661: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 10:57:13.661: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 10:57:13.661: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
Aug 24 10:57:13.669: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.669: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 10:57:13.669: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.669: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 10:57:13.669: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 10:57:13.669: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.669: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 10:57:13.669: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.669: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 10:57:13.669: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.669: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 10:57:13.669: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 10:57:13.669: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 10:57:13.669: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 10:57:13.669: INFO: sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8 from sysctl-4219 started at 2023-08-24 10:57:09 +0000 UTC (1 container statuses recorded)
Aug 24 10:57:13.669: INFO: 	Container test-container ready: false, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 10:57:13.67
Aug 24 10:57:13.680: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3880" to be "running"
Aug 24 10:57:13.686: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722663ms
Aug 24 10:57:15.691: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010824853s
Aug 24 10:57:15.691: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 10:57:15.694
STEP: Trying to apply a random label on the found node. 08/24/23 10:57:15.739
STEP: verifying the node has the label kubernetes.io/e2e-d1d6887d-da98-4be2-9b8f-99261b474b65 95 08/24/23 10:57:15.756
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/24/23 10:57:15.768
Aug 24 10:57:15.775: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3880" to be "not pending"
Aug 24 10:57:15.782: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.435678ms
Aug 24 10:57:17.786: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010910953s
Aug 24 10:57:17.786: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.17 on the node which pod4 resides and expect not scheduled 08/24/23 10:57:17.786
Aug 24 10:57:17.799: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3880" to be "not pending"
Aug 24 10:57:17.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.134777ms
Aug 24 10:57:19.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017154982s
Aug 24 10:57:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018857861s
Aug 24 10:57:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018094702s
Aug 24 10:57:25.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025514146s
Aug 24 10:57:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018371042s
Aug 24 10:57:29.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019189179s
Aug 24 10:57:31.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019334357s
Aug 24 10:57:33.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017133963s
Aug 24 10:57:35.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024691783s
Aug 24 10:57:37.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018178907s
Aug 24 10:57:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019260963s
Aug 24 10:57:41.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019663232s
Aug 24 10:57:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.018243903s
Aug 24 10:57:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017177338s
Aug 24 10:57:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017379494s
Aug 24 10:57:49.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018445752s
Aug 24 10:57:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018070931s
Aug 24 10:57:53.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.017961448s
Aug 24 10:57:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.018797381s
Aug 24 10:57:57.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.017347005s
Aug 24 10:57:59.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017785057s
Aug 24 10:58:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.018594701s
Aug 24 10:58:03.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018944227s
Aug 24 10:58:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.018487358s
Aug 24 10:58:07.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016958952s
Aug 24 10:58:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019246404s
Aug 24 10:58:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.018979427s
Aug 24 10:58:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.018690564s
Aug 24 10:58:15.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018873585s
Aug 24 10:58:17.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.018256144s
Aug 24 10:58:19.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.020713213s
Aug 24 10:58:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.018480898s
Aug 24 10:58:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018379498s
Aug 24 10:58:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018425441s
Aug 24 10:58:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.017748716s
Aug 24 10:58:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.018435102s
Aug 24 10:58:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017848027s
Aug 24 10:58:33.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018994468s
Aug 24 10:58:35.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01945562s
Aug 24 10:58:37.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018655976s
Aug 24 10:58:39.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017516768s
Aug 24 10:58:41.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.017219451s
Aug 24 10:58:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018055417s
Aug 24 10:58:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017258828s
Aug 24 10:58:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016948674s
Aug 24 10:58:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019280396s
Aug 24 10:58:51.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024654169s
Aug 24 10:58:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019251545s
Aug 24 10:58:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.019347483s
Aug 24 10:58:57.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02144943s
Aug 24 10:58:59.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.016883172s
Aug 24 10:59:01.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.027436282s
Aug 24 10:59:03.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.018352845s
Aug 24 10:59:05.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021920338s
Aug 24 10:59:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019115667s
Aug 24 10:59:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018983571s
Aug 24 10:59:11.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.020310216s
Aug 24 10:59:13.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018551247s
Aug 24 10:59:15.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017683708s
Aug 24 10:59:17.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018655153s
Aug 24 10:59:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.018430587s
Aug 24 10:59:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.018058685s
Aug 24 10:59:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.01849784s
Aug 24 10:59:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017782309s
Aug 24 10:59:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01762383s
Aug 24 10:59:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.017807908s
Aug 24 10:59:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.017763416s
Aug 24 10:59:33.829: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.030326659s
Aug 24 10:59:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018465228s
Aug 24 10:59:37.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.017460302s
Aug 24 10:59:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.019157285s
Aug 24 10:59:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.018079323s
Aug 24 10:59:43.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.019063485s
Aug 24 10:59:45.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.019360817s
Aug 24 10:59:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.017485442s
Aug 24 10:59:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.018620726s
Aug 24 10:59:51.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.019080333s
Aug 24 10:59:53.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.016593971s
Aug 24 10:59:55.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.019979354s
Aug 24 10:59:57.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.018613774s
Aug 24 10:59:59.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.021163298s
Aug 24 11:00:01.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.017309557s
Aug 24 11:00:03.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020989953s
Aug 24 11:00:05.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.020183012s
Aug 24 11:00:07.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.017978777s
Aug 24 11:00:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.019048229s
Aug 24 11:00:11.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.017372209s
Aug 24 11:00:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.019355043s
Aug 24 11:00:15.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.019602962s
Aug 24 11:00:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.017188826s
Aug 24 11:00:19.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019431057s
Aug 24 11:00:21.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.017344538s
Aug 24 11:00:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.017906264s
Aug 24 11:00:25.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.019356422s
Aug 24 11:00:27.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.021936515s
Aug 24 11:00:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.017644235s
Aug 24 11:00:31.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.018686201s
Aug 24 11:00:33.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.018536668s
Aug 24 11:00:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.018428188s
Aug 24 11:00:37.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.018141799s
Aug 24 11:00:39.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.017999591s
Aug 24 11:00:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017985165s
Aug 24 11:00:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.017922949s
Aug 24 11:00:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.017455724s
Aug 24 11:00:47.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.01832427s
Aug 24 11:00:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018691635s
Aug 24 11:00:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.018249956s
Aug 24 11:00:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.018965006s
Aug 24 11:00:55.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.018109145s
Aug 24 11:00:57.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.018093101s
Aug 24 11:00:59.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.018876731s
Aug 24 11:01:01.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.018434991s
Aug 24 11:01:03.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.018543732s
Aug 24 11:01:05.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019054239s
Aug 24 11:01:07.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.017239329s
Aug 24 11:01:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.019093507s
Aug 24 11:01:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.018776036s
Aug 24 11:01:13.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.017106755s
Aug 24 11:01:15.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.018770895s
Aug 24 11:01:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.017232406s
Aug 24 11:01:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.018337872s
Aug 24 11:01:21.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.019709586s
Aug 24 11:01:23.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.019236401s
Aug 24 11:01:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.018097579s
Aug 24 11:01:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.019134767s
Aug 24 11:01:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017775285s
Aug 24 11:01:31.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018821967s
Aug 24 11:01:33.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.020694647s
Aug 24 11:01:35.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.016935467s
Aug 24 11:01:37.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.017193973s
Aug 24 11:01:39.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01837846s
Aug 24 11:01:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.017807468s
Aug 24 11:01:43.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.017363475s
Aug 24 11:01:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016661933s
Aug 24 11:01:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.017224874s
Aug 24 11:01:49.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.020357501s
Aug 24 11:01:51.830: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.030816106s
Aug 24 11:01:53.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.018363938s
Aug 24 11:01:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.018958128s
Aug 24 11:01:57.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021991197s
Aug 24 11:01:59.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.017423785s
Aug 24 11:02:01.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016908691s
Aug 24 11:02:03.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.021196996s
Aug 24 11:02:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018320939s
Aug 24 11:02:07.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.0167117s
Aug 24 11:02:09.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.018496597s
Aug 24 11:02:11.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.017944843s
Aug 24 11:02:13.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017689811s
Aug 24 11:02:15.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.018359801s
Aug 24 11:02:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017155832s
Aug 24 11:02:17.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020236214s
STEP: removing the label kubernetes.io/e2e-d1d6887d-da98-4be2-9b8f-99261b474b65 off the node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 11:02:17.819
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d1d6887d-da98-4be2-9b8f-99261b474b65 08/24/23 11:02:17.848
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:02:17.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3880" for this suite. 08/24/23 11:02:17.864
------------------------------
â€¢ [SLOW TEST] [304.285 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 10:57:13.588
    Aug 24 10:57:13.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-pred 08/24/23 10:57:13.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 10:57:13.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 10:57:13.624
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 10:57:13.630: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 10:57:13.638: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 10:57:13.642: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
    Aug 24 10:57:13.652: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container grafana ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.652: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 10:57:13.652: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.653: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 24 10:57:13.653: INFO: 	Container prometheus ready: true, restart count 0
    Aug 24 10:57:13.653: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.653: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 10:57:13.653: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 10:57:13.653: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.653: INFO: 	Container busybox ready: true, restart count 0
    Aug 24 10:57:13.653: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.653: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 10:57:13.653: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.653: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 10:57:13.653: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
    Aug 24 10:57:13.661: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.661: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.661: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.661: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.661: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.661: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.661: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 10:57:13.661: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
    Aug 24 10:57:13.669: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.669: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.669: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.669: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.669: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.669: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 10:57:13.669: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 10:57:13.669: INFO: sysctl-c21e3b25-20e0-42ee-9fb7-c78b278688c8 from sysctl-4219 started at 2023-08-24 10:57:09 +0000 UTC (1 container statuses recorded)
    Aug 24 10:57:13.669: INFO: 	Container test-container ready: false, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 10:57:13.67
    Aug 24 10:57:13.680: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3880" to be "running"
    Aug 24 10:57:13.686: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.722663ms
    Aug 24 10:57:15.691: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010824853s
    Aug 24 10:57:15.691: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 10:57:15.694
    STEP: Trying to apply a random label on the found node. 08/24/23 10:57:15.739
    STEP: verifying the node has the label kubernetes.io/e2e-d1d6887d-da98-4be2-9b8f-99261b474b65 95 08/24/23 10:57:15.756
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/24/23 10:57:15.768
    Aug 24 10:57:15.775: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3880" to be "not pending"
    Aug 24 10:57:15.782: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.435678ms
    Aug 24 10:57:17.786: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010910953s
    Aug 24 10:57:17.786: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.17 on the node which pod4 resides and expect not scheduled 08/24/23 10:57:17.786
    Aug 24 10:57:17.799: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3880" to be "not pending"
    Aug 24 10:57:17.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.134777ms
    Aug 24 10:57:19.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017154982s
    Aug 24 10:57:21.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018857861s
    Aug 24 10:57:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018094702s
    Aug 24 10:57:25.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025514146s
    Aug 24 10:57:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018371042s
    Aug 24 10:57:29.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019189179s
    Aug 24 10:57:31.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019334357s
    Aug 24 10:57:33.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.017133963s
    Aug 24 10:57:35.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.024691783s
    Aug 24 10:57:37.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018178907s
    Aug 24 10:57:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019260963s
    Aug 24 10:57:41.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.019663232s
    Aug 24 10:57:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.018243903s
    Aug 24 10:57:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.017177338s
    Aug 24 10:57:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.017379494s
    Aug 24 10:57:49.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018445752s
    Aug 24 10:57:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018070931s
    Aug 24 10:57:53.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.017961448s
    Aug 24 10:57:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.018797381s
    Aug 24 10:57:57.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.017347005s
    Aug 24 10:57:59.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.017785057s
    Aug 24 10:58:01.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.018594701s
    Aug 24 10:58:03.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018944227s
    Aug 24 10:58:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.018487358s
    Aug 24 10:58:07.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016958952s
    Aug 24 10:58:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019246404s
    Aug 24 10:58:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.018979427s
    Aug 24 10:58:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.018690564s
    Aug 24 10:58:15.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.018873585s
    Aug 24 10:58:17.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.018256144s
    Aug 24 10:58:19.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.020713213s
    Aug 24 10:58:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.018480898s
    Aug 24 10:58:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018379498s
    Aug 24 10:58:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018425441s
    Aug 24 10:58:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.017748716s
    Aug 24 10:58:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.018435102s
    Aug 24 10:58:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.017848027s
    Aug 24 10:58:33.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.018994468s
    Aug 24 10:58:35.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.01945562s
    Aug 24 10:58:37.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018655976s
    Aug 24 10:58:39.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017516768s
    Aug 24 10:58:41.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.017219451s
    Aug 24 10:58:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018055417s
    Aug 24 10:58:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017258828s
    Aug 24 10:58:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.016948674s
    Aug 24 10:58:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019280396s
    Aug 24 10:58:51.824: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024654169s
    Aug 24 10:58:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019251545s
    Aug 24 10:58:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.019347483s
    Aug 24 10:58:57.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.02144943s
    Aug 24 10:58:59.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.016883172s
    Aug 24 10:59:01.826: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.027436282s
    Aug 24 10:59:03.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.018352845s
    Aug 24 10:59:05.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021920338s
    Aug 24 10:59:07.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.019115667s
    Aug 24 10:59:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018983571s
    Aug 24 10:59:11.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.020310216s
    Aug 24 10:59:13.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018551247s
    Aug 24 10:59:15.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017683708s
    Aug 24 10:59:17.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018655153s
    Aug 24 10:59:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.018430587s
    Aug 24 10:59:21.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.018058685s
    Aug 24 10:59:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.01849784s
    Aug 24 10:59:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017782309s
    Aug 24 10:59:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01762383s
    Aug 24 10:59:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.017807908s
    Aug 24 10:59:31.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.017763416s
    Aug 24 10:59:33.829: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.030326659s
    Aug 24 10:59:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018465228s
    Aug 24 10:59:37.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.017460302s
    Aug 24 10:59:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.019157285s
    Aug 24 10:59:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.018079323s
    Aug 24 10:59:43.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.019063485s
    Aug 24 10:59:45.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.019360817s
    Aug 24 10:59:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.017485442s
    Aug 24 10:59:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.018620726s
    Aug 24 10:59:51.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.019080333s
    Aug 24 10:59:53.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.016593971s
    Aug 24 10:59:55.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.019979354s
    Aug 24 10:59:57.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.018613774s
    Aug 24 10:59:59.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.021163298s
    Aug 24 11:00:01.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.017309557s
    Aug 24 11:00:03.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.020989953s
    Aug 24 11:00:05.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.020183012s
    Aug 24 11:00:07.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.017978777s
    Aug 24 11:00:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.019048229s
    Aug 24 11:00:11.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.017372209s
    Aug 24 11:00:13.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.019355043s
    Aug 24 11:00:15.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.019602962s
    Aug 24 11:00:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.017188826s
    Aug 24 11:00:19.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019431057s
    Aug 24 11:00:21.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.017344538s
    Aug 24 11:00:23.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.017906264s
    Aug 24 11:00:25.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.019356422s
    Aug 24 11:00:27.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.021936515s
    Aug 24 11:00:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.017644235s
    Aug 24 11:00:31.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.018686201s
    Aug 24 11:00:33.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.018536668s
    Aug 24 11:00:35.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.018428188s
    Aug 24 11:00:37.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.018141799s
    Aug 24 11:00:39.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.017999591s
    Aug 24 11:00:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017985165s
    Aug 24 11:00:43.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.017922949s
    Aug 24 11:00:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.017455724s
    Aug 24 11:00:47.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.01832427s
    Aug 24 11:00:49.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018691635s
    Aug 24 11:00:51.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.018249956s
    Aug 24 11:00:53.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.018965006s
    Aug 24 11:00:55.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.018109145s
    Aug 24 11:00:57.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.018093101s
    Aug 24 11:00:59.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.018876731s
    Aug 24 11:01:01.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.018434991s
    Aug 24 11:01:03.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.018543732s
    Aug 24 11:01:05.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019054239s
    Aug 24 11:01:07.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.017239329s
    Aug 24 11:01:09.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.019093507s
    Aug 24 11:01:11.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.018776036s
    Aug 24 11:01:13.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.017106755s
    Aug 24 11:01:15.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.018770895s
    Aug 24 11:01:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.017232406s
    Aug 24 11:01:19.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.018337872s
    Aug 24 11:01:21.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.019709586s
    Aug 24 11:01:23.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.019236401s
    Aug 24 11:01:25.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.018097579s
    Aug 24 11:01:27.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.019134767s
    Aug 24 11:01:29.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017775285s
    Aug 24 11:01:31.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.018821967s
    Aug 24 11:01:33.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.020694647s
    Aug 24 11:01:35.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.016935467s
    Aug 24 11:01:37.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.017193973s
    Aug 24 11:01:39.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.01837846s
    Aug 24 11:01:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.017807468s
    Aug 24 11:01:43.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.017363475s
    Aug 24 11:01:45.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016661933s
    Aug 24 11:01:47.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.017224874s
    Aug 24 11:01:49.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.020357501s
    Aug 24 11:01:51.830: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.030816106s
    Aug 24 11:01:53.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.018363938s
    Aug 24 11:01:55.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.018958128s
    Aug 24 11:01:57.821: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.021991197s
    Aug 24 11:01:59.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.017423785s
    Aug 24 11:02:01.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016908691s
    Aug 24 11:02:03.820: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.021196996s
    Aug 24 11:02:05.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.018320939s
    Aug 24 11:02:07.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.0167117s
    Aug 24 11:02:09.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.018496597s
    Aug 24 11:02:11.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.017944843s
    Aug 24 11:02:13.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017689811s
    Aug 24 11:02:15.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.018359801s
    Aug 24 11:02:17.816: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017155832s
    Aug 24 11:02:17.819: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020236214s
    STEP: removing the label kubernetes.io/e2e-d1d6887d-da98-4be2-9b8f-99261b474b65 off the node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 11:02:17.819
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d1d6887d-da98-4be2-9b8f-99261b474b65 08/24/23 11:02:17.848
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:02:17.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3880" for this suite. 08/24/23 11:02:17.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:02:17.873
Aug 24 11:02:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 11:02:17.875
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:02:17.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:02:17.906
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/24/23 11:02:17.916
STEP: waiting for Deployment to be created 08/24/23 11:02:17.926
STEP: waiting for all Replicas to be Ready 08/24/23 11:02:17.929
Aug 24 11:02:17.931: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:17.931: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:17.951: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:17.951: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:17.974: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:17.974: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:18.061: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:18.061: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 24 11:02:18.994: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 24 11:02:18.994: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 24 11:02:19.011: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/24/23 11:02:19.011
W0824 11:02:19.030475      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 24 11:02:19.033: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/24/23 11:02:19.033
Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.053: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.053: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.094: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.094: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:19.134: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:19.134: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:19.201: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:19.201: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:21.069: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:21.069: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:21.114: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
STEP: listing Deployments 08/24/23 11:02:21.114
Aug 24 11:02:21.120: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/24/23 11:02:21.12
Aug 24 11:02:21.142: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/24/23 11:02:21.142
Aug 24 11:02:21.158: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:21.164: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:21.274: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:21.311: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:23.051: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:23.080: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:23.183: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:23.198: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 24 11:02:25.040: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/24/23 11:02:25.093
STEP: fetching the DeploymentStatus 08/24/23 11:02:25.105
Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3
Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3
STEP: deleting the Deployment 08/24/23 11:02:25.112
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
Aug 24 11:02:25.142: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 11:02:25.175: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 24 11:02:25.184: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1947  5ed239f0-23b3-4181-b0b3-cf51d872fdbe 13197 2 2023-08-24 11:02:21 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 907fca89-dcc0-4714-926b-4ec3feb9e9b9 0xc004998277 0xc004998278}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907fca89-dcc0-4714-926b-4ec3feb9e9b9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:02:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004998300 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 24 11:02:25.196: INFO: pod: "test-deployment-7b7876f9d6-569wg":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-569wg test-deployment-7b7876f9d6- deployment-1947  f8e08ffc-1b42-4d50-b486-18523527d787 13196 0 2023-08-24 11:02:23 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:a8afef6d09b42d71016fad18523c024451c3fe505c13bd00b03590abf53f104f cni.projectcalico.org/podIP:10.100.45.150/32 cni.projectcalico.org/podIPs:10.100.45.150/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 5ed239f0-23b3-4181-b0b3-cf51d872fdbe 0xc0048fb197 0xc0048fb198}] [] [{Go-http-client Update v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed239f0-23b3-4181-b0b3-cf51d872fdbe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kj4z7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kj4z7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.150,StartTime:2023-08-24 11:02:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://00d22f163e7dd79818e8c558364c67f38710ac6d88496a0597a7f0cae1867750,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 24 11:02:25.197: INFO: pod: "test-deployment-7b7876f9d6-58l5j":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-58l5j test-deployment-7b7876f9d6- deployment-1947  5de29c5a-cfe4-492e-b3c8-45b295d821f4 13148 0 2023-08-24 11:02:21 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:84e9acfe674f1e2b1cc5704a3fef850608decb4f8ea6d949ef6c6437f9f53fbb cni.projectcalico.org/podIP:10.100.181.180/32 cni.projectcalico.org/podIPs:10.100.181.180/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 5ed239f0-23b3-4181-b0b3-cf51d872fdbe 0xc0048fb847 0xc0048fb848}] [] [{Go-http-client Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed239f0-23b3-4181-b0b3-cf51d872fdbe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvcng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvcng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.180,StartTime:2023-08-24 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://688709523364557a3bab4585e3c5c0141191c7d6ee8b86f5c7a6f77d5bea91ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 24 11:02:25.197: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1947  af6e83e3-888b-4594-8a30-96dabb355074 13206 4 2023-08-24 11:02:19 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 907fca89-dcc0-4714-926b-4ec3feb9e9b9 0xc004998367 0xc004998368}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907fca89-dcc0-4714-926b-4ec3feb9e9b9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:02:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049983f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 24 11:02:25.202: INFO: pod: "test-deployment-7df74c55ff-8wl2g":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-8wl2g test-deployment-7df74c55ff- deployment-1947  f2f91828-673e-4b6e-84e7-56911517939d 13201 0 2023-08-24 11:02:21 +0000 UTC 2023-08-24 11:02:26 +0000 UTC 0xc002ea4e78 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:bd72f326e2ba0c2929f3dd52853678ce55adc1a4b10cb1ee3f0a3692bdbfb409 cni.projectcalico.org/podIP:10.100.45.149/32 cni.projectcalico.org/podIPs:10.100.45.149/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff af6e83e3-888b-4594-8a30-96dabb355074 0xc002ea4ec7 0xc002ea4ec8}] [] [{Go-http-client Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af6e83e3-888b-4594-8a30-96dabb355074\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:02:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vrnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vrnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.149,StartTime:2023-08-24 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://622625228e35b8c0292e4ec4d72c1d8d8905df68b00cee35750f52d148c95ee1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 24 11:02:25.202: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1947  1c7a6eb8-2030-49af-ab7d-c25390687246 13079 3 2023-08-24 11:02:17 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 907fca89-dcc0-4714-926b-4ec3feb9e9b9 0xc004998457 0xc004998458}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907fca89-dcc0-4714-926b-4ec3feb9e9b9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049984f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 11:02:25.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1947" for this suite. 08/24/23 11:02:25.212
------------------------------
â€¢ [SLOW TEST] [7.372 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:02:17.873
    Aug 24 11:02:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 11:02:17.875
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:02:17.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:02:17.906
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/24/23 11:02:17.916
    STEP: waiting for Deployment to be created 08/24/23 11:02:17.926
    STEP: waiting for all Replicas to be Ready 08/24/23 11:02:17.929
    Aug 24 11:02:17.931: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:17.931: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:17.951: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:17.951: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:17.974: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:17.974: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:18.061: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:18.061: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 24 11:02:18.994: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 24 11:02:18.994: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 24 11:02:19.011: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/24/23 11:02:19.011
    W0824 11:02:19.030475      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 24 11:02:19.033: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/24/23 11:02:19.033
    Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.035: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 0
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.036: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.053: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.053: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.094: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.094: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:19.134: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:19.134: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:19.201: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:19.201: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:21.069: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:21.069: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:21.114: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    STEP: listing Deployments 08/24/23 11:02:21.114
    Aug 24 11:02:21.120: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/24/23 11:02:21.12
    Aug 24 11:02:21.142: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/24/23 11:02:21.142
    Aug 24 11:02:21.158: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:21.164: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:21.274: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:21.311: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:23.051: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:23.080: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:23.183: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:23.198: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 24 11:02:25.040: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/24/23 11:02:25.093
    STEP: fetching the DeploymentStatus 08/24/23 11:02:25.105
    Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 1
    Aug 24 11:02:25.111: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3
    Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 2
    Aug 24 11:02:25.112: INFO: observed Deployment test-deployment in namespace deployment-1947 with ReadyReplicas 3
    STEP: deleting the Deployment 08/24/23 11:02:25.112
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    Aug 24 11:02:25.142: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 11:02:25.175: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 24 11:02:25.184: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1947  5ed239f0-23b3-4181-b0b3-cf51d872fdbe 13197 2 2023-08-24 11:02:21 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 907fca89-dcc0-4714-926b-4ec3feb9e9b9 0xc004998277 0xc004998278}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907fca89-dcc0-4714-926b-4ec3feb9e9b9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:02:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004998300 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Aug 24 11:02:25.196: INFO: pod: "test-deployment-7b7876f9d6-569wg":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-569wg test-deployment-7b7876f9d6- deployment-1947  f8e08ffc-1b42-4d50-b486-18523527d787 13196 0 2023-08-24 11:02:23 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:a8afef6d09b42d71016fad18523c024451c3fe505c13bd00b03590abf53f104f cni.projectcalico.org/podIP:10.100.45.150/32 cni.projectcalico.org/podIPs:10.100.45.150/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 5ed239f0-23b3-4181-b0b3-cf51d872fdbe 0xc0048fb197 0xc0048fb198}] [] [{Go-http-client Update v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed239f0-23b3-4181-b0b3-cf51d872fdbe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kj4z7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kj4z7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.150,StartTime:2023-08-24 11:02:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://00d22f163e7dd79818e8c558364c67f38710ac6d88496a0597a7f0cae1867750,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 24 11:02:25.197: INFO: pod: "test-deployment-7b7876f9d6-58l5j":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-58l5j test-deployment-7b7876f9d6- deployment-1947  5de29c5a-cfe4-492e-b3c8-45b295d821f4 13148 0 2023-08-24 11:02:21 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:84e9acfe674f1e2b1cc5704a3fef850608decb4f8ea6d949ef6c6437f9f53fbb cni.projectcalico.org/podIP:10.100.181.180/32 cni.projectcalico.org/podIPs:10.100.181.180/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 5ed239f0-23b3-4181-b0b3-cf51d872fdbe 0xc0048fb847 0xc0048fb848}] [] [{Go-http-client Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed239f0-23b3-4181-b0b3-cf51d872fdbe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvcng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvcng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.180,StartTime:2023-08-24 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://688709523364557a3bab4585e3c5c0141191c7d6ee8b86f5c7a6f77d5bea91ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 24 11:02:25.197: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1947  af6e83e3-888b-4594-8a30-96dabb355074 13206 4 2023-08-24 11:02:19 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 907fca89-dcc0-4714-926b-4ec3feb9e9b9 0xc004998367 0xc004998368}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907fca89-dcc0-4714-926b-4ec3feb9e9b9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:02:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049983f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 24 11:02:25.202: INFO: pod: "test-deployment-7df74c55ff-8wl2g":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-8wl2g test-deployment-7df74c55ff- deployment-1947  f2f91828-673e-4b6e-84e7-56911517939d 13201 0 2023-08-24 11:02:21 +0000 UTC 2023-08-24 11:02:26 +0000 UTC 0xc002ea4e78 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:bd72f326e2ba0c2929f3dd52853678ce55adc1a4b10cb1ee3f0a3692bdbfb409 cni.projectcalico.org/podIP:10.100.45.149/32 cni.projectcalico.org/podIPs:10.100.45.149/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff af6e83e3-888b-4594-8a30-96dabb355074 0xc002ea4ec7 0xc002ea4ec8}] [] [{Go-http-client Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af6e83e3-888b-4594-8a30-96dabb355074\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:02:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vrnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vrnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.149,StartTime:2023-08-24 11:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://622625228e35b8c0292e4ec4d72c1d8d8905df68b00cee35750f52d148c95ee1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 24 11:02:25.202: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1947  1c7a6eb8-2030-49af-ab7d-c25390687246 13079 3 2023-08-24 11:02:17 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 907fca89-dcc0-4714-926b-4ec3feb9e9b9 0xc004998457 0xc004998458}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"907fca89-dcc0-4714-926b-4ec3feb9e9b9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:02:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049984f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:02:25.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1947" for this suite. 08/24/23 11:02:25.212
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:02:25.248
Aug 24 11:02:25.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 11:02:25.25
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:02:25.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:02:25.296
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-03dada4c-d298-4b75-a697-606d7beef269 in namespace container-probe-9559 08/24/23 11:02:25.302
Aug 24 11:02:25.315: INFO: Waiting up to 5m0s for pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269" in namespace "container-probe-9559" to be "not pending"
Aug 24 11:02:25.327: INFO: Pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269": Phase="Pending", Reason="", readiness=false. Elapsed: 11.690002ms
Aug 24 11:02:27.335: INFO: Pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269": Phase="Running", Reason="", readiness=true. Elapsed: 2.020198075s
Aug 24 11:02:27.336: INFO: Pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269" satisfied condition "not pending"
Aug 24 11:02:27.336: INFO: Started pod test-webserver-03dada4c-d298-4b75-a697-606d7beef269 in namespace container-probe-9559
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:02:27.336
Aug 24 11:02:27.339: INFO: Initial restart count of pod test-webserver-03dada4c-d298-4b75-a697-606d7beef269 is 0
STEP: deleting the pod 08/24/23 11:06:28.04
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:28.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9559" for this suite. 08/24/23 11:06:28.097
------------------------------
â€¢ [SLOW TEST] [242.860 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:02:25.248
    Aug 24 11:02:25.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 11:02:25.25
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:02:25.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:02:25.296
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-03dada4c-d298-4b75-a697-606d7beef269 in namespace container-probe-9559 08/24/23 11:02:25.302
    Aug 24 11:02:25.315: INFO: Waiting up to 5m0s for pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269" in namespace "container-probe-9559" to be "not pending"
    Aug 24 11:02:25.327: INFO: Pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269": Phase="Pending", Reason="", readiness=false. Elapsed: 11.690002ms
    Aug 24 11:02:27.335: INFO: Pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269": Phase="Running", Reason="", readiness=true. Elapsed: 2.020198075s
    Aug 24 11:02:27.336: INFO: Pod "test-webserver-03dada4c-d298-4b75-a697-606d7beef269" satisfied condition "not pending"
    Aug 24 11:02:27.336: INFO: Started pod test-webserver-03dada4c-d298-4b75-a697-606d7beef269 in namespace container-probe-9559
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:02:27.336
    Aug 24 11:02:27.339: INFO: Initial restart count of pod test-webserver-03dada4c-d298-4b75-a697-606d7beef269 is 0
    STEP: deleting the pod 08/24/23 11:06:28.04
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:28.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9559" for this suite. 08/24/23 11:06:28.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:28.109
Aug 24 11:06:28.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 11:06:28.114
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:28.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:28.149
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/24/23 11:06:28.157
Aug 24 11:06:28.167: INFO: Waiting up to 5m0s for pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9" in namespace "pods-8579" to be "running and ready"
Aug 24 11:06:28.181: INFO: Pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.914548ms
Aug 24 11:06:28.181: INFO: The phase of Pod pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:06:30.187: INFO: Pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.019506043s
Aug 24 11:06:30.187: INFO: The phase of Pod pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9 is Running (Ready = true)
Aug 24 11:06:30.187: INFO: Pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9" satisfied condition "running and ready"
Aug 24 11:06:30.192: INFO: Pod pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9 has hostIP: 10.0.0.17
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:30.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8579" for this suite. 08/24/23 11:06:30.195
------------------------------
â€¢ [2.094 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:28.109
    Aug 24 11:06:28.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 11:06:28.114
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:28.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:28.149
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/24/23 11:06:28.157
    Aug 24 11:06:28.167: INFO: Waiting up to 5m0s for pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9" in namespace "pods-8579" to be "running and ready"
    Aug 24 11:06:28.181: INFO: Pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.914548ms
    Aug 24 11:06:28.181: INFO: The phase of Pod pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:06:30.187: INFO: Pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.019506043s
    Aug 24 11:06:30.187: INFO: The phase of Pod pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9 is Running (Ready = true)
    Aug 24 11:06:30.187: INFO: Pod "pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9" satisfied condition "running and ready"
    Aug 24 11:06:30.192: INFO: Pod pod-hostip-81d19bcd-3c64-4d53-abf2-ae4c303e46a9 has hostIP: 10.0.0.17
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:30.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8579" for this suite. 08/24/23 11:06:30.195
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:30.204
Aug 24 11:06:30.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:06:30.205
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:30.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:30.275
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:06:30.282
Aug 24 11:06:30.294: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f" in namespace "projected-5126" to be "Succeeded or Failed"
Aug 24 11:06:30.301: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.00574ms
Aug 24 11:06:32.306: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012125962s
Aug 24 11:06:34.308: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013457577s
STEP: Saw pod success 08/24/23 11:06:34.308
Aug 24 11:06:34.308: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f" satisfied condition "Succeeded or Failed"
Aug 24 11:06:34.312: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f container client-container: <nil>
STEP: delete the pod 08/24/23 11:06:34.378
Aug 24 11:06:34.401: INFO: Waiting for pod downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f to disappear
Aug 24 11:06:34.407: INFO: Pod downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5126" for this suite. 08/24/23 11:06:34.411
------------------------------
â€¢ [4.216 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:30.204
    Aug 24 11:06:30.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:06:30.205
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:30.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:30.275
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:06:30.282
    Aug 24 11:06:30.294: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f" in namespace "projected-5126" to be "Succeeded or Failed"
    Aug 24 11:06:30.301: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.00574ms
    Aug 24 11:06:32.306: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012125962s
    Aug 24 11:06:34.308: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013457577s
    STEP: Saw pod success 08/24/23 11:06:34.308
    Aug 24 11:06:34.308: INFO: Pod "downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f" satisfied condition "Succeeded or Failed"
    Aug 24 11:06:34.312: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f container client-container: <nil>
    STEP: delete the pod 08/24/23 11:06:34.378
    Aug 24 11:06:34.401: INFO: Waiting for pod downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f to disappear
    Aug 24 11:06:34.407: INFO: Pod downwardapi-volume-78a66442-12b0-4e59-bb43-da6f0d90583f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:34.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5126" for this suite. 08/24/23 11:06:34.411
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:34.421
Aug 24 11:06:34.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:06:34.423
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:34.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:34.46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-e70ac099-b502-4699-a5a2-b457533ea71f 08/24/23 11:06:34.467
STEP: Creating a pod to test consume configMaps 08/24/23 11:06:34.478
Aug 24 11:06:34.491: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df" in namespace "projected-1484" to be "Succeeded or Failed"
Aug 24 11:06:34.495: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007394ms
Aug 24 11:06:36.499: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00769782s
Aug 24 11:06:38.500: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008717716s
STEP: Saw pod success 08/24/23 11:06:38.5
Aug 24 11:06:38.501: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df" satisfied condition "Succeeded or Failed"
Aug 24 11:06:38.503: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/24/23 11:06:38.51
Aug 24 11:06:38.528: INFO: Waiting for pod pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df to disappear
Aug 24 11:06:38.535: INFO: Pod pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:38.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1484" for this suite. 08/24/23 11:06:38.539
------------------------------
â€¢ [4.129 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:34.421
    Aug 24 11:06:34.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:06:34.423
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:34.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:34.46
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-e70ac099-b502-4699-a5a2-b457533ea71f 08/24/23 11:06:34.467
    STEP: Creating a pod to test consume configMaps 08/24/23 11:06:34.478
    Aug 24 11:06:34.491: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df" in namespace "projected-1484" to be "Succeeded or Failed"
    Aug 24 11:06:34.495: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007394ms
    Aug 24 11:06:36.499: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00769782s
    Aug 24 11:06:38.500: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008717716s
    STEP: Saw pod success 08/24/23 11:06:38.5
    Aug 24 11:06:38.501: INFO: Pod "pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df" satisfied condition "Succeeded or Failed"
    Aug 24 11:06:38.503: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:06:38.51
    Aug 24 11:06:38.528: INFO: Waiting for pod pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df to disappear
    Aug 24 11:06:38.535: INFO: Pod pod-projected-configmaps-31ac5588-1f12-4a4d-87d6-5b684b10f7df no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:38.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1484" for this suite. 08/24/23 11:06:38.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:38.555
Aug 24 11:06:38.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:06:38.557
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:38.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:38.584
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/24/23 11:06:38.591
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/24/23 11:06:38.594
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 11:06:38.594
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/24/23 11:06:38.594
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/24/23 11:06:38.597
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:06:38.597
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:06:38.6
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:38.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4528" for this suite. 08/24/23 11:06:38.604
------------------------------
â€¢ [0.056 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:38.555
    Aug 24 11:06:38.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:06:38.557
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:38.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:38.584
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/24/23 11:06:38.591
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/24/23 11:06:38.594
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/24/23 11:06:38.594
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/24/23 11:06:38.594
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/24/23 11:06:38.597
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:06:38.597
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/24/23 11:06:38.6
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:38.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4528" for this suite. 08/24/23 11:06:38.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:38.611
Aug 24 11:06:38.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svc-latency 08/24/23 11:06:38.613
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:38.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:38.645
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 24 11:06:38.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8671 08/24/23 11:06:38.653
I0824 11:06:38.666523      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8671, replica count: 1
I0824 11:06:39.718269      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:06:40.719108      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:06:40.840: INFO: Created: latency-svc-v2sfp
Aug 24 11:06:40.859: INFO: Got endpoints: latency-svc-v2sfp [38.90402ms]
Aug 24 11:06:40.894: INFO: Created: latency-svc-rnbwv
Aug 24 11:06:40.916: INFO: Got endpoints: latency-svc-rnbwv [56.668447ms]
Aug 24 11:06:40.943: INFO: Created: latency-svc-qfxhq
Aug 24 11:06:40.959: INFO: Got endpoints: latency-svc-qfxhq [100.077696ms]
Aug 24 11:06:40.969: INFO: Created: latency-svc-vhj92
Aug 24 11:06:40.987: INFO: Created: latency-svc-bcmn7
Aug 24 11:06:41.000: INFO: Got endpoints: latency-svc-vhj92 [141.029808ms]
Aug 24 11:06:41.026: INFO: Created: latency-svc-dp9nv
Aug 24 11:06:41.029: INFO: Got endpoints: latency-svc-bcmn7 [170.576825ms]
Aug 24 11:06:41.041: INFO: Created: latency-svc-9xk9v
Aug 24 11:06:41.064: INFO: Got endpoints: latency-svc-dp9nv [204.30721ms]
Aug 24 11:06:41.065: INFO: Created: latency-svc-l9m2x
Aug 24 11:06:41.068: INFO: Got endpoints: latency-svc-9xk9v [208.226751ms]
Aug 24 11:06:41.098: INFO: Got endpoints: latency-svc-l9m2x [239.353081ms]
Aug 24 11:06:41.103: INFO: Created: latency-svc-dvkps
Aug 24 11:06:41.120: INFO: Got endpoints: latency-svc-dvkps [260.46105ms]
Aug 24 11:06:41.262: INFO: Created: latency-svc-m4dsx
Aug 24 11:06:41.278: INFO: Got endpoints: latency-svc-m4dsx [418.95745ms]
Aug 24 11:06:41.280: INFO: Created: latency-svc-rq4mv
Aug 24 11:06:41.282: INFO: Created: latency-svc-4d7ql
Aug 24 11:06:41.282: INFO: Created: latency-svc-l6cvd
Aug 24 11:06:41.283: INFO: Created: latency-svc-sx4kp
Aug 24 11:06:41.283: INFO: Created: latency-svc-x6gwf
Aug 24 11:06:41.283: INFO: Created: latency-svc-6hppx
Aug 24 11:06:41.283: INFO: Created: latency-svc-jx8fx
Aug 24 11:06:41.283: INFO: Created: latency-svc-vjchf
Aug 24 11:06:41.283: INFO: Created: latency-svc-v72qr
Aug 24 11:06:41.283: INFO: Created: latency-svc-zpbbk
Aug 24 11:06:41.287: INFO: Created: latency-svc-p7mxw
Aug 24 11:06:41.287: INFO: Created: latency-svc-5th6p
Aug 24 11:06:41.287: INFO: Created: latency-svc-bl5cz
Aug 24 11:06:41.291: INFO: Created: latency-svc-q2kgq
Aug 24 11:06:41.346: INFO: Got endpoints: latency-svc-4d7ql [278.263988ms]
Aug 24 11:06:41.369: INFO: Got endpoints: latency-svc-p7mxw [270.441013ms]
Aug 24 11:06:41.369: INFO: Got endpoints: latency-svc-bl5cz [248.993827ms]
Aug 24 11:06:41.379: INFO: Got endpoints: latency-svc-vjchf [519.869334ms]
Aug 24 11:06:41.387: INFO: Created: latency-svc-gzcbm
Aug 24 11:06:41.395: INFO: Got endpoints: latency-svc-6hppx [536.302442ms]
Aug 24 11:06:41.417: INFO: Got endpoints: latency-svc-rq4mv [416.906853ms]
Aug 24 11:06:41.417: INFO: Got endpoints: latency-svc-v72qr [557.652697ms]
Aug 24 11:06:41.423: INFO: Created: latency-svc-wjzzk
Aug 24 11:06:41.435: INFO: Got endpoints: latency-svc-sx4kp [575.474654ms]
Aug 24 11:06:41.435: INFO: Got endpoints: latency-svc-x6gwf [575.75445ms]
Aug 24 11:06:41.451: INFO: Created: latency-svc-hcxw4
Aug 24 11:06:41.452: INFO: Got endpoints: latency-svc-zpbbk [422.845674ms]
Aug 24 11:06:41.452: INFO: Got endpoints: latency-svc-q2kgq [492.921868ms]
Aug 24 11:06:41.452: INFO: Got endpoints: latency-svc-5th6p [593.26269ms]
Aug 24 11:06:41.473: INFO: Got endpoints: latency-svc-l6cvd [409.186634ms]
Aug 24 11:06:41.473: INFO: Got endpoints: latency-svc-jx8fx [557.215103ms]
Aug 24 11:06:41.474: INFO: Got endpoints: latency-svc-wjzzk [127.584137ms]
Aug 24 11:06:41.482: INFO: Created: latency-svc-5frl5
Aug 24 11:06:41.488: INFO: Got endpoints: latency-svc-gzcbm [210.589048ms]
Aug 24 11:06:41.488: INFO: Got endpoints: latency-svc-hcxw4 [119.351961ms]
Aug 24 11:06:41.506: INFO: Got endpoints: latency-svc-5frl5 [126.520949ms]
Aug 24 11:06:41.607: INFO: Created: latency-svc-tdtzx
Aug 24 11:06:41.615: INFO: Created: latency-svc-cz7md
Aug 24 11:06:41.615: INFO: Created: latency-svc-hlz5j
Aug 24 11:06:41.616: INFO: Created: latency-svc-8qzx9
Aug 24 11:06:41.624: INFO: Created: latency-svc-9kjgs
Aug 24 11:06:41.624: INFO: Created: latency-svc-p7xlc
Aug 24 11:06:41.624: INFO: Created: latency-svc-bnxfs
Aug 24 11:06:41.624: INFO: Created: latency-svc-xl8jk
Aug 24 11:06:41.624: INFO: Created: latency-svc-kh66q
Aug 24 11:06:41.624: INFO: Created: latency-svc-l9j9x
Aug 24 11:06:41.625: INFO: Created: latency-svc-j2dzs
Aug 24 11:06:41.626: INFO: Created: latency-svc-q4jc4
Aug 24 11:06:41.627: INFO: Created: latency-svc-tjwpz
Aug 24 11:06:41.627: INFO: Created: latency-svc-286zx
Aug 24 11:06:41.628: INFO: Created: latency-svc-rf7nv
Aug 24 11:06:41.668: INFO: Got endpoints: latency-svc-q4jc4 [251.303888ms]
Aug 24 11:06:41.668: INFO: Got endpoints: latency-svc-tdtzx [194.964584ms]
Aug 24 11:06:41.668: INFO: Got endpoints: latency-svc-tjwpz [299.38354ms]
Aug 24 11:06:41.677: INFO: Got endpoints: latency-svc-hlz5j [171.076271ms]
Aug 24 11:06:41.677: INFO: Got endpoints: latency-svc-9kjgs [224.955095ms]
Aug 24 11:06:41.704: INFO: Got endpoints: latency-svc-l9j9x [251.098675ms]
Aug 24 11:06:41.704: INFO: Got endpoints: latency-svc-bnxfs [252.138981ms]
Aug 24 11:06:41.705: INFO: Got endpoints: latency-svc-j2dzs [231.574044ms]
Aug 24 11:06:41.711: INFO: Got endpoints: latency-svc-rf7nv [315.589145ms]
Aug 24 11:06:41.711: INFO: Got endpoints: latency-svc-286zx [294.449617ms]
Aug 24 11:06:41.733: INFO: Got endpoints: latency-svc-xl8jk [297.114347ms]
Aug 24 11:06:41.733: INFO: Created: latency-svc-2j5r9
Aug 24 11:06:41.742: INFO: Got endpoints: latency-svc-p7xlc [253.638943ms]
Aug 24 11:06:41.745: INFO: Got endpoints: latency-svc-cz7md [271.721947ms]
Aug 24 11:06:41.752: INFO: Got endpoints: latency-svc-8qzx9 [264.152236ms]
Aug 24 11:06:41.753: INFO: Got endpoints: latency-svc-kh66q [317.411347ms]
Aug 24 11:06:41.755: INFO: Got endpoints: latency-svc-2j5r9 [85.764952ms]
Aug 24 11:06:41.857: INFO: Created: latency-svc-fpd46
Aug 24 11:06:41.870: INFO: Created: latency-svc-65rcc
Aug 24 11:06:41.871: INFO: Created: latency-svc-x6q22
Aug 24 11:06:41.871: INFO: Created: latency-svc-qgzrb
Aug 24 11:06:41.871: INFO: Created: latency-svc-xhmqn
Aug 24 11:06:41.871: INFO: Created: latency-svc-z9cvv
Aug 24 11:06:41.872: INFO: Created: latency-svc-lt22v
Aug 24 11:06:41.872: INFO: Created: latency-svc-csk7f
Aug 24 11:06:41.872: INFO: Created: latency-svc-fbfsb
Aug 24 11:06:41.872: INFO: Created: latency-svc-lvxgf
Aug 24 11:06:41.871: INFO: Created: latency-svc-mffcr
Aug 24 11:06:41.872: INFO: Created: latency-svc-46n6w
Aug 24 11:06:41.872: INFO: Created: latency-svc-vrfp6
Aug 24 11:06:41.872: INFO: Created: latency-svc-xw8dm
Aug 24 11:06:41.872: INFO: Created: latency-svc-vggtv
Aug 24 11:06:41.873: INFO: Got endpoints: latency-svc-fpd46 [120.358116ms]
Aug 24 11:06:41.960: INFO: Got endpoints: latency-svc-lvxgf [249.147755ms]
Aug 24 11:06:41.961: INFO: Got endpoints: latency-svc-xw8dm [255.923794ms]
Aug 24 11:06:41.961: INFO: Got endpoints: latency-svc-xhmqn [206.02882ms]
Aug 24 11:06:41.972: INFO: Got endpoints: latency-svc-46n6w [266.928026ms]
Aug 24 11:06:41.972: INFO: Got endpoints: latency-svc-csk7f [239.454093ms]
Aug 24 11:06:41.995: INFO: Created: latency-svc-tqlc4
Aug 24 11:06:41.996: INFO: Got endpoints: latency-svc-vrfp6 [318.326939ms]
Aug 24 11:06:41.996: INFO: Got endpoints: latency-svc-qgzrb [243.144149ms]
Aug 24 11:06:42.017: INFO: Created: latency-svc-fx9pp
Aug 24 11:06:42.017: INFO: Got endpoints: latency-svc-lt22v [347.74564ms]
Aug 24 11:06:42.054: INFO: Got endpoints: latency-svc-vggtv [312.076443ms]
Aug 24 11:06:42.093: INFO: Created: latency-svc-xvl8h
Aug 24 11:06:42.102: INFO: Created: latency-svc-zd8dp
Aug 24 11:06:42.102: INFO: Created: latency-svc-rwtj8
Aug 24 11:06:42.102: INFO: Created: latency-svc-58pqp
Aug 24 11:06:42.102: INFO: Created: latency-svc-n8f9n
Aug 24 11:06:42.103: INFO: Created: latency-svc-55qgq
Aug 24 11:06:42.103: INFO: Created: latency-svc-7lc4d
Aug 24 11:06:42.103: INFO: Created: latency-svc-gz8pm
Aug 24 11:06:42.115: INFO: Got endpoints: latency-svc-mffcr [444.641081ms]
Aug 24 11:06:42.155: INFO: Created: latency-svc-vpnrd
Aug 24 11:06:42.164: INFO: Got endpoints: latency-svc-z9cvv [486.695278ms]
Aug 24 11:06:42.181: INFO: Created: latency-svc-n4qd6
Aug 24 11:06:42.206: INFO: Got endpoints: latency-svc-fbfsb [502.481874ms]
Aug 24 11:06:42.224: INFO: Created: latency-svc-7flhq
Aug 24 11:06:42.252: INFO: Got endpoints: latency-svc-x6q22 [505.962602ms]
Aug 24 11:06:42.270: INFO: Created: latency-svc-hzmv5
Aug 24 11:06:42.302: INFO: Got endpoints: latency-svc-65rcc [590.628059ms]
Aug 24 11:06:42.322: INFO: Created: latency-svc-lvn7p
Aug 24 11:06:42.363: INFO: Got endpoints: latency-svc-tqlc4 [490.397305ms]
Aug 24 11:06:42.381: INFO: Created: latency-svc-zf5zv
Aug 24 11:06:42.409: INFO: Got endpoints: latency-svc-fx9pp [448.145665ms]
Aug 24 11:06:42.423: INFO: Created: latency-svc-zsbtt
Aug 24 11:06:42.452: INFO: Got endpoints: latency-svc-xvl8h [435.134202ms]
Aug 24 11:06:42.475: INFO: Created: latency-svc-s8w8p
Aug 24 11:06:42.504: INFO: Got endpoints: latency-svc-rwtj8 [449.281166ms]
Aug 24 11:06:42.520: INFO: Created: latency-svc-gpdkb
Aug 24 11:06:42.554: INFO: Got endpoints: latency-svc-gz8pm [592.719265ms]
Aug 24 11:06:42.577: INFO: Created: latency-svc-97whh
Aug 24 11:06:42.602: INFO: Got endpoints: latency-svc-n8f9n [606.675862ms]
Aug 24 11:06:42.621: INFO: Created: latency-svc-rn8fr
Aug 24 11:06:42.657: INFO: Got endpoints: latency-svc-zd8dp [696.522218ms]
Aug 24 11:06:42.674: INFO: Created: latency-svc-d8jvc
Aug 24 11:06:42.708: INFO: Got endpoints: latency-svc-7lc4d [735.656225ms]
Aug 24 11:06:42.724: INFO: Created: latency-svc-j92t8
Aug 24 11:06:42.755: INFO: Got endpoints: latency-svc-55qgq [758.877518ms]
Aug 24 11:06:42.770: INFO: Created: latency-svc-6j2s7
Aug 24 11:06:42.805: INFO: Got endpoints: latency-svc-58pqp [833.139603ms]
Aug 24 11:06:42.822: INFO: Created: latency-svc-89wtb
Aug 24 11:06:42.859: INFO: Got endpoints: latency-svc-vpnrd [744.01112ms]
Aug 24 11:06:42.881: INFO: Created: latency-svc-qwxtj
Aug 24 11:06:42.912: INFO: Got endpoints: latency-svc-n4qd6 [748.182842ms]
Aug 24 11:06:42.933: INFO: Created: latency-svc-kbx99
Aug 24 11:06:42.961: INFO: Got endpoints: latency-svc-7flhq [754.608577ms]
Aug 24 11:06:42.985: INFO: Created: latency-svc-km7n8
Aug 24 11:06:43.009: INFO: Got endpoints: latency-svc-hzmv5 [757.896784ms]
Aug 24 11:06:43.029: INFO: Created: latency-svc-p72vn
Aug 24 11:06:43.052: INFO: Got endpoints: latency-svc-lvn7p [750.258393ms]
Aug 24 11:06:43.079: INFO: Created: latency-svc-fdhdn
Aug 24 11:06:43.103: INFO: Got endpoints: latency-svc-zf5zv [739.601238ms]
Aug 24 11:06:43.129: INFO: Created: latency-svc-n9qrk
Aug 24 11:06:43.160: INFO: Got endpoints: latency-svc-zsbtt [751.469366ms]
Aug 24 11:06:43.179: INFO: Created: latency-svc-d2vx5
Aug 24 11:06:43.203: INFO: Got endpoints: latency-svc-s8w8p [751.119995ms]
Aug 24 11:06:43.226: INFO: Created: latency-svc-sqg8r
Aug 24 11:06:43.260: INFO: Got endpoints: latency-svc-gpdkb [756.503203ms]
Aug 24 11:06:43.276: INFO: Created: latency-svc-22njr
Aug 24 11:06:43.307: INFO: Got endpoints: latency-svc-97whh [753.654437ms]
Aug 24 11:06:43.324: INFO: Created: latency-svc-hxk8s
Aug 24 11:06:43.352: INFO: Got endpoints: latency-svc-rn8fr [749.56092ms]
Aug 24 11:06:43.374: INFO: Created: latency-svc-kq9kx
Aug 24 11:06:43.407: INFO: Got endpoints: latency-svc-d8jvc [749.026119ms]
Aug 24 11:06:43.426: INFO: Created: latency-svc-lknmk
Aug 24 11:06:43.457: INFO: Got endpoints: latency-svc-j92t8 [748.791703ms]
Aug 24 11:06:43.473: INFO: Created: latency-svc-dqqlh
Aug 24 11:06:43.510: INFO: Got endpoints: latency-svc-6j2s7 [755.507669ms]
Aug 24 11:06:43.528: INFO: Created: latency-svc-b28gp
Aug 24 11:06:43.559: INFO: Got endpoints: latency-svc-89wtb [754.175968ms]
Aug 24 11:06:43.626: INFO: Created: latency-svc-n22tr
Aug 24 11:06:43.703: INFO: Got endpoints: latency-svc-qwxtj [843.455106ms]
Aug 24 11:06:43.713: INFO: Got endpoints: latency-svc-kbx99 [801.090921ms]
Aug 24 11:06:43.723: INFO: Got endpoints: latency-svc-km7n8 [762.12822ms]
Aug 24 11:06:43.747: INFO: Created: latency-svc-48c46
Aug 24 11:06:43.766: INFO: Got endpoints: latency-svc-p72vn [756.232606ms]
Aug 24 11:06:43.768: INFO: Created: latency-svc-jt766
Aug 24 11:06:43.787: INFO: Created: latency-svc-kfbrc
Aug 24 11:06:43.797: INFO: Created: latency-svc-6qpb5
Aug 24 11:06:43.809: INFO: Got endpoints: latency-svc-fdhdn [756.873677ms]
Aug 24 11:06:43.832: INFO: Created: latency-svc-j74cd
Aug 24 11:06:43.867: INFO: Got endpoints: latency-svc-n9qrk [764.025862ms]
Aug 24 11:06:43.899: INFO: Created: latency-svc-kqqq5
Aug 24 11:06:43.909: INFO: Got endpoints: latency-svc-d2vx5 [749.084621ms]
Aug 24 11:06:43.937: INFO: Created: latency-svc-9j2qp
Aug 24 11:06:43.971: INFO: Got endpoints: latency-svc-sqg8r [768.163889ms]
Aug 24 11:06:43.999: INFO: Created: latency-svc-c7gjj
Aug 24 11:06:44.014: INFO: Got endpoints: latency-svc-22njr [753.548614ms]
Aug 24 11:06:44.034: INFO: Created: latency-svc-2wtwt
Aug 24 11:06:44.059: INFO: Got endpoints: latency-svc-hxk8s [751.828788ms]
Aug 24 11:06:44.075: INFO: Created: latency-svc-hh4wp
Aug 24 11:06:44.107: INFO: Got endpoints: latency-svc-kq9kx [755.529381ms]
Aug 24 11:06:44.137: INFO: Created: latency-svc-tlkdf
Aug 24 11:06:44.167: INFO: Got endpoints: latency-svc-lknmk [759.884016ms]
Aug 24 11:06:44.192: INFO: Created: latency-svc-bfwh7
Aug 24 11:06:44.206: INFO: Got endpoints: latency-svc-dqqlh [749.456613ms]
Aug 24 11:06:44.238: INFO: Created: latency-svc-4bmll
Aug 24 11:06:44.255: INFO: Got endpoints: latency-svc-b28gp [744.523428ms]
Aug 24 11:06:44.279: INFO: Created: latency-svc-95nrh
Aug 24 11:06:44.315: INFO: Got endpoints: latency-svc-n22tr [755.437769ms]
Aug 24 11:06:44.337: INFO: Created: latency-svc-29rws
Aug 24 11:06:44.359: INFO: Got endpoints: latency-svc-48c46 [656.468927ms]
Aug 24 11:06:44.377: INFO: Created: latency-svc-8gkqn
Aug 24 11:06:44.407: INFO: Got endpoints: latency-svc-jt766 [693.598513ms]
Aug 24 11:06:44.442: INFO: Created: latency-svc-8xtlc
Aug 24 11:06:44.454: INFO: Got endpoints: latency-svc-kfbrc [730.328082ms]
Aug 24 11:06:44.473: INFO: Created: latency-svc-m26g9
Aug 24 11:06:44.509: INFO: Got endpoints: latency-svc-6qpb5 [743.176813ms]
Aug 24 11:06:44.538: INFO: Created: latency-svc-wp4bk
Aug 24 11:06:44.554: INFO: Got endpoints: latency-svc-j74cd [744.513313ms]
Aug 24 11:06:44.573: INFO: Created: latency-svc-b29gt
Aug 24 11:06:44.606: INFO: Got endpoints: latency-svc-kqqq5 [738.231877ms]
Aug 24 11:06:44.621: INFO: Created: latency-svc-lq9rn
Aug 24 11:06:44.658: INFO: Got endpoints: latency-svc-9j2qp [748.298593ms]
Aug 24 11:06:44.672: INFO: Created: latency-svc-mwjxc
Aug 24 11:06:44.709: INFO: Got endpoints: latency-svc-c7gjj [736.945708ms]
Aug 24 11:06:44.728: INFO: Created: latency-svc-rlrlx
Aug 24 11:06:44.758: INFO: Got endpoints: latency-svc-2wtwt [743.799183ms]
Aug 24 11:06:44.776: INFO: Created: latency-svc-6z9nd
Aug 24 11:06:44.802: INFO: Got endpoints: latency-svc-hh4wp [743.039339ms]
Aug 24 11:06:44.825: INFO: Created: latency-svc-mm64f
Aug 24 11:06:44.858: INFO: Got endpoints: latency-svc-tlkdf [750.341101ms]
Aug 24 11:06:44.884: INFO: Created: latency-svc-68464
Aug 24 11:06:44.904: INFO: Got endpoints: latency-svc-bfwh7 [736.870461ms]
Aug 24 11:06:44.926: INFO: Created: latency-svc-km5hb
Aug 24 11:06:44.977: INFO: Got endpoints: latency-svc-4bmll [770.696345ms]
Aug 24 11:06:45.012: INFO: Created: latency-svc-4wh6h
Aug 24 11:06:45.017: INFO: Got endpoints: latency-svc-95nrh [762.321311ms]
Aug 24 11:06:45.039: INFO: Created: latency-svc-ftldd
Aug 24 11:06:45.058: INFO: Got endpoints: latency-svc-29rws [743.800701ms]
Aug 24 11:06:45.080: INFO: Created: latency-svc-l6kb6
Aug 24 11:06:45.101: INFO: Got endpoints: latency-svc-8gkqn [742.103183ms]
Aug 24 11:06:45.125: INFO: Created: latency-svc-b52xh
Aug 24 11:06:45.155: INFO: Got endpoints: latency-svc-8xtlc [748.315052ms]
Aug 24 11:06:45.170: INFO: Created: latency-svc-8v7jj
Aug 24 11:06:45.206: INFO: Got endpoints: latency-svc-m26g9 [752.326257ms]
Aug 24 11:06:45.234: INFO: Created: latency-svc-sqs6v
Aug 24 11:06:45.252: INFO: Got endpoints: latency-svc-wp4bk [743.158296ms]
Aug 24 11:06:45.269: INFO: Created: latency-svc-k8d7c
Aug 24 11:06:45.303: INFO: Got endpoints: latency-svc-b29gt [748.399443ms]
Aug 24 11:06:45.321: INFO: Created: latency-svc-qqk5h
Aug 24 11:06:45.357: INFO: Got endpoints: latency-svc-lq9rn [751.168339ms]
Aug 24 11:06:45.374: INFO: Created: latency-svc-vpgwq
Aug 24 11:06:45.402: INFO: Got endpoints: latency-svc-mwjxc [743.802133ms]
Aug 24 11:06:45.420: INFO: Created: latency-svc-j27gl
Aug 24 11:06:45.456: INFO: Got endpoints: latency-svc-rlrlx [747.296291ms]
Aug 24 11:06:45.473: INFO: Created: latency-svc-b85h5
Aug 24 11:06:45.509: INFO: Got endpoints: latency-svc-6z9nd [750.697583ms]
Aug 24 11:06:45.526: INFO: Created: latency-svc-44f4f
Aug 24 11:06:45.559: INFO: Got endpoints: latency-svc-mm64f [756.867739ms]
Aug 24 11:06:45.575: INFO: Created: latency-svc-8nnbm
Aug 24 11:06:45.609: INFO: Got endpoints: latency-svc-68464 [751.30257ms]
Aug 24 11:06:45.631: INFO: Created: latency-svc-d5snc
Aug 24 11:06:45.655: INFO: Got endpoints: latency-svc-km5hb [751.230679ms]
Aug 24 11:06:45.670: INFO: Created: latency-svc-nk9rf
Aug 24 11:06:45.707: INFO: Got endpoints: latency-svc-4wh6h [729.545246ms]
Aug 24 11:06:45.731: INFO: Created: latency-svc-tjrv7
Aug 24 11:06:45.756: INFO: Got endpoints: latency-svc-ftldd [738.0504ms]
Aug 24 11:06:45.775: INFO: Created: latency-svc-sxzqk
Aug 24 11:06:45.813: INFO: Got endpoints: latency-svc-l6kb6 [754.653211ms]
Aug 24 11:06:45.836: INFO: Created: latency-svc-c955p
Aug 24 11:06:45.870: INFO: Got endpoints: latency-svc-b52xh [768.76021ms]
Aug 24 11:06:45.941: INFO: Got endpoints: latency-svc-8v7jj [785.231143ms]
Aug 24 11:06:45.952: INFO: Created: latency-svc-jhfq7
Aug 24 11:06:45.976: INFO: Got endpoints: latency-svc-sqs6v [770.424681ms]
Aug 24 11:06:45.997: INFO: Created: latency-svc-pj8rl
Aug 24 11:06:46.014: INFO: Got endpoints: latency-svc-k8d7c [762.323534ms]
Aug 24 11:06:46.016: INFO: Created: latency-svc-hdkpn
Aug 24 11:06:46.069: INFO: Got endpoints: latency-svc-qqk5h [766.76604ms]
Aug 24 11:06:46.077: INFO: Created: latency-svc-lbbfp
Aug 24 11:06:46.090: INFO: Created: latency-svc-95955
Aug 24 11:06:46.103: INFO: Got endpoints: latency-svc-vpgwq [745.47658ms]
Aug 24 11:06:46.124: INFO: Created: latency-svc-fhm5p
Aug 24 11:06:46.174: INFO: Got endpoints: latency-svc-j27gl [771.995796ms]
Aug 24 11:06:46.202: INFO: Created: latency-svc-br8xl
Aug 24 11:06:46.208: INFO: Got endpoints: latency-svc-b85h5 [751.190457ms]
Aug 24 11:06:46.239: INFO: Created: latency-svc-jn85s
Aug 24 11:06:46.260: INFO: Got endpoints: latency-svc-44f4f [751.469441ms]
Aug 24 11:06:46.281: INFO: Created: latency-svc-spvfz
Aug 24 11:06:46.307: INFO: Got endpoints: latency-svc-8nnbm [747.750083ms]
Aug 24 11:06:46.330: INFO: Created: latency-svc-dztd9
Aug 24 11:06:46.351: INFO: Got endpoints: latency-svc-d5snc [741.998064ms]
Aug 24 11:06:46.372: INFO: Created: latency-svc-qst5m
Aug 24 11:06:46.408: INFO: Got endpoints: latency-svc-nk9rf [753.052167ms]
Aug 24 11:06:46.426: INFO: Created: latency-svc-nq2fl
Aug 24 11:06:46.466: INFO: Got endpoints: latency-svc-tjrv7 [759.64131ms]
Aug 24 11:06:46.481: INFO: Created: latency-svc-hcpll
Aug 24 11:06:46.503: INFO: Got endpoints: latency-svc-sxzqk [747.839483ms]
Aug 24 11:06:46.522: INFO: Created: latency-svc-77qwl
Aug 24 11:06:46.559: INFO: Got endpoints: latency-svc-c955p [745.427662ms]
Aug 24 11:06:46.581: INFO: Created: latency-svc-ght4z
Aug 24 11:06:46.608: INFO: Got endpoints: latency-svc-jhfq7 [737.567173ms]
Aug 24 11:06:46.636: INFO: Created: latency-svc-jc2mm
Aug 24 11:06:46.656: INFO: Got endpoints: latency-svc-pj8rl [714.828684ms]
Aug 24 11:06:46.672: INFO: Created: latency-svc-8bd75
Aug 24 11:06:46.717: INFO: Got endpoints: latency-svc-hdkpn [741.061137ms]
Aug 24 11:06:46.734: INFO: Created: latency-svc-l296h
Aug 24 11:06:46.754: INFO: Got endpoints: latency-svc-lbbfp [738.900233ms]
Aug 24 11:06:46.771: INFO: Created: latency-svc-755ll
Aug 24 11:06:46.808: INFO: Got endpoints: latency-svc-95955 [738.471491ms]
Aug 24 11:06:46.830: INFO: Created: latency-svc-dmk64
Aug 24 11:06:46.853: INFO: Got endpoints: latency-svc-fhm5p [750.576144ms]
Aug 24 11:06:46.889: INFO: Created: latency-svc-78q67
Aug 24 11:06:46.921: INFO: Got endpoints: latency-svc-br8xl [747.445784ms]
Aug 24 11:06:46.948: INFO: Created: latency-svc-fwkh9
Aug 24 11:06:46.966: INFO: Got endpoints: latency-svc-jn85s [757.751541ms]
Aug 24 11:06:46.984: INFO: Created: latency-svc-m4s7t
Aug 24 11:06:47.003: INFO: Got endpoints: latency-svc-spvfz [742.824254ms]
Aug 24 11:06:47.025: INFO: Created: latency-svc-xfh4r
Aug 24 11:06:47.051: INFO: Got endpoints: latency-svc-dztd9 [744.256825ms]
Aug 24 11:06:47.076: INFO: Created: latency-svc-bbd4g
Aug 24 11:06:47.106: INFO: Got endpoints: latency-svc-qst5m [754.376336ms]
Aug 24 11:06:47.140: INFO: Created: latency-svc-9w9tg
Aug 24 11:06:47.156: INFO: Got endpoints: latency-svc-nq2fl [747.810792ms]
Aug 24 11:06:47.177: INFO: Created: latency-svc-hhhf5
Aug 24 11:06:47.202: INFO: Got endpoints: latency-svc-hcpll [736.049449ms]
Aug 24 11:06:47.219: INFO: Created: latency-svc-8hczg
Aug 24 11:06:47.269: INFO: Got endpoints: latency-svc-77qwl [765.116804ms]
Aug 24 11:06:47.291: INFO: Created: latency-svc-t5b5z
Aug 24 11:06:47.303: INFO: Got endpoints: latency-svc-ght4z [744.535775ms]
Aug 24 11:06:47.326: INFO: Created: latency-svc-7vwtm
Aug 24 11:06:47.371: INFO: Got endpoints: latency-svc-jc2mm [763.069584ms]
Aug 24 11:06:47.388: INFO: Created: latency-svc-2z7nb
Aug 24 11:06:47.406: INFO: Got endpoints: latency-svc-8bd75 [750.425901ms]
Aug 24 11:06:47.425: INFO: Created: latency-svc-dqcbw
Aug 24 11:06:47.460: INFO: Got endpoints: latency-svc-l296h [742.59212ms]
Aug 24 11:06:47.479: INFO: Created: latency-svc-dfvlb
Aug 24 11:06:47.515: INFO: Got endpoints: latency-svc-755ll [761.101677ms]
Aug 24 11:06:47.535: INFO: Created: latency-svc-75gvm
Aug 24 11:06:47.557: INFO: Got endpoints: latency-svc-dmk64 [748.810668ms]
Aug 24 11:06:47.574: INFO: Created: latency-svc-rb7m6
Aug 24 11:06:47.603: INFO: Got endpoints: latency-svc-78q67 [750.104566ms]
Aug 24 11:06:47.620: INFO: Created: latency-svc-qrbb7
Aug 24 11:06:47.655: INFO: Got endpoints: latency-svc-fwkh9 [733.63047ms]
Aug 24 11:06:47.668: INFO: Created: latency-svc-rs54d
Aug 24 11:06:47.709: INFO: Got endpoints: latency-svc-m4s7t [742.716531ms]
Aug 24 11:06:47.743: INFO: Created: latency-svc-jqtx7
Aug 24 11:06:47.755: INFO: Got endpoints: latency-svc-xfh4r [751.793354ms]
Aug 24 11:06:47.773: INFO: Created: latency-svc-ltxpz
Aug 24 11:06:47.812: INFO: Got endpoints: latency-svc-bbd4g [760.704799ms]
Aug 24 11:06:47.826: INFO: Created: latency-svc-cznrf
Aug 24 11:06:47.859: INFO: Got endpoints: latency-svc-9w9tg [753.629829ms]
Aug 24 11:06:47.883: INFO: Created: latency-svc-sn78c
Aug 24 11:06:47.907: INFO: Got endpoints: latency-svc-hhhf5 [750.913935ms]
Aug 24 11:06:47.931: INFO: Created: latency-svc-pvll5
Aug 24 11:06:47.960: INFO: Got endpoints: latency-svc-8hczg [757.923098ms]
Aug 24 11:06:47.985: INFO: Created: latency-svc-hfkjw
Aug 24 11:06:48.025: INFO: Got endpoints: latency-svc-t5b5z [756.27789ms]
Aug 24 11:06:48.079: INFO: Created: latency-svc-h8586
Aug 24 11:06:48.084: INFO: Got endpoints: latency-svc-7vwtm [780.355292ms]
Aug 24 11:06:48.127: INFO: Got endpoints: latency-svc-2z7nb [756.459843ms]
Aug 24 11:06:48.143: INFO: Created: latency-svc-fnqvm
Aug 24 11:06:48.158: INFO: Created: latency-svc-65fkh
Aug 24 11:06:48.173: INFO: Got endpoints: latency-svc-dqcbw [766.629742ms]
Aug 24 11:06:48.215: INFO: Created: latency-svc-bm8qc
Aug 24 11:06:48.224: INFO: Got endpoints: latency-svc-dfvlb [763.607645ms]
Aug 24 11:06:48.253: INFO: Created: latency-svc-jccjn
Aug 24 11:06:48.266: INFO: Got endpoints: latency-svc-75gvm [750.760244ms]
Aug 24 11:06:48.284: INFO: Created: latency-svc-sg4vr
Aug 24 11:06:48.310: INFO: Got endpoints: latency-svc-rb7m6 [753.500478ms]
Aug 24 11:06:48.332: INFO: Created: latency-svc-d25vp
Aug 24 11:06:48.352: INFO: Got endpoints: latency-svc-qrbb7 [748.550088ms]
Aug 24 11:06:48.369: INFO: Created: latency-svc-45hps
Aug 24 11:06:48.409: INFO: Got endpoints: latency-svc-rs54d [754.236321ms]
Aug 24 11:06:48.425: INFO: Created: latency-svc-vgdf2
Aug 24 11:06:48.462: INFO: Got endpoints: latency-svc-jqtx7 [752.498009ms]
Aug 24 11:06:48.480: INFO: Created: latency-svc-t749q
Aug 24 11:06:48.507: INFO: Got endpoints: latency-svc-ltxpz [751.724657ms]
Aug 24 11:06:48.523: INFO: Created: latency-svc-xxcq7
Aug 24 11:06:48.557: INFO: Got endpoints: latency-svc-cznrf [744.283903ms]
Aug 24 11:06:48.583: INFO: Created: latency-svc-2sdpx
Aug 24 11:06:48.607: INFO: Got endpoints: latency-svc-sn78c [747.295965ms]
Aug 24 11:06:48.624: INFO: Created: latency-svc-tgdjp
Aug 24 11:06:48.677: INFO: Got endpoints: latency-svc-pvll5 [770.105828ms]
Aug 24 11:06:48.692: INFO: Created: latency-svc-g8l2b
Aug 24 11:06:48.710: INFO: Got endpoints: latency-svc-hfkjw [749.178208ms]
Aug 24 11:06:48.753: INFO: Got endpoints: latency-svc-h8586 [728.176756ms]
Aug 24 11:06:48.803: INFO: Got endpoints: latency-svc-fnqvm [719.449936ms]
Aug 24 11:06:48.856: INFO: Got endpoints: latency-svc-65fkh [728.770255ms]
Aug 24 11:06:48.905: INFO: Got endpoints: latency-svc-bm8qc [731.846479ms]
Aug 24 11:06:48.964: INFO: Got endpoints: latency-svc-jccjn [739.992585ms]
Aug 24 11:06:49.010: INFO: Got endpoints: latency-svc-sg4vr [744.528106ms]
Aug 24 11:06:49.056: INFO: Got endpoints: latency-svc-d25vp [745.56673ms]
Aug 24 11:06:49.107: INFO: Got endpoints: latency-svc-45hps [755.003681ms]
Aug 24 11:06:49.152: INFO: Got endpoints: latency-svc-vgdf2 [742.607542ms]
Aug 24 11:06:49.220: INFO: Got endpoints: latency-svc-t749q [757.816628ms]
Aug 24 11:06:49.255: INFO: Got endpoints: latency-svc-xxcq7 [748.513539ms]
Aug 24 11:06:49.312: INFO: Got endpoints: latency-svc-2sdpx [755.584542ms]
Aug 24 11:06:49.361: INFO: Got endpoints: latency-svc-tgdjp [754.488727ms]
Aug 24 11:06:49.405: INFO: Got endpoints: latency-svc-g8l2b [727.657204ms]
Aug 24 11:06:49.405: INFO: Latencies: [56.668447ms 85.764952ms 100.077696ms 119.351961ms 120.358116ms 126.520949ms 127.584137ms 141.029808ms 170.576825ms 171.076271ms 194.964584ms 204.30721ms 206.02882ms 208.226751ms 210.589048ms 224.955095ms 231.574044ms 239.353081ms 239.454093ms 243.144149ms 248.993827ms 249.147755ms 251.098675ms 251.303888ms 252.138981ms 253.638943ms 255.923794ms 260.46105ms 264.152236ms 266.928026ms 270.441013ms 271.721947ms 278.263988ms 294.449617ms 297.114347ms 299.38354ms 312.076443ms 315.589145ms 317.411347ms 318.326939ms 347.74564ms 409.186634ms 416.906853ms 418.95745ms 422.845674ms 435.134202ms 444.641081ms 448.145665ms 449.281166ms 486.695278ms 490.397305ms 492.921868ms 502.481874ms 505.962602ms 519.869334ms 536.302442ms 557.215103ms 557.652697ms 575.474654ms 575.75445ms 590.628059ms 592.719265ms 593.26269ms 606.675862ms 656.468927ms 693.598513ms 696.522218ms 714.828684ms 719.449936ms 727.657204ms 728.176756ms 728.770255ms 729.545246ms 730.328082ms 731.846479ms 733.63047ms 735.656225ms 736.049449ms 736.870461ms 736.945708ms 737.567173ms 738.0504ms 738.231877ms 738.471491ms 738.900233ms 739.601238ms 739.992585ms 741.061137ms 741.998064ms 742.103183ms 742.59212ms 742.607542ms 742.716531ms 742.824254ms 743.039339ms 743.158296ms 743.176813ms 743.799183ms 743.800701ms 743.802133ms 744.01112ms 744.256825ms 744.283903ms 744.513313ms 744.523428ms 744.528106ms 744.535775ms 745.427662ms 745.47658ms 745.56673ms 747.295965ms 747.296291ms 747.445784ms 747.750083ms 747.810792ms 747.839483ms 748.182842ms 748.298593ms 748.315052ms 748.399443ms 748.513539ms 748.550088ms 748.791703ms 748.810668ms 749.026119ms 749.084621ms 749.178208ms 749.456613ms 749.56092ms 750.104566ms 750.258393ms 750.341101ms 750.425901ms 750.576144ms 750.697583ms 750.760244ms 750.913935ms 751.119995ms 751.168339ms 751.190457ms 751.230679ms 751.30257ms 751.469366ms 751.469441ms 751.724657ms 751.793354ms 751.828788ms 752.326257ms 752.498009ms 753.052167ms 753.500478ms 753.548614ms 753.629829ms 753.654437ms 754.175968ms 754.236321ms 754.376336ms 754.488727ms 754.608577ms 754.653211ms 755.003681ms 755.437769ms 755.507669ms 755.529381ms 755.584542ms 756.232606ms 756.27789ms 756.459843ms 756.503203ms 756.867739ms 756.873677ms 757.751541ms 757.816628ms 757.896784ms 757.923098ms 758.877518ms 759.64131ms 759.884016ms 760.704799ms 761.101677ms 762.12822ms 762.321311ms 762.323534ms 763.069584ms 763.607645ms 764.025862ms 765.116804ms 766.629742ms 766.76604ms 768.163889ms 768.76021ms 770.105828ms 770.424681ms 770.696345ms 771.995796ms 780.355292ms 785.231143ms 801.090921ms 833.139603ms 843.455106ms]
Aug 24 11:06:49.405: INFO: 50 %ile: 744.01112ms
Aug 24 11:06:49.405: INFO: 90 %ile: 762.12822ms
Aug 24 11:06:49.405: INFO: 99 %ile: 833.139603ms
Aug 24 11:06:49.405: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:49.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-8671" for this suite. 08/24/23 11:06:49.416
------------------------------
â€¢ [SLOW TEST] [10.817 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:38.611
    Aug 24 11:06:38.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svc-latency 08/24/23 11:06:38.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:38.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:38.645
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 24 11:06:38.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-8671 08/24/23 11:06:38.653
    I0824 11:06:38.666523      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8671, replica count: 1
    I0824 11:06:39.718269      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:06:40.719108      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:06:40.840: INFO: Created: latency-svc-v2sfp
    Aug 24 11:06:40.859: INFO: Got endpoints: latency-svc-v2sfp [38.90402ms]
    Aug 24 11:06:40.894: INFO: Created: latency-svc-rnbwv
    Aug 24 11:06:40.916: INFO: Got endpoints: latency-svc-rnbwv [56.668447ms]
    Aug 24 11:06:40.943: INFO: Created: latency-svc-qfxhq
    Aug 24 11:06:40.959: INFO: Got endpoints: latency-svc-qfxhq [100.077696ms]
    Aug 24 11:06:40.969: INFO: Created: latency-svc-vhj92
    Aug 24 11:06:40.987: INFO: Created: latency-svc-bcmn7
    Aug 24 11:06:41.000: INFO: Got endpoints: latency-svc-vhj92 [141.029808ms]
    Aug 24 11:06:41.026: INFO: Created: latency-svc-dp9nv
    Aug 24 11:06:41.029: INFO: Got endpoints: latency-svc-bcmn7 [170.576825ms]
    Aug 24 11:06:41.041: INFO: Created: latency-svc-9xk9v
    Aug 24 11:06:41.064: INFO: Got endpoints: latency-svc-dp9nv [204.30721ms]
    Aug 24 11:06:41.065: INFO: Created: latency-svc-l9m2x
    Aug 24 11:06:41.068: INFO: Got endpoints: latency-svc-9xk9v [208.226751ms]
    Aug 24 11:06:41.098: INFO: Got endpoints: latency-svc-l9m2x [239.353081ms]
    Aug 24 11:06:41.103: INFO: Created: latency-svc-dvkps
    Aug 24 11:06:41.120: INFO: Got endpoints: latency-svc-dvkps [260.46105ms]
    Aug 24 11:06:41.262: INFO: Created: latency-svc-m4dsx
    Aug 24 11:06:41.278: INFO: Got endpoints: latency-svc-m4dsx [418.95745ms]
    Aug 24 11:06:41.280: INFO: Created: latency-svc-rq4mv
    Aug 24 11:06:41.282: INFO: Created: latency-svc-4d7ql
    Aug 24 11:06:41.282: INFO: Created: latency-svc-l6cvd
    Aug 24 11:06:41.283: INFO: Created: latency-svc-sx4kp
    Aug 24 11:06:41.283: INFO: Created: latency-svc-x6gwf
    Aug 24 11:06:41.283: INFO: Created: latency-svc-6hppx
    Aug 24 11:06:41.283: INFO: Created: latency-svc-jx8fx
    Aug 24 11:06:41.283: INFO: Created: latency-svc-vjchf
    Aug 24 11:06:41.283: INFO: Created: latency-svc-v72qr
    Aug 24 11:06:41.283: INFO: Created: latency-svc-zpbbk
    Aug 24 11:06:41.287: INFO: Created: latency-svc-p7mxw
    Aug 24 11:06:41.287: INFO: Created: latency-svc-5th6p
    Aug 24 11:06:41.287: INFO: Created: latency-svc-bl5cz
    Aug 24 11:06:41.291: INFO: Created: latency-svc-q2kgq
    Aug 24 11:06:41.346: INFO: Got endpoints: latency-svc-4d7ql [278.263988ms]
    Aug 24 11:06:41.369: INFO: Got endpoints: latency-svc-p7mxw [270.441013ms]
    Aug 24 11:06:41.369: INFO: Got endpoints: latency-svc-bl5cz [248.993827ms]
    Aug 24 11:06:41.379: INFO: Got endpoints: latency-svc-vjchf [519.869334ms]
    Aug 24 11:06:41.387: INFO: Created: latency-svc-gzcbm
    Aug 24 11:06:41.395: INFO: Got endpoints: latency-svc-6hppx [536.302442ms]
    Aug 24 11:06:41.417: INFO: Got endpoints: latency-svc-rq4mv [416.906853ms]
    Aug 24 11:06:41.417: INFO: Got endpoints: latency-svc-v72qr [557.652697ms]
    Aug 24 11:06:41.423: INFO: Created: latency-svc-wjzzk
    Aug 24 11:06:41.435: INFO: Got endpoints: latency-svc-sx4kp [575.474654ms]
    Aug 24 11:06:41.435: INFO: Got endpoints: latency-svc-x6gwf [575.75445ms]
    Aug 24 11:06:41.451: INFO: Created: latency-svc-hcxw4
    Aug 24 11:06:41.452: INFO: Got endpoints: latency-svc-zpbbk [422.845674ms]
    Aug 24 11:06:41.452: INFO: Got endpoints: latency-svc-q2kgq [492.921868ms]
    Aug 24 11:06:41.452: INFO: Got endpoints: latency-svc-5th6p [593.26269ms]
    Aug 24 11:06:41.473: INFO: Got endpoints: latency-svc-l6cvd [409.186634ms]
    Aug 24 11:06:41.473: INFO: Got endpoints: latency-svc-jx8fx [557.215103ms]
    Aug 24 11:06:41.474: INFO: Got endpoints: latency-svc-wjzzk [127.584137ms]
    Aug 24 11:06:41.482: INFO: Created: latency-svc-5frl5
    Aug 24 11:06:41.488: INFO: Got endpoints: latency-svc-gzcbm [210.589048ms]
    Aug 24 11:06:41.488: INFO: Got endpoints: latency-svc-hcxw4 [119.351961ms]
    Aug 24 11:06:41.506: INFO: Got endpoints: latency-svc-5frl5 [126.520949ms]
    Aug 24 11:06:41.607: INFO: Created: latency-svc-tdtzx
    Aug 24 11:06:41.615: INFO: Created: latency-svc-cz7md
    Aug 24 11:06:41.615: INFO: Created: latency-svc-hlz5j
    Aug 24 11:06:41.616: INFO: Created: latency-svc-8qzx9
    Aug 24 11:06:41.624: INFO: Created: latency-svc-9kjgs
    Aug 24 11:06:41.624: INFO: Created: latency-svc-p7xlc
    Aug 24 11:06:41.624: INFO: Created: latency-svc-bnxfs
    Aug 24 11:06:41.624: INFO: Created: latency-svc-xl8jk
    Aug 24 11:06:41.624: INFO: Created: latency-svc-kh66q
    Aug 24 11:06:41.624: INFO: Created: latency-svc-l9j9x
    Aug 24 11:06:41.625: INFO: Created: latency-svc-j2dzs
    Aug 24 11:06:41.626: INFO: Created: latency-svc-q4jc4
    Aug 24 11:06:41.627: INFO: Created: latency-svc-tjwpz
    Aug 24 11:06:41.627: INFO: Created: latency-svc-286zx
    Aug 24 11:06:41.628: INFO: Created: latency-svc-rf7nv
    Aug 24 11:06:41.668: INFO: Got endpoints: latency-svc-q4jc4 [251.303888ms]
    Aug 24 11:06:41.668: INFO: Got endpoints: latency-svc-tdtzx [194.964584ms]
    Aug 24 11:06:41.668: INFO: Got endpoints: latency-svc-tjwpz [299.38354ms]
    Aug 24 11:06:41.677: INFO: Got endpoints: latency-svc-hlz5j [171.076271ms]
    Aug 24 11:06:41.677: INFO: Got endpoints: latency-svc-9kjgs [224.955095ms]
    Aug 24 11:06:41.704: INFO: Got endpoints: latency-svc-l9j9x [251.098675ms]
    Aug 24 11:06:41.704: INFO: Got endpoints: latency-svc-bnxfs [252.138981ms]
    Aug 24 11:06:41.705: INFO: Got endpoints: latency-svc-j2dzs [231.574044ms]
    Aug 24 11:06:41.711: INFO: Got endpoints: latency-svc-rf7nv [315.589145ms]
    Aug 24 11:06:41.711: INFO: Got endpoints: latency-svc-286zx [294.449617ms]
    Aug 24 11:06:41.733: INFO: Got endpoints: latency-svc-xl8jk [297.114347ms]
    Aug 24 11:06:41.733: INFO: Created: latency-svc-2j5r9
    Aug 24 11:06:41.742: INFO: Got endpoints: latency-svc-p7xlc [253.638943ms]
    Aug 24 11:06:41.745: INFO: Got endpoints: latency-svc-cz7md [271.721947ms]
    Aug 24 11:06:41.752: INFO: Got endpoints: latency-svc-8qzx9 [264.152236ms]
    Aug 24 11:06:41.753: INFO: Got endpoints: latency-svc-kh66q [317.411347ms]
    Aug 24 11:06:41.755: INFO: Got endpoints: latency-svc-2j5r9 [85.764952ms]
    Aug 24 11:06:41.857: INFO: Created: latency-svc-fpd46
    Aug 24 11:06:41.870: INFO: Created: latency-svc-65rcc
    Aug 24 11:06:41.871: INFO: Created: latency-svc-x6q22
    Aug 24 11:06:41.871: INFO: Created: latency-svc-qgzrb
    Aug 24 11:06:41.871: INFO: Created: latency-svc-xhmqn
    Aug 24 11:06:41.871: INFO: Created: latency-svc-z9cvv
    Aug 24 11:06:41.872: INFO: Created: latency-svc-lt22v
    Aug 24 11:06:41.872: INFO: Created: latency-svc-csk7f
    Aug 24 11:06:41.872: INFO: Created: latency-svc-fbfsb
    Aug 24 11:06:41.872: INFO: Created: latency-svc-lvxgf
    Aug 24 11:06:41.871: INFO: Created: latency-svc-mffcr
    Aug 24 11:06:41.872: INFO: Created: latency-svc-46n6w
    Aug 24 11:06:41.872: INFO: Created: latency-svc-vrfp6
    Aug 24 11:06:41.872: INFO: Created: latency-svc-xw8dm
    Aug 24 11:06:41.872: INFO: Created: latency-svc-vggtv
    Aug 24 11:06:41.873: INFO: Got endpoints: latency-svc-fpd46 [120.358116ms]
    Aug 24 11:06:41.960: INFO: Got endpoints: latency-svc-lvxgf [249.147755ms]
    Aug 24 11:06:41.961: INFO: Got endpoints: latency-svc-xw8dm [255.923794ms]
    Aug 24 11:06:41.961: INFO: Got endpoints: latency-svc-xhmqn [206.02882ms]
    Aug 24 11:06:41.972: INFO: Got endpoints: latency-svc-46n6w [266.928026ms]
    Aug 24 11:06:41.972: INFO: Got endpoints: latency-svc-csk7f [239.454093ms]
    Aug 24 11:06:41.995: INFO: Created: latency-svc-tqlc4
    Aug 24 11:06:41.996: INFO: Got endpoints: latency-svc-vrfp6 [318.326939ms]
    Aug 24 11:06:41.996: INFO: Got endpoints: latency-svc-qgzrb [243.144149ms]
    Aug 24 11:06:42.017: INFO: Created: latency-svc-fx9pp
    Aug 24 11:06:42.017: INFO: Got endpoints: latency-svc-lt22v [347.74564ms]
    Aug 24 11:06:42.054: INFO: Got endpoints: latency-svc-vggtv [312.076443ms]
    Aug 24 11:06:42.093: INFO: Created: latency-svc-xvl8h
    Aug 24 11:06:42.102: INFO: Created: latency-svc-zd8dp
    Aug 24 11:06:42.102: INFO: Created: latency-svc-rwtj8
    Aug 24 11:06:42.102: INFO: Created: latency-svc-58pqp
    Aug 24 11:06:42.102: INFO: Created: latency-svc-n8f9n
    Aug 24 11:06:42.103: INFO: Created: latency-svc-55qgq
    Aug 24 11:06:42.103: INFO: Created: latency-svc-7lc4d
    Aug 24 11:06:42.103: INFO: Created: latency-svc-gz8pm
    Aug 24 11:06:42.115: INFO: Got endpoints: latency-svc-mffcr [444.641081ms]
    Aug 24 11:06:42.155: INFO: Created: latency-svc-vpnrd
    Aug 24 11:06:42.164: INFO: Got endpoints: latency-svc-z9cvv [486.695278ms]
    Aug 24 11:06:42.181: INFO: Created: latency-svc-n4qd6
    Aug 24 11:06:42.206: INFO: Got endpoints: latency-svc-fbfsb [502.481874ms]
    Aug 24 11:06:42.224: INFO: Created: latency-svc-7flhq
    Aug 24 11:06:42.252: INFO: Got endpoints: latency-svc-x6q22 [505.962602ms]
    Aug 24 11:06:42.270: INFO: Created: latency-svc-hzmv5
    Aug 24 11:06:42.302: INFO: Got endpoints: latency-svc-65rcc [590.628059ms]
    Aug 24 11:06:42.322: INFO: Created: latency-svc-lvn7p
    Aug 24 11:06:42.363: INFO: Got endpoints: latency-svc-tqlc4 [490.397305ms]
    Aug 24 11:06:42.381: INFO: Created: latency-svc-zf5zv
    Aug 24 11:06:42.409: INFO: Got endpoints: latency-svc-fx9pp [448.145665ms]
    Aug 24 11:06:42.423: INFO: Created: latency-svc-zsbtt
    Aug 24 11:06:42.452: INFO: Got endpoints: latency-svc-xvl8h [435.134202ms]
    Aug 24 11:06:42.475: INFO: Created: latency-svc-s8w8p
    Aug 24 11:06:42.504: INFO: Got endpoints: latency-svc-rwtj8 [449.281166ms]
    Aug 24 11:06:42.520: INFO: Created: latency-svc-gpdkb
    Aug 24 11:06:42.554: INFO: Got endpoints: latency-svc-gz8pm [592.719265ms]
    Aug 24 11:06:42.577: INFO: Created: latency-svc-97whh
    Aug 24 11:06:42.602: INFO: Got endpoints: latency-svc-n8f9n [606.675862ms]
    Aug 24 11:06:42.621: INFO: Created: latency-svc-rn8fr
    Aug 24 11:06:42.657: INFO: Got endpoints: latency-svc-zd8dp [696.522218ms]
    Aug 24 11:06:42.674: INFO: Created: latency-svc-d8jvc
    Aug 24 11:06:42.708: INFO: Got endpoints: latency-svc-7lc4d [735.656225ms]
    Aug 24 11:06:42.724: INFO: Created: latency-svc-j92t8
    Aug 24 11:06:42.755: INFO: Got endpoints: latency-svc-55qgq [758.877518ms]
    Aug 24 11:06:42.770: INFO: Created: latency-svc-6j2s7
    Aug 24 11:06:42.805: INFO: Got endpoints: latency-svc-58pqp [833.139603ms]
    Aug 24 11:06:42.822: INFO: Created: latency-svc-89wtb
    Aug 24 11:06:42.859: INFO: Got endpoints: latency-svc-vpnrd [744.01112ms]
    Aug 24 11:06:42.881: INFO: Created: latency-svc-qwxtj
    Aug 24 11:06:42.912: INFO: Got endpoints: latency-svc-n4qd6 [748.182842ms]
    Aug 24 11:06:42.933: INFO: Created: latency-svc-kbx99
    Aug 24 11:06:42.961: INFO: Got endpoints: latency-svc-7flhq [754.608577ms]
    Aug 24 11:06:42.985: INFO: Created: latency-svc-km7n8
    Aug 24 11:06:43.009: INFO: Got endpoints: latency-svc-hzmv5 [757.896784ms]
    Aug 24 11:06:43.029: INFO: Created: latency-svc-p72vn
    Aug 24 11:06:43.052: INFO: Got endpoints: latency-svc-lvn7p [750.258393ms]
    Aug 24 11:06:43.079: INFO: Created: latency-svc-fdhdn
    Aug 24 11:06:43.103: INFO: Got endpoints: latency-svc-zf5zv [739.601238ms]
    Aug 24 11:06:43.129: INFO: Created: latency-svc-n9qrk
    Aug 24 11:06:43.160: INFO: Got endpoints: latency-svc-zsbtt [751.469366ms]
    Aug 24 11:06:43.179: INFO: Created: latency-svc-d2vx5
    Aug 24 11:06:43.203: INFO: Got endpoints: latency-svc-s8w8p [751.119995ms]
    Aug 24 11:06:43.226: INFO: Created: latency-svc-sqg8r
    Aug 24 11:06:43.260: INFO: Got endpoints: latency-svc-gpdkb [756.503203ms]
    Aug 24 11:06:43.276: INFO: Created: latency-svc-22njr
    Aug 24 11:06:43.307: INFO: Got endpoints: latency-svc-97whh [753.654437ms]
    Aug 24 11:06:43.324: INFO: Created: latency-svc-hxk8s
    Aug 24 11:06:43.352: INFO: Got endpoints: latency-svc-rn8fr [749.56092ms]
    Aug 24 11:06:43.374: INFO: Created: latency-svc-kq9kx
    Aug 24 11:06:43.407: INFO: Got endpoints: latency-svc-d8jvc [749.026119ms]
    Aug 24 11:06:43.426: INFO: Created: latency-svc-lknmk
    Aug 24 11:06:43.457: INFO: Got endpoints: latency-svc-j92t8 [748.791703ms]
    Aug 24 11:06:43.473: INFO: Created: latency-svc-dqqlh
    Aug 24 11:06:43.510: INFO: Got endpoints: latency-svc-6j2s7 [755.507669ms]
    Aug 24 11:06:43.528: INFO: Created: latency-svc-b28gp
    Aug 24 11:06:43.559: INFO: Got endpoints: latency-svc-89wtb [754.175968ms]
    Aug 24 11:06:43.626: INFO: Created: latency-svc-n22tr
    Aug 24 11:06:43.703: INFO: Got endpoints: latency-svc-qwxtj [843.455106ms]
    Aug 24 11:06:43.713: INFO: Got endpoints: latency-svc-kbx99 [801.090921ms]
    Aug 24 11:06:43.723: INFO: Got endpoints: latency-svc-km7n8 [762.12822ms]
    Aug 24 11:06:43.747: INFO: Created: latency-svc-48c46
    Aug 24 11:06:43.766: INFO: Got endpoints: latency-svc-p72vn [756.232606ms]
    Aug 24 11:06:43.768: INFO: Created: latency-svc-jt766
    Aug 24 11:06:43.787: INFO: Created: latency-svc-kfbrc
    Aug 24 11:06:43.797: INFO: Created: latency-svc-6qpb5
    Aug 24 11:06:43.809: INFO: Got endpoints: latency-svc-fdhdn [756.873677ms]
    Aug 24 11:06:43.832: INFO: Created: latency-svc-j74cd
    Aug 24 11:06:43.867: INFO: Got endpoints: latency-svc-n9qrk [764.025862ms]
    Aug 24 11:06:43.899: INFO: Created: latency-svc-kqqq5
    Aug 24 11:06:43.909: INFO: Got endpoints: latency-svc-d2vx5 [749.084621ms]
    Aug 24 11:06:43.937: INFO: Created: latency-svc-9j2qp
    Aug 24 11:06:43.971: INFO: Got endpoints: latency-svc-sqg8r [768.163889ms]
    Aug 24 11:06:43.999: INFO: Created: latency-svc-c7gjj
    Aug 24 11:06:44.014: INFO: Got endpoints: latency-svc-22njr [753.548614ms]
    Aug 24 11:06:44.034: INFO: Created: latency-svc-2wtwt
    Aug 24 11:06:44.059: INFO: Got endpoints: latency-svc-hxk8s [751.828788ms]
    Aug 24 11:06:44.075: INFO: Created: latency-svc-hh4wp
    Aug 24 11:06:44.107: INFO: Got endpoints: latency-svc-kq9kx [755.529381ms]
    Aug 24 11:06:44.137: INFO: Created: latency-svc-tlkdf
    Aug 24 11:06:44.167: INFO: Got endpoints: latency-svc-lknmk [759.884016ms]
    Aug 24 11:06:44.192: INFO: Created: latency-svc-bfwh7
    Aug 24 11:06:44.206: INFO: Got endpoints: latency-svc-dqqlh [749.456613ms]
    Aug 24 11:06:44.238: INFO: Created: latency-svc-4bmll
    Aug 24 11:06:44.255: INFO: Got endpoints: latency-svc-b28gp [744.523428ms]
    Aug 24 11:06:44.279: INFO: Created: latency-svc-95nrh
    Aug 24 11:06:44.315: INFO: Got endpoints: latency-svc-n22tr [755.437769ms]
    Aug 24 11:06:44.337: INFO: Created: latency-svc-29rws
    Aug 24 11:06:44.359: INFO: Got endpoints: latency-svc-48c46 [656.468927ms]
    Aug 24 11:06:44.377: INFO: Created: latency-svc-8gkqn
    Aug 24 11:06:44.407: INFO: Got endpoints: latency-svc-jt766 [693.598513ms]
    Aug 24 11:06:44.442: INFO: Created: latency-svc-8xtlc
    Aug 24 11:06:44.454: INFO: Got endpoints: latency-svc-kfbrc [730.328082ms]
    Aug 24 11:06:44.473: INFO: Created: latency-svc-m26g9
    Aug 24 11:06:44.509: INFO: Got endpoints: latency-svc-6qpb5 [743.176813ms]
    Aug 24 11:06:44.538: INFO: Created: latency-svc-wp4bk
    Aug 24 11:06:44.554: INFO: Got endpoints: latency-svc-j74cd [744.513313ms]
    Aug 24 11:06:44.573: INFO: Created: latency-svc-b29gt
    Aug 24 11:06:44.606: INFO: Got endpoints: latency-svc-kqqq5 [738.231877ms]
    Aug 24 11:06:44.621: INFO: Created: latency-svc-lq9rn
    Aug 24 11:06:44.658: INFO: Got endpoints: latency-svc-9j2qp [748.298593ms]
    Aug 24 11:06:44.672: INFO: Created: latency-svc-mwjxc
    Aug 24 11:06:44.709: INFO: Got endpoints: latency-svc-c7gjj [736.945708ms]
    Aug 24 11:06:44.728: INFO: Created: latency-svc-rlrlx
    Aug 24 11:06:44.758: INFO: Got endpoints: latency-svc-2wtwt [743.799183ms]
    Aug 24 11:06:44.776: INFO: Created: latency-svc-6z9nd
    Aug 24 11:06:44.802: INFO: Got endpoints: latency-svc-hh4wp [743.039339ms]
    Aug 24 11:06:44.825: INFO: Created: latency-svc-mm64f
    Aug 24 11:06:44.858: INFO: Got endpoints: latency-svc-tlkdf [750.341101ms]
    Aug 24 11:06:44.884: INFO: Created: latency-svc-68464
    Aug 24 11:06:44.904: INFO: Got endpoints: latency-svc-bfwh7 [736.870461ms]
    Aug 24 11:06:44.926: INFO: Created: latency-svc-km5hb
    Aug 24 11:06:44.977: INFO: Got endpoints: latency-svc-4bmll [770.696345ms]
    Aug 24 11:06:45.012: INFO: Created: latency-svc-4wh6h
    Aug 24 11:06:45.017: INFO: Got endpoints: latency-svc-95nrh [762.321311ms]
    Aug 24 11:06:45.039: INFO: Created: latency-svc-ftldd
    Aug 24 11:06:45.058: INFO: Got endpoints: latency-svc-29rws [743.800701ms]
    Aug 24 11:06:45.080: INFO: Created: latency-svc-l6kb6
    Aug 24 11:06:45.101: INFO: Got endpoints: latency-svc-8gkqn [742.103183ms]
    Aug 24 11:06:45.125: INFO: Created: latency-svc-b52xh
    Aug 24 11:06:45.155: INFO: Got endpoints: latency-svc-8xtlc [748.315052ms]
    Aug 24 11:06:45.170: INFO: Created: latency-svc-8v7jj
    Aug 24 11:06:45.206: INFO: Got endpoints: latency-svc-m26g9 [752.326257ms]
    Aug 24 11:06:45.234: INFO: Created: latency-svc-sqs6v
    Aug 24 11:06:45.252: INFO: Got endpoints: latency-svc-wp4bk [743.158296ms]
    Aug 24 11:06:45.269: INFO: Created: latency-svc-k8d7c
    Aug 24 11:06:45.303: INFO: Got endpoints: latency-svc-b29gt [748.399443ms]
    Aug 24 11:06:45.321: INFO: Created: latency-svc-qqk5h
    Aug 24 11:06:45.357: INFO: Got endpoints: latency-svc-lq9rn [751.168339ms]
    Aug 24 11:06:45.374: INFO: Created: latency-svc-vpgwq
    Aug 24 11:06:45.402: INFO: Got endpoints: latency-svc-mwjxc [743.802133ms]
    Aug 24 11:06:45.420: INFO: Created: latency-svc-j27gl
    Aug 24 11:06:45.456: INFO: Got endpoints: latency-svc-rlrlx [747.296291ms]
    Aug 24 11:06:45.473: INFO: Created: latency-svc-b85h5
    Aug 24 11:06:45.509: INFO: Got endpoints: latency-svc-6z9nd [750.697583ms]
    Aug 24 11:06:45.526: INFO: Created: latency-svc-44f4f
    Aug 24 11:06:45.559: INFO: Got endpoints: latency-svc-mm64f [756.867739ms]
    Aug 24 11:06:45.575: INFO: Created: latency-svc-8nnbm
    Aug 24 11:06:45.609: INFO: Got endpoints: latency-svc-68464 [751.30257ms]
    Aug 24 11:06:45.631: INFO: Created: latency-svc-d5snc
    Aug 24 11:06:45.655: INFO: Got endpoints: latency-svc-km5hb [751.230679ms]
    Aug 24 11:06:45.670: INFO: Created: latency-svc-nk9rf
    Aug 24 11:06:45.707: INFO: Got endpoints: latency-svc-4wh6h [729.545246ms]
    Aug 24 11:06:45.731: INFO: Created: latency-svc-tjrv7
    Aug 24 11:06:45.756: INFO: Got endpoints: latency-svc-ftldd [738.0504ms]
    Aug 24 11:06:45.775: INFO: Created: latency-svc-sxzqk
    Aug 24 11:06:45.813: INFO: Got endpoints: latency-svc-l6kb6 [754.653211ms]
    Aug 24 11:06:45.836: INFO: Created: latency-svc-c955p
    Aug 24 11:06:45.870: INFO: Got endpoints: latency-svc-b52xh [768.76021ms]
    Aug 24 11:06:45.941: INFO: Got endpoints: latency-svc-8v7jj [785.231143ms]
    Aug 24 11:06:45.952: INFO: Created: latency-svc-jhfq7
    Aug 24 11:06:45.976: INFO: Got endpoints: latency-svc-sqs6v [770.424681ms]
    Aug 24 11:06:45.997: INFO: Created: latency-svc-pj8rl
    Aug 24 11:06:46.014: INFO: Got endpoints: latency-svc-k8d7c [762.323534ms]
    Aug 24 11:06:46.016: INFO: Created: latency-svc-hdkpn
    Aug 24 11:06:46.069: INFO: Got endpoints: latency-svc-qqk5h [766.76604ms]
    Aug 24 11:06:46.077: INFO: Created: latency-svc-lbbfp
    Aug 24 11:06:46.090: INFO: Created: latency-svc-95955
    Aug 24 11:06:46.103: INFO: Got endpoints: latency-svc-vpgwq [745.47658ms]
    Aug 24 11:06:46.124: INFO: Created: latency-svc-fhm5p
    Aug 24 11:06:46.174: INFO: Got endpoints: latency-svc-j27gl [771.995796ms]
    Aug 24 11:06:46.202: INFO: Created: latency-svc-br8xl
    Aug 24 11:06:46.208: INFO: Got endpoints: latency-svc-b85h5 [751.190457ms]
    Aug 24 11:06:46.239: INFO: Created: latency-svc-jn85s
    Aug 24 11:06:46.260: INFO: Got endpoints: latency-svc-44f4f [751.469441ms]
    Aug 24 11:06:46.281: INFO: Created: latency-svc-spvfz
    Aug 24 11:06:46.307: INFO: Got endpoints: latency-svc-8nnbm [747.750083ms]
    Aug 24 11:06:46.330: INFO: Created: latency-svc-dztd9
    Aug 24 11:06:46.351: INFO: Got endpoints: latency-svc-d5snc [741.998064ms]
    Aug 24 11:06:46.372: INFO: Created: latency-svc-qst5m
    Aug 24 11:06:46.408: INFO: Got endpoints: latency-svc-nk9rf [753.052167ms]
    Aug 24 11:06:46.426: INFO: Created: latency-svc-nq2fl
    Aug 24 11:06:46.466: INFO: Got endpoints: latency-svc-tjrv7 [759.64131ms]
    Aug 24 11:06:46.481: INFO: Created: latency-svc-hcpll
    Aug 24 11:06:46.503: INFO: Got endpoints: latency-svc-sxzqk [747.839483ms]
    Aug 24 11:06:46.522: INFO: Created: latency-svc-77qwl
    Aug 24 11:06:46.559: INFO: Got endpoints: latency-svc-c955p [745.427662ms]
    Aug 24 11:06:46.581: INFO: Created: latency-svc-ght4z
    Aug 24 11:06:46.608: INFO: Got endpoints: latency-svc-jhfq7 [737.567173ms]
    Aug 24 11:06:46.636: INFO: Created: latency-svc-jc2mm
    Aug 24 11:06:46.656: INFO: Got endpoints: latency-svc-pj8rl [714.828684ms]
    Aug 24 11:06:46.672: INFO: Created: latency-svc-8bd75
    Aug 24 11:06:46.717: INFO: Got endpoints: latency-svc-hdkpn [741.061137ms]
    Aug 24 11:06:46.734: INFO: Created: latency-svc-l296h
    Aug 24 11:06:46.754: INFO: Got endpoints: latency-svc-lbbfp [738.900233ms]
    Aug 24 11:06:46.771: INFO: Created: latency-svc-755ll
    Aug 24 11:06:46.808: INFO: Got endpoints: latency-svc-95955 [738.471491ms]
    Aug 24 11:06:46.830: INFO: Created: latency-svc-dmk64
    Aug 24 11:06:46.853: INFO: Got endpoints: latency-svc-fhm5p [750.576144ms]
    Aug 24 11:06:46.889: INFO: Created: latency-svc-78q67
    Aug 24 11:06:46.921: INFO: Got endpoints: latency-svc-br8xl [747.445784ms]
    Aug 24 11:06:46.948: INFO: Created: latency-svc-fwkh9
    Aug 24 11:06:46.966: INFO: Got endpoints: latency-svc-jn85s [757.751541ms]
    Aug 24 11:06:46.984: INFO: Created: latency-svc-m4s7t
    Aug 24 11:06:47.003: INFO: Got endpoints: latency-svc-spvfz [742.824254ms]
    Aug 24 11:06:47.025: INFO: Created: latency-svc-xfh4r
    Aug 24 11:06:47.051: INFO: Got endpoints: latency-svc-dztd9 [744.256825ms]
    Aug 24 11:06:47.076: INFO: Created: latency-svc-bbd4g
    Aug 24 11:06:47.106: INFO: Got endpoints: latency-svc-qst5m [754.376336ms]
    Aug 24 11:06:47.140: INFO: Created: latency-svc-9w9tg
    Aug 24 11:06:47.156: INFO: Got endpoints: latency-svc-nq2fl [747.810792ms]
    Aug 24 11:06:47.177: INFO: Created: latency-svc-hhhf5
    Aug 24 11:06:47.202: INFO: Got endpoints: latency-svc-hcpll [736.049449ms]
    Aug 24 11:06:47.219: INFO: Created: latency-svc-8hczg
    Aug 24 11:06:47.269: INFO: Got endpoints: latency-svc-77qwl [765.116804ms]
    Aug 24 11:06:47.291: INFO: Created: latency-svc-t5b5z
    Aug 24 11:06:47.303: INFO: Got endpoints: latency-svc-ght4z [744.535775ms]
    Aug 24 11:06:47.326: INFO: Created: latency-svc-7vwtm
    Aug 24 11:06:47.371: INFO: Got endpoints: latency-svc-jc2mm [763.069584ms]
    Aug 24 11:06:47.388: INFO: Created: latency-svc-2z7nb
    Aug 24 11:06:47.406: INFO: Got endpoints: latency-svc-8bd75 [750.425901ms]
    Aug 24 11:06:47.425: INFO: Created: latency-svc-dqcbw
    Aug 24 11:06:47.460: INFO: Got endpoints: latency-svc-l296h [742.59212ms]
    Aug 24 11:06:47.479: INFO: Created: latency-svc-dfvlb
    Aug 24 11:06:47.515: INFO: Got endpoints: latency-svc-755ll [761.101677ms]
    Aug 24 11:06:47.535: INFO: Created: latency-svc-75gvm
    Aug 24 11:06:47.557: INFO: Got endpoints: latency-svc-dmk64 [748.810668ms]
    Aug 24 11:06:47.574: INFO: Created: latency-svc-rb7m6
    Aug 24 11:06:47.603: INFO: Got endpoints: latency-svc-78q67 [750.104566ms]
    Aug 24 11:06:47.620: INFO: Created: latency-svc-qrbb7
    Aug 24 11:06:47.655: INFO: Got endpoints: latency-svc-fwkh9 [733.63047ms]
    Aug 24 11:06:47.668: INFO: Created: latency-svc-rs54d
    Aug 24 11:06:47.709: INFO: Got endpoints: latency-svc-m4s7t [742.716531ms]
    Aug 24 11:06:47.743: INFO: Created: latency-svc-jqtx7
    Aug 24 11:06:47.755: INFO: Got endpoints: latency-svc-xfh4r [751.793354ms]
    Aug 24 11:06:47.773: INFO: Created: latency-svc-ltxpz
    Aug 24 11:06:47.812: INFO: Got endpoints: latency-svc-bbd4g [760.704799ms]
    Aug 24 11:06:47.826: INFO: Created: latency-svc-cznrf
    Aug 24 11:06:47.859: INFO: Got endpoints: latency-svc-9w9tg [753.629829ms]
    Aug 24 11:06:47.883: INFO: Created: latency-svc-sn78c
    Aug 24 11:06:47.907: INFO: Got endpoints: latency-svc-hhhf5 [750.913935ms]
    Aug 24 11:06:47.931: INFO: Created: latency-svc-pvll5
    Aug 24 11:06:47.960: INFO: Got endpoints: latency-svc-8hczg [757.923098ms]
    Aug 24 11:06:47.985: INFO: Created: latency-svc-hfkjw
    Aug 24 11:06:48.025: INFO: Got endpoints: latency-svc-t5b5z [756.27789ms]
    Aug 24 11:06:48.079: INFO: Created: latency-svc-h8586
    Aug 24 11:06:48.084: INFO: Got endpoints: latency-svc-7vwtm [780.355292ms]
    Aug 24 11:06:48.127: INFO: Got endpoints: latency-svc-2z7nb [756.459843ms]
    Aug 24 11:06:48.143: INFO: Created: latency-svc-fnqvm
    Aug 24 11:06:48.158: INFO: Created: latency-svc-65fkh
    Aug 24 11:06:48.173: INFO: Got endpoints: latency-svc-dqcbw [766.629742ms]
    Aug 24 11:06:48.215: INFO: Created: latency-svc-bm8qc
    Aug 24 11:06:48.224: INFO: Got endpoints: latency-svc-dfvlb [763.607645ms]
    Aug 24 11:06:48.253: INFO: Created: latency-svc-jccjn
    Aug 24 11:06:48.266: INFO: Got endpoints: latency-svc-75gvm [750.760244ms]
    Aug 24 11:06:48.284: INFO: Created: latency-svc-sg4vr
    Aug 24 11:06:48.310: INFO: Got endpoints: latency-svc-rb7m6 [753.500478ms]
    Aug 24 11:06:48.332: INFO: Created: latency-svc-d25vp
    Aug 24 11:06:48.352: INFO: Got endpoints: latency-svc-qrbb7 [748.550088ms]
    Aug 24 11:06:48.369: INFO: Created: latency-svc-45hps
    Aug 24 11:06:48.409: INFO: Got endpoints: latency-svc-rs54d [754.236321ms]
    Aug 24 11:06:48.425: INFO: Created: latency-svc-vgdf2
    Aug 24 11:06:48.462: INFO: Got endpoints: latency-svc-jqtx7 [752.498009ms]
    Aug 24 11:06:48.480: INFO: Created: latency-svc-t749q
    Aug 24 11:06:48.507: INFO: Got endpoints: latency-svc-ltxpz [751.724657ms]
    Aug 24 11:06:48.523: INFO: Created: latency-svc-xxcq7
    Aug 24 11:06:48.557: INFO: Got endpoints: latency-svc-cznrf [744.283903ms]
    Aug 24 11:06:48.583: INFO: Created: latency-svc-2sdpx
    Aug 24 11:06:48.607: INFO: Got endpoints: latency-svc-sn78c [747.295965ms]
    Aug 24 11:06:48.624: INFO: Created: latency-svc-tgdjp
    Aug 24 11:06:48.677: INFO: Got endpoints: latency-svc-pvll5 [770.105828ms]
    Aug 24 11:06:48.692: INFO: Created: latency-svc-g8l2b
    Aug 24 11:06:48.710: INFO: Got endpoints: latency-svc-hfkjw [749.178208ms]
    Aug 24 11:06:48.753: INFO: Got endpoints: latency-svc-h8586 [728.176756ms]
    Aug 24 11:06:48.803: INFO: Got endpoints: latency-svc-fnqvm [719.449936ms]
    Aug 24 11:06:48.856: INFO: Got endpoints: latency-svc-65fkh [728.770255ms]
    Aug 24 11:06:48.905: INFO: Got endpoints: latency-svc-bm8qc [731.846479ms]
    Aug 24 11:06:48.964: INFO: Got endpoints: latency-svc-jccjn [739.992585ms]
    Aug 24 11:06:49.010: INFO: Got endpoints: latency-svc-sg4vr [744.528106ms]
    Aug 24 11:06:49.056: INFO: Got endpoints: latency-svc-d25vp [745.56673ms]
    Aug 24 11:06:49.107: INFO: Got endpoints: latency-svc-45hps [755.003681ms]
    Aug 24 11:06:49.152: INFO: Got endpoints: latency-svc-vgdf2 [742.607542ms]
    Aug 24 11:06:49.220: INFO: Got endpoints: latency-svc-t749q [757.816628ms]
    Aug 24 11:06:49.255: INFO: Got endpoints: latency-svc-xxcq7 [748.513539ms]
    Aug 24 11:06:49.312: INFO: Got endpoints: latency-svc-2sdpx [755.584542ms]
    Aug 24 11:06:49.361: INFO: Got endpoints: latency-svc-tgdjp [754.488727ms]
    Aug 24 11:06:49.405: INFO: Got endpoints: latency-svc-g8l2b [727.657204ms]
    Aug 24 11:06:49.405: INFO: Latencies: [56.668447ms 85.764952ms 100.077696ms 119.351961ms 120.358116ms 126.520949ms 127.584137ms 141.029808ms 170.576825ms 171.076271ms 194.964584ms 204.30721ms 206.02882ms 208.226751ms 210.589048ms 224.955095ms 231.574044ms 239.353081ms 239.454093ms 243.144149ms 248.993827ms 249.147755ms 251.098675ms 251.303888ms 252.138981ms 253.638943ms 255.923794ms 260.46105ms 264.152236ms 266.928026ms 270.441013ms 271.721947ms 278.263988ms 294.449617ms 297.114347ms 299.38354ms 312.076443ms 315.589145ms 317.411347ms 318.326939ms 347.74564ms 409.186634ms 416.906853ms 418.95745ms 422.845674ms 435.134202ms 444.641081ms 448.145665ms 449.281166ms 486.695278ms 490.397305ms 492.921868ms 502.481874ms 505.962602ms 519.869334ms 536.302442ms 557.215103ms 557.652697ms 575.474654ms 575.75445ms 590.628059ms 592.719265ms 593.26269ms 606.675862ms 656.468927ms 693.598513ms 696.522218ms 714.828684ms 719.449936ms 727.657204ms 728.176756ms 728.770255ms 729.545246ms 730.328082ms 731.846479ms 733.63047ms 735.656225ms 736.049449ms 736.870461ms 736.945708ms 737.567173ms 738.0504ms 738.231877ms 738.471491ms 738.900233ms 739.601238ms 739.992585ms 741.061137ms 741.998064ms 742.103183ms 742.59212ms 742.607542ms 742.716531ms 742.824254ms 743.039339ms 743.158296ms 743.176813ms 743.799183ms 743.800701ms 743.802133ms 744.01112ms 744.256825ms 744.283903ms 744.513313ms 744.523428ms 744.528106ms 744.535775ms 745.427662ms 745.47658ms 745.56673ms 747.295965ms 747.296291ms 747.445784ms 747.750083ms 747.810792ms 747.839483ms 748.182842ms 748.298593ms 748.315052ms 748.399443ms 748.513539ms 748.550088ms 748.791703ms 748.810668ms 749.026119ms 749.084621ms 749.178208ms 749.456613ms 749.56092ms 750.104566ms 750.258393ms 750.341101ms 750.425901ms 750.576144ms 750.697583ms 750.760244ms 750.913935ms 751.119995ms 751.168339ms 751.190457ms 751.230679ms 751.30257ms 751.469366ms 751.469441ms 751.724657ms 751.793354ms 751.828788ms 752.326257ms 752.498009ms 753.052167ms 753.500478ms 753.548614ms 753.629829ms 753.654437ms 754.175968ms 754.236321ms 754.376336ms 754.488727ms 754.608577ms 754.653211ms 755.003681ms 755.437769ms 755.507669ms 755.529381ms 755.584542ms 756.232606ms 756.27789ms 756.459843ms 756.503203ms 756.867739ms 756.873677ms 757.751541ms 757.816628ms 757.896784ms 757.923098ms 758.877518ms 759.64131ms 759.884016ms 760.704799ms 761.101677ms 762.12822ms 762.321311ms 762.323534ms 763.069584ms 763.607645ms 764.025862ms 765.116804ms 766.629742ms 766.76604ms 768.163889ms 768.76021ms 770.105828ms 770.424681ms 770.696345ms 771.995796ms 780.355292ms 785.231143ms 801.090921ms 833.139603ms 843.455106ms]
    Aug 24 11:06:49.405: INFO: 50 %ile: 744.01112ms
    Aug 24 11:06:49.405: INFO: 90 %ile: 762.12822ms
    Aug 24 11:06:49.405: INFO: 99 %ile: 833.139603ms
    Aug 24 11:06:49.405: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:49.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-8671" for this suite. 08/24/23 11:06:49.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:49.433
Aug 24 11:06:49.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename disruption 08/24/23 11:06:49.435
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:49.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:49.467
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/24/23 11:06:49.477
STEP: Waiting for the pdb to be processed 08/24/23 11:06:49.489
STEP: updating the pdb 08/24/23 11:06:49.501
STEP: Waiting for the pdb to be processed 08/24/23 11:06:49.533
STEP: patching the pdb 08/24/23 11:06:51.559
STEP: Waiting for the pdb to be processed 08/24/23 11:06:51.574
STEP: Waiting for the pdb to be deleted 08/24/23 11:06:51.594
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:51.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2568" for this suite. 08/24/23 11:06:51.61
------------------------------
â€¢ [2.186 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:49.433
    Aug 24 11:06:49.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename disruption 08/24/23 11:06:49.435
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:49.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:49.467
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/24/23 11:06:49.477
    STEP: Waiting for the pdb to be processed 08/24/23 11:06:49.489
    STEP: updating the pdb 08/24/23 11:06:49.501
    STEP: Waiting for the pdb to be processed 08/24/23 11:06:49.533
    STEP: patching the pdb 08/24/23 11:06:51.559
    STEP: Waiting for the pdb to be processed 08/24/23 11:06:51.574
    STEP: Waiting for the pdb to be deleted 08/24/23 11:06:51.594
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:51.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2568" for this suite. 08/24/23 11:06:51.61
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:51.619
Aug 24 11:06:51.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:06:51.621
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:51.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:51.649
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:06:51.658
Aug 24 11:06:51.673: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5" in namespace "projected-3587" to be "Succeeded or Failed"
Aug 24 11:06:51.682: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.260604ms
Aug 24 11:06:53.687: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013761527s
Aug 24 11:06:55.700: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02681939s
STEP: Saw pod success 08/24/23 11:06:55.7
Aug 24 11:06:55.701: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5" satisfied condition "Succeeded or Failed"
Aug 24 11:06:55.708: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5 container client-container: <nil>
STEP: delete the pod 08/24/23 11:06:55.803
Aug 24 11:06:55.850: INFO: Waiting for pod downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5 to disappear
Aug 24 11:06:55.871: INFO: Pod downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 11:06:55.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3587" for this suite. 08/24/23 11:06:55.881
------------------------------
â€¢ [4.279 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:51.619
    Aug 24 11:06:51.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:06:51.621
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:51.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:51.649
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:06:51.658
    Aug 24 11:06:51.673: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5" in namespace "projected-3587" to be "Succeeded or Failed"
    Aug 24 11:06:51.682: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.260604ms
    Aug 24 11:06:53.687: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013761527s
    Aug 24 11:06:55.700: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02681939s
    STEP: Saw pod success 08/24/23 11:06:55.7
    Aug 24 11:06:55.701: INFO: Pod "downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5" satisfied condition "Succeeded or Failed"
    Aug 24 11:06:55.708: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:06:55.803
    Aug 24 11:06:55.850: INFO: Waiting for pod downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5 to disappear
    Aug 24 11:06:55.871: INFO: Pod downwardapi-volume-8679275e-dbd6-41ad-b341-59072133aac5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:06:55.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3587" for this suite. 08/24/23 11:06:55.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:06:55.899
Aug 24 11:06:55.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:06:55.901
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:55.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:55.959
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/24/23 11:06:55.975
Aug 24 11:06:55.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: mark a version not serverd 08/24/23 11:07:03.726
STEP: check the unserved version gets removed 08/24/23 11:07:03.836
STEP: check the other version is not changed 08/24/23 11:07:05.952
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:07:10.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6657" for this suite. 08/24/23 11:07:10.669
------------------------------
â€¢ [SLOW TEST] [14.788 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:06:55.899
    Aug 24 11:06:55.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:06:55.901
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:06:55.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:06:55.959
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/24/23 11:06:55.975
    Aug 24 11:06:55.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: mark a version not serverd 08/24/23 11:07:03.726
    STEP: check the unserved version gets removed 08/24/23 11:07:03.836
    STEP: check the other version is not changed 08/24/23 11:07:05.952
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:07:10.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6657" for this suite. 08/24/23 11:07:10.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:07:10.689
Aug 24 11:07:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:07:10.69
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:07:10.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:07:10.729
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:07:10.734
Aug 24 11:07:10.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7" in namespace "downward-api-9886" to be "Succeeded or Failed"
Aug 24 11:07:10.759: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.257173ms
Aug 24 11:07:12.765: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016811933s
Aug 24 11:07:14.764: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015606122s
STEP: Saw pod success 08/24/23 11:07:14.764
Aug 24 11:07:14.764: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7" satisfied condition "Succeeded or Failed"
Aug 24 11:07:14.767: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7 container client-container: <nil>
STEP: delete the pod 08/24/23 11:07:14.776
Aug 24 11:07:14.824: INFO: Waiting for pod downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7 to disappear
Aug 24 11:07:14.830: INFO: Pod downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 11:07:14.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9886" for this suite. 08/24/23 11:07:14.835
------------------------------
â€¢ [4.158 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:07:10.689
    Aug 24 11:07:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:07:10.69
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:07:10.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:07:10.729
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:07:10.734
    Aug 24 11:07:10.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7" in namespace "downward-api-9886" to be "Succeeded or Failed"
    Aug 24 11:07:10.759: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.257173ms
    Aug 24 11:07:12.765: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016811933s
    Aug 24 11:07:14.764: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015606122s
    STEP: Saw pod success 08/24/23 11:07:14.764
    Aug 24 11:07:14.764: INFO: Pod "downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7" satisfied condition "Succeeded or Failed"
    Aug 24 11:07:14.767: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:07:14.776
    Aug 24 11:07:14.824: INFO: Waiting for pod downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7 to disappear
    Aug 24 11:07:14.830: INFO: Pod downwardapi-volume-b4f2a915-a511-45dc-a9db-aa2b611457e7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:07:14.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9886" for this suite. 08/24/23 11:07:14.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:07:14.848
Aug 24 11:07:14.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 11:07:14.85
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:07:14.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:07:14.879
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7230 08/24/23 11:07:14.892
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/24/23 11:07:14.908
Aug 24 11:07:14.938: INFO: Found 0 stateful pods, waiting for 3
Aug 24 11:07:24.943: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:07:24.943: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:07:24.943: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:07:24.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 11:07:25.251: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 11:07:25.251: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 11:07:25.251: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 11:07:35.267
Aug 24 11:07:35.291: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/24/23 11:07:35.291
STEP: Updating Pods in reverse ordinal order 08/24/23 11:07:45.308
Aug 24 11:07:45.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 11:07:45.592: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 11:07:45.592: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 11:07:45.592: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 11:07:55.622: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:07:55.623: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:07:55.623: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:05.630: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:08:05.630: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:05.630: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:15.632: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:08:15.632: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:15.632: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:25.634: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:08:25.634: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:25.634: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:35.631: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:08:35.631: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:45.630: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:08:45.630: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:08:55.632: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:08:55.632: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:09:05.633: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:09:05.633: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Rolling back to a previous revision 08/24/23 11:09:15.632
Aug 24 11:09:15.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 11:09:15.972: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 11:09:15.972: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 11:09:15.972: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 11:09:26.030: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/24/23 11:09:36.054
Aug 24 11:09:36.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 11:09:36.322: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 11:09:36.322: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 11:09:36.322: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 11:09:46.345: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
Aug 24 11:09:46.345: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
Aug 24 11:09:46.345: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
Aug 24 11:09:46.345: INFO: Waiting for Pod statefulset-7230/ss2-2 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 11:09:56.354: INFO: Deleting all statefulset in ns statefulset-7230
Aug 24 11:09:56.357: INFO: Scaling statefulset ss2 to 0
Aug 24 11:10:06.382: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 11:10:06.386: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:06.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7230" for this suite. 08/24/23 11:10:06.44
------------------------------
â€¢ [SLOW TEST] [171.601 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:07:14.848
    Aug 24 11:07:14.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 11:07:14.85
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:07:14.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:07:14.879
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7230 08/24/23 11:07:14.892
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/24/23 11:07:14.908
    Aug 24 11:07:14.938: INFO: Found 0 stateful pods, waiting for 3
    Aug 24 11:07:24.943: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:07:24.943: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:07:24.943: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:07:24.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 11:07:25.251: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 11:07:25.251: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 11:07:25.251: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 11:07:35.267
    Aug 24 11:07:35.291: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/24/23 11:07:35.291
    STEP: Updating Pods in reverse ordinal order 08/24/23 11:07:45.308
    Aug 24 11:07:45.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 11:07:45.592: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 11:07:45.592: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 11:07:45.592: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 11:07:55.622: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:07:55.623: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:07:55.623: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:05.630: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:08:05.630: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:05.630: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:15.632: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:08:15.632: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:15.632: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:25.634: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:08:25.634: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:25.634: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:35.631: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:08:35.631: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:45.630: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:08:45.630: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:08:55.632: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:08:55.632: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:09:05.633: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:09:05.633: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Rolling back to a previous revision 08/24/23 11:09:15.632
    Aug 24 11:09:15.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 11:09:15.972: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 11:09:15.972: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 11:09:15.972: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 11:09:26.030: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/24/23 11:09:36.054
    Aug 24 11:09:36.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-7230 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 11:09:36.322: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 11:09:36.322: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 11:09:36.322: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 11:09:46.345: INFO: Waiting for StatefulSet statefulset-7230/ss2 to complete update
    Aug 24 11:09:46.345: INFO: Waiting for Pod statefulset-7230/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
    Aug 24 11:09:46.345: INFO: Waiting for Pod statefulset-7230/ss2-1 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
    Aug 24 11:09:46.345: INFO: Waiting for Pod statefulset-7230/ss2-2 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 11:09:56.354: INFO: Deleting all statefulset in ns statefulset-7230
    Aug 24 11:09:56.357: INFO: Scaling statefulset ss2 to 0
    Aug 24 11:10:06.382: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 11:10:06.386: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:06.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7230" for this suite. 08/24/23 11:10:06.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:06.453
Aug 24 11:10:06.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-runtime 08/24/23 11:10:06.456
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:06.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:06.486
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/24/23 11:10:06.493
STEP: wait for the container to reach Succeeded 08/24/23 11:10:06.515
STEP: get the container status 08/24/23 11:10:10.543
STEP: the container should be terminated 08/24/23 11:10:10.546
STEP: the termination message should be set 08/24/23 11:10:10.546
Aug 24 11:10:10.546: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/24/23 11:10:10.546
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:10.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9247" for this suite. 08/24/23 11:10:10.579
------------------------------
â€¢ [4.135 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:06.453
    Aug 24 11:10:06.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-runtime 08/24/23 11:10:06.456
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:06.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:06.486
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/24/23 11:10:06.493
    STEP: wait for the container to reach Succeeded 08/24/23 11:10:06.515
    STEP: get the container status 08/24/23 11:10:10.543
    STEP: the container should be terminated 08/24/23 11:10:10.546
    STEP: the termination message should be set 08/24/23 11:10:10.546
    Aug 24 11:10:10.546: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/24/23 11:10:10.546
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:10.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9247" for this suite. 08/24/23 11:10:10.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:10.59
Aug 24 11:10:10.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:10:10.591
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:10.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:10.621
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/24/23 11:10:10.628
Aug 24 11:10:10.650: INFO: Waiting up to 5m0s for pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c" in namespace "downward-api-6468" to be "Succeeded or Failed"
Aug 24 11:10:10.657: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.203919ms
Aug 24 11:10:12.664: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01430888s
Aug 24 11:10:14.663: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013270655s
STEP: Saw pod success 08/24/23 11:10:14.663
Aug 24 11:10:14.663: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c" satisfied condition "Succeeded or Failed"
Aug 24 11:10:14.666: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c container dapi-container: <nil>
STEP: delete the pod 08/24/23 11:10:14.719
Aug 24 11:10:14.738: INFO: Waiting for pod downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c to disappear
Aug 24 11:10:14.743: INFO: Pod downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6468" for this suite. 08/24/23 11:10:14.748
------------------------------
â€¢ [4.169 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:10.59
    Aug 24 11:10:10.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:10:10.591
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:10.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:10.621
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/24/23 11:10:10.628
    Aug 24 11:10:10.650: INFO: Waiting up to 5m0s for pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c" in namespace "downward-api-6468" to be "Succeeded or Failed"
    Aug 24 11:10:10.657: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.203919ms
    Aug 24 11:10:12.664: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01430888s
    Aug 24 11:10:14.663: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013270655s
    STEP: Saw pod success 08/24/23 11:10:14.663
    Aug 24 11:10:14.663: INFO: Pod "downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c" satisfied condition "Succeeded or Failed"
    Aug 24 11:10:14.666: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c container dapi-container: <nil>
    STEP: delete the pod 08/24/23 11:10:14.719
    Aug 24 11:10:14.738: INFO: Waiting for pod downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c to disappear
    Aug 24 11:10:14.743: INFO: Pod downward-api-730f9cf6-9fb9-46b2-baa9-acdd6f98d17c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:14.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6468" for this suite. 08/24/23 11:10:14.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:14.76
Aug 24 11:10:14.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replicaset 08/24/23 11:10:14.762
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:14.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:14.791
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/24/23 11:10:14.805
STEP: Verify that the required pods have come up. 08/24/23 11:10:14.814
Aug 24 11:10:14.818: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 24 11:10:19.825: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 11:10:19.825
STEP: Getting /status 08/24/23 11:10:19.826
Aug 24 11:10:19.833: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/24/23 11:10:19.833
Aug 24 11:10:19.855: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/24/23 11:10:19.855
Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: ADDED
Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 11:10:19.860: INFO: Found replicaset test-rs in namespace replicaset-6439 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 11:10:19.860: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/24/23 11:10:19.86
Aug 24 11:10:19.860: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 24 11:10:19.874: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/24/23 11:10:19.875
Aug 24 11:10:19.878: INFO: Observed &ReplicaSet event: ADDED
Aug 24 11:10:19.878: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 11:10:19.878: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 11:10:19.879: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 11:10:19.879: INFO: Observed replicaset test-rs in namespace replicaset-6439 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 11:10:19.879: INFO: Observed &ReplicaSet event: MODIFIED
Aug 24 11:10:19.879: INFO: Found replicaset test-rs in namespace replicaset-6439 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 24 11:10:19.879: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:19.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6439" for this suite. 08/24/23 11:10:19.885
------------------------------
â€¢ [SLOW TEST] [5.137 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:14.76
    Aug 24 11:10:14.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replicaset 08/24/23 11:10:14.762
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:14.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:14.791
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/24/23 11:10:14.805
    STEP: Verify that the required pods have come up. 08/24/23 11:10:14.814
    Aug 24 11:10:14.818: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 24 11:10:19.825: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 11:10:19.825
    STEP: Getting /status 08/24/23 11:10:19.826
    Aug 24 11:10:19.833: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/24/23 11:10:19.833
    Aug 24 11:10:19.855: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/24/23 11:10:19.855
    Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: ADDED
    Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 11:10:19.860: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 11:10:19.860: INFO: Found replicaset test-rs in namespace replicaset-6439 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 11:10:19.860: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/24/23 11:10:19.86
    Aug 24 11:10:19.860: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 24 11:10:19.874: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/24/23 11:10:19.875
    Aug 24 11:10:19.878: INFO: Observed &ReplicaSet event: ADDED
    Aug 24 11:10:19.878: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 11:10:19.878: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 11:10:19.879: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 11:10:19.879: INFO: Observed replicaset test-rs in namespace replicaset-6439 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 11:10:19.879: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 24 11:10:19.879: INFO: Found replicaset test-rs in namespace replicaset-6439 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 24 11:10:19.879: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:19.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6439" for this suite. 08/24/23 11:10:19.885
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:19.898
Aug 24 11:10:19.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename cronjob 08/24/23 11:10:19.9
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:19.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:19.929
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/24/23 11:10:19.938
STEP: creating 08/24/23 11:10:19.939
STEP: getting 08/24/23 11:10:19.95
STEP: listing 08/24/23 11:10:19.955
STEP: watching 08/24/23 11:10:19.963
Aug 24 11:10:19.963: INFO: starting watch
STEP: cluster-wide listing 08/24/23 11:10:19.965
STEP: cluster-wide watching 08/24/23 11:10:19.971
Aug 24 11:10:19.971: INFO: starting watch
STEP: patching 08/24/23 11:10:19.974
STEP: updating 08/24/23 11:10:20.002
Aug 24 11:10:20.017: INFO: waiting for watch events with expected annotations
Aug 24 11:10:20.017: INFO: saw patched and updated annotations
STEP: patching /status 08/24/23 11:10:20.017
STEP: updating /status 08/24/23 11:10:20.042
STEP: get /status 08/24/23 11:10:20.051
STEP: deleting 08/24/23 11:10:20.057
STEP: deleting a collection 08/24/23 11:10:20.077
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8336" for this suite. 08/24/23 11:10:20.095
------------------------------
â€¢ [0.216 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:19.898
    Aug 24 11:10:19.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename cronjob 08/24/23 11:10:19.9
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:19.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:19.929
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/24/23 11:10:19.938
    STEP: creating 08/24/23 11:10:19.939
    STEP: getting 08/24/23 11:10:19.95
    STEP: listing 08/24/23 11:10:19.955
    STEP: watching 08/24/23 11:10:19.963
    Aug 24 11:10:19.963: INFO: starting watch
    STEP: cluster-wide listing 08/24/23 11:10:19.965
    STEP: cluster-wide watching 08/24/23 11:10:19.971
    Aug 24 11:10:19.971: INFO: starting watch
    STEP: patching 08/24/23 11:10:19.974
    STEP: updating 08/24/23 11:10:20.002
    Aug 24 11:10:20.017: INFO: waiting for watch events with expected annotations
    Aug 24 11:10:20.017: INFO: saw patched and updated annotations
    STEP: patching /status 08/24/23 11:10:20.017
    STEP: updating /status 08/24/23 11:10:20.042
    STEP: get /status 08/24/23 11:10:20.051
    STEP: deleting 08/24/23 11:10:20.057
    STEP: deleting a collection 08/24/23 11:10:20.077
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:20.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8336" for this suite. 08/24/23 11:10:20.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:20.118
Aug 24 11:10:20.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:10:20.12
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:20.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:20.172
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/24/23 11:10:20.179
Aug 24 11:10:20.191: INFO: Waiting up to 5m0s for pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c" in namespace "downward-api-1830" to be "running and ready"
Aug 24 11:10:20.210: INFO: Pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.194862ms
Aug 24 11:10:20.210: INFO: The phase of Pod annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:10:22.214: INFO: Pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c": Phase="Running", Reason="", readiness=true. Elapsed: 2.023703407s
Aug 24 11:10:22.214: INFO: The phase of Pod annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c is Running (Ready = true)
Aug 24 11:10:22.214: INFO: Pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c" satisfied condition "running and ready"
Aug 24 11:10:22.802: INFO: Successfully updated pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:26.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1830" for this suite. 08/24/23 11:10:26.835
------------------------------
â€¢ [SLOW TEST] [6.729 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:20.118
    Aug 24 11:10:20.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:10:20.12
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:20.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:20.172
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/24/23 11:10:20.179
    Aug 24 11:10:20.191: INFO: Waiting up to 5m0s for pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c" in namespace "downward-api-1830" to be "running and ready"
    Aug 24 11:10:20.210: INFO: Pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c": Phase="Pending", Reason="", readiness=false. Elapsed: 19.194862ms
    Aug 24 11:10:20.210: INFO: The phase of Pod annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:10:22.214: INFO: Pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c": Phase="Running", Reason="", readiness=true. Elapsed: 2.023703407s
    Aug 24 11:10:22.214: INFO: The phase of Pod annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c is Running (Ready = true)
    Aug 24 11:10:22.214: INFO: Pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c" satisfied condition "running and ready"
    Aug 24 11:10:22.802: INFO: Successfully updated pod "annotationupdated3dee657-1ce8-40d0-8676-27fd683d566c"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:26.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1830" for this suite. 08/24/23 11:10:26.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:26.851
Aug 24 11:10:26.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename job 08/24/23 11:10:26.852
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:26.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:26.888
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/24/23 11:10:26.895
STEP: Ensuring job reaches completions 08/24/23 11:10:26.91
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1380" for this suite. 08/24/23 11:10:38.921
------------------------------
â€¢ [SLOW TEST] [12.079 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:26.851
    Aug 24 11:10:26.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename job 08/24/23 11:10:26.852
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:26.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:26.888
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/24/23 11:10:26.895
    STEP: Ensuring job reaches completions 08/24/23 11:10:26.91
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1380" for this suite. 08/24/23 11:10:38.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:38.93
Aug 24 11:10:38.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:10:38.932
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:38.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:38.958
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/24/23 11:10:38.968
Aug 24 11:10:38.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5149 cluster-info'
Aug 24 11:10:39.100: INFO: stderr: ""
Aug 24 11:10:39.100: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:39.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5149" for this suite. 08/24/23 11:10:39.106
------------------------------
â€¢ [0.189 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:38.93
    Aug 24 11:10:38.930: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:10:38.932
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:38.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:38.958
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/24/23 11:10:38.968
    Aug 24 11:10:38.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5149 cluster-info'
    Aug 24 11:10:39.100: INFO: stderr: ""
    Aug 24 11:10:39.100: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:39.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5149" for this suite. 08/24/23 11:10:39.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:39.12
Aug 24 11:10:39.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:10:39.121
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:39.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:39.159
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:10:39.168
Aug 24 11:10:39.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524" in namespace "projected-587" to be "Succeeded or Failed"
Aug 24 11:10:39.204: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524": Phase="Pending", Reason="", readiness=false. Elapsed: 16.588551ms
Aug 24 11:10:41.210: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023210586s
Aug 24 11:10:43.211: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023357039s
STEP: Saw pod success 08/24/23 11:10:43.211
Aug 24 11:10:43.211: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524" satisfied condition "Succeeded or Failed"
Aug 24 11:10:43.214: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524 container client-container: <nil>
STEP: delete the pod 08/24/23 11:10:43.222
Aug 24 11:10:43.252: INFO: Waiting for pod downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524 to disappear
Aug 24 11:10:43.258: INFO: Pod downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 11:10:43.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-587" for this suite. 08/24/23 11:10:43.264
------------------------------
â€¢ [4.156 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:39.12
    Aug 24 11:10:39.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:10:39.121
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:39.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:39.159
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:10:39.168
    Aug 24 11:10:39.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524" in namespace "projected-587" to be "Succeeded or Failed"
    Aug 24 11:10:39.204: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524": Phase="Pending", Reason="", readiness=false. Elapsed: 16.588551ms
    Aug 24 11:10:41.210: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023210586s
    Aug 24 11:10:43.211: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023357039s
    STEP: Saw pod success 08/24/23 11:10:43.211
    Aug 24 11:10:43.211: INFO: Pod "downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524" satisfied condition "Succeeded or Failed"
    Aug 24 11:10:43.214: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:10:43.222
    Aug 24 11:10:43.252: INFO: Waiting for pod downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524 to disappear
    Aug 24 11:10:43.258: INFO: Pod downwardapi-volume-baaad479-7441-4464-8217-5b2dee1b5524 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:10:43.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-587" for this suite. 08/24/23 11:10:43.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:10:43.277
Aug 24 11:10:43.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-watch 08/24/23 11:10:43.279
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:43.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:43.337
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 24 11:10:43.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Creating first CR  08/24/23 11:10:45.977
Aug 24 11:10:45.984: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:10:45Z]] name:name1 resourceVersion:17068 uid:cfbae8e9-9bb1-41e6-b15d-4be1a7a2ca3c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/24/23 11:10:55.985
Aug 24 11:10:55.994: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:10:55Z]] name:name2 resourceVersion:17097 uid:c8e4534e-d4b8-4dc4-9f59-0e3cc7d8d44f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/24/23 11:11:05.995
Aug 24 11:11:06.011: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:05Z]] name:name1 resourceVersion:17120 uid:cfbae8e9-9bb1-41e6-b15d-4be1a7a2ca3c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/24/23 11:11:16.012
Aug 24 11:11:16.025: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:16Z]] name:name2 resourceVersion:17142 uid:c8e4534e-d4b8-4dc4-9f59-0e3cc7d8d44f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/24/23 11:11:26.029
Aug 24 11:11:26.042: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:05Z]] name:name1 resourceVersion:17165 uid:cfbae8e9-9bb1-41e6-b15d-4be1a7a2ca3c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/24/23 11:11:36.043
Aug 24 11:11:36.054: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:16Z]] name:name2 resourceVersion:17189 uid:c8e4534e-d4b8-4dc4-9f59-0e3cc7d8d44f] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:11:46.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-7907" for this suite. 08/24/23 11:11:46.588
------------------------------
â€¢ [SLOW TEST] [63.320 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:10:43.277
    Aug 24 11:10:43.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-watch 08/24/23 11:10:43.279
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:10:43.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:10:43.337
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 24 11:10:43.363: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Creating first CR  08/24/23 11:10:45.977
    Aug 24 11:10:45.984: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:10:45Z]] name:name1 resourceVersion:17068 uid:cfbae8e9-9bb1-41e6-b15d-4be1a7a2ca3c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/24/23 11:10:55.985
    Aug 24 11:10:55.994: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:10:55Z]] name:name2 resourceVersion:17097 uid:c8e4534e-d4b8-4dc4-9f59-0e3cc7d8d44f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/24/23 11:11:05.995
    Aug 24 11:11:06.011: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:05Z]] name:name1 resourceVersion:17120 uid:cfbae8e9-9bb1-41e6-b15d-4be1a7a2ca3c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/24/23 11:11:16.012
    Aug 24 11:11:16.025: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:16Z]] name:name2 resourceVersion:17142 uid:c8e4534e-d4b8-4dc4-9f59-0e3cc7d8d44f] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/24/23 11:11:26.029
    Aug 24 11:11:26.042: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:05Z]] name:name1 resourceVersion:17165 uid:cfbae8e9-9bb1-41e6-b15d-4be1a7a2ca3c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/24/23 11:11:36.043
    Aug 24 11:11:36.054: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:10:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:11:16Z]] name:name2 resourceVersion:17189 uid:c8e4534e-d4b8-4dc4-9f59-0e3cc7d8d44f] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:11:46.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-7907" for this suite. 08/24/23 11:11:46.588
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:11:46.598
Aug 24 11:11:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename containers 08/24/23 11:11:46.6
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:46.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:46.643
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/24/23 11:11:46.653
Aug 24 11:11:46.666: INFO: Waiting up to 5m0s for pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a" in namespace "containers-1724" to be "Succeeded or Failed"
Aug 24 11:11:46.673: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.113527ms
Aug 24 11:11:48.677: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010452901s
Aug 24 11:11:50.679: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012270813s
STEP: Saw pod success 08/24/23 11:11:50.679
Aug 24 11:11:50.679: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a" satisfied condition "Succeeded or Failed"
Aug 24 11:11:50.683: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod client-containers-c8e79c44-b628-484e-b8fd-714430e8075a container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:11:50.7
Aug 24 11:11:50.726: INFO: Waiting for pod client-containers-c8e79c44-b628-484e-b8fd-714430e8075a to disappear
Aug 24 11:11:50.731: INFO: Pod client-containers-c8e79c44-b628-484e-b8fd-714430e8075a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 11:11:50.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1724" for this suite. 08/24/23 11:11:50.736
------------------------------
â€¢ [4.149 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:11:46.598
    Aug 24 11:11:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename containers 08/24/23 11:11:46.6
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:46.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:46.643
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/24/23 11:11:46.653
    Aug 24 11:11:46.666: INFO: Waiting up to 5m0s for pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a" in namespace "containers-1724" to be "Succeeded or Failed"
    Aug 24 11:11:46.673: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.113527ms
    Aug 24 11:11:48.677: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010452901s
    Aug 24 11:11:50.679: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012270813s
    STEP: Saw pod success 08/24/23 11:11:50.679
    Aug 24 11:11:50.679: INFO: Pod "client-containers-c8e79c44-b628-484e-b8fd-714430e8075a" satisfied condition "Succeeded or Failed"
    Aug 24 11:11:50.683: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod client-containers-c8e79c44-b628-484e-b8fd-714430e8075a container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:11:50.7
    Aug 24 11:11:50.726: INFO: Waiting for pod client-containers-c8e79c44-b628-484e-b8fd-714430e8075a to disappear
    Aug 24 11:11:50.731: INFO: Pod client-containers-c8e79c44-b628-484e-b8fd-714430e8075a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:11:50.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1724" for this suite. 08/24/23 11:11:50.736
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:11:50.75
Aug 24 11:11:50.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename proxy 08/24/23 11:11:50.751
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:50.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:50.788
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 24 11:11:50.799: INFO: Creating pod...
Aug 24 11:11:50.813: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7332" to be "running"
Aug 24 11:11:50.823: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 10.063025ms
Aug 24 11:11:52.827: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013525274s
Aug 24 11:11:52.827: INFO: Pod "agnhost" satisfied condition "running"
Aug 24 11:11:52.827: INFO: Creating service...
Aug 24 11:11:52.870: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=DELETE
Aug 24 11:11:52.911: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 11:11:52.911: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=OPTIONS
Aug 24 11:11:52.918: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 11:11:52.919: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=PATCH
Aug 24 11:11:52.924: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 11:11:52.924: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=POST
Aug 24 11:11:52.931: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 11:11:52.931: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=PUT
Aug 24 11:11:52.937: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 24 11:11:52.937: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 24 11:11:52.944: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 11:11:52.944: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 24 11:11:52.950: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 11:11:52.950: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 24 11:11:52.958: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 11:11:52.958: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=POST
Aug 24 11:11:52.964: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 11:11:52.964: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=PUT
Aug 24 11:11:52.969: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 24 11:11:52.970: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=GET
Aug 24 11:11:52.975: INFO: http.Client request:GET StatusCode:301
Aug 24 11:11:52.975: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=GET
Aug 24 11:11:52.979: INFO: http.Client request:GET StatusCode:301
Aug 24 11:11:52.979: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=HEAD
Aug 24 11:11:52.983: INFO: http.Client request:HEAD StatusCode:301
Aug 24 11:11:52.983: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 24 11:11:52.996: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 24 11:11:52.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7332" for this suite. 08/24/23 11:11:53
------------------------------
â€¢ [2.260 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:11:50.75
    Aug 24 11:11:50.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename proxy 08/24/23 11:11:50.751
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:50.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:50.788
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 24 11:11:50.799: INFO: Creating pod...
    Aug 24 11:11:50.813: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7332" to be "running"
    Aug 24 11:11:50.823: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 10.063025ms
    Aug 24 11:11:52.827: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013525274s
    Aug 24 11:11:52.827: INFO: Pod "agnhost" satisfied condition "running"
    Aug 24 11:11:52.827: INFO: Creating service...
    Aug 24 11:11:52.870: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=DELETE
    Aug 24 11:11:52.911: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 11:11:52.911: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=OPTIONS
    Aug 24 11:11:52.918: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 11:11:52.919: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=PATCH
    Aug 24 11:11:52.924: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 11:11:52.924: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=POST
    Aug 24 11:11:52.931: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 11:11:52.931: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=PUT
    Aug 24 11:11:52.937: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 24 11:11:52.937: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 24 11:11:52.944: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 11:11:52.944: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 24 11:11:52.950: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 11:11:52.950: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 24 11:11:52.958: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 11:11:52.958: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=POST
    Aug 24 11:11:52.964: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 11:11:52.964: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 24 11:11:52.969: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 24 11:11:52.970: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=GET
    Aug 24 11:11:52.975: INFO: http.Client request:GET StatusCode:301
    Aug 24 11:11:52.975: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=GET
    Aug 24 11:11:52.979: INFO: http.Client request:GET StatusCode:301
    Aug 24 11:11:52.979: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/pods/agnhost/proxy?method=HEAD
    Aug 24 11:11:52.983: INFO: http.Client request:HEAD StatusCode:301
    Aug 24 11:11:52.983: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7332/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 24 11:11:52.996: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:11:52.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7332" for this suite. 08/24/23 11:11:53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:11:53.011
Aug 24 11:11:53.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubelet-test 08/24/23 11:11:53.013
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:53.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:53.046
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:11:57.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9793" for this suite. 08/24/23 11:11:57.082
------------------------------
â€¢ [4.080 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:11:53.011
    Aug 24 11:11:53.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 11:11:53.013
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:53.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:53.046
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:11:57.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9793" for this suite. 08/24/23 11:11:57.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:11:57.092
Aug 24 11:11:57.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:11:57.093
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:57.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:57.118
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-4428 08/24/23 11:11:57.13
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[] 08/24/23 11:11:57.153
Aug 24 11:11:57.162: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 24 11:11:58.177: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4428 08/24/23 11:11:58.177
Aug 24 11:11:58.203: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4428" to be "running and ready"
Aug 24 11:11:58.217: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.415744ms
Aug 24 11:11:58.217: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:12:00.222: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018631501s
Aug 24 11:12:00.222: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 24 11:12:00.222: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[pod1:[100]] 08/24/23 11:12:00.225
Aug 24 11:12:00.235: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4428 08/24/23 11:12:00.235
Aug 24 11:12:00.245: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4428" to be "running and ready"
Aug 24 11:12:00.263: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.209145ms
Aug 24 11:12:00.263: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:12:02.268: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.022918407s
Aug 24 11:12:02.268: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 24 11:12:02.268: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[pod1:[100] pod2:[101]] 08/24/23 11:12:02.276
Aug 24 11:12:02.294: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/24/23 11:12:02.294
Aug 24 11:12:02.294: INFO: Creating new exec pod
Aug 24 11:12:02.306: INFO: Waiting up to 5m0s for pod "execpodd6dwp" in namespace "services-4428" to be "running"
Aug 24 11:12:02.317: INFO: Pod "execpodd6dwp": Phase="Pending", Reason="", readiness=false. Elapsed: 10.528463ms
Aug 24 11:12:04.323: INFO: Pod "execpodd6dwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.0169117s
Aug 24 11:12:04.323: INFO: Pod "execpodd6dwp" satisfied condition "running"
Aug 24 11:12:05.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 24 11:12:05.592: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 24 11:12:05.592: INFO: stdout: ""
Aug 24 11:12:05.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 10.254.87.172 80'
Aug 24 11:12:05.859: INFO: stderr: "+ nc -v -z -w 2 10.254.87.172 80\nConnection to 10.254.87.172 80 port [tcp/http] succeeded!\n"
Aug 24 11:12:05.859: INFO: stdout: ""
Aug 24 11:12:05.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 24 11:12:06.103: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 24 11:12:06.103: INFO: stdout: ""
Aug 24 11:12:06.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 10.254.87.172 81'
Aug 24 11:12:06.377: INFO: stderr: "+ nc -v -z -w 2 10.254.87.172 81\nConnection to 10.254.87.172 81 port [tcp/*] succeeded!\n"
Aug 24 11:12:06.377: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4428 08/24/23 11:12:06.377
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[pod2:[101]] 08/24/23 11:12:06.403
Aug 24 11:12:06.446: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4428 08/24/23 11:12:06.446
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[] 08/24/23 11:12:06.489
Aug 24 11:12:07.510: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:07.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4428" for this suite. 08/24/23 11:12:07.571
------------------------------
â€¢ [SLOW TEST] [10.495 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:11:57.092
    Aug 24 11:11:57.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:11:57.093
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:11:57.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:11:57.118
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-4428 08/24/23 11:11:57.13
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[] 08/24/23 11:11:57.153
    Aug 24 11:11:57.162: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Aug 24 11:11:58.177: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4428 08/24/23 11:11:58.177
    Aug 24 11:11:58.203: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4428" to be "running and ready"
    Aug 24 11:11:58.217: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.415744ms
    Aug 24 11:11:58.217: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:12:00.222: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.018631501s
    Aug 24 11:12:00.222: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 24 11:12:00.222: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[pod1:[100]] 08/24/23 11:12:00.225
    Aug 24 11:12:00.235: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4428 08/24/23 11:12:00.235
    Aug 24 11:12:00.245: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4428" to be "running and ready"
    Aug 24 11:12:00.263: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.209145ms
    Aug 24 11:12:00.263: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:12:02.268: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.022918407s
    Aug 24 11:12:02.268: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 24 11:12:02.268: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[pod1:[100] pod2:[101]] 08/24/23 11:12:02.276
    Aug 24 11:12:02.294: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/24/23 11:12:02.294
    Aug 24 11:12:02.294: INFO: Creating new exec pod
    Aug 24 11:12:02.306: INFO: Waiting up to 5m0s for pod "execpodd6dwp" in namespace "services-4428" to be "running"
    Aug 24 11:12:02.317: INFO: Pod "execpodd6dwp": Phase="Pending", Reason="", readiness=false. Elapsed: 10.528463ms
    Aug 24 11:12:04.323: INFO: Pod "execpodd6dwp": Phase="Running", Reason="", readiness=true. Elapsed: 2.0169117s
    Aug 24 11:12:04.323: INFO: Pod "execpodd6dwp" satisfied condition "running"
    Aug 24 11:12:05.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 24 11:12:05.592: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 24 11:12:05.592: INFO: stdout: ""
    Aug 24 11:12:05.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 10.254.87.172 80'
    Aug 24 11:12:05.859: INFO: stderr: "+ nc -v -z -w 2 10.254.87.172 80\nConnection to 10.254.87.172 80 port [tcp/http] succeeded!\n"
    Aug 24 11:12:05.859: INFO: stdout: ""
    Aug 24 11:12:05.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 24 11:12:06.103: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 24 11:12:06.103: INFO: stdout: ""
    Aug 24 11:12:06.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-4428 exec execpodd6dwp -- /bin/sh -x -c nc -v -z -w 2 10.254.87.172 81'
    Aug 24 11:12:06.377: INFO: stderr: "+ nc -v -z -w 2 10.254.87.172 81\nConnection to 10.254.87.172 81 port [tcp/*] succeeded!\n"
    Aug 24 11:12:06.377: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4428 08/24/23 11:12:06.377
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[pod2:[101]] 08/24/23 11:12:06.403
    Aug 24 11:12:06.446: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4428 08/24/23 11:12:06.446
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4428 to expose endpoints map[] 08/24/23 11:12:06.489
    Aug 24 11:12:07.510: INFO: successfully validated that service multi-endpoint-test in namespace services-4428 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:07.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4428" for this suite. 08/24/23 11:12:07.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:07.592
Aug 24 11:12:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replicaset 08/24/23 11:12:07.593
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:07.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:07.636
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/24/23 11:12:07.644
STEP: Verify that the required pods have come up 08/24/23 11:12:07.655
Aug 24 11:12:07.662: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 24 11:12:12.666: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/24/23 11:12:12.666
Aug 24 11:12:12.669: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/24/23 11:12:12.67
STEP: DeleteCollection of the ReplicaSets 08/24/23 11:12:12.676
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/24/23 11:12:12.687
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:12.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3492" for this suite. 08/24/23 11:12:12.709
------------------------------
â€¢ [SLOW TEST] [5.158 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:07.592
    Aug 24 11:12:07.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replicaset 08/24/23 11:12:07.593
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:07.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:07.636
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/24/23 11:12:07.644
    STEP: Verify that the required pods have come up 08/24/23 11:12:07.655
    Aug 24 11:12:07.662: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 24 11:12:12.666: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/24/23 11:12:12.666
    Aug 24 11:12:12.669: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/24/23 11:12:12.67
    STEP: DeleteCollection of the ReplicaSets 08/24/23 11:12:12.676
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/24/23 11:12:12.687
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:12.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3492" for this suite. 08/24/23 11:12:12.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:12.75
Aug 24 11:12:12.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename security-context-test 08/24/23 11:12:12.752
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:12.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:12.798
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 24 11:12:12.835: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba" in namespace "security-context-test-7592" to be "Succeeded or Failed"
Aug 24 11:12:12.849: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": Phase="Pending", Reason="", readiness=false. Elapsed: 14.357251ms
Aug 24 11:12:14.854: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018532309s
Aug 24 11:12:16.855: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02003142s
Aug 24 11:12:16.855: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba" satisfied condition "Succeeded or Failed"
Aug 24 11:12:16.917: INFO: Got logs for pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:16.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7592" for this suite. 08/24/23 11:12:16.929
------------------------------
â€¢ [4.188 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:12.75
    Aug 24 11:12:12.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename security-context-test 08/24/23 11:12:12.752
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:12.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:12.798
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 24 11:12:12.835: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba" in namespace "security-context-test-7592" to be "Succeeded or Failed"
    Aug 24 11:12:12.849: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": Phase="Pending", Reason="", readiness=false. Elapsed: 14.357251ms
    Aug 24 11:12:14.854: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018532309s
    Aug 24 11:12:16.855: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02003142s
    Aug 24 11:12:16.855: INFO: Pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba" satisfied condition "Succeeded or Failed"
    Aug 24 11:12:16.917: INFO: Got logs for pod "busybox-privileged-false-2ca192a8-5a85-48e8-af05-de79b3a401ba": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:16.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7592" for this suite. 08/24/23 11:12:16.929
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:16.939
Aug 24 11:12:16.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:12:16.941
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:16.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:16.979
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:12:16.987
Aug 24 11:12:17.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112" in namespace "downward-api-503" to be "Succeeded or Failed"
Aug 24 11:12:17.011: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112": Phase="Pending", Reason="", readiness=false. Elapsed: 7.637829ms
Aug 24 11:12:19.017: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014366364s
Aug 24 11:12:21.016: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013392805s
STEP: Saw pod success 08/24/23 11:12:21.016
Aug 24 11:12:21.017: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112" satisfied condition "Succeeded or Failed"
Aug 24 11:12:21.020: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112 container client-container: <nil>
STEP: delete the pod 08/24/23 11:12:21.033
Aug 24 11:12:21.053: INFO: Waiting for pod downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112 to disappear
Aug 24 11:12:21.058: INFO: Pod downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:21.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-503" for this suite. 08/24/23 11:12:21.062
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:16.939
    Aug 24 11:12:16.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:12:16.941
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:16.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:16.979
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:12:16.987
    Aug 24 11:12:17.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112" in namespace "downward-api-503" to be "Succeeded or Failed"
    Aug 24 11:12:17.011: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112": Phase="Pending", Reason="", readiness=false. Elapsed: 7.637829ms
    Aug 24 11:12:19.017: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014366364s
    Aug 24 11:12:21.016: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013392805s
    STEP: Saw pod success 08/24/23 11:12:21.016
    Aug 24 11:12:21.017: INFO: Pod "downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112" satisfied condition "Succeeded or Failed"
    Aug 24 11:12:21.020: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:12:21.033
    Aug 24 11:12:21.053: INFO: Waiting for pod downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112 to disappear
    Aug 24 11:12:21.058: INFO: Pod downwardapi-volume-ab889002-079d-4c01-b400-eed24aa7e112 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:21.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-503" for this suite. 08/24/23 11:12:21.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:21.075
Aug 24 11:12:21.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-webhook 08/24/23 11:12:21.076
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:21.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:21.105
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/24/23 11:12:21.114
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 11:12:22.475
STEP: Deploying the custom resource conversion webhook pod 08/24/23 11:12:22.491
STEP: Wait for the deployment to be ready 08/24/23 11:12:22.517
Aug 24 11:12:22.547: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 11:12:24.558
STEP: Verifying the service has paired with the endpoint 08/24/23 11:12:24.575
Aug 24 11:12:25.575: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 24 11:12:25.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Creating a v1 custom resource 08/24/23 11:12:28.202
STEP: v2 custom resource should be converted 08/24/23 11:12:28.21
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:28.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6124" for this suite. 08/24/23 11:12:28.944
------------------------------
â€¢ [SLOW TEST] [7.910 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:21.075
    Aug 24 11:12:21.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-webhook 08/24/23 11:12:21.076
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:21.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:21.105
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/24/23 11:12:21.114
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 11:12:22.475
    STEP: Deploying the custom resource conversion webhook pod 08/24/23 11:12:22.491
    STEP: Wait for the deployment to be ready 08/24/23 11:12:22.517
    Aug 24 11:12:22.547: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 11:12:24.558
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:12:24.575
    Aug 24 11:12:25.575: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 24 11:12:25.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Creating a v1 custom resource 08/24/23 11:12:28.202
    STEP: v2 custom resource should be converted 08/24/23 11:12:28.21
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:28.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6124" for this suite. 08/24/23 11:12:28.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:28.987
Aug 24 11:12:28.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:12:28.989
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:29.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:29.044
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 24 11:12:29.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:30.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8768" for this suite. 08/24/23 11:12:30.106
------------------------------
â€¢ [1.132 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:28.987
    Aug 24 11:12:28.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:12:28.989
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:29.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:29.044
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 24 11:12:29.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:30.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8768" for this suite. 08/24/23 11:12:30.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:30.124
Aug 24 11:12:30.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename endpointslicemirroring 08/24/23 11:12:30.125
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:30.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:30.161
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/24/23 11:12:30.192
Aug 24 11:12:30.220: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/24/23 11:12:32.225
Aug 24 11:12:32.237: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/24/23 11:12:34.241
Aug 24 11:12:34.257: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:36.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-4388" for this suite. 08/24/23 11:12:36.265
------------------------------
â€¢ [SLOW TEST] [6.151 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:30.124
    Aug 24 11:12:30.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename endpointslicemirroring 08/24/23 11:12:30.125
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:30.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:30.161
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/24/23 11:12:30.192
    Aug 24 11:12:30.220: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/24/23 11:12:32.225
    Aug 24 11:12:32.237: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/24/23 11:12:34.241
    Aug 24 11:12:34.257: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:36.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-4388" for this suite. 08/24/23 11:12:36.265
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:36.275
Aug 24 11:12:36.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename proxy 08/24/23 11:12:36.277
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:36.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:36.303
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/24/23 11:12:36.332
STEP: creating replication controller proxy-service-562tp in namespace proxy-5350 08/24/23 11:12:36.332
I0824 11:12:36.352748      22 runners.go:193] Created replication controller with name: proxy-service-562tp, namespace: proxy-5350, replica count: 1
I0824 11:12:37.404370      22 runners.go:193] proxy-service-562tp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:12:38.405695      22 runners.go:193] proxy-service-562tp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0824 11:12:39.406570      22 runners.go:193] proxy-service-562tp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:12:39.414: INFO: setup took 3.102431425s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/24/23 11:12:39.414
Aug 24 11:12:39.427: INFO: (0) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 12.894205ms)
Aug 24 11:12:39.427: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 13.009965ms)
Aug 24 11:12:39.427: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.981114ms)
Aug 24 11:12:39.435: INFO: (0) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 21.1782ms)
Aug 24 11:12:39.442: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 28.100629ms)
Aug 24 11:12:39.442: INFO: (0) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 28.158788ms)
Aug 24 11:12:39.443: INFO: (0) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 29.126174ms)
Aug 24 11:12:39.443: INFO: (0) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 28.887406ms)
Aug 24 11:12:39.448: INFO: (0) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 34.425974ms)
Aug 24 11:12:39.448: INFO: (0) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 34.149863ms)
Aug 24 11:12:39.448: INFO: (0) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 34.155764ms)
Aug 24 11:12:39.449: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 34.409659ms)
Aug 24 11:12:39.450: INFO: (0) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 36.288559ms)
Aug 24 11:12:39.450: INFO: (0) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 36.23648ms)
Aug 24 11:12:39.450: INFO: (0) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 36.141108ms)
Aug 24 11:12:39.451: INFO: (0) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 36.905103ms)
Aug 24 11:12:39.467: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 16.113592ms)
Aug 24 11:12:39.467: INFO: (1) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.083571ms)
Aug 24 11:12:39.471: INFO: (1) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 20.365674ms)
Aug 24 11:12:39.472: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.223765ms)
Aug 24 11:12:39.472: INFO: (1) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 21.268847ms)
Aug 24 11:12:39.472: INFO: (1) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.127205ms)
Aug 24 11:12:39.473: INFO: (1) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 21.451738ms)
Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 23.95ms)
Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 23.942015ms)
Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 24.310747ms)
Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 24.363932ms)
Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 24.028582ms)
Aug 24 11:12:39.476: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 24.2409ms)
Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 24.267195ms)
Aug 24 11:12:39.477: INFO: (1) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 25.292324ms)
Aug 24 11:12:39.477: INFO: (1) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 25.588885ms)
Aug 24 11:12:39.490: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.459908ms)
Aug 24 11:12:39.491: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 13.668678ms)
Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 16.423693ms)
Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 16.393265ms)
Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 16.47765ms)
Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.537836ms)
Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 16.889984ms)
Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 17.263717ms)
Aug 24 11:12:39.495: INFO: (2) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 17.288365ms)
Aug 24 11:12:39.495: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 17.350712ms)
Aug 24 11:12:39.497: INFO: (2) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 19.682079ms)
Aug 24 11:12:39.499: INFO: (2) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 22.102519ms)
Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 23.452041ms)
Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 23.48496ms)
Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 23.529996ms)
Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 23.585812ms)
Aug 24 11:12:39.508: INFO: (3) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 6.779506ms)
Aug 24 11:12:39.509: INFO: (3) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 7.543771ms)
Aug 24 11:12:39.511: INFO: (3) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 10.132546ms)
Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.225533ms)
Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.372823ms)
Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 20.88149ms)
Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.365497ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 23.946716ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 24.503428ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 24.68316ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 24.720078ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 24.064647ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 24.645048ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 24.702059ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 24.815631ms)
Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 24.832067ms)
Aug 24 11:12:39.536: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 9.870586ms)
Aug 24 11:12:39.539: INFO: (4) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 12.525131ms)
Aug 24 11:12:39.541: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 15.012383ms)
Aug 24 11:12:39.541: INFO: (4) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 14.977813ms)
Aug 24 11:12:39.544: INFO: (4) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 17.504029ms)
Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 21.638464ms)
Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.786415ms)
Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 21.880053ms)
Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.773615ms)
Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.834723ms)
Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 24.314678ms)
Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 24.329478ms)
Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 24.568628ms)
Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 24.251183ms)
Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 24.252851ms)
Aug 24 11:12:39.552: INFO: (4) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 25.448951ms)
Aug 24 11:12:39.560: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 7.439816ms)
Aug 24 11:12:39.562: INFO: (5) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 8.980134ms)
Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.656736ms)
Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 12.456481ms)
Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 12.561109ms)
Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 12.722793ms)
Aug 24 11:12:39.568: INFO: (5) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 15.351305ms)
Aug 24 11:12:39.568: INFO: (5) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 15.639187ms)
Aug 24 11:12:39.569: INFO: (5) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 16.22417ms)
Aug 24 11:12:39.569: INFO: (5) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 16.469744ms)
Aug 24 11:12:39.569: INFO: (5) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 16.697455ms)
Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 27.31591ms)
Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 27.414509ms)
Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 27.231539ms)
Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 27.325412ms)
Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 28.092016ms)
Aug 24 11:12:39.591: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 10.791327ms)
Aug 24 11:12:39.591: INFO: (6) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 10.773567ms)
Aug 24 11:12:39.599: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 18.032231ms)
Aug 24 11:12:39.599: INFO: (6) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 18.16501ms)
Aug 24 11:12:39.600: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 19.619145ms)
Aug 24 11:12:39.603: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.907467ms)
Aug 24 11:12:39.603: INFO: (6) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 22.087572ms)
Aug 24 11:12:39.603: INFO: (6) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 22.053943ms)
Aug 24 11:12:39.608: INFO: (6) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 26.951567ms)
Aug 24 11:12:39.608: INFO: (6) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 27.626015ms)
Aug 24 11:12:39.609: INFO: (6) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 28.063172ms)
Aug 24 11:12:39.609: INFO: (6) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 28.09935ms)
Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 28.907374ms)
Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 28.922346ms)
Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 28.926211ms)
Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 28.993595ms)
Aug 24 11:12:39.615: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 4.525818ms)
Aug 24 11:12:39.623: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.956372ms)
Aug 24 11:12:39.623: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 13.001611ms)
Aug 24 11:12:39.623: INFO: (7) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 13.023817ms)
Aug 24 11:12:39.624: INFO: (7) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 13.532268ms)
Aug 24 11:12:39.629: INFO: (7) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 18.768741ms)
Aug 24 11:12:39.629: INFO: (7) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 19.040755ms)
Aug 24 11:12:39.630: INFO: (7) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 19.755864ms)
Aug 24 11:12:39.630: INFO: (7) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 19.770702ms)
Aug 24 11:12:39.631: INFO: (7) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 20.58072ms)
Aug 24 11:12:39.631: INFO: (7) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 21.108207ms)
Aug 24 11:12:39.633: INFO: (7) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 22.265626ms)
Aug 24 11:12:39.633: INFO: (7) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 22.348786ms)
Aug 24 11:12:39.633: INFO: (7) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 23.145889ms)
Aug 24 11:12:39.634: INFO: (7) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 24.208209ms)
Aug 24 11:12:39.635: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 24.683224ms)
Aug 24 11:12:39.649: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 13.704826ms)
Aug 24 11:12:39.655: INFO: (8) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 18.54569ms)
Aug 24 11:12:39.655: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 19.028985ms)
Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.100498ms)
Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 20.781415ms)
Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 20.953545ms)
Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 21.314972ms)
Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 21.478461ms)
Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 21.781091ms)
Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 22.197514ms)
Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 21.989327ms)
Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 22.15242ms)
Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 22.114742ms)
Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 22.587176ms)
Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 22.507871ms)
Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 22.409846ms)
Aug 24 11:12:39.662: INFO: (9) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 4.396637ms)
Aug 24 11:12:39.672: INFO: (9) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 14.493953ms)
Aug 24 11:12:39.675: INFO: (9) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 16.934702ms)
Aug 24 11:12:39.675: INFO: (9) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.862194ms)
Aug 24 11:12:39.678: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 19.737846ms)
Aug 24 11:12:39.679: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 20.903656ms)
Aug 24 11:12:39.680: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.547194ms)
Aug 24 11:12:39.680: INFO: (9) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 22.107812ms)
Aug 24 11:12:39.680: INFO: (9) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 22.243037ms)
Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 22.853474ms)
Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 22.77513ms)
Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 22.83869ms)
Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 22.679686ms)
Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 23.035352ms)
Aug 24 11:12:39.682: INFO: (9) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 23.39149ms)
Aug 24 11:12:39.682: INFO: (9) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 23.959038ms)
Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 25.40042ms)
Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 25.561019ms)
Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 25.849251ms)
Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 25.991463ms)
Aug 24 11:12:39.710: INFO: (10) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 27.572745ms)
Aug 24 11:12:39.710: INFO: (10) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 27.352188ms)
Aug 24 11:12:39.710: INFO: (10) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 28.014374ms)
Aug 24 11:12:39.711: INFO: (10) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 28.450176ms)
Aug 24 11:12:39.714: INFO: (10) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 31.199935ms)
Aug 24 11:12:39.716: INFO: (10) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 33.882011ms)
Aug 24 11:12:39.716: INFO: (10) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 33.934337ms)
Aug 24 11:12:39.716: INFO: (10) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 34.330472ms)
Aug 24 11:12:39.717: INFO: (10) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 34.544312ms)
Aug 24 11:12:39.717: INFO: (10) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 34.27529ms)
Aug 24 11:12:39.717: INFO: (10) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 34.225068ms)
Aug 24 11:12:39.718: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 35.535053ms)
Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 18.128587ms)
Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 17.922634ms)
Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 18.309033ms)
Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 18.085472ms)
Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 18.027161ms)
Aug 24 11:12:39.737: INFO: (11) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 18.569286ms)
Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 18.375475ms)
Aug 24 11:12:39.741: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 23.046416ms)
Aug 24 11:12:39.741: INFO: (11) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 23.231808ms)
Aug 24 11:12:39.742: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 23.435129ms)
Aug 24 11:12:39.754: INFO: (11) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 35.624906ms)
Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 36.482004ms)
Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 36.654945ms)
Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 36.693517ms)
Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 36.709482ms)
Aug 24 11:12:39.761: INFO: (11) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 42.773699ms)
Aug 24 11:12:39.769: INFO: (12) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 7.705726ms)
Aug 24 11:12:39.771: INFO: (12) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 9.034675ms)
Aug 24 11:12:39.771: INFO: (12) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 9.118976ms)
Aug 24 11:12:39.771: INFO: (12) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 9.116921ms)
Aug 24 11:12:39.772: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 10.683921ms)
Aug 24 11:12:39.772: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 10.760092ms)
Aug 24 11:12:39.772: INFO: (12) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 10.781785ms)
Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 12.183742ms)
Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 12.124738ms)
Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.134893ms)
Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.224651ms)
Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 12.748548ms)
Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 12.907331ms)
Aug 24 11:12:39.775: INFO: (12) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 13.238474ms)
Aug 24 11:12:39.775: INFO: (12) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 13.576287ms)
Aug 24 11:12:39.776: INFO: (12) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 14.226239ms)
Aug 24 11:12:39.784: INFO: (13) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 8.179701ms)
Aug 24 11:12:39.784: INFO: (13) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 8.4067ms)
Aug 24 11:12:39.784: INFO: (13) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 8.278332ms)
Aug 24 11:12:39.786: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 9.869096ms)
Aug 24 11:12:39.786: INFO: (13) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 10.258282ms)
Aug 24 11:12:39.787: INFO: (13) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 10.713207ms)
Aug 24 11:12:39.789: INFO: (13) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 12.969769ms)
Aug 24 11:12:39.789: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.867247ms)
Aug 24 11:12:39.790: INFO: (13) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 14.50324ms)
Aug 24 11:12:39.790: INFO: (13) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 14.243141ms)
Aug 24 11:12:39.790: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 14.109577ms)
Aug 24 11:12:39.791: INFO: (13) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 14.589596ms)
Aug 24 11:12:39.791: INFO: (13) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 14.969359ms)
Aug 24 11:12:39.792: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 15.84761ms)
Aug 24 11:12:39.792: INFO: (13) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 16.119978ms)
Aug 24 11:12:39.793: INFO: (13) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 16.786374ms)
Aug 24 11:12:39.800: INFO: (14) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 7.216401ms)
Aug 24 11:12:39.801: INFO: (14) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 8.042875ms)
Aug 24 11:12:39.801: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 8.133561ms)
Aug 24 11:12:39.802: INFO: (14) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 9.288527ms)
Aug 24 11:12:39.803: INFO: (14) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 10.250919ms)
Aug 24 11:12:39.804: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 10.829597ms)
Aug 24 11:12:39.806: INFO: (14) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 12.436487ms)
Aug 24 11:12:39.807: INFO: (14) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 14.20262ms)
Aug 24 11:12:39.808: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 15.051987ms)
Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 15.711302ms)
Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 15.780075ms)
Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 15.874146ms)
Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 15.867431ms)
Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 15.948335ms)
Aug 24 11:12:39.810: INFO: (14) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 16.403862ms)
Aug 24 11:12:39.810: INFO: (14) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 17.329453ms)
Aug 24 11:12:39.822: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 11.544805ms)
Aug 24 11:12:39.822: INFO: (15) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 12.206543ms)
Aug 24 11:12:39.823: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.146251ms)
Aug 24 11:12:39.823: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.185732ms)
Aug 24 11:12:39.824: INFO: (15) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 13.664317ms)
Aug 24 11:12:39.824: INFO: (15) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 13.257941ms)
Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 16.481929ms)
Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 16.292207ms)
Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 16.206635ms)
Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 16.502385ms)
Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 18.083519ms)
Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 18.130774ms)
Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 18.084669ms)
Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 18.234221ms)
Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 18.246923ms)
Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 18.258384ms)
Aug 24 11:12:39.844: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 14.930841ms)
Aug 24 11:12:39.845: INFO: (16) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 15.963726ms)
Aug 24 11:12:39.845: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 15.785463ms)
Aug 24 11:12:39.851: INFO: (16) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 22.343086ms)
Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 35.614005ms)
Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 35.98138ms)
Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 35.994601ms)
Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 35.851177ms)
Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 35.954207ms)
Aug 24 11:12:39.867: INFO: (16) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 37.427219ms)
Aug 24 11:12:39.869: INFO: (16) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 40.048934ms)
Aug 24 11:12:39.869: INFO: (16) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 40.158436ms)
Aug 24 11:12:39.872: INFO: (16) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 43.259973ms)
Aug 24 11:12:39.874: INFO: (16) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 44.70541ms)
Aug 24 11:12:39.875: INFO: (16) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 45.49907ms)
Aug 24 11:12:39.875: INFO: (16) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 45.988404ms)
Aug 24 11:12:39.897: INFO: (17) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.60053ms)
Aug 24 11:12:39.897: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.697136ms)
Aug 24 11:12:39.902: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 26.348988ms)
Aug 24 11:12:39.902: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 26.554471ms)
Aug 24 11:12:39.904: INFO: (17) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 28.789386ms)
Aug 24 11:12:39.905: INFO: (17) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 29.749256ms)
Aug 24 11:12:39.905: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 29.700172ms)
Aug 24 11:12:39.905: INFO: (17) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 29.979675ms)
Aug 24 11:12:39.907: INFO: (17) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 31.075398ms)
Aug 24 11:12:39.912: INFO: (17) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 36.269235ms)
Aug 24 11:12:39.912: INFO: (17) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 36.346209ms)
Aug 24 11:12:39.913: INFO: (17) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 37.076628ms)
Aug 24 11:12:39.913: INFO: (17) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 37.120639ms)
Aug 24 11:12:39.913: INFO: (17) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 37.198342ms)
Aug 24 11:12:39.914: INFO: (17) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 38.530593ms)
Aug 24 11:12:39.914: INFO: (17) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 38.709419ms)
Aug 24 11:12:39.929: INFO: (18) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 14.462966ms)
Aug 24 11:12:39.929: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 14.47784ms)
Aug 24 11:12:39.930: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 15.948336ms)
Aug 24 11:12:39.930: INFO: (18) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 15.891098ms)
Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 16.670099ms)
Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.767861ms)
Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 17.027106ms)
Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 16.847385ms)
Aug 24 11:12:39.933: INFO: (18) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 18.907775ms)
Aug 24 11:12:39.933: INFO: (18) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 19.225152ms)
Aug 24 11:12:39.935: INFO: (18) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 20.43056ms)
Aug 24 11:12:39.936: INFO: (18) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 21.245591ms)
Aug 24 11:12:39.937: INFO: (18) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 22.459195ms)
Aug 24 11:12:39.937: INFO: (18) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 23.003759ms)
Aug 24 11:12:39.938: INFO: (18) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 23.392056ms)
Aug 24 11:12:39.939: INFO: (18) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 24.571334ms)
Aug 24 11:12:39.947: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 7.675132ms)
Aug 24 11:12:39.948: INFO: (19) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 8.268699ms)
Aug 24 11:12:39.949: INFO: (19) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 9.23371ms)
Aug 24 11:12:39.949: INFO: (19) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 9.584801ms)
Aug 24 11:12:39.958: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 18.677937ms)
Aug 24 11:12:39.958: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 18.651913ms)
Aug 24 11:12:39.960: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 20.612523ms)
Aug 24 11:12:39.961: INFO: (19) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 21.155132ms)
Aug 24 11:12:39.961: INFO: (19) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 21.371221ms)
Aug 24 11:12:39.961: INFO: (19) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 21.784286ms)
Aug 24 11:12:39.962: INFO: (19) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 22.67715ms)
Aug 24 11:12:39.963: INFO: (19) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 23.125038ms)
Aug 24 11:12:39.964: INFO: (19) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 24.382394ms)
Aug 24 11:12:39.964: INFO: (19) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 24.473762ms)
Aug 24 11:12:39.964: INFO: (19) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 24.503539ms)
Aug 24 11:12:39.965: INFO: (19) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 24.810922ms)
STEP: deleting ReplicationController proxy-service-562tp in namespace proxy-5350, will wait for the garbage collector to delete the pods 08/24/23 11:12:39.965
Aug 24 11:12:40.031: INFO: Deleting ReplicationController proxy-service-562tp took: 10.683952ms
Aug 24 11:12:40.132: INFO: Terminating ReplicationController proxy-service-562tp pods took: 100.801943ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:41.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5350" for this suite. 08/24/23 11:12:41.738
------------------------------
â€¢ [SLOW TEST] [5.471 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:36.275
    Aug 24 11:12:36.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename proxy 08/24/23 11:12:36.277
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:36.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:36.303
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/24/23 11:12:36.332
    STEP: creating replication controller proxy-service-562tp in namespace proxy-5350 08/24/23 11:12:36.332
    I0824 11:12:36.352748      22 runners.go:193] Created replication controller with name: proxy-service-562tp, namespace: proxy-5350, replica count: 1
    I0824 11:12:37.404370      22 runners.go:193] proxy-service-562tp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:12:38.405695      22 runners.go:193] proxy-service-562tp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0824 11:12:39.406570      22 runners.go:193] proxy-service-562tp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:12:39.414: INFO: setup took 3.102431425s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/24/23 11:12:39.414
    Aug 24 11:12:39.427: INFO: (0) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 12.894205ms)
    Aug 24 11:12:39.427: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 13.009965ms)
    Aug 24 11:12:39.427: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.981114ms)
    Aug 24 11:12:39.435: INFO: (0) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 21.1782ms)
    Aug 24 11:12:39.442: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 28.100629ms)
    Aug 24 11:12:39.442: INFO: (0) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 28.158788ms)
    Aug 24 11:12:39.443: INFO: (0) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 29.126174ms)
    Aug 24 11:12:39.443: INFO: (0) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 28.887406ms)
    Aug 24 11:12:39.448: INFO: (0) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 34.425974ms)
    Aug 24 11:12:39.448: INFO: (0) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 34.149863ms)
    Aug 24 11:12:39.448: INFO: (0) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 34.155764ms)
    Aug 24 11:12:39.449: INFO: (0) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 34.409659ms)
    Aug 24 11:12:39.450: INFO: (0) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 36.288559ms)
    Aug 24 11:12:39.450: INFO: (0) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 36.23648ms)
    Aug 24 11:12:39.450: INFO: (0) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 36.141108ms)
    Aug 24 11:12:39.451: INFO: (0) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 36.905103ms)
    Aug 24 11:12:39.467: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 16.113592ms)
    Aug 24 11:12:39.467: INFO: (1) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.083571ms)
    Aug 24 11:12:39.471: INFO: (1) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 20.365674ms)
    Aug 24 11:12:39.472: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.223765ms)
    Aug 24 11:12:39.472: INFO: (1) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 21.268847ms)
    Aug 24 11:12:39.472: INFO: (1) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.127205ms)
    Aug 24 11:12:39.473: INFO: (1) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 21.451738ms)
    Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 23.95ms)
    Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 23.942015ms)
    Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 24.310747ms)
    Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 24.363932ms)
    Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 24.028582ms)
    Aug 24 11:12:39.476: INFO: (1) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 24.2409ms)
    Aug 24 11:12:39.475: INFO: (1) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 24.267195ms)
    Aug 24 11:12:39.477: INFO: (1) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 25.292324ms)
    Aug 24 11:12:39.477: INFO: (1) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 25.588885ms)
    Aug 24 11:12:39.490: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.459908ms)
    Aug 24 11:12:39.491: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 13.668678ms)
    Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 16.423693ms)
    Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 16.393265ms)
    Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 16.47765ms)
    Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.537836ms)
    Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 16.889984ms)
    Aug 24 11:12:39.494: INFO: (2) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 17.263717ms)
    Aug 24 11:12:39.495: INFO: (2) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 17.288365ms)
    Aug 24 11:12:39.495: INFO: (2) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 17.350712ms)
    Aug 24 11:12:39.497: INFO: (2) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 19.682079ms)
    Aug 24 11:12:39.499: INFO: (2) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 22.102519ms)
    Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 23.452041ms)
    Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 23.48496ms)
    Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 23.529996ms)
    Aug 24 11:12:39.501: INFO: (2) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 23.585812ms)
    Aug 24 11:12:39.508: INFO: (3) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 6.779506ms)
    Aug 24 11:12:39.509: INFO: (3) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 7.543771ms)
    Aug 24 11:12:39.511: INFO: (3) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 10.132546ms)
    Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.225533ms)
    Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.372823ms)
    Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 20.88149ms)
    Aug 24 11:12:39.523: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.365497ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 23.946716ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 24.503428ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 24.68316ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 24.720078ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 24.064647ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 24.645048ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 24.702059ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 24.815631ms)
    Aug 24 11:12:39.526: INFO: (3) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 24.832067ms)
    Aug 24 11:12:39.536: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 9.870586ms)
    Aug 24 11:12:39.539: INFO: (4) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 12.525131ms)
    Aug 24 11:12:39.541: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 15.012383ms)
    Aug 24 11:12:39.541: INFO: (4) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 14.977813ms)
    Aug 24 11:12:39.544: INFO: (4) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 17.504029ms)
    Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 21.638464ms)
    Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.786415ms)
    Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 21.880053ms)
    Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.773615ms)
    Aug 24 11:12:39.548: INFO: (4) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.834723ms)
    Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 24.314678ms)
    Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 24.329478ms)
    Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 24.568628ms)
    Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 24.251183ms)
    Aug 24 11:12:39.551: INFO: (4) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 24.252851ms)
    Aug 24 11:12:39.552: INFO: (4) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 25.448951ms)
    Aug 24 11:12:39.560: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 7.439816ms)
    Aug 24 11:12:39.562: INFO: (5) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 8.980134ms)
    Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.656736ms)
    Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 12.456481ms)
    Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 12.561109ms)
    Aug 24 11:12:39.565: INFO: (5) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 12.722793ms)
    Aug 24 11:12:39.568: INFO: (5) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 15.351305ms)
    Aug 24 11:12:39.568: INFO: (5) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 15.639187ms)
    Aug 24 11:12:39.569: INFO: (5) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 16.22417ms)
    Aug 24 11:12:39.569: INFO: (5) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 16.469744ms)
    Aug 24 11:12:39.569: INFO: (5) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 16.697455ms)
    Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 27.31591ms)
    Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 27.414509ms)
    Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 27.231539ms)
    Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 27.325412ms)
    Aug 24 11:12:39.580: INFO: (5) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 28.092016ms)
    Aug 24 11:12:39.591: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 10.791327ms)
    Aug 24 11:12:39.591: INFO: (6) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 10.773567ms)
    Aug 24 11:12:39.599: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 18.032231ms)
    Aug 24 11:12:39.599: INFO: (6) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 18.16501ms)
    Aug 24 11:12:39.600: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 19.619145ms)
    Aug 24 11:12:39.603: INFO: (6) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 21.907467ms)
    Aug 24 11:12:39.603: INFO: (6) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 22.087572ms)
    Aug 24 11:12:39.603: INFO: (6) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 22.053943ms)
    Aug 24 11:12:39.608: INFO: (6) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 26.951567ms)
    Aug 24 11:12:39.608: INFO: (6) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 27.626015ms)
    Aug 24 11:12:39.609: INFO: (6) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 28.063172ms)
    Aug 24 11:12:39.609: INFO: (6) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 28.09935ms)
    Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 28.907374ms)
    Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 28.922346ms)
    Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 28.926211ms)
    Aug 24 11:12:39.610: INFO: (6) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 28.993595ms)
    Aug 24 11:12:39.615: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 4.525818ms)
    Aug 24 11:12:39.623: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.956372ms)
    Aug 24 11:12:39.623: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 13.001611ms)
    Aug 24 11:12:39.623: INFO: (7) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 13.023817ms)
    Aug 24 11:12:39.624: INFO: (7) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 13.532268ms)
    Aug 24 11:12:39.629: INFO: (7) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 18.768741ms)
    Aug 24 11:12:39.629: INFO: (7) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 19.040755ms)
    Aug 24 11:12:39.630: INFO: (7) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 19.755864ms)
    Aug 24 11:12:39.630: INFO: (7) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 19.770702ms)
    Aug 24 11:12:39.631: INFO: (7) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 20.58072ms)
    Aug 24 11:12:39.631: INFO: (7) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 21.108207ms)
    Aug 24 11:12:39.633: INFO: (7) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 22.265626ms)
    Aug 24 11:12:39.633: INFO: (7) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 22.348786ms)
    Aug 24 11:12:39.633: INFO: (7) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 23.145889ms)
    Aug 24 11:12:39.634: INFO: (7) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 24.208209ms)
    Aug 24 11:12:39.635: INFO: (7) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 24.683224ms)
    Aug 24 11:12:39.649: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 13.704826ms)
    Aug 24 11:12:39.655: INFO: (8) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 18.54569ms)
    Aug 24 11:12:39.655: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 19.028985ms)
    Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.100498ms)
    Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 20.781415ms)
    Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 20.953545ms)
    Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 21.314972ms)
    Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 21.478461ms)
    Aug 24 11:12:39.657: INFO: (8) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 21.781091ms)
    Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 22.197514ms)
    Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 21.989327ms)
    Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 22.15242ms)
    Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 22.114742ms)
    Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 22.587176ms)
    Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 22.507871ms)
    Aug 24 11:12:39.658: INFO: (8) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 22.409846ms)
    Aug 24 11:12:39.662: INFO: (9) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 4.396637ms)
    Aug 24 11:12:39.672: INFO: (9) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 14.493953ms)
    Aug 24 11:12:39.675: INFO: (9) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 16.934702ms)
    Aug 24 11:12:39.675: INFO: (9) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.862194ms)
    Aug 24 11:12:39.678: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 19.737846ms)
    Aug 24 11:12:39.679: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 20.903656ms)
    Aug 24 11:12:39.680: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.547194ms)
    Aug 24 11:12:39.680: INFO: (9) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 22.107812ms)
    Aug 24 11:12:39.680: INFO: (9) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 22.243037ms)
    Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 22.853474ms)
    Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 22.77513ms)
    Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 22.83869ms)
    Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 22.679686ms)
    Aug 24 11:12:39.681: INFO: (9) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 23.035352ms)
    Aug 24 11:12:39.682: INFO: (9) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 23.39149ms)
    Aug 24 11:12:39.682: INFO: (9) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 23.959038ms)
    Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 25.40042ms)
    Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 25.561019ms)
    Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 25.849251ms)
    Aug 24 11:12:39.708: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 25.991463ms)
    Aug 24 11:12:39.710: INFO: (10) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 27.572745ms)
    Aug 24 11:12:39.710: INFO: (10) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 27.352188ms)
    Aug 24 11:12:39.710: INFO: (10) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 28.014374ms)
    Aug 24 11:12:39.711: INFO: (10) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 28.450176ms)
    Aug 24 11:12:39.714: INFO: (10) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 31.199935ms)
    Aug 24 11:12:39.716: INFO: (10) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 33.882011ms)
    Aug 24 11:12:39.716: INFO: (10) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 33.934337ms)
    Aug 24 11:12:39.716: INFO: (10) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 34.330472ms)
    Aug 24 11:12:39.717: INFO: (10) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 34.544312ms)
    Aug 24 11:12:39.717: INFO: (10) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 34.27529ms)
    Aug 24 11:12:39.717: INFO: (10) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 34.225068ms)
    Aug 24 11:12:39.718: INFO: (10) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 35.535053ms)
    Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 18.128587ms)
    Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 17.922634ms)
    Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 18.309033ms)
    Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 18.085472ms)
    Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 18.027161ms)
    Aug 24 11:12:39.737: INFO: (11) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 18.569286ms)
    Aug 24 11:12:39.736: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 18.375475ms)
    Aug 24 11:12:39.741: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 23.046416ms)
    Aug 24 11:12:39.741: INFO: (11) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 23.231808ms)
    Aug 24 11:12:39.742: INFO: (11) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 23.435129ms)
    Aug 24 11:12:39.754: INFO: (11) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 35.624906ms)
    Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 36.482004ms)
    Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 36.654945ms)
    Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 36.693517ms)
    Aug 24 11:12:39.755: INFO: (11) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 36.709482ms)
    Aug 24 11:12:39.761: INFO: (11) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 42.773699ms)
    Aug 24 11:12:39.769: INFO: (12) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 7.705726ms)
    Aug 24 11:12:39.771: INFO: (12) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 9.034675ms)
    Aug 24 11:12:39.771: INFO: (12) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 9.118976ms)
    Aug 24 11:12:39.771: INFO: (12) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 9.116921ms)
    Aug 24 11:12:39.772: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 10.683921ms)
    Aug 24 11:12:39.772: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 10.760092ms)
    Aug 24 11:12:39.772: INFO: (12) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 10.781785ms)
    Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 12.183742ms)
    Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 12.124738ms)
    Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.134893ms)
    Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.224651ms)
    Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 12.748548ms)
    Aug 24 11:12:39.774: INFO: (12) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 12.907331ms)
    Aug 24 11:12:39.775: INFO: (12) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 13.238474ms)
    Aug 24 11:12:39.775: INFO: (12) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 13.576287ms)
    Aug 24 11:12:39.776: INFO: (12) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 14.226239ms)
    Aug 24 11:12:39.784: INFO: (13) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 8.179701ms)
    Aug 24 11:12:39.784: INFO: (13) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 8.4067ms)
    Aug 24 11:12:39.784: INFO: (13) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 8.278332ms)
    Aug 24 11:12:39.786: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 9.869096ms)
    Aug 24 11:12:39.786: INFO: (13) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 10.258282ms)
    Aug 24 11:12:39.787: INFO: (13) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 10.713207ms)
    Aug 24 11:12:39.789: INFO: (13) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 12.969769ms)
    Aug 24 11:12:39.789: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.867247ms)
    Aug 24 11:12:39.790: INFO: (13) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 14.50324ms)
    Aug 24 11:12:39.790: INFO: (13) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 14.243141ms)
    Aug 24 11:12:39.790: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 14.109577ms)
    Aug 24 11:12:39.791: INFO: (13) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 14.589596ms)
    Aug 24 11:12:39.791: INFO: (13) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 14.969359ms)
    Aug 24 11:12:39.792: INFO: (13) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 15.84761ms)
    Aug 24 11:12:39.792: INFO: (13) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 16.119978ms)
    Aug 24 11:12:39.793: INFO: (13) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 16.786374ms)
    Aug 24 11:12:39.800: INFO: (14) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 7.216401ms)
    Aug 24 11:12:39.801: INFO: (14) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 8.042875ms)
    Aug 24 11:12:39.801: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 8.133561ms)
    Aug 24 11:12:39.802: INFO: (14) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 9.288527ms)
    Aug 24 11:12:39.803: INFO: (14) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 10.250919ms)
    Aug 24 11:12:39.804: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 10.829597ms)
    Aug 24 11:12:39.806: INFO: (14) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 12.436487ms)
    Aug 24 11:12:39.807: INFO: (14) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 14.20262ms)
    Aug 24 11:12:39.808: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 15.051987ms)
    Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 15.711302ms)
    Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 15.780075ms)
    Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 15.874146ms)
    Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 15.867431ms)
    Aug 24 11:12:39.809: INFO: (14) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 15.948335ms)
    Aug 24 11:12:39.810: INFO: (14) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 16.403862ms)
    Aug 24 11:12:39.810: INFO: (14) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 17.329453ms)
    Aug 24 11:12:39.822: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 11.544805ms)
    Aug 24 11:12:39.822: INFO: (15) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 12.206543ms)
    Aug 24 11:12:39.823: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 12.146251ms)
    Aug 24 11:12:39.823: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 12.185732ms)
    Aug 24 11:12:39.824: INFO: (15) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 13.664317ms)
    Aug 24 11:12:39.824: INFO: (15) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 13.257941ms)
    Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 16.481929ms)
    Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 16.292207ms)
    Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 16.206635ms)
    Aug 24 11:12:39.827: INFO: (15) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 16.502385ms)
    Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 18.083519ms)
    Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 18.130774ms)
    Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 18.084669ms)
    Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 18.234221ms)
    Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 18.246923ms)
    Aug 24 11:12:39.829: INFO: (15) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 18.258384ms)
    Aug 24 11:12:39.844: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 14.930841ms)
    Aug 24 11:12:39.845: INFO: (16) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 15.963726ms)
    Aug 24 11:12:39.845: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 15.785463ms)
    Aug 24 11:12:39.851: INFO: (16) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 22.343086ms)
    Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 35.614005ms)
    Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 35.98138ms)
    Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 35.994601ms)
    Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 35.851177ms)
    Aug 24 11:12:39.865: INFO: (16) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 35.954207ms)
    Aug 24 11:12:39.867: INFO: (16) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 37.427219ms)
    Aug 24 11:12:39.869: INFO: (16) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 40.048934ms)
    Aug 24 11:12:39.869: INFO: (16) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 40.158436ms)
    Aug 24 11:12:39.872: INFO: (16) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 43.259973ms)
    Aug 24 11:12:39.874: INFO: (16) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 44.70541ms)
    Aug 24 11:12:39.875: INFO: (16) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 45.49907ms)
    Aug 24 11:12:39.875: INFO: (16) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 45.988404ms)
    Aug 24 11:12:39.897: INFO: (17) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 21.60053ms)
    Aug 24 11:12:39.897: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 21.697136ms)
    Aug 24 11:12:39.902: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 26.348988ms)
    Aug 24 11:12:39.902: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 26.554471ms)
    Aug 24 11:12:39.904: INFO: (17) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 28.789386ms)
    Aug 24 11:12:39.905: INFO: (17) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 29.749256ms)
    Aug 24 11:12:39.905: INFO: (17) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 29.700172ms)
    Aug 24 11:12:39.905: INFO: (17) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 29.979675ms)
    Aug 24 11:12:39.907: INFO: (17) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 31.075398ms)
    Aug 24 11:12:39.912: INFO: (17) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 36.269235ms)
    Aug 24 11:12:39.912: INFO: (17) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 36.346209ms)
    Aug 24 11:12:39.913: INFO: (17) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 37.076628ms)
    Aug 24 11:12:39.913: INFO: (17) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 37.120639ms)
    Aug 24 11:12:39.913: INFO: (17) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 37.198342ms)
    Aug 24 11:12:39.914: INFO: (17) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 38.530593ms)
    Aug 24 11:12:39.914: INFO: (17) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 38.709419ms)
    Aug 24 11:12:39.929: INFO: (18) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 14.462966ms)
    Aug 24 11:12:39.929: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 14.47784ms)
    Aug 24 11:12:39.930: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 15.948336ms)
    Aug 24 11:12:39.930: INFO: (18) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 15.891098ms)
    Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 16.670099ms)
    Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 16.767861ms)
    Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 17.027106ms)
    Aug 24 11:12:39.931: INFO: (18) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 16.847385ms)
    Aug 24 11:12:39.933: INFO: (18) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 18.907775ms)
    Aug 24 11:12:39.933: INFO: (18) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 19.225152ms)
    Aug 24 11:12:39.935: INFO: (18) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 20.43056ms)
    Aug 24 11:12:39.936: INFO: (18) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 21.245591ms)
    Aug 24 11:12:39.937: INFO: (18) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 22.459195ms)
    Aug 24 11:12:39.937: INFO: (18) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 23.003759ms)
    Aug 24 11:12:39.938: INFO: (18) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 23.392056ms)
    Aug 24 11:12:39.939: INFO: (18) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 24.571334ms)
    Aug 24 11:12:39.947: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:162/proxy/: bar (200; 7.675132ms)
    Aug 24 11:12:39.948: INFO: (19) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname2/proxy/: tls qux (200; 8.268699ms)
    Aug 24 11:12:39.949: INFO: (19) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:1080/proxy/rewriteme">... (200; 9.23371ms)
    Aug 24 11:12:39.949: INFO: (19) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:162/proxy/: bar (200; 9.584801ms)
    Aug 24 11:12:39.958: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:160/proxy/: foo (200; 18.677937ms)
    Aug 24 11:12:39.958: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw/proxy/rewriteme">test</a> (200; 18.651913ms)
    Aug 24 11:12:39.960: INFO: (19) /api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/proxy-service-562tp-mbmjw:1080/proxy/rewriteme">test<... (200; 20.612523ms)
    Aug 24 11:12:39.961: INFO: (19) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:460/proxy/: tls baz (200; 21.155132ms)
    Aug 24 11:12:39.961: INFO: (19) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:462/proxy/: tls qux (200; 21.371221ms)
    Aug 24 11:12:39.961: INFO: (19) /api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/: <a href="/api/v1/namespaces/proxy-5350/pods/https:proxy-service-562tp-mbmjw:443/proxy/tlsrewritem... (200; 21.784286ms)
    Aug 24 11:12:39.962: INFO: (19) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname2/proxy/: bar (200; 22.67715ms)
    Aug 24 11:12:39.963: INFO: (19) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname2/proxy/: bar (200; 23.125038ms)
    Aug 24 11:12:39.964: INFO: (19) /api/v1/namespaces/proxy-5350/services/http:proxy-service-562tp:portname1/proxy/: foo (200; 24.382394ms)
    Aug 24 11:12:39.964: INFO: (19) /api/v1/namespaces/proxy-5350/services/proxy-service-562tp:portname1/proxy/: foo (200; 24.473762ms)
    Aug 24 11:12:39.964: INFO: (19) /api/v1/namespaces/proxy-5350/pods/http:proxy-service-562tp-mbmjw:160/proxy/: foo (200; 24.503539ms)
    Aug 24 11:12:39.965: INFO: (19) /api/v1/namespaces/proxy-5350/services/https:proxy-service-562tp:tlsportname1/proxy/: tls baz (200; 24.810922ms)
    STEP: deleting ReplicationController proxy-service-562tp in namespace proxy-5350, will wait for the garbage collector to delete the pods 08/24/23 11:12:39.965
    Aug 24 11:12:40.031: INFO: Deleting ReplicationController proxy-service-562tp took: 10.683952ms
    Aug 24 11:12:40.132: INFO: Terminating ReplicationController proxy-service-562tp pods took: 100.801943ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:41.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5350" for this suite. 08/24/23 11:12:41.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:41.75
Aug 24 11:12:41.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename init-container 08/24/23 11:12:41.751
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:41.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:41.79
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/24/23 11:12:41.797
Aug 24 11:12:41.797: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:12:46.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9239" for this suite. 08/24/23 11:12:46.684
------------------------------
â€¢ [4.950 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:41.75
    Aug 24 11:12:41.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename init-container 08/24/23 11:12:41.751
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:41.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:41.79
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/24/23 11:12:41.797
    Aug 24 11:12:41.797: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:12:46.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9239" for this suite. 08/24/23 11:12:46.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:12:46.703
Aug 24 11:12:46.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename security-context-test 08/24/23 11:12:46.705
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:46.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:46.734
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 24 11:12:46.793: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97" in namespace "security-context-test-6613" to be "Succeeded or Failed"
Aug 24 11:12:46.809: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 16.843281ms
Aug 24 11:12:48.814: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021347462s
Aug 24 11:12:50.816: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023259178s
Aug 24 11:12:52.814: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021018768s
Aug 24 11:12:54.814: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021533907s
Aug 24 11:12:56.820: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027677652s
Aug 24 11:12:58.815: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022224407s
Aug 24 11:13:00.817: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024174768s
Aug 24 11:13:02.813: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.020755106s
Aug 24 11:13:02.813: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 11:13:02.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6613" for this suite. 08/24/23 11:13:02.826
------------------------------
â€¢ [SLOW TEST] [16.136 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:12:46.703
    Aug 24 11:12:46.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename security-context-test 08/24/23 11:12:46.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:12:46.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:12:46.734
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 24 11:12:46.793: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97" in namespace "security-context-test-6613" to be "Succeeded or Failed"
    Aug 24 11:12:46.809: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 16.843281ms
    Aug 24 11:12:48.814: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021347462s
    Aug 24 11:12:50.816: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023259178s
    Aug 24 11:12:52.814: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021018768s
    Aug 24 11:12:54.814: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021533907s
    Aug 24 11:12:56.820: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027677652s
    Aug 24 11:12:58.815: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 12.022224407s
    Aug 24 11:13:00.817: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Pending", Reason="", readiness=false. Elapsed: 14.024174768s
    Aug 24 11:13:02.813: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.020755106s
    Aug 24 11:13:02.813: INFO: Pod "alpine-nnp-false-7d3f987f-4aa2-461d-a89f-1569ac8f8d97" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:13:02.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6613" for this suite. 08/24/23 11:13:02.826
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:13:02.839
Aug 24 11:13:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename watch 08/24/23 11:13:02.841
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:02.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:02.874
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/24/23 11:13:02.88
STEP: creating a watch on configmaps with label B 08/24/23 11:13:02.883
STEP: creating a watch on configmaps with label A or B 08/24/23 11:13:02.884
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/24/23 11:13:02.886
Aug 24 11:13:02.899: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17949 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:13:02.899: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17949 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/24/23 11:13:02.899
Aug 24 11:13:02.915: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17950 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:13:02.915: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17950 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/24/23 11:13:02.915
Aug 24 11:13:02.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17951 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:13:02.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17951 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/24/23 11:13:02.925
Aug 24 11:13:02.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17952 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:13:02.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17952 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/24/23 11:13:02.935
Aug 24 11:13:02.942: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17953 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:13:02.942: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17953 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/24/23 11:13:12.942
Aug 24 11:13:12.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17989 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 11:13:12.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17989 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 11:13:22.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7478" for this suite. 08/24/23 11:13:22.963
------------------------------
â€¢ [SLOW TEST] [20.137 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:13:02.839
    Aug 24 11:13:02.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename watch 08/24/23 11:13:02.841
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:02.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:02.874
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/24/23 11:13:02.88
    STEP: creating a watch on configmaps with label B 08/24/23 11:13:02.883
    STEP: creating a watch on configmaps with label A or B 08/24/23 11:13:02.884
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/24/23 11:13:02.886
    Aug 24 11:13:02.899: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17949 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:13:02.899: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17949 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/24/23 11:13:02.899
    Aug 24 11:13:02.915: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17950 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:13:02.915: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17950 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/24/23 11:13:02.915
    Aug 24 11:13:02.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17951 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:13:02.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17951 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/24/23 11:13:02.925
    Aug 24 11:13:02.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17952 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:13:02.935: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7478  899452b6-8c04-4a75-9bb7-1833cd02fa09 17952 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/24/23 11:13:02.935
    Aug 24 11:13:02.942: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17953 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:13:02.942: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17953 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/24/23 11:13:12.942
    Aug 24 11:13:12.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17989 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 11:13:12.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7478  febb6a1d-ba52-4c71-9f91-18f7681fd4b3 17989 0 2023-08-24 11:13:02 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:13:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:13:22.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7478" for this suite. 08/24/23 11:13:22.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:13:22.977
Aug 24 11:13:22.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 11:13:22.978
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:23.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:23.013
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 11:13:23.024
Aug 24 11:13:23.049: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-241" to be "running and ready"
Aug 24 11:13:23.063: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.863621ms
Aug 24 11:13:23.063: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:13:25.068: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019185645s
Aug 24 11:13:25.068: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 11:13:25.068: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/24/23 11:13:25.072
Aug 24 11:13:25.079: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-241" to be "running and ready"
Aug 24 11:13:25.087: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.233861ms
Aug 24 11:13:25.087: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:13:27.092: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012392033s
Aug 24 11:13:27.092: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 24 11:13:27.092: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/24/23 11:13:27.095
Aug 24 11:13:27.109: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 24 11:13:27.116: INFO: Pod pod-with-prestop-http-hook still exists
Aug 24 11:13:29.117: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 24 11:13:29.121: INFO: Pod pod-with-prestop-http-hook still exists
Aug 24 11:13:31.118: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 24 11:13:31.125: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/24/23 11:13:31.125
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 11:13:31.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-241" for this suite. 08/24/23 11:13:31.168
------------------------------
â€¢ [SLOW TEST] [8.200 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:13:22.977
    Aug 24 11:13:22.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 11:13:22.978
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:23.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:23.013
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 11:13:23.024
    Aug 24 11:13:23.049: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-241" to be "running and ready"
    Aug 24 11:13:23.063: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 13.863621ms
    Aug 24 11:13:23.063: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:13:25.068: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.019185645s
    Aug 24 11:13:25.068: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 11:13:25.068: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/24/23 11:13:25.072
    Aug 24 11:13:25.079: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-241" to be "running and ready"
    Aug 24 11:13:25.087: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 7.233861ms
    Aug 24 11:13:25.087: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:13:27.092: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012392033s
    Aug 24 11:13:27.092: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 24 11:13:27.092: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/24/23 11:13:27.095
    Aug 24 11:13:27.109: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 24 11:13:27.116: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 24 11:13:29.117: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 24 11:13:29.121: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 24 11:13:31.118: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 24 11:13:31.125: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/24/23 11:13:31.125
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:13:31.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-241" for this suite. 08/24/23 11:13:31.168
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:13:31.178
Aug 24 11:13:31.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 11:13:31.179
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:31.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:31.205
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/24/23 11:13:31.211
STEP: Creating a ResourceQuota 08/24/23 11:13:36.22
STEP: Ensuring resource quota status is calculated 08/24/23 11:13:36.232
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 11:13:38.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1593" for this suite. 08/24/23 11:13:38.249
------------------------------
â€¢ [SLOW TEST] [7.083 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:13:31.178
    Aug 24 11:13:31.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 11:13:31.179
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:31.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:31.205
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/24/23 11:13:31.211
    STEP: Creating a ResourceQuota 08/24/23 11:13:36.22
    STEP: Ensuring resource quota status is calculated 08/24/23 11:13:36.232
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:13:38.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1593" for this suite. 08/24/23 11:13:38.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:13:38.262
Aug 24 11:13:38.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 11:13:38.264
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:38.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:38.292
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 in namespace container-probe-6597 08/24/23 11:13:38.299
Aug 24 11:13:38.312: INFO: Waiting up to 5m0s for pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8" in namespace "container-probe-6597" to be "not pending"
Aug 24 11:13:38.322: INFO: Pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.586846ms
Aug 24 11:13:40.326: INFO: Pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013853577s
Aug 24 11:13:40.326: INFO: Pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8" satisfied condition "not pending"
Aug 24 11:13:40.326: INFO: Started pod liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 in namespace container-probe-6597
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:13:40.326
Aug 24 11:13:40.331: INFO: Initial restart count of pod liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is 0
Aug 24 11:14:00.468: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 1 (20.136890605s elapsed)
Aug 24 11:14:20.525: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 2 (40.194740528s elapsed)
Aug 24 11:14:40.577: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 3 (1m0.245954681s elapsed)
Aug 24 11:15:00.629: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 4 (1m20.298082841s elapsed)
Aug 24 11:16:00.779: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 5 (2m20.448771098s elapsed)
STEP: deleting the pod 08/24/23 11:16:00.779
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:00.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6597" for this suite. 08/24/23 11:16:00.815
------------------------------
â€¢ [SLOW TEST] [142.561 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:13:38.262
    Aug 24 11:13:38.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 11:13:38.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:13:38.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:13:38.292
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 in namespace container-probe-6597 08/24/23 11:13:38.299
    Aug 24 11:13:38.312: INFO: Waiting up to 5m0s for pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8" in namespace "container-probe-6597" to be "not pending"
    Aug 24 11:13:38.322: INFO: Pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.586846ms
    Aug 24 11:13:40.326: INFO: Pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.013853577s
    Aug 24 11:13:40.326: INFO: Pod "liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8" satisfied condition "not pending"
    Aug 24 11:13:40.326: INFO: Started pod liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 in namespace container-probe-6597
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:13:40.326
    Aug 24 11:13:40.331: INFO: Initial restart count of pod liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is 0
    Aug 24 11:14:00.468: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 1 (20.136890605s elapsed)
    Aug 24 11:14:20.525: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 2 (40.194740528s elapsed)
    Aug 24 11:14:40.577: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 3 (1m0.245954681s elapsed)
    Aug 24 11:15:00.629: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 4 (1m20.298082841s elapsed)
    Aug 24 11:16:00.779: INFO: Restart count of pod container-probe-6597/liveness-e5125c8a-4389-4443-bb8a-9c9abf0f47d8 is now 5 (2m20.448771098s elapsed)
    STEP: deleting the pod 08/24/23 11:16:00.779
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:00.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6597" for this suite. 08/24/23 11:16:00.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:00.826
Aug 24 11:16:00.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:16:00.828
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:00.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:00.857
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 24 11:16:00.909: INFO: created pod pod-service-account-defaultsa
Aug 24 11:16:00.909: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 24 11:16:00.930: INFO: created pod pod-service-account-mountsa
Aug 24 11:16:00.930: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 24 11:16:00.953: INFO: created pod pod-service-account-nomountsa
Aug 24 11:16:00.953: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 24 11:16:00.975: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 24 11:16:00.975: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 24 11:16:01.020: INFO: created pod pod-service-account-mountsa-mountspec
Aug 24 11:16:01.020: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 24 11:16:01.065: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 24 11:16:01.065: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 24 11:16:01.086: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 24 11:16:01.086: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 24 11:16:01.121: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 24 11:16:01.121: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 24 11:16:01.166: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 24 11:16:01.167: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:01.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5984" for this suite. 08/24/23 11:16:01.191
------------------------------
â€¢ [0.406 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:00.826
    Aug 24 11:16:00.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:16:00.828
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:00.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:00.857
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 24 11:16:00.909: INFO: created pod pod-service-account-defaultsa
    Aug 24 11:16:00.909: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 24 11:16:00.930: INFO: created pod pod-service-account-mountsa
    Aug 24 11:16:00.930: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 24 11:16:00.953: INFO: created pod pod-service-account-nomountsa
    Aug 24 11:16:00.953: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 24 11:16:00.975: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 24 11:16:00.975: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 24 11:16:01.020: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 24 11:16:01.020: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 24 11:16:01.065: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 24 11:16:01.065: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 24 11:16:01.086: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 24 11:16:01.086: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 24 11:16:01.121: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 24 11:16:01.121: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 24 11:16:01.166: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 24 11:16:01.167: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:01.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5984" for this suite. 08/24/23 11:16:01.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:01.234
Aug 24 11:16:01.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:16:01.236
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:01.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:01.325
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-2eb191fa-749d-49c4-be81-c16d1a05db82 08/24/23 11:16:01.337
STEP: Creating a pod to test consume secrets 08/24/23 11:16:01.35
Aug 24 11:16:01.387: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07" in namespace "projected-4831" to be "Succeeded or Failed"
Aug 24 11:16:01.399: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Pending", Reason="", readiness=false. Elapsed: 11.824494ms
Aug 24 11:16:03.413: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025943826s
Aug 24 11:16:05.404: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01707783s
Aug 24 11:16:07.413: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026000243s
STEP: Saw pod success 08/24/23 11:16:07.413
Aug 24 11:16:07.413: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07" satisfied condition "Succeeded or Failed"
Aug 24 11:16:07.419: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 11:16:07.473
Aug 24 11:16:07.510: INFO: Waiting for pod pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07 to disappear
Aug 24 11:16:07.518: INFO: Pod pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:07.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4831" for this suite. 08/24/23 11:16:07.523
------------------------------
â€¢ [SLOW TEST] [6.303 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:01.234
    Aug 24 11:16:01.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:16:01.236
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:01.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:01.325
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-2eb191fa-749d-49c4-be81-c16d1a05db82 08/24/23 11:16:01.337
    STEP: Creating a pod to test consume secrets 08/24/23 11:16:01.35
    Aug 24 11:16:01.387: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07" in namespace "projected-4831" to be "Succeeded or Failed"
    Aug 24 11:16:01.399: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Pending", Reason="", readiness=false. Elapsed: 11.824494ms
    Aug 24 11:16:03.413: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025943826s
    Aug 24 11:16:05.404: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01707783s
    Aug 24 11:16:07.413: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026000243s
    STEP: Saw pod success 08/24/23 11:16:07.413
    Aug 24 11:16:07.413: INFO: Pod "pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07" satisfied condition "Succeeded or Failed"
    Aug 24 11:16:07.419: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:16:07.473
    Aug 24 11:16:07.510: INFO: Waiting for pod pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07 to disappear
    Aug 24 11:16:07.518: INFO: Pod pod-projected-secrets-e077201c-6ba4-494c-8001-04aec5c2af07 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:07.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4831" for this suite. 08/24/23 11:16:07.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:07.538
Aug 24 11:16:07.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 11:16:07.539
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:07.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:07.652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:16:07.686
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:16:08.328
STEP: Deploying the webhook pod 08/24/23 11:16:08.338
STEP: Wait for the deployment to be ready 08/24/23 11:16:08.364
Aug 24 11:16:08.382: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 24 11:16:10.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/24/23 11:16:12.4
STEP: Verifying the service has paired with the endpoint 08/24/23 11:16:12.421
Aug 24 11:16:13.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/24/23 11:16:13.433
STEP: create a namespace for the webhook 08/24/23 11:16:13.465
STEP: create a configmap should be unconditionally rejected by the webhook 08/24/23 11:16:13.476
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:13.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9168" for this suite. 08/24/23 11:16:13.829
STEP: Destroying namespace "webhook-9168-markers" for this suite. 08/24/23 11:16:13.867
------------------------------
â€¢ [SLOW TEST] [6.340 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:07.538
    Aug 24 11:16:07.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 11:16:07.539
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:07.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:07.652
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:16:07.686
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:16:08.328
    STEP: Deploying the webhook pod 08/24/23 11:16:08.338
    STEP: Wait for the deployment to be ready 08/24/23 11:16:08.364
    Aug 24 11:16:08.382: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 24 11:16:10.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 16, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/24/23 11:16:12.4
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:16:12.421
    Aug 24 11:16:13.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/24/23 11:16:13.433
    STEP: create a namespace for the webhook 08/24/23 11:16:13.465
    STEP: create a configmap should be unconditionally rejected by the webhook 08/24/23 11:16:13.476
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:13.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9168" for this suite. 08/24/23 11:16:13.829
    STEP: Destroying namespace "webhook-9168-markers" for this suite. 08/24/23 11:16:13.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:13.882
Aug 24 11:16:13.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename namespaces 08/24/23 11:16:13.884
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:13.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:13.934
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-4845" 08/24/23 11:16:13.944
Aug 24 11:16:13.959: INFO: Namespace "namespaces-4845" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b80dffc4-45ea-4898-a190-ca68b5b82212", "kubernetes.io/metadata.name":"namespaces-4845", "namespaces-4845":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:13.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4845" for this suite. 08/24/23 11:16:13.966
------------------------------
â€¢ [0.102 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:13.882
    Aug 24 11:16:13.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename namespaces 08/24/23 11:16:13.884
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:13.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:13.934
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-4845" 08/24/23 11:16:13.944
    Aug 24 11:16:13.959: INFO: Namespace "namespaces-4845" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b80dffc4-45ea-4898-a190-ca68b5b82212", "kubernetes.io/metadata.name":"namespaces-4845", "namespaces-4845":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:13.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4845" for this suite. 08/24/23 11:16:13.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:13.986
Aug 24 11:16:13.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 11:16:13.987
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:14.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:14.032
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7606 08/24/23 11:16:14.054
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-7606 08/24/23 11:16:14.091
Aug 24 11:16:14.187: INFO: Found 0 stateful pods, waiting for 1
Aug 24 11:16:24.191: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/24/23 11:16:24.198
STEP: updating a scale subresource 08/24/23 11:16:24.203
STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 11:16:24.217
STEP: Patch a scale subresource 08/24/23 11:16:24.226
STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 11:16:24.27
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 11:16:24.275: INFO: Deleting all statefulset in ns statefulset-7606
Aug 24 11:16:24.282: INFO: Scaling statefulset ss to 0
Aug 24 11:16:34.311: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 11:16:34.318: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:34.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7606" for this suite. 08/24/23 11:16:34.361
------------------------------
â€¢ [SLOW TEST] [20.389 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:13.986
    Aug 24 11:16:13.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 11:16:13.987
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:14.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:14.032
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7606 08/24/23 11:16:14.054
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-7606 08/24/23 11:16:14.091
    Aug 24 11:16:14.187: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 11:16:24.191: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/24/23 11:16:24.198
    STEP: updating a scale subresource 08/24/23 11:16:24.203
    STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 11:16:24.217
    STEP: Patch a scale subresource 08/24/23 11:16:24.226
    STEP: verifying the statefulset Spec.Replicas was modified 08/24/23 11:16:24.27
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 11:16:24.275: INFO: Deleting all statefulset in ns statefulset-7606
    Aug 24 11:16:24.282: INFO: Scaling statefulset ss to 0
    Aug 24 11:16:34.311: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 11:16:34.318: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:34.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7606" for this suite. 08/24/23 11:16:34.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:34.377
Aug 24 11:16:34.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename namespaces 08/24/23 11:16:34.379
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:34.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:34.413
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-8jfrv" 08/24/23 11:16:34.421
Aug 24 11:16:34.441: INFO: Namespace "e2e-ns-8jfrv-1904" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-8jfrv-1904" 08/24/23 11:16:34.441
Aug 24 11:16:34.459: INFO: Namespace "e2e-ns-8jfrv-1904" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-8jfrv-1904" 08/24/23 11:16:34.459
Aug 24 11:16:34.487: INFO: Namespace "e2e-ns-8jfrv-1904" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:34.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5318" for this suite. 08/24/23 11:16:34.5
STEP: Destroying namespace "e2e-ns-8jfrv-1904" for this suite. 08/24/23 11:16:34.518
------------------------------
â€¢ [0.151 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:34.377
    Aug 24 11:16:34.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename namespaces 08/24/23 11:16:34.379
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:34.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:34.413
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-8jfrv" 08/24/23 11:16:34.421
    Aug 24 11:16:34.441: INFO: Namespace "e2e-ns-8jfrv-1904" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-8jfrv-1904" 08/24/23 11:16:34.441
    Aug 24 11:16:34.459: INFO: Namespace "e2e-ns-8jfrv-1904" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-8jfrv-1904" 08/24/23 11:16:34.459
    Aug 24 11:16:34.487: INFO: Namespace "e2e-ns-8jfrv-1904" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:34.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5318" for this suite. 08/24/23 11:16:34.5
    STEP: Destroying namespace "e2e-ns-8jfrv-1904" for this suite. 08/24/23 11:16:34.518
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:34.529
Aug 24 11:16:34.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename gc 08/24/23 11:16:34.53
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:34.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:34.578
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/24/23 11:16:34.6
STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 11:16:34.613
STEP: delete the deployment 08/24/23 11:16:35.126
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/24/23 11:16:35.135
STEP: Gathering metrics 08/24/23 11:16:35.685
W0824 11:16:35.712739      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 24 11:16:35.713: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:35.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6998" for this suite. 08/24/23 11:16:35.718
------------------------------
â€¢ [1.199 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:34.529
    Aug 24 11:16:34.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename gc 08/24/23 11:16:34.53
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:34.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:34.578
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/24/23 11:16:34.6
    STEP: Wait for the Deployment to create new ReplicaSet 08/24/23 11:16:34.613
    STEP: delete the deployment 08/24/23 11:16:35.126
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/24/23 11:16:35.135
    STEP: Gathering metrics 08/24/23 11:16:35.685
    W0824 11:16:35.712739      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 24 11:16:35.713: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:35.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6998" for this suite. 08/24/23 11:16:35.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:35.73
Aug 24 11:16:35.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:16:35.733
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:35.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:35.772
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:16:35.778
Aug 24 11:16:35.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 24 11:16:35.924: INFO: stderr: ""
Aug 24 11:16:35.924: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/24/23 11:16:35.924
STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 11:16:40.975
Aug 24 11:16:40.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 get pod e2e-test-httpd-pod -o json'
Aug 24 11:16:41.156: INFO: stderr: ""
Aug 24 11:16:41.156: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"22d05720805a579224b8c3127fa1174cafdf0668111c941d0ae31d5e69375799\",\n            \"cni.projectcalico.org/podIP\": \"10.100.181.136/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.181.136/32\"\n        },\n        \"creationTimestamp\": \"2023-08-24T11:16:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8079\",\n        \"resourceVersion\": \"18982\",\n        \"uid\": \"2ad9d62b-2ec2-4a5b-b39a-4f19e67770cb\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-q9mm7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"gitlab-1-26-36460-guscsyka22xa-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-q9mm7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:37Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a06ca0fd3197e58b3366000a3b90a063aada0c3b8a8649662846c9e18ad0d564\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-24T11:16:36Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.181.136\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.181.136\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-24T11:16:35Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/24/23 11:16:41.156
Aug 24 11:16:41.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 replace -f -'
Aug 24 11:16:42.951: INFO: stderr: ""
Aug 24 11:16:42.951: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/24/23 11:16:42.951
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 24 11:16:42.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 delete pods e2e-test-httpd-pod'
Aug 24 11:16:44.420: INFO: stderr: ""
Aug 24 11:16:44.420: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:44.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8079" for this suite. 08/24/23 11:16:44.427
------------------------------
â€¢ [SLOW TEST] [8.708 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:35.73
    Aug 24 11:16:35.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:16:35.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:35.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:35.772
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:16:35.778
    Aug 24 11:16:35.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 24 11:16:35.924: INFO: stderr: ""
    Aug 24 11:16:35.924: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/24/23 11:16:35.924
    STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 11:16:40.975
    Aug 24 11:16:40.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 get pod e2e-test-httpd-pod -o json'
    Aug 24 11:16:41.156: INFO: stderr: ""
    Aug 24 11:16:41.156: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"22d05720805a579224b8c3127fa1174cafdf0668111c941d0ae31d5e69375799\",\n            \"cni.projectcalico.org/podIP\": \"10.100.181.136/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.181.136/32\"\n        },\n        \"creationTimestamp\": \"2023-08-24T11:16:35Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8079\",\n        \"resourceVersion\": \"18982\",\n        \"uid\": \"2ad9d62b-2ec2-4a5b-b39a-4f19e67770cb\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-q9mm7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"gitlab-1-26-36460-guscsyka22xa-node-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-q9mm7\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:37Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T11:16:35Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a06ca0fd3197e58b3366000a3b90a063aada0c3b8a8649662846c9e18ad0d564\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-24T11:16:36Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.181.136\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.181.136\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-24T11:16:35Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/24/23 11:16:41.156
    Aug 24 11:16:41.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 replace -f -'
    Aug 24 11:16:42.951: INFO: stderr: ""
    Aug 24 11:16:42.951: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/24/23 11:16:42.951
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 24 11:16:42.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8079 delete pods e2e-test-httpd-pod'
    Aug 24 11:16:44.420: INFO: stderr: ""
    Aug 24 11:16:44.420: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:44.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8079" for this suite. 08/24/23 11:16:44.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:44.44
Aug 24 11:16:44.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 11:16:44.441
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:44.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:44.478
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/24/23 11:16:44.483
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_tcp@PTR;sleep 1; done
 08/24/23 11:16:44.553
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_tcp@PTR;sleep 1; done
 08/24/23 11:16:44.553
STEP: creating a pod to probe DNS 08/24/23 11:16:44.553
STEP: submitting the pod to kubernetes 08/24/23 11:16:44.553
Aug 24 11:16:44.586: INFO: Waiting up to 15m0s for pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603" in namespace "dns-9956" to be "running"
Aug 24 11:16:44.598: INFO: Pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603": Phase="Pending", Reason="", readiness=false. Elapsed: 12.732036ms
Aug 24 11:16:46.604: INFO: Pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603": Phase="Running", Reason="", readiness=true. Elapsed: 2.018290163s
Aug 24 11:16:46.604: INFO: Pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603" satisfied condition "running"
STEP: retrieving the pod 08/24/23 11:16:46.604
STEP: looking for the results for each expected name from probers 08/24/23 11:16:46.608
Aug 24 11:16:46.615: INFO: Unable to read wheezy_udp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.618: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.622: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.631: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.657: INFO: Unable to read jessie_udp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.673: INFO: Unable to read jessie_tcp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.702: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.715: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
Aug 24 11:16:46.736: INFO: Lookups using dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603 failed for: [wheezy_udp@dns-test-service.dns-9956.svc.cluster.local wheezy_tcp@dns-test-service.dns-9956.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local jessie_udp@dns-test-service.dns-9956.svc.cluster.local jessie_tcp@dns-test-service.dns-9956.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local]

Aug 24 11:16:51.819: INFO: DNS probes using dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603 succeeded

STEP: deleting the pod 08/24/23 11:16:51.819
STEP: deleting the test service 08/24/23 11:16:51.894
STEP: deleting the test headless service 08/24/23 11:16:51.99
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:52.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9956" for this suite. 08/24/23 11:16:52.033
------------------------------
â€¢ [SLOW TEST] [7.604 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:44.44
    Aug 24 11:16:44.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 11:16:44.441
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:44.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:44.478
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/24/23 11:16:44.483
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_tcp@PTR;sleep 1; done
     08/24/23 11:16:44.553
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9956.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9956.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9956.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_udp@PTR;check="$$(dig +tcp +noall +answer +search 227.221.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.221.227_tcp@PTR;sleep 1; done
     08/24/23 11:16:44.553
    STEP: creating a pod to probe DNS 08/24/23 11:16:44.553
    STEP: submitting the pod to kubernetes 08/24/23 11:16:44.553
    Aug 24 11:16:44.586: INFO: Waiting up to 15m0s for pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603" in namespace "dns-9956" to be "running"
    Aug 24 11:16:44.598: INFO: Pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603": Phase="Pending", Reason="", readiness=false. Elapsed: 12.732036ms
    Aug 24 11:16:46.604: INFO: Pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603": Phase="Running", Reason="", readiness=true. Elapsed: 2.018290163s
    Aug 24 11:16:46.604: INFO: Pod "dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 11:16:46.604
    STEP: looking for the results for each expected name from probers 08/24/23 11:16:46.608
    Aug 24 11:16:46.615: INFO: Unable to read wheezy_udp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.618: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.622: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.631: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.657: INFO: Unable to read jessie_udp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.673: INFO: Unable to read jessie_tcp@dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.702: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.715: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local from pod dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603: the server could not find the requested resource (get pods dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603)
    Aug 24 11:16:46.736: INFO: Lookups using dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603 failed for: [wheezy_udp@dns-test-service.dns-9956.svc.cluster.local wheezy_tcp@dns-test-service.dns-9956.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local jessie_udp@dns-test-service.dns-9956.svc.cluster.local jessie_tcp@dns-test-service.dns-9956.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9956.svc.cluster.local]

    Aug 24 11:16:51.819: INFO: DNS probes using dns-9956/dns-test-bb1bbe9a-70de-4964-b3ec-2732a9a00603 succeeded

    STEP: deleting the pod 08/24/23 11:16:51.819
    STEP: deleting the test service 08/24/23 11:16:51.894
    STEP: deleting the test headless service 08/24/23 11:16:51.99
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:52.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9956" for this suite. 08/24/23 11:16:52.033
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:52.045
Aug 24 11:16:52.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:16:52.047
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:52.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:52.091
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/24/23 11:16:52.101
STEP: watching for the Service to be added 08/24/23 11:16:52.122
Aug 24 11:16:52.130: INFO: Found Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 24 11:16:52.131: INFO: Service test-service-bfsnt created
STEP: Getting /status 08/24/23 11:16:52.131
Aug 24 11:16:52.140: INFO: Service test-service-bfsnt has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/24/23 11:16:52.14
STEP: watching for the Service to be patched 08/24/23 11:16:52.156
Aug 24 11:16:52.163: INFO: observed Service test-service-bfsnt in namespace services-8679 with annotations: map[] & LoadBalancer: {[]}
Aug 24 11:16:52.163: INFO: Found Service test-service-bfsnt in namespace services-8679 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 24 11:16:52.163: INFO: Service test-service-bfsnt has service status patched
STEP: updating the ServiceStatus 08/24/23 11:16:52.164
Aug 24 11:16:52.201: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/24/23 11:16:52.201
Aug 24 11:16:52.204: INFO: Observed Service test-service-bfsnt in namespace services-8679 with annotations: map[] & Conditions: {[]}
Aug 24 11:16:52.204: INFO: Observed event: &Service{ObjectMeta:{test-service-bfsnt  services-8679  d0a6b70c-8653-419c-8693-db749703cb9c 19155 0 2023-08-24 11:16:52 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-24 11:16:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-24 11:16:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.106.4,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.106.4],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 24 11:16:52.205: INFO: Found Service test-service-bfsnt in namespace services-8679 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 11:16:52.205: INFO: Service test-service-bfsnt has service status updated
STEP: patching the service 08/24/23 11:16:52.205
STEP: watching for the Service to be patched 08/24/23 11:16:52.222
Aug 24 11:16:52.227: INFO: observed Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true]
Aug 24 11:16:52.227: INFO: observed Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true]
Aug 24 11:16:52.227: INFO: observed Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true]
Aug 24 11:16:52.227: INFO: Found Service test-service-bfsnt in namespace services-8679 with labels: map[test-service:patched test-service-static:true]
Aug 24 11:16:52.227: INFO: Service test-service-bfsnt patched
STEP: deleting the service 08/24/23 11:16:52.227
STEP: watching for the Service to be deleted 08/24/23 11:16:52.25
Aug 24 11:16:52.253: INFO: Observed event: ADDED
Aug 24 11:16:52.253: INFO: Observed event: MODIFIED
Aug 24 11:16:52.253: INFO: Observed event: MODIFIED
Aug 24 11:16:52.253: INFO: Observed event: MODIFIED
Aug 24 11:16:52.253: INFO: Found Service test-service-bfsnt in namespace services-8679 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 24 11:16:52.253: INFO: Service test-service-bfsnt deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8679" for this suite. 08/24/23 11:16:52.258
------------------------------
â€¢ [0.222 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:52.045
    Aug 24 11:16:52.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:16:52.047
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:52.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:52.091
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/24/23 11:16:52.101
    STEP: watching for the Service to be added 08/24/23 11:16:52.122
    Aug 24 11:16:52.130: INFO: Found Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 24 11:16:52.131: INFO: Service test-service-bfsnt created
    STEP: Getting /status 08/24/23 11:16:52.131
    Aug 24 11:16:52.140: INFO: Service test-service-bfsnt has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/24/23 11:16:52.14
    STEP: watching for the Service to be patched 08/24/23 11:16:52.156
    Aug 24 11:16:52.163: INFO: observed Service test-service-bfsnt in namespace services-8679 with annotations: map[] & LoadBalancer: {[]}
    Aug 24 11:16:52.163: INFO: Found Service test-service-bfsnt in namespace services-8679 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 24 11:16:52.163: INFO: Service test-service-bfsnt has service status patched
    STEP: updating the ServiceStatus 08/24/23 11:16:52.164
    Aug 24 11:16:52.201: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/24/23 11:16:52.201
    Aug 24 11:16:52.204: INFO: Observed Service test-service-bfsnt in namespace services-8679 with annotations: map[] & Conditions: {[]}
    Aug 24 11:16:52.204: INFO: Observed event: &Service{ObjectMeta:{test-service-bfsnt  services-8679  d0a6b70c-8653-419c-8693-db749703cb9c 19155 0 2023-08-24 11:16:52 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-24 11:16:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-24 11:16:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.106.4,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.106.4],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 24 11:16:52.205: INFO: Found Service test-service-bfsnt in namespace services-8679 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 11:16:52.205: INFO: Service test-service-bfsnt has service status updated
    STEP: patching the service 08/24/23 11:16:52.205
    STEP: watching for the Service to be patched 08/24/23 11:16:52.222
    Aug 24 11:16:52.227: INFO: observed Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true]
    Aug 24 11:16:52.227: INFO: observed Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true]
    Aug 24 11:16:52.227: INFO: observed Service test-service-bfsnt in namespace services-8679 with labels: map[test-service-static:true]
    Aug 24 11:16:52.227: INFO: Found Service test-service-bfsnt in namespace services-8679 with labels: map[test-service:patched test-service-static:true]
    Aug 24 11:16:52.227: INFO: Service test-service-bfsnt patched
    STEP: deleting the service 08/24/23 11:16:52.227
    STEP: watching for the Service to be deleted 08/24/23 11:16:52.25
    Aug 24 11:16:52.253: INFO: Observed event: ADDED
    Aug 24 11:16:52.253: INFO: Observed event: MODIFIED
    Aug 24 11:16:52.253: INFO: Observed event: MODIFIED
    Aug 24 11:16:52.253: INFO: Observed event: MODIFIED
    Aug 24 11:16:52.253: INFO: Found Service test-service-bfsnt in namespace services-8679 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 24 11:16:52.253: INFO: Service test-service-bfsnt deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8679" for this suite. 08/24/23 11:16:52.258
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:52.267
Aug 24 11:16:52.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 11:16:52.269
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:52.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:52.306
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/24/23 11:16:52.312
STEP: submitting the pod to kubernetes 08/24/23 11:16:52.312
STEP: verifying QOS class is set on the pod 08/24/23 11:16:52.328
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:52.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8528" for this suite. 08/24/23 11:16:52.348
------------------------------
â€¢ [0.096 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:52.267
    Aug 24 11:16:52.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 11:16:52.269
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:52.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:52.306
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/24/23 11:16:52.312
    STEP: submitting the pod to kubernetes 08/24/23 11:16:52.312
    STEP: verifying QOS class is set on the pod 08/24/23 11:16:52.328
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:52.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8528" for this suite. 08/24/23 11:16:52.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:52.365
Aug 24 11:16:52.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:16:52.366
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:52.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:52.399
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:16:52.405
Aug 24 11:16:52.422: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585" in namespace "downward-api-4824" to be "Succeeded or Failed"
Aug 24 11:16:52.437: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585": Phase="Pending", Reason="", readiness=false. Elapsed: 14.999738ms
Aug 24 11:16:54.442: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020166487s
Aug 24 11:16:56.443: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02106454s
STEP: Saw pod success 08/24/23 11:16:56.443
Aug 24 11:16:56.444: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585" satisfied condition "Succeeded or Failed"
Aug 24 11:16:56.447: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585 container client-container: <nil>
STEP: delete the pod 08/24/23 11:16:56.455
Aug 24 11:16:56.483: INFO: Waiting for pod downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585 to disappear
Aug 24 11:16:56.489: INFO: Pod downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 11:16:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4824" for this suite. 08/24/23 11:16:56.494
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:52.365
    Aug 24 11:16:52.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:16:52.366
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:52.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:52.399
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:16:52.405
    Aug 24 11:16:52.422: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585" in namespace "downward-api-4824" to be "Succeeded or Failed"
    Aug 24 11:16:52.437: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585": Phase="Pending", Reason="", readiness=false. Elapsed: 14.999738ms
    Aug 24 11:16:54.442: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020166487s
    Aug 24 11:16:56.443: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02106454s
    STEP: Saw pod success 08/24/23 11:16:56.443
    Aug 24 11:16:56.444: INFO: Pod "downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585" satisfied condition "Succeeded or Failed"
    Aug 24 11:16:56.447: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:16:56.455
    Aug 24 11:16:56.483: INFO: Waiting for pod downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585 to disappear
    Aug 24 11:16:56.489: INFO: Pod downwardapi-volume-3e95ad04-fd42-4b51-a52d-8c6ae5ec1585 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:16:56.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4824" for this suite. 08/24/23 11:16:56.494
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:16:56.508
Aug 24 11:16:56.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 11:16:56.51
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:56.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:56.541
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8970 08/24/23 11:16:56.548
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/24/23 11:16:56.557
STEP: Creating stateful set ss in namespace statefulset-8970 08/24/23 11:16:56.575
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8970 08/24/23 11:16:56.593
Aug 24 11:16:56.598: INFO: Found 0 stateful pods, waiting for 1
Aug 24 11:17:06.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/24/23 11:17:06.603
Aug 24 11:17:06.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 11:17:06.861: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 11:17:06.861: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 11:17:06.861: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 11:17:06.864: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 24 11:17:16.868: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 11:17:16.868: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 11:17:16.888: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999733s
Aug 24 11:17:17.892: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997113687s
Aug 24 11:17:18.915: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.9930818s
Aug 24 11:17:19.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.969713935s
Aug 24 11:17:20.923: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.965830629s
Aug 24 11:17:21.939: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961409126s
Aug 24 11:17:22.943: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.9459086s
Aug 24 11:17:23.947: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.941716156s
Aug 24 11:17:24.952: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.9377722s
Aug 24 11:17:25.955: INFO: Verifying statefulset ss doesn't scale past 1 for another 932.982549ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8970 08/24/23 11:17:26.956
Aug 24 11:17:26.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 11:17:27.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 11:17:27.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 11:17:27.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 11:17:27.207: INFO: Found 1 stateful pods, waiting for 3
Aug 24 11:17:37.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:17:37.231: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:17:37.231: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/24/23 11:17:37.231
STEP: Scale down will halt with unhealthy stateful pod 08/24/23 11:17:37.231
Aug 24 11:17:37.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 11:17:37.506: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 11:17:37.506: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 11:17:37.506: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 11:17:37.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 11:17:37.810: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 11:17:37.810: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 11:17:37.810: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 11:17:37.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 11:17:38.175: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 11:17:38.175: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 11:17:38.175: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 11:17:38.175: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 11:17:38.178: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 24 11:17:48.189: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 11:17:48.189: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 11:17:48.189: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 11:17:48.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999742s
Aug 24 11:17:49.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993911738s
Aug 24 11:17:50.219: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988889418s
Aug 24 11:17:51.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983603274s
Aug 24 11:17:52.228: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978778733s
Aug 24 11:17:53.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97369435s
Aug 24 11:17:54.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965557427s
Aug 24 11:17:55.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95955589s
Aug 24 11:17:56.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955424377s
Aug 24 11:17:57.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.970983ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8970 08/24/23 11:17:58.259
Aug 24 11:17:58.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 11:17:58.513: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 11:17:58.513: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 11:17:58.513: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 11:17:58.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 11:17:58.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 11:17:58.790: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 11:17:58.790: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 11:17:58.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 11:17:59.102: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 11:17:59.102: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 11:17:59.102: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 11:17:59.102: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/24/23 11:18:09.122
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 11:18:09.122: INFO: Deleting all statefulset in ns statefulset-8970
Aug 24 11:18:09.127: INFO: Scaling statefulset ss to 0
Aug 24 11:18:09.137: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 11:18:09.139: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:18:09.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8970" for this suite. 08/24/23 11:18:09.198
------------------------------
â€¢ [SLOW TEST] [72.707 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:16:56.508
    Aug 24 11:16:56.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 11:16:56.51
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:16:56.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:16:56.541
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8970 08/24/23 11:16:56.548
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/24/23 11:16:56.557
    STEP: Creating stateful set ss in namespace statefulset-8970 08/24/23 11:16:56.575
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8970 08/24/23 11:16:56.593
    Aug 24 11:16:56.598: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 11:17:06.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/24/23 11:17:06.603
    Aug 24 11:17:06.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 11:17:06.861: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 11:17:06.861: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 11:17:06.861: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 11:17:06.864: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 24 11:17:16.868: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 11:17:16.868: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 11:17:16.888: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999733s
    Aug 24 11:17:17.892: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997113687s
    Aug 24 11:17:18.915: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.9930818s
    Aug 24 11:17:19.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.969713935s
    Aug 24 11:17:20.923: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.965830629s
    Aug 24 11:17:21.939: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.961409126s
    Aug 24 11:17:22.943: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.9459086s
    Aug 24 11:17:23.947: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.941716156s
    Aug 24 11:17:24.952: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.9377722s
    Aug 24 11:17:25.955: INFO: Verifying statefulset ss doesn't scale past 1 for another 932.982549ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8970 08/24/23 11:17:26.956
    Aug 24 11:17:26.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 11:17:27.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 11:17:27.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 11:17:27.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 11:17:27.207: INFO: Found 1 stateful pods, waiting for 3
    Aug 24 11:17:37.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:17:37.231: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:17:37.231: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/24/23 11:17:37.231
    STEP: Scale down will halt with unhealthy stateful pod 08/24/23 11:17:37.231
    Aug 24 11:17:37.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 11:17:37.506: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 11:17:37.506: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 11:17:37.506: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 11:17:37.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 11:17:37.810: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 11:17:37.810: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 11:17:37.810: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 11:17:37.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 11:17:38.175: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 11:17:38.175: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 11:17:38.175: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 11:17:38.175: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 11:17:38.178: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 24 11:17:48.189: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 11:17:48.189: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 11:17:48.189: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 11:17:48.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999742s
    Aug 24 11:17:49.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993911738s
    Aug 24 11:17:50.219: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988889418s
    Aug 24 11:17:51.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983603274s
    Aug 24 11:17:52.228: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978778733s
    Aug 24 11:17:53.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97369435s
    Aug 24 11:17:54.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965557427s
    Aug 24 11:17:55.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95955589s
    Aug 24 11:17:56.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955424377s
    Aug 24 11:17:57.259: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.970983ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8970 08/24/23 11:17:58.259
    Aug 24 11:17:58.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 11:17:58.513: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 11:17:58.513: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 11:17:58.513: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 11:17:58.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 11:17:58.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 11:17:58.790: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 11:17:58.790: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 11:17:58.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-8970 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 11:17:59.102: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 11:17:59.102: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 11:17:59.102: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 11:17:59.102: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/24/23 11:18:09.122
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 11:18:09.122: INFO: Deleting all statefulset in ns statefulset-8970
    Aug 24 11:18:09.127: INFO: Scaling statefulset ss to 0
    Aug 24 11:18:09.137: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 11:18:09.139: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:18:09.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8970" for this suite. 08/24/23 11:18:09.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:18:09.219
Aug 24 11:18:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename runtimeclass 08/24/23 11:18:09.221
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:09.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:09.267
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-8908-delete-me 08/24/23 11:18:09.282
STEP: Waiting for the RuntimeClass to disappear 08/24/23 11:18:09.291
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 11:18:09.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8908" for this suite. 08/24/23 11:18:09.32
------------------------------
â€¢ [0.126 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:18:09.219
    Aug 24 11:18:09.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 11:18:09.221
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:09.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:09.267
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-8908-delete-me 08/24/23 11:18:09.282
    STEP: Waiting for the RuntimeClass to disappear 08/24/23 11:18:09.291
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:18:09.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8908" for this suite. 08/24/23 11:18:09.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:18:09.353
Aug 24 11:18:09.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:18:09.354
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:09.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:09.411
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/24/23 11:18:09.426
STEP: watching for the ServiceAccount to be added 08/24/23 11:18:09.44
STEP: patching the ServiceAccount 08/24/23 11:18:09.444
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/24/23 11:18:09.454
STEP: deleting the ServiceAccount 08/24/23 11:18:09.466
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 11:18:09.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8171" for this suite. 08/24/23 11:18:09.49
------------------------------
â€¢ [0.149 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:18:09.353
    Aug 24 11:18:09.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:18:09.354
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:09.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:09.411
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/24/23 11:18:09.426
    STEP: watching for the ServiceAccount to be added 08/24/23 11:18:09.44
    STEP: patching the ServiceAccount 08/24/23 11:18:09.444
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/24/23 11:18:09.454
    STEP: deleting the ServiceAccount 08/24/23 11:18:09.466
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:18:09.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8171" for this suite. 08/24/23 11:18:09.49
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:18:09.503
Aug 24 11:18:09.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-runtime 08/24/23 11:18:09.504
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:09.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:09.53
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/24/23 11:18:09.538
STEP: wait for the container to reach Succeeded 08/24/23 11:18:09.559
STEP: get the container status 08/24/23 11:18:12.602
STEP: the container should be terminated 08/24/23 11:18:12.604
STEP: the termination message should be set 08/24/23 11:18:12.604
Aug 24 11:18:12.604: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/24/23 11:18:12.604
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 11:18:12.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2239" for this suite. 08/24/23 11:18:12.652
------------------------------
â€¢ [3.174 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:18:09.503
    Aug 24 11:18:09.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-runtime 08/24/23 11:18:09.504
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:09.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:09.53
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/24/23 11:18:09.538
    STEP: wait for the container to reach Succeeded 08/24/23 11:18:09.559
    STEP: get the container status 08/24/23 11:18:12.602
    STEP: the container should be terminated 08/24/23 11:18:12.604
    STEP: the termination message should be set 08/24/23 11:18:12.604
    Aug 24 11:18:12.604: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/24/23 11:18:12.604
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:18:12.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2239" for this suite. 08/24/23 11:18:12.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:18:12.677
Aug 24 11:18:12.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubelet-test 08/24/23 11:18:12.686
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:12.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:12.725
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:18:12.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2679" for this suite. 08/24/23 11:18:12.843
------------------------------
â€¢ [0.181 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:18:12.677
    Aug 24 11:18:12.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 11:18:12.686
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:12.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:12.725
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:18:12.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2679" for this suite. 08/24/23 11:18:12.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:18:12.859
Aug 24 11:18:12.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename events 08/24/23 11:18:12.86
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:12.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:12.885
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/24/23 11:18:12.893
STEP: listing events in all namespaces 08/24/23 11:18:12.908
STEP: listing events in test namespace 08/24/23 11:18:12.917
STEP: listing events with field selection filtering on source 08/24/23 11:18:12.922
STEP: listing events with field selection filtering on reportingController 08/24/23 11:18:12.927
STEP: getting the test event 08/24/23 11:18:12.93
STEP: patching the test event 08/24/23 11:18:12.934
STEP: getting the test event 08/24/23 11:18:12.947
STEP: updating the test event 08/24/23 11:18:12.95
STEP: getting the test event 08/24/23 11:18:12.961
STEP: deleting the test event 08/24/23 11:18:12.965
STEP: listing events in all namespaces 08/24/23 11:18:12.973
STEP: listing events in test namespace 08/24/23 11:18:12.98
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 24 11:18:12.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4055" for this suite. 08/24/23 11:18:12.987
------------------------------
â€¢ [0.144 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:18:12.859
    Aug 24 11:18:12.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename events 08/24/23 11:18:12.86
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:12.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:12.885
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/24/23 11:18:12.893
    STEP: listing events in all namespaces 08/24/23 11:18:12.908
    STEP: listing events in test namespace 08/24/23 11:18:12.917
    STEP: listing events with field selection filtering on source 08/24/23 11:18:12.922
    STEP: listing events with field selection filtering on reportingController 08/24/23 11:18:12.927
    STEP: getting the test event 08/24/23 11:18:12.93
    STEP: patching the test event 08/24/23 11:18:12.934
    STEP: getting the test event 08/24/23 11:18:12.947
    STEP: updating the test event 08/24/23 11:18:12.95
    STEP: getting the test event 08/24/23 11:18:12.961
    STEP: deleting the test event 08/24/23 11:18:12.965
    STEP: listing events in all namespaces 08/24/23 11:18:12.973
    STEP: listing events in test namespace 08/24/23 11:18:12.98
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:18:12.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4055" for this suite. 08/24/23 11:18:12.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:18:13.004
Aug 24 11:18:13.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename aggregator 08/24/23 11:18:13.006
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:13.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:13.04
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 24 11:18:13.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/24/23 11:18:13.048
Aug 24 11:18:14.094: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 24 11:18:16.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:18.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:20.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:22.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:24.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:26.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:28.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:30.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:32.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:34.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:36.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:38.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:40.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:42.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:44.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:46.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:48.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:50.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:52.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:54.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:56.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:18:58.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:00.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:02.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:04.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:06.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:08.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:10.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:12.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:14.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:16.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:18.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:20.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:22.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:24.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:26.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:28.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:30.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:32.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:34.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:36.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:38.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:40.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:42.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:44.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:46.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:48.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:50.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:52.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:54.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:56.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:19:58.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:20:00.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:20:02.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 11:20:04.388: INFO: Waited 189.282078ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/24/23 11:20:04.493
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/24/23 11:20:04.497
STEP: List APIServices 08/24/23 11:20:04.513
Aug 24 11:20:04.522: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 24 11:20:05.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-700" for this suite. 08/24/23 11:20:05.112
------------------------------
â€¢ [SLOW TEST] [112.121 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:18:13.004
    Aug 24 11:18:13.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename aggregator 08/24/23 11:18:13.006
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:18:13.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:18:13.04
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 24 11:18:13.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/24/23 11:18:13.048
    Aug 24 11:18:14.094: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Aug 24 11:18:16.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:18.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:20.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:22.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:24.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:26.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:28.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:30.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:32.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:34.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:36.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:38.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:40.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:42.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:44.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:46.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:48.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:50.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:52.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:54.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:56.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:18:58.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:00.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:02.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:04.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:06.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:08.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:10.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:12.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:14.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:16.200: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:18.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:20.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:22.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:24.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:26.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:28.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:30.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:32.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:34.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:36.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:38.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:40.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:42.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:44.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:46.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:48.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:50.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:52.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:54.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:56.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:19:58.184: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:20:00.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:20:02.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 18, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 11:20:04.388: INFO: Waited 189.282078ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/24/23 11:20:04.493
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/24/23 11:20:04.497
    STEP: List APIServices 08/24/23 11:20:04.513
    Aug 24 11:20:04.522: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:20:05.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-700" for this suite. 08/24/23 11:20:05.112
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:20:05.128
Aug 24 11:20:05.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:20:05.13
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:05.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:05.168
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/24/23 11:20:05.178
Aug 24 11:20:05.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:20:08.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:20:18.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3078" for this suite. 08/24/23 11:20:18.136
------------------------------
â€¢ [SLOW TEST] [13.022 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:20:05.128
    Aug 24 11:20:05.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:20:05.13
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:05.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:05.168
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/24/23 11:20:05.178
    Aug 24 11:20:05.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:20:08.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:20:18.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3078" for this suite. 08/24/23 11:20:18.136
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:20:18.151
Aug 24 11:20:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 11:20:18.155
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:18.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:18.185
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/24/23 11:20:18.191
STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:20:18.199
STEP: Creating a ResourceQuota with not best effort scope 08/24/23 11:20:20.204
STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:20:20.212
STEP: Creating a best-effort pod 08/24/23 11:20:22.217
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/24/23 11:20:22.242
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/24/23 11:20:24.246
STEP: Deleting the pod 08/24/23 11:20:26.25
STEP: Ensuring resource quota status released the pod usage 08/24/23 11:20:26.266
STEP: Creating a not best-effort pod 08/24/23 11:20:28.271
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/24/23 11:20:28.291
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/24/23 11:20:30.299
STEP: Deleting the pod 08/24/23 11:20:32.304
STEP: Ensuring resource quota status released the pod usage 08/24/23 11:20:32.331
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 11:20:34.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6118" for this suite. 08/24/23 11:20:34.341
------------------------------
â€¢ [SLOW TEST] [16.203 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:20:18.151
    Aug 24 11:20:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 11:20:18.155
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:18.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:18.185
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/24/23 11:20:18.191
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:20:18.199
    STEP: Creating a ResourceQuota with not best effort scope 08/24/23 11:20:20.204
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:20:20.212
    STEP: Creating a best-effort pod 08/24/23 11:20:22.217
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/24/23 11:20:22.242
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/24/23 11:20:24.246
    STEP: Deleting the pod 08/24/23 11:20:26.25
    STEP: Ensuring resource quota status released the pod usage 08/24/23 11:20:26.266
    STEP: Creating a not best-effort pod 08/24/23 11:20:28.271
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/24/23 11:20:28.291
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/24/23 11:20:30.299
    STEP: Deleting the pod 08/24/23 11:20:32.304
    STEP: Ensuring resource quota status released the pod usage 08/24/23 11:20:32.331
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:20:34.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6118" for this suite. 08/24/23 11:20:34.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:20:34.356
Aug 24 11:20:34.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:20:34.357
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:34.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:34.389
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1768 08/24/23 11:20:34.396
STEP: creating replication controller nodeport-test in namespace services-1768 08/24/23 11:20:34.417
I0824 11:20:34.450742      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1768, replica count: 2
I0824 11:20:37.502114      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:20:37.502: INFO: Creating new exec pod
Aug 24 11:20:37.517: INFO: Waiting up to 5m0s for pod "execpodh7bgt" in namespace "services-1768" to be "running"
Aug 24 11:20:37.523: INFO: Pod "execpodh7bgt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.220322ms
Aug 24 11:20:39.531: INFO: Pod "execpodh7bgt": Phase="Running", Reason="", readiness=true. Elapsed: 2.014366s
Aug 24 11:20:39.531: INFO: Pod "execpodh7bgt" satisfied condition "running"
Aug 24 11:20:40.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 24 11:20:40.797: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 24 11:20:40.797: INFO: stdout: ""
Aug 24 11:20:40.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 10.254.80.183 80'
Aug 24 11:20:41.058: INFO: stderr: "+ nc -v -z -w 2 10.254.80.183 80\nConnection to 10.254.80.183 80 port [tcp/http] succeeded!\n"
Aug 24 11:20:41.058: INFO: stdout: ""
Aug 24 11:20:41.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 10.0.0.17 32067'
Aug 24 11:20:41.323: INFO: stderr: "+ nc -v -z -w 2 10.0.0.17 32067\nConnection to 10.0.0.17 32067 port [tcp/*] succeeded!\n"
Aug 24 11:20:41.323: INFO: stdout: ""
Aug 24 11:20:41.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 32067'
Aug 24 11:20:41.566: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 32067\nConnection to 10.0.0.4 32067 port [tcp/*] succeeded!\n"
Aug 24 11:20:41.566: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:20:41.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1768" for this suite. 08/24/23 11:20:41.57
------------------------------
â€¢ [SLOW TEST] [7.222 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:20:34.356
    Aug 24 11:20:34.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:20:34.357
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:34.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:34.389
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1768 08/24/23 11:20:34.396
    STEP: creating replication controller nodeport-test in namespace services-1768 08/24/23 11:20:34.417
    I0824 11:20:34.450742      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1768, replica count: 2
    I0824 11:20:37.502114      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:20:37.502: INFO: Creating new exec pod
    Aug 24 11:20:37.517: INFO: Waiting up to 5m0s for pod "execpodh7bgt" in namespace "services-1768" to be "running"
    Aug 24 11:20:37.523: INFO: Pod "execpodh7bgt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.220322ms
    Aug 24 11:20:39.531: INFO: Pod "execpodh7bgt": Phase="Running", Reason="", readiness=true. Elapsed: 2.014366s
    Aug 24 11:20:39.531: INFO: Pod "execpodh7bgt" satisfied condition "running"
    Aug 24 11:20:40.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 24 11:20:40.797: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 24 11:20:40.797: INFO: stdout: ""
    Aug 24 11:20:40.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 10.254.80.183 80'
    Aug 24 11:20:41.058: INFO: stderr: "+ nc -v -z -w 2 10.254.80.183 80\nConnection to 10.254.80.183 80 port [tcp/http] succeeded!\n"
    Aug 24 11:20:41.058: INFO: stdout: ""
    Aug 24 11:20:41.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 10.0.0.17 32067'
    Aug 24 11:20:41.323: INFO: stderr: "+ nc -v -z -w 2 10.0.0.17 32067\nConnection to 10.0.0.17 32067 port [tcp/*] succeeded!\n"
    Aug 24 11:20:41.323: INFO: stdout: ""
    Aug 24 11:20:41.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1768 exec execpodh7bgt -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 32067'
    Aug 24 11:20:41.566: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 32067\nConnection to 10.0.0.4 32067 port [tcp/*] succeeded!\n"
    Aug 24 11:20:41.566: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:20:41.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1768" for this suite. 08/24/23 11:20:41.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:20:41.579
Aug 24 11:20:41.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 11:20:41.581
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:41.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:41.603
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/24/23 11:20:41.611
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-423;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-423;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +notcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_tcp@PTR;sleep 1; done
 08/24/23 11:20:41.646
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-423;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-423;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +notcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_tcp@PTR;sleep 1; done
 08/24/23 11:20:41.647
STEP: creating a pod to probe DNS 08/24/23 11:20:41.647
STEP: submitting the pod to kubernetes 08/24/23 11:20:41.647
Aug 24 11:20:41.685: INFO: Waiting up to 15m0s for pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96" in namespace "dns-423" to be "running"
Aug 24 11:20:41.695: INFO: Pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96": Phase="Pending", Reason="", readiness=false. Elapsed: 9.955573ms
Aug 24 11:20:43.699: INFO: Pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96": Phase="Running", Reason="", readiness=true. Elapsed: 2.013667799s
Aug 24 11:20:43.699: INFO: Pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96" satisfied condition "running"
STEP: retrieving the pod 08/24/23 11:20:43.699
STEP: looking for the results for each expected name from probers 08/24/23 11:20:43.708
Aug 24 11:20:43.713: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.717: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.720: INFO: Unable to read wheezy_udp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.723: INFO: Unable to read wheezy_tcp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.727: INFO: Unable to read wheezy_udp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.733: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.736: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.740: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.742: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.753: INFO: Unable to read jessie_udp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.755: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.758: INFO: Unable to read jessie_udp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.762: INFO: Unable to read jessie_tcp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.765: INFO: Unable to read jessie_udp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.771: INFO: Unable to read jessie_tcp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:43.791: INFO: Lookups using dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-423 wheezy_tcp@dns-test-service.dns-423 wheezy_udp@dns-test-service.dns-423.svc wheezy_tcp@dns-test-service.dns-423.svc wheezy_udp@_http._tcp.dns-test-service.dns-423.svc wheezy_tcp@_http._tcp.dns-test-service.dns-423.svc wheezy_udp@_http._tcp.test-service-2.dns-423.svc wheezy_tcp@_http._tcp.test-service-2.dns-423.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-423 jessie_tcp@dns-test-service.dns-423 jessie_udp@dns-test-service.dns-423.svc jessie_tcp@dns-test-service.dns-423.svc]

Aug 24 11:20:48.839: INFO: Unable to read jessie_udp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:48.842: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:48.846: INFO: Unable to read jessie_udp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:48.849: INFO: Unable to read jessie_tcp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:48.853: INFO: Unable to read jessie_udp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:48.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
Aug 24 11:20:48.915: INFO: Lookups using dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-423 jessie_tcp@dns-test-service.dns-423 jessie_udp@dns-test-service.dns-423.svc jessie_tcp@dns-test-service.dns-423.svc]

Aug 24 11:20:53.877: INFO: DNS probes using dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96 succeeded

STEP: deleting the pod 08/24/23 11:20:53.877
STEP: deleting the test service 08/24/23 11:20:53.939
STEP: deleting the test headless service 08/24/23 11:20:54.083
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 11:20:54.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-423" for this suite. 08/24/23 11:20:54.179
------------------------------
â€¢ [SLOW TEST] [12.614 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:20:41.579
    Aug 24 11:20:41.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 11:20:41.581
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:41.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:41.603
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/24/23 11:20:41.611
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-423;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-423;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +notcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_tcp@PTR;sleep 1; done
     08/24/23 11:20:41.646
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-423;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-423;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-423.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-423.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-423.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-423.svc;check="$$(dig +notcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.188.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.188.113_tcp@PTR;sleep 1; done
     08/24/23 11:20:41.647
    STEP: creating a pod to probe DNS 08/24/23 11:20:41.647
    STEP: submitting the pod to kubernetes 08/24/23 11:20:41.647
    Aug 24 11:20:41.685: INFO: Waiting up to 15m0s for pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96" in namespace "dns-423" to be "running"
    Aug 24 11:20:41.695: INFO: Pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96": Phase="Pending", Reason="", readiness=false. Elapsed: 9.955573ms
    Aug 24 11:20:43.699: INFO: Pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96": Phase="Running", Reason="", readiness=true. Elapsed: 2.013667799s
    Aug 24 11:20:43.699: INFO: Pod "dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 11:20:43.699
    STEP: looking for the results for each expected name from probers 08/24/23 11:20:43.708
    Aug 24 11:20:43.713: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.717: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.720: INFO: Unable to read wheezy_udp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.723: INFO: Unable to read wheezy_tcp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.727: INFO: Unable to read wheezy_udp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.733: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.736: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.740: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.742: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.753: INFO: Unable to read jessie_udp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.755: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.758: INFO: Unable to read jessie_udp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.762: INFO: Unable to read jessie_tcp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.765: INFO: Unable to read jessie_udp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.771: INFO: Unable to read jessie_tcp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:43.791: INFO: Lookups using dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-423 wheezy_tcp@dns-test-service.dns-423 wheezy_udp@dns-test-service.dns-423.svc wheezy_tcp@dns-test-service.dns-423.svc wheezy_udp@_http._tcp.dns-test-service.dns-423.svc wheezy_tcp@_http._tcp.dns-test-service.dns-423.svc wheezy_udp@_http._tcp.test-service-2.dns-423.svc wheezy_tcp@_http._tcp.test-service-2.dns-423.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-423 jessie_tcp@dns-test-service.dns-423 jessie_udp@dns-test-service.dns-423.svc jessie_tcp@dns-test-service.dns-423.svc]

    Aug 24 11:20:48.839: INFO: Unable to read jessie_udp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:48.842: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:48.846: INFO: Unable to read jessie_udp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:48.849: INFO: Unable to read jessie_tcp@dns-test-service.dns-423 from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:48.853: INFO: Unable to read jessie_udp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:48.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-423.svc from pod dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96: the server could not find the requested resource (get pods dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96)
    Aug 24 11:20:48.915: INFO: Lookups using dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-423 jessie_tcp@dns-test-service.dns-423 jessie_udp@dns-test-service.dns-423.svc jessie_tcp@dns-test-service.dns-423.svc]

    Aug 24 11:20:53.877: INFO: DNS probes using dns-423/dns-test-a7e49d52-a0a1-4477-8d7e-3f531b5bad96 succeeded

    STEP: deleting the pod 08/24/23 11:20:53.877
    STEP: deleting the test service 08/24/23 11:20:53.939
    STEP: deleting the test headless service 08/24/23 11:20:54.083
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:20:54.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-423" for this suite. 08/24/23 11:20:54.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:20:54.193
Aug 24 11:20:54.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:20:54.195
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:54.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:54.231
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:20:54.24
Aug 24 11:20:54.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8805 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 24 11:20:54.365: INFO: stderr: ""
Aug 24 11:20:54.365: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/24/23 11:20:54.365
Aug 24 11:20:54.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8805 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 24 11:20:55.619: INFO: stderr: ""
Aug 24 11:20:55.619: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:20:55.619
Aug 24 11:20:55.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8805 delete pods e2e-test-httpd-pod'
Aug 24 11:20:57.979: INFO: stderr: ""
Aug 24 11:20:57.979: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:20:57.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8805" for this suite. 08/24/23 11:20:57.993
------------------------------
â€¢ [3.818 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:20:54.193
    Aug 24 11:20:54.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:20:54.195
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:54.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:54.231
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:20:54.24
    Aug 24 11:20:54.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8805 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 24 11:20:54.365: INFO: stderr: ""
    Aug 24 11:20:54.365: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/24/23 11:20:54.365
    Aug 24 11:20:54.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8805 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 24 11:20:55.619: INFO: stderr: ""
    Aug 24 11:20:55.619: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:20:55.619
    Aug 24 11:20:55.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8805 delete pods e2e-test-httpd-pod'
    Aug 24 11:20:57.979: INFO: stderr: ""
    Aug 24 11:20:57.979: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:20:57.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8805" for this suite. 08/24/23 11:20:57.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:20:58.012
Aug 24 11:20:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:20:58.014
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:58.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:58.056
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/24/23 11:20:58.063
Aug 24 11:20:58.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 create -f -'
Aug 24 11:20:58.552: INFO: stderr: ""
Aug 24 11:20:58.552: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:20:58.552
Aug 24 11:20:58.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:20:58.682: INFO: stderr: ""
Aug 24 11:20:58.682: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:20:58.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:20:58.806: INFO: stderr: ""
Aug 24 11:20:58.806: INFO: stdout: ""
Aug 24 11:20:58.806: INFO: update-demo-nautilus-68fx2 is created but not running
Aug 24 11:21:03.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:03.928: INFO: stderr: ""
Aug 24 11:21:03.928: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:21:03.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:04.037: INFO: stderr: ""
Aug 24 11:21:04.037: INFO: stdout: ""
Aug 24 11:21:04.037: INFO: update-demo-nautilus-68fx2 is created but not running
Aug 24 11:21:09.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:09.163: INFO: stderr: ""
Aug 24 11:21:09.163: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:21:09.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:09.274: INFO: stderr: ""
Aug 24 11:21:09.274: INFO: stdout: ""
Aug 24 11:21:09.274: INFO: update-demo-nautilus-68fx2 is created but not running
Aug 24 11:21:14.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:14.394: INFO: stderr: ""
Aug 24 11:21:14.394: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:21:14.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:14.499: INFO: stderr: ""
Aug 24 11:21:14.499: INFO: stdout: ""
Aug 24 11:21:14.499: INFO: update-demo-nautilus-68fx2 is created but not running
Aug 24 11:21:19.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:19.633: INFO: stderr: ""
Aug 24 11:21:19.634: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:21:19.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:19.744: INFO: stderr: ""
Aug 24 11:21:19.744: INFO: stdout: ""
Aug 24 11:21:19.744: INFO: update-demo-nautilus-68fx2 is created but not running
Aug 24 11:21:24.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:24.874: INFO: stderr: ""
Aug 24 11:21:24.874: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:21:24.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:25.008: INFO: stderr: ""
Aug 24 11:21:25.008: INFO: stdout: ""
Aug 24 11:21:25.008: INFO: update-demo-nautilus-68fx2 is created but not running
Aug 24 11:21:30.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:30.124: INFO: stderr: ""
Aug 24 11:21:30.124: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:21:30.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:30.248: INFO: stderr: ""
Aug 24 11:21:30.248: INFO: stdout: ""
Aug 24 11:21:30.248: INFO: update-demo-nautilus-68fx2 is created but not running
Aug 24 11:21:35.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:35.376: INFO: stderr: ""
Aug 24 11:21:35.377: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
Aug 24 11:21:35.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:35.502: INFO: stderr: ""
Aug 24 11:21:35.502: INFO: stdout: "true"
Aug 24 11:21:35.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:21:35.623: INFO: stderr: ""
Aug 24 11:21:35.623: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:21:35.623: INFO: validating pod update-demo-nautilus-68fx2
Aug 24 11:21:35.631: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:21:35.631: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:21:35.631: INFO: update-demo-nautilus-68fx2 is verified up and running
Aug 24 11:21:35.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:35.750: INFO: stderr: ""
Aug 24 11:21:35.750: INFO: stdout: "true"
Aug 24 11:21:35.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:21:35.859: INFO: stderr: ""
Aug 24 11:21:35.859: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:21:35.859: INFO: validating pod update-demo-nautilus-pkhr6
Aug 24 11:21:35.865: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:21:35.865: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:21:35.865: INFO: update-demo-nautilus-pkhr6 is verified up and running
STEP: scaling down the replication controller 08/24/23 11:21:35.865
Aug 24 11:21:35.869: INFO: scanned /root for discovery docs: <nil>
Aug 24 11:21:35.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 24 11:21:37.029: INFO: stderr: ""
Aug 24 11:21:37.029: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:21:37.029
Aug 24 11:21:37.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:37.150: INFO: stderr: ""
Aug 24 11:21:37.150: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/24/23 11:21:37.15
Aug 24 11:21:42.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:42.278: INFO: stderr: ""
Aug 24 11:21:42.279: INFO: stdout: "update-demo-nautilus-pkhr6 "
Aug 24 11:21:42.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:42.378: INFO: stderr: ""
Aug 24 11:21:42.378: INFO: stdout: "true"
Aug 24 11:21:42.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:21:42.514: INFO: stderr: ""
Aug 24 11:21:42.514: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:21:42.514: INFO: validating pod update-demo-nautilus-pkhr6
Aug 24 11:21:42.521: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:21:42.521: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:21:42.521: INFO: update-demo-nautilus-pkhr6 is verified up and running
STEP: scaling up the replication controller 08/24/23 11:21:42.521
Aug 24 11:21:42.523: INFO: scanned /root for discovery docs: <nil>
Aug 24 11:21:42.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 24 11:21:43.668: INFO: stderr: ""
Aug 24 11:21:43.668: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:21:43.668
Aug 24 11:21:43.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:43.789: INFO: stderr: ""
Aug 24 11:21:43.789: INFO: stdout: "update-demo-nautilus-pkhr6 update-demo-nautilus-t55nq "
Aug 24 11:21:43.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:43.926: INFO: stderr: ""
Aug 24 11:21:43.926: INFO: stdout: "true"
Aug 24 11:21:43.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:21:44.065: INFO: stderr: ""
Aug 24 11:21:44.065: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:21:44.065: INFO: validating pod update-demo-nautilus-pkhr6
Aug 24 11:21:44.071: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:21:44.071: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:21:44.071: INFO: update-demo-nautilus-pkhr6 is verified up and running
Aug 24 11:21:44.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-t55nq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:44.199: INFO: stderr: ""
Aug 24 11:21:44.199: INFO: stdout: ""
Aug 24 11:21:44.199: INFO: update-demo-nautilus-t55nq is created but not running
Aug 24 11:21:49.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:21:49.340: INFO: stderr: ""
Aug 24 11:21:49.340: INFO: stdout: "update-demo-nautilus-pkhr6 update-demo-nautilus-t55nq "
Aug 24 11:21:49.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:49.451: INFO: stderr: ""
Aug 24 11:21:49.451: INFO: stdout: "true"
Aug 24 11:21:49.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:21:49.554: INFO: stderr: ""
Aug 24 11:21:49.554: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:21:49.554: INFO: validating pod update-demo-nautilus-pkhr6
Aug 24 11:21:49.561: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:21:49.561: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:21:49.561: INFO: update-demo-nautilus-pkhr6 is verified up and running
Aug 24 11:21:49.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-t55nq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:21:49.672: INFO: stderr: ""
Aug 24 11:21:49.672: INFO: stdout: "true"
Aug 24 11:21:49.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-t55nq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:21:49.809: INFO: stderr: ""
Aug 24 11:21:49.809: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:21:49.809: INFO: validating pod update-demo-nautilus-t55nq
Aug 24 11:21:49.828: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:21:49.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:21:49.828: INFO: update-demo-nautilus-t55nq is verified up and running
STEP: using delete to clean up resources 08/24/23 11:21:49.828
Aug 24 11:21:49.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 delete --grace-period=0 --force -f -'
Aug 24 11:21:49.960: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:21:49.960: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 24 11:21:49.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get rc,svc -l name=update-demo --no-headers'
Aug 24 11:21:50.101: INFO: stderr: "No resources found in kubectl-9001 namespace.\n"
Aug 24 11:21:50.101: INFO: stdout: ""
Aug 24 11:21:50.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 24 11:21:50.245: INFO: stderr: ""
Aug 24 11:21:50.245: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:21:50.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9001" for this suite. 08/24/23 11:21:50.261
------------------------------
â€¢ [SLOW TEST] [52.268 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:20:58.012
    Aug 24 11:20:58.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:20:58.014
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:20:58.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:20:58.056
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/24/23 11:20:58.063
    Aug 24 11:20:58.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 create -f -'
    Aug 24 11:20:58.552: INFO: stderr: ""
    Aug 24 11:20:58.552: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:20:58.552
    Aug 24 11:20:58.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:20:58.682: INFO: stderr: ""
    Aug 24 11:20:58.682: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:20:58.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:20:58.806: INFO: stderr: ""
    Aug 24 11:20:58.806: INFO: stdout: ""
    Aug 24 11:20:58.806: INFO: update-demo-nautilus-68fx2 is created but not running
    Aug 24 11:21:03.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:03.928: INFO: stderr: ""
    Aug 24 11:21:03.928: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:21:03.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:04.037: INFO: stderr: ""
    Aug 24 11:21:04.037: INFO: stdout: ""
    Aug 24 11:21:04.037: INFO: update-demo-nautilus-68fx2 is created but not running
    Aug 24 11:21:09.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:09.163: INFO: stderr: ""
    Aug 24 11:21:09.163: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:21:09.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:09.274: INFO: stderr: ""
    Aug 24 11:21:09.274: INFO: stdout: ""
    Aug 24 11:21:09.274: INFO: update-demo-nautilus-68fx2 is created but not running
    Aug 24 11:21:14.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:14.394: INFO: stderr: ""
    Aug 24 11:21:14.394: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:21:14.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:14.499: INFO: stderr: ""
    Aug 24 11:21:14.499: INFO: stdout: ""
    Aug 24 11:21:14.499: INFO: update-demo-nautilus-68fx2 is created but not running
    Aug 24 11:21:19.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:19.633: INFO: stderr: ""
    Aug 24 11:21:19.634: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:21:19.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:19.744: INFO: stderr: ""
    Aug 24 11:21:19.744: INFO: stdout: ""
    Aug 24 11:21:19.744: INFO: update-demo-nautilus-68fx2 is created but not running
    Aug 24 11:21:24.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:24.874: INFO: stderr: ""
    Aug 24 11:21:24.874: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:21:24.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:25.008: INFO: stderr: ""
    Aug 24 11:21:25.008: INFO: stdout: ""
    Aug 24 11:21:25.008: INFO: update-demo-nautilus-68fx2 is created but not running
    Aug 24 11:21:30.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:30.124: INFO: stderr: ""
    Aug 24 11:21:30.124: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:21:30.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:30.248: INFO: stderr: ""
    Aug 24 11:21:30.248: INFO: stdout: ""
    Aug 24 11:21:30.248: INFO: update-demo-nautilus-68fx2 is created but not running
    Aug 24 11:21:35.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:35.376: INFO: stderr: ""
    Aug 24 11:21:35.377: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    Aug 24 11:21:35.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:35.502: INFO: stderr: ""
    Aug 24 11:21:35.502: INFO: stdout: "true"
    Aug 24 11:21:35.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-68fx2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:21:35.623: INFO: stderr: ""
    Aug 24 11:21:35.623: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:21:35.623: INFO: validating pod update-demo-nautilus-68fx2
    Aug 24 11:21:35.631: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:21:35.631: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:21:35.631: INFO: update-demo-nautilus-68fx2 is verified up and running
    Aug 24 11:21:35.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:35.750: INFO: stderr: ""
    Aug 24 11:21:35.750: INFO: stdout: "true"
    Aug 24 11:21:35.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:21:35.859: INFO: stderr: ""
    Aug 24 11:21:35.859: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:21:35.859: INFO: validating pod update-demo-nautilus-pkhr6
    Aug 24 11:21:35.865: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:21:35.865: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:21:35.865: INFO: update-demo-nautilus-pkhr6 is verified up and running
    STEP: scaling down the replication controller 08/24/23 11:21:35.865
    Aug 24 11:21:35.869: INFO: scanned /root for discovery docs: <nil>
    Aug 24 11:21:35.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 24 11:21:37.029: INFO: stderr: ""
    Aug 24 11:21:37.029: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:21:37.029
    Aug 24 11:21:37.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:37.150: INFO: stderr: ""
    Aug 24 11:21:37.150: INFO: stdout: "update-demo-nautilus-68fx2 update-demo-nautilus-pkhr6 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/24/23 11:21:37.15
    Aug 24 11:21:42.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:42.278: INFO: stderr: ""
    Aug 24 11:21:42.279: INFO: stdout: "update-demo-nautilus-pkhr6 "
    Aug 24 11:21:42.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:42.378: INFO: stderr: ""
    Aug 24 11:21:42.378: INFO: stdout: "true"
    Aug 24 11:21:42.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:21:42.514: INFO: stderr: ""
    Aug 24 11:21:42.514: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:21:42.514: INFO: validating pod update-demo-nautilus-pkhr6
    Aug 24 11:21:42.521: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:21:42.521: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:21:42.521: INFO: update-demo-nautilus-pkhr6 is verified up and running
    STEP: scaling up the replication controller 08/24/23 11:21:42.521
    Aug 24 11:21:42.523: INFO: scanned /root for discovery docs: <nil>
    Aug 24 11:21:42.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 24 11:21:43.668: INFO: stderr: ""
    Aug 24 11:21:43.668: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:21:43.668
    Aug 24 11:21:43.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:43.789: INFO: stderr: ""
    Aug 24 11:21:43.789: INFO: stdout: "update-demo-nautilus-pkhr6 update-demo-nautilus-t55nq "
    Aug 24 11:21:43.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:43.926: INFO: stderr: ""
    Aug 24 11:21:43.926: INFO: stdout: "true"
    Aug 24 11:21:43.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:21:44.065: INFO: stderr: ""
    Aug 24 11:21:44.065: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:21:44.065: INFO: validating pod update-demo-nautilus-pkhr6
    Aug 24 11:21:44.071: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:21:44.071: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:21:44.071: INFO: update-demo-nautilus-pkhr6 is verified up and running
    Aug 24 11:21:44.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-t55nq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:44.199: INFO: stderr: ""
    Aug 24 11:21:44.199: INFO: stdout: ""
    Aug 24 11:21:44.199: INFO: update-demo-nautilus-t55nq is created but not running
    Aug 24 11:21:49.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:21:49.340: INFO: stderr: ""
    Aug 24 11:21:49.340: INFO: stdout: "update-demo-nautilus-pkhr6 update-demo-nautilus-t55nq "
    Aug 24 11:21:49.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:49.451: INFO: stderr: ""
    Aug 24 11:21:49.451: INFO: stdout: "true"
    Aug 24 11:21:49.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-pkhr6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:21:49.554: INFO: stderr: ""
    Aug 24 11:21:49.554: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:21:49.554: INFO: validating pod update-demo-nautilus-pkhr6
    Aug 24 11:21:49.561: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:21:49.561: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:21:49.561: INFO: update-demo-nautilus-pkhr6 is verified up and running
    Aug 24 11:21:49.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-t55nq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:21:49.672: INFO: stderr: ""
    Aug 24 11:21:49.672: INFO: stdout: "true"
    Aug 24 11:21:49.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods update-demo-nautilus-t55nq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:21:49.809: INFO: stderr: ""
    Aug 24 11:21:49.809: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:21:49.809: INFO: validating pod update-demo-nautilus-t55nq
    Aug 24 11:21:49.828: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:21:49.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:21:49.828: INFO: update-demo-nautilus-t55nq is verified up and running
    STEP: using delete to clean up resources 08/24/23 11:21:49.828
    Aug 24 11:21:49.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 delete --grace-period=0 --force -f -'
    Aug 24 11:21:49.960: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:21:49.960: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 24 11:21:49.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get rc,svc -l name=update-demo --no-headers'
    Aug 24 11:21:50.101: INFO: stderr: "No resources found in kubectl-9001 namespace.\n"
    Aug 24 11:21:50.101: INFO: stdout: ""
    Aug 24 11:21:50.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-9001 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 24 11:21:50.245: INFO: stderr: ""
    Aug 24 11:21:50.245: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:21:50.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9001" for this suite. 08/24/23 11:21:50.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:21:50.282
Aug 24 11:21:50.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:21:50.285
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:21:50.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:21:50.319
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/24/23 11:21:50.326
Aug 24 11:21:50.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4712 api-versions'
Aug 24 11:21:50.513: INFO: stderr: ""
Aug 24 11:21:50.513: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresource.k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:21:50.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4712" for this suite. 08/24/23 11:21:50.524
------------------------------
â€¢ [0.251 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:21:50.282
    Aug 24 11:21:50.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:21:50.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:21:50.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:21:50.319
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/24/23 11:21:50.326
    Aug 24 11:21:50.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4712 api-versions'
    Aug 24 11:21:50.513: INFO: stderr: ""
    Aug 24 11:21:50.513: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresource.k8s.io/v1alpha1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:21:50.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4712" for this suite. 08/24/23 11:21:50.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:21:50.534
Aug 24 11:21:50.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename cronjob 08/24/23 11:21:50.536
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:21:50.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:21:50.572
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/24/23 11:21:50.577
STEP: Ensuring a job is scheduled 08/24/23 11:21:50.588
STEP: Ensuring exactly one is scheduled 08/24/23 11:22:00.593
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 11:22:00.597
STEP: Ensuring no more jobs are scheduled 08/24/23 11:22:00.604
STEP: Removing cronjob 08/24/23 11:27:00.612
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 11:27:00.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7376" for this suite. 08/24/23 11:27:00.631
------------------------------
â€¢ [SLOW TEST] [310.127 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:21:50.534
    Aug 24 11:21:50.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename cronjob 08/24/23 11:21:50.536
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:21:50.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:21:50.572
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/24/23 11:21:50.577
    STEP: Ensuring a job is scheduled 08/24/23 11:21:50.588
    STEP: Ensuring exactly one is scheduled 08/24/23 11:22:00.593
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 11:22:00.597
    STEP: Ensuring no more jobs are scheduled 08/24/23 11:22:00.604
    STEP: Removing cronjob 08/24/23 11:27:00.612
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:27:00.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7376" for this suite. 08/24/23 11:27:00.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:27:00.664
Aug 24 11:27:00.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:27:00.666
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:27:00.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:27:00.712
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-5871 08/24/23 11:27:00.719
STEP: creating service affinity-nodeport-transition in namespace services-5871 08/24/23 11:27:00.719
STEP: creating replication controller affinity-nodeport-transition in namespace services-5871 08/24/23 11:27:00.751
I0824 11:27:00.768829      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5871, replica count: 3
I0824 11:27:03.819893      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:27:06.820334      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:27:06.833: INFO: Creating new exec pod
Aug 24 11:27:06.856: INFO: Waiting up to 5m0s for pod "execpod-affinity6d65b" in namespace "services-5871" to be "running"
Aug 24 11:27:06.869: INFO: Pod "execpod-affinity6d65b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.086264ms
Aug 24 11:27:08.873: INFO: Pod "execpod-affinity6d65b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016984759s
Aug 24 11:27:08.873: INFO: Pod "execpod-affinity6d65b" satisfied condition "running"
Aug 24 11:27:09.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 24 11:27:10.136: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 24 11:27:10.136: INFO: stdout: ""
Aug 24 11:27:10.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 10.254.5.187 80'
Aug 24 11:27:10.358: INFO: stderr: "+ nc -v -z -w 2 10.254.5.187 80\nConnection to 10.254.5.187 80 port [tcp/http] succeeded!\n"
Aug 24 11:27:10.358: INFO: stdout: ""
Aug 24 11:27:10.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 10.0.0.18 31482'
Aug 24 11:27:10.622: INFO: stderr: "+ nc -v -z -w 2 10.0.0.18 31482\nConnection to 10.0.0.18 31482 port [tcp/*] succeeded!\n"
Aug 24 11:27:10.622: INFO: stdout: ""
Aug 24 11:27:10.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 31482'
Aug 24 11:27:10.884: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 31482\nConnection to 10.0.0.4 31482 port [tcp/*] succeeded!\n"
Aug 24 11:27:10.884: INFO: stdout: ""
Aug 24 11:27:10.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:31482/ ; done'
Aug 24 11:27:11.284: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n"
Aug 24 11:27:11.284: INFO: stdout: "\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-zdllp"
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
Aug 24 11:27:11.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:31482/ ; done'
Aug 24 11:27:11.690: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n"
Aug 24 11:27:11.690: INFO: stdout: "\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh"
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
Aug 24 11:27:11.690: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5871, will wait for the garbage collector to delete the pods 08/24/23 11:27:11.72
Aug 24 11:27:11.790: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.755244ms
Aug 24 11:27:11.891: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.77744ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:27:14.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5871" for this suite. 08/24/23 11:27:14.151
------------------------------
â€¢ [SLOW TEST] [13.505 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:27:00.664
    Aug 24 11:27:00.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:27:00.666
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:27:00.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:27:00.712
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-5871 08/24/23 11:27:00.719
    STEP: creating service affinity-nodeport-transition in namespace services-5871 08/24/23 11:27:00.719
    STEP: creating replication controller affinity-nodeport-transition in namespace services-5871 08/24/23 11:27:00.751
    I0824 11:27:00.768829      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5871, replica count: 3
    I0824 11:27:03.819893      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:27:06.820334      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:27:06.833: INFO: Creating new exec pod
    Aug 24 11:27:06.856: INFO: Waiting up to 5m0s for pod "execpod-affinity6d65b" in namespace "services-5871" to be "running"
    Aug 24 11:27:06.869: INFO: Pod "execpod-affinity6d65b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.086264ms
    Aug 24 11:27:08.873: INFO: Pod "execpod-affinity6d65b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016984759s
    Aug 24 11:27:08.873: INFO: Pod "execpod-affinity6d65b" satisfied condition "running"
    Aug 24 11:27:09.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 24 11:27:10.136: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 24 11:27:10.136: INFO: stdout: ""
    Aug 24 11:27:10.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 10.254.5.187 80'
    Aug 24 11:27:10.358: INFO: stderr: "+ nc -v -z -w 2 10.254.5.187 80\nConnection to 10.254.5.187 80 port [tcp/http] succeeded!\n"
    Aug 24 11:27:10.358: INFO: stdout: ""
    Aug 24 11:27:10.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 10.0.0.18 31482'
    Aug 24 11:27:10.622: INFO: stderr: "+ nc -v -z -w 2 10.0.0.18 31482\nConnection to 10.0.0.18 31482 port [tcp/*] succeeded!\n"
    Aug 24 11:27:10.622: INFO: stdout: ""
    Aug 24 11:27:10.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 31482'
    Aug 24 11:27:10.884: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 31482\nConnection to 10.0.0.4 31482 port [tcp/*] succeeded!\n"
    Aug 24 11:27:10.884: INFO: stdout: ""
    Aug 24 11:27:10.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:31482/ ; done'
    Aug 24 11:27:11.284: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n"
    Aug 24 11:27:11.284: INFO: stdout: "\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-rr8x9\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-zdllp\naffinity-nodeport-transition-zdllp"
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-rr8x9
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
    Aug 24 11:27:11.284: INFO: Received response from host: affinity-nodeport-transition-zdllp
    Aug 24 11:27:11.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5871 exec execpod-affinity6d65b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:31482/ ; done'
    Aug 24 11:27:11.690: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:31482/\n"
    Aug 24 11:27:11.690: INFO: stdout: "\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh\naffinity-nodeport-transition-nd9xh"
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Received response from host: affinity-nodeport-transition-nd9xh
    Aug 24 11:27:11.690: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5871, will wait for the garbage collector to delete the pods 08/24/23 11:27:11.72
    Aug 24 11:27:11.790: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.755244ms
    Aug 24 11:27:11.891: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.77744ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:27:14.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5871" for this suite. 08/24/23 11:27:14.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:27:14.179
Aug 24 11:27:14.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename daemonsets 08/24/23 11:27:14.181
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:27:14.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:27:14.219
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 08/24/23 11:27:14.254
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 11:27:14.264
Aug 24 11:27:14.269: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:14.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 11:27:14.277: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 11:27:15.296: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:15.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 11:27:15.301: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 11:27:16.283: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:16.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 11:27:16.286: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/24/23 11:27:16.29
Aug 24 11:27:16.308: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:16.311: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 11:27:16.311: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
Aug 24 11:27:17.316: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:17.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 11:27:17.320: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
Aug 24 11:27:18.318: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:18.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 11:27:18.322: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
Aug 24 11:27:19.332: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:19.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 11:27:19.341: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
Aug 24 11:27:20.317: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:20.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 11:27:20.321: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
Aug 24 11:27:21.317: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 11:27:21.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 11:27:21.321: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 11:27:21.325
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8936, will wait for the garbage collector to delete the pods 08/24/23 11:27:21.325
Aug 24 11:27:21.389: INFO: Deleting DaemonSet.extensions daemon-set took: 10.590787ms
Aug 24 11:27:21.489: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.189965ms
Aug 24 11:27:23.996: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 11:27:23.996: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 11:27:24.000: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21718"},"items":null}

Aug 24 11:27:24.004: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21718"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:27:24.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8936" for this suite. 08/24/23 11:27:24.066
------------------------------
â€¢ [SLOW TEST] [9.899 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:27:14.179
    Aug 24 11:27:14.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename daemonsets 08/24/23 11:27:14.181
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:27:14.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:27:14.219
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 08/24/23 11:27:14.254
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 11:27:14.264
    Aug 24 11:27:14.269: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:14.277: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 11:27:14.277: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 11:27:15.296: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:15.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 11:27:15.301: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 11:27:16.283: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:16.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 11:27:16.286: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/24/23 11:27:16.29
    Aug 24 11:27:16.308: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:16.311: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 11:27:16.311: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
    Aug 24 11:27:17.316: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:17.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 11:27:17.320: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
    Aug 24 11:27:18.318: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:18.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 11:27:18.322: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
    Aug 24 11:27:19.332: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:19.341: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 11:27:19.341: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
    Aug 24 11:27:20.317: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:20.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 11:27:20.321: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
    Aug 24 11:27:21.317: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 11:27:21.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 11:27:21.321: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 11:27:21.325
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8936, will wait for the garbage collector to delete the pods 08/24/23 11:27:21.325
    Aug 24 11:27:21.389: INFO: Deleting DaemonSet.extensions daemon-set took: 10.590787ms
    Aug 24 11:27:21.489: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.189965ms
    Aug 24 11:27:23.996: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 11:27:23.996: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 11:27:24.000: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21718"},"items":null}

    Aug 24 11:27:24.004: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21718"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:27:24.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8936" for this suite. 08/24/23 11:27:24.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:27:24.082
Aug 24 11:27:24.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 11:27:24.083
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:27:24.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:27:24.117
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/24/23 11:27:24.126
Aug 24 11:27:24.139: INFO: Waiting up to 2m0s for pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" in namespace "var-expansion-5861" to be "running"
Aug 24 11:27:24.165: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 25.85359ms
Aug 24 11:27:26.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031169018s
Aug 24 11:27:28.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029865016s
Aug 24 11:27:30.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03074001s
Aug 24 11:27:32.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031913879s
Aug 24 11:27:34.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030848808s
Aug 24 11:27:36.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030919827s
Aug 24 11:27:38.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 14.030087736s
Aug 24 11:27:40.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029773268s
Aug 24 11:27:42.175: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 18.036009724s
Aug 24 11:27:44.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 20.03088929s
Aug 24 11:27:46.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 22.031209316s
Aug 24 11:27:48.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030928759s
Aug 24 11:27:50.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 26.031828701s
Aug 24 11:27:52.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 28.030842885s
Aug 24 11:27:54.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 30.033395484s
Aug 24 11:27:56.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030513567s
Aug 24 11:27:58.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0303489s
Aug 24 11:28:00.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 36.032682368s
Aug 24 11:28:02.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 38.031610618s
Aug 24 11:28:04.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 40.030817898s
Aug 24 11:28:06.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 42.030105975s
Aug 24 11:28:08.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 44.030390061s
Aug 24 11:28:10.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030815541s
Aug 24 11:28:12.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 48.030399956s
Aug 24 11:28:14.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031067449s
Aug 24 11:28:16.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 52.032794188s
Aug 24 11:28:18.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031556803s
Aug 24 11:28:20.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 56.030927704s
Aug 24 11:28:22.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 58.031273188s
Aug 24 11:28:24.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.031909845s
Aug 24 11:28:26.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.030702202s
Aug 24 11:28:28.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.031003164s
Aug 24 11:28:30.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030116074s
Aug 24 11:28:32.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.031183495s
Aug 24 11:28:34.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.031436883s
Aug 24 11:28:36.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.03054015s
Aug 24 11:28:38.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.030507702s
Aug 24 11:28:40.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.031935001s
Aug 24 11:28:42.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.033494832s
Aug 24 11:28:44.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.033899092s
Aug 24 11:28:46.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.031228689s
Aug 24 11:28:48.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.030507048s
Aug 24 11:28:50.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.032289734s
Aug 24 11:28:52.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.030990274s
Aug 24 11:28:54.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.030208962s
Aug 24 11:28:56.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030562521s
Aug 24 11:28:58.190: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.05102145s
Aug 24 11:29:00.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031249828s
Aug 24 11:29:02.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.03032061s
Aug 24 11:29:04.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.030996044s
Aug 24 11:29:06.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.033985807s
Aug 24 11:29:08.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.031377533s
Aug 24 11:29:10.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.033114649s
Aug 24 11:29:12.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.031093067s
Aug 24 11:29:14.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.031740344s
Aug 24 11:29:16.183: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.04392491s
Aug 24 11:29:18.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.029751913s
Aug 24 11:29:20.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.031039811s
Aug 24 11:29:22.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.031362458s
Aug 24 11:29:24.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03047584s
Aug 24 11:29:24.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.033505464s
STEP: updating the pod 08/24/23 11:29:24.173
Aug 24 11:29:24.694: INFO: Successfully updated pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408"
STEP: waiting for pod running 08/24/23 11:29:24.694
Aug 24 11:29:24.694: INFO: Waiting up to 2m0s for pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" in namespace "var-expansion-5861" to be "running"
Aug 24 11:29:24.706: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 11.898756ms
Aug 24 11:29:26.712: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Running", Reason="", readiness=true. Elapsed: 2.017726291s
Aug 24 11:29:26.712: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" satisfied condition "running"
STEP: deleting the pod gracefully 08/24/23 11:29:26.712
Aug 24 11:29:26.712: INFO: Deleting pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" in namespace "var-expansion-5861"
Aug 24 11:29:26.726: INFO: Wait up to 5m0s for pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 11:29:58.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5861" for this suite. 08/24/23 11:29:58.747
------------------------------
â€¢ [SLOW TEST] [154.675 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:27:24.082
    Aug 24 11:27:24.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 11:27:24.083
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:27:24.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:27:24.117
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/24/23 11:27:24.126
    Aug 24 11:27:24.139: INFO: Waiting up to 2m0s for pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" in namespace "var-expansion-5861" to be "running"
    Aug 24 11:27:24.165: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 25.85359ms
    Aug 24 11:27:26.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031169018s
    Aug 24 11:27:28.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029865016s
    Aug 24 11:27:30.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03074001s
    Aug 24 11:27:32.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031913879s
    Aug 24 11:27:34.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030848808s
    Aug 24 11:27:36.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030919827s
    Aug 24 11:27:38.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 14.030087736s
    Aug 24 11:27:40.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029773268s
    Aug 24 11:27:42.175: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 18.036009724s
    Aug 24 11:27:44.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 20.03088929s
    Aug 24 11:27:46.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 22.031209316s
    Aug 24 11:27:48.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 24.030928759s
    Aug 24 11:27:50.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 26.031828701s
    Aug 24 11:27:52.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 28.030842885s
    Aug 24 11:27:54.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 30.033395484s
    Aug 24 11:27:56.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 32.030513567s
    Aug 24 11:27:58.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 34.0303489s
    Aug 24 11:28:00.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 36.032682368s
    Aug 24 11:28:02.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 38.031610618s
    Aug 24 11:28:04.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 40.030817898s
    Aug 24 11:28:06.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 42.030105975s
    Aug 24 11:28:08.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 44.030390061s
    Aug 24 11:28:10.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030815541s
    Aug 24 11:28:12.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 48.030399956s
    Aug 24 11:28:14.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 50.031067449s
    Aug 24 11:28:16.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 52.032794188s
    Aug 24 11:28:18.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031556803s
    Aug 24 11:28:20.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 56.030927704s
    Aug 24 11:28:22.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 58.031273188s
    Aug 24 11:28:24.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.031909845s
    Aug 24 11:28:26.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.030702202s
    Aug 24 11:28:28.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.031003164s
    Aug 24 11:28:30.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.030116074s
    Aug 24 11:28:32.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.031183495s
    Aug 24 11:28:34.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.031436883s
    Aug 24 11:28:36.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.03054015s
    Aug 24 11:28:38.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.030507702s
    Aug 24 11:28:40.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.031935001s
    Aug 24 11:28:42.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.033494832s
    Aug 24 11:28:44.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.033899092s
    Aug 24 11:28:46.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.031228689s
    Aug 24 11:28:48.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.030507048s
    Aug 24 11:28:50.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.032289734s
    Aug 24 11:28:52.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.030990274s
    Aug 24 11:28:54.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.030208962s
    Aug 24 11:28:56.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030562521s
    Aug 24 11:28:58.190: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.05102145s
    Aug 24 11:29:00.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031249828s
    Aug 24 11:29:02.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.03032061s
    Aug 24 11:29:04.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.030996044s
    Aug 24 11:29:06.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.033985807s
    Aug 24 11:29:08.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.031377533s
    Aug 24 11:29:10.172: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.033114649s
    Aug 24 11:29:12.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.031093067s
    Aug 24 11:29:14.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.031740344s
    Aug 24 11:29:16.183: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.04392491s
    Aug 24 11:29:18.169: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.029751913s
    Aug 24 11:29:20.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.031039811s
    Aug 24 11:29:22.171: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.031362458s
    Aug 24 11:29:24.170: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.03047584s
    Aug 24 11:29:24.173: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.033505464s
    STEP: updating the pod 08/24/23 11:29:24.173
    Aug 24 11:29:24.694: INFO: Successfully updated pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408"
    STEP: waiting for pod running 08/24/23 11:29:24.694
    Aug 24 11:29:24.694: INFO: Waiting up to 2m0s for pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" in namespace "var-expansion-5861" to be "running"
    Aug 24 11:29:24.706: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Pending", Reason="", readiness=false. Elapsed: 11.898756ms
    Aug 24 11:29:26.712: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408": Phase="Running", Reason="", readiness=true. Elapsed: 2.017726291s
    Aug 24 11:29:26.712: INFO: Pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" satisfied condition "running"
    STEP: deleting the pod gracefully 08/24/23 11:29:26.712
    Aug 24 11:29:26.712: INFO: Deleting pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" in namespace "var-expansion-5861"
    Aug 24 11:29:26.726: INFO: Wait up to 5m0s for pod "var-expansion-cbdd55cc-4707-4232-b18e-afcad41c7408" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:29:58.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5861" for this suite. 08/24/23 11:29:58.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:29:58.759
Aug 24 11:29:58.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename gc 08/24/23 11:29:58.761
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:29:58.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:29:58.809
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/24/23 11:29:58.825
STEP: delete the rc 08/24/23 11:30:03.983
STEP: wait for the rc to be deleted 08/24/23 11:30:04.093
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/24/23 11:30:09.113
STEP: Gathering metrics 08/24/23 11:30:39.136
W0824 11:30:39.152580      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 24 11:30:39.152: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 24 11:30:39.152: INFO: Deleting pod "simpletest.rc-26d6d" in namespace "gc-7829"
Aug 24 11:30:39.181: INFO: Deleting pod "simpletest.rc-28b2b" in namespace "gc-7829"
Aug 24 11:30:39.211: INFO: Deleting pod "simpletest.rc-4bxqt" in namespace "gc-7829"
Aug 24 11:30:39.243: INFO: Deleting pod "simpletest.rc-5fsp5" in namespace "gc-7829"
Aug 24 11:30:39.270: INFO: Deleting pod "simpletest.rc-5lgm5" in namespace "gc-7829"
Aug 24 11:30:39.321: INFO: Deleting pod "simpletest.rc-66s59" in namespace "gc-7829"
Aug 24 11:30:39.356: INFO: Deleting pod "simpletest.rc-6c66z" in namespace "gc-7829"
Aug 24 11:30:39.431: INFO: Deleting pod "simpletest.rc-6dhv9" in namespace "gc-7829"
Aug 24 11:30:39.475: INFO: Deleting pod "simpletest.rc-6gsl9" in namespace "gc-7829"
Aug 24 11:30:39.520: INFO: Deleting pod "simpletest.rc-6p8xz" in namespace "gc-7829"
Aug 24 11:30:39.604: INFO: Deleting pod "simpletest.rc-7dn9z" in namespace "gc-7829"
Aug 24 11:30:39.721: INFO: Deleting pod "simpletest.rc-7mnqd" in namespace "gc-7829"
Aug 24 11:30:39.769: INFO: Deleting pod "simpletest.rc-7q4kq" in namespace "gc-7829"
Aug 24 11:30:39.807: INFO: Deleting pod "simpletest.rc-7r9st" in namespace "gc-7829"
Aug 24 11:30:39.837: INFO: Deleting pod "simpletest.rc-8p7pk" in namespace "gc-7829"
Aug 24 11:30:39.876: INFO: Deleting pod "simpletest.rc-8pxnp" in namespace "gc-7829"
Aug 24 11:30:39.928: INFO: Deleting pod "simpletest.rc-8qjpc" in namespace "gc-7829"
Aug 24 11:30:39.985: INFO: Deleting pod "simpletest.rc-8xdzs" in namespace "gc-7829"
Aug 24 11:30:40.024: INFO: Deleting pod "simpletest.rc-b497p" in namespace "gc-7829"
Aug 24 11:30:40.090: INFO: Deleting pod "simpletest.rc-b9p88" in namespace "gc-7829"
Aug 24 11:30:40.123: INFO: Deleting pod "simpletest.rc-bc5l5" in namespace "gc-7829"
Aug 24 11:30:40.188: INFO: Deleting pod "simpletest.rc-bqprr" in namespace "gc-7829"
Aug 24 11:30:40.244: INFO: Deleting pod "simpletest.rc-bvpfl" in namespace "gc-7829"
Aug 24 11:30:40.357: INFO: Deleting pod "simpletest.rc-bvxjz" in namespace "gc-7829"
Aug 24 11:30:40.425: INFO: Deleting pod "simpletest.rc-c7kcm" in namespace "gc-7829"
Aug 24 11:30:40.606: INFO: Deleting pod "simpletest.rc-cdgfv" in namespace "gc-7829"
Aug 24 11:30:40.674: INFO: Deleting pod "simpletest.rc-cpk4v" in namespace "gc-7829"
Aug 24 11:30:40.756: INFO: Deleting pod "simpletest.rc-d78qp" in namespace "gc-7829"
Aug 24 11:30:40.852: INFO: Deleting pod "simpletest.rc-dzh54" in namespace "gc-7829"
Aug 24 11:30:41.011: INFO: Deleting pod "simpletest.rc-fq2sh" in namespace "gc-7829"
Aug 24 11:30:41.074: INFO: Deleting pod "simpletest.rc-gdk9n" in namespace "gc-7829"
Aug 24 11:30:41.152: INFO: Deleting pod "simpletest.rc-gmzl8" in namespace "gc-7829"
Aug 24 11:30:41.188: INFO: Deleting pod "simpletest.rc-grhp4" in namespace "gc-7829"
Aug 24 11:30:41.263: INFO: Deleting pod "simpletest.rc-gtmr7" in namespace "gc-7829"
Aug 24 11:30:41.314: INFO: Deleting pod "simpletest.rc-hgk82" in namespace "gc-7829"
Aug 24 11:30:41.377: INFO: Deleting pod "simpletest.rc-hh2gn" in namespace "gc-7829"
Aug 24 11:30:41.469: INFO: Deleting pod "simpletest.rc-hptfr" in namespace "gc-7829"
Aug 24 11:30:41.529: INFO: Deleting pod "simpletest.rc-hzv2w" in namespace "gc-7829"
Aug 24 11:30:41.587: INFO: Deleting pod "simpletest.rc-jf2pz" in namespace "gc-7829"
Aug 24 11:30:41.685: INFO: Deleting pod "simpletest.rc-jjhxp" in namespace "gc-7829"
Aug 24 11:30:41.729: INFO: Deleting pod "simpletest.rc-jq9l9" in namespace "gc-7829"
Aug 24 11:30:41.770: INFO: Deleting pod "simpletest.rc-jqqxb" in namespace "gc-7829"
Aug 24 11:30:41.809: INFO: Deleting pod "simpletest.rc-jwcgs" in namespace "gc-7829"
Aug 24 11:30:41.860: INFO: Deleting pod "simpletest.rc-k9l27" in namespace "gc-7829"
Aug 24 11:30:41.944: INFO: Deleting pod "simpletest.rc-km6vc" in namespace "gc-7829"
Aug 24 11:30:42.011: INFO: Deleting pod "simpletest.rc-kqzvf" in namespace "gc-7829"
Aug 24 11:30:42.071: INFO: Deleting pod "simpletest.rc-l95nj" in namespace "gc-7829"
Aug 24 11:30:42.133: INFO: Deleting pod "simpletest.rc-ln7zk" in namespace "gc-7829"
Aug 24 11:30:42.214: INFO: Deleting pod "simpletest.rc-m82zq" in namespace "gc-7829"
Aug 24 11:30:42.273: INFO: Deleting pod "simpletest.rc-mrg5m" in namespace "gc-7829"
Aug 24 11:30:42.321: INFO: Deleting pod "simpletest.rc-mvkqm" in namespace "gc-7829"
Aug 24 11:30:42.379: INFO: Deleting pod "simpletest.rc-mvp8q" in namespace "gc-7829"
Aug 24 11:30:42.406: INFO: Deleting pod "simpletest.rc-mxwjv" in namespace "gc-7829"
Aug 24 11:30:42.500: INFO: Deleting pod "simpletest.rc-nflgp" in namespace "gc-7829"
Aug 24 11:30:42.572: INFO: Deleting pod "simpletest.rc-nqg9b" in namespace "gc-7829"
Aug 24 11:30:42.661: INFO: Deleting pod "simpletest.rc-nxzfj" in namespace "gc-7829"
Aug 24 11:30:42.764: INFO: Deleting pod "simpletest.rc-p655w" in namespace "gc-7829"
Aug 24 11:30:42.866: INFO: Deleting pod "simpletest.rc-pmbln" in namespace "gc-7829"
Aug 24 11:30:42.943: INFO: Deleting pod "simpletest.rc-pxvkt" in namespace "gc-7829"
Aug 24 11:30:43.007: INFO: Deleting pod "simpletest.rc-q4gxt" in namespace "gc-7829"
Aug 24 11:30:43.104: INFO: Deleting pod "simpletest.rc-q6m52" in namespace "gc-7829"
Aug 24 11:30:43.159: INFO: Deleting pod "simpletest.rc-qbfrt" in namespace "gc-7829"
Aug 24 11:30:43.211: INFO: Deleting pod "simpletest.rc-qrb78" in namespace "gc-7829"
Aug 24 11:30:43.258: INFO: Deleting pod "simpletest.rc-qvt6j" in namespace "gc-7829"
Aug 24 11:30:43.334: INFO: Deleting pod "simpletest.rc-r4mx6" in namespace "gc-7829"
Aug 24 11:30:43.393: INFO: Deleting pod "simpletest.rc-r4wn6" in namespace "gc-7829"
Aug 24 11:30:43.440: INFO: Deleting pod "simpletest.rc-r8422" in namespace "gc-7829"
Aug 24 11:30:43.486: INFO: Deleting pod "simpletest.rc-rl2hf" in namespace "gc-7829"
Aug 24 11:30:43.528: INFO: Deleting pod "simpletest.rc-s9j75" in namespace "gc-7829"
Aug 24 11:30:43.568: INFO: Deleting pod "simpletest.rc-sfrkt" in namespace "gc-7829"
Aug 24 11:30:43.639: INFO: Deleting pod "simpletest.rc-smwxt" in namespace "gc-7829"
Aug 24 11:30:43.688: INFO: Deleting pod "simpletest.rc-sxtq7" in namespace "gc-7829"
Aug 24 11:30:43.761: INFO: Deleting pod "simpletest.rc-t8m4v" in namespace "gc-7829"
Aug 24 11:30:43.819: INFO: Deleting pod "simpletest.rc-tcf5h" in namespace "gc-7829"
Aug 24 11:30:43.895: INFO: Deleting pod "simpletest.rc-tcjsp" in namespace "gc-7829"
Aug 24 11:30:43.961: INFO: Deleting pod "simpletest.rc-tfgqb" in namespace "gc-7829"
Aug 24 11:30:44.021: INFO: Deleting pod "simpletest.rc-tgghg" in namespace "gc-7829"
Aug 24 11:30:44.127: INFO: Deleting pod "simpletest.rc-tghb2" in namespace "gc-7829"
Aug 24 11:30:44.189: INFO: Deleting pod "simpletest.rc-vchv6" in namespace "gc-7829"
Aug 24 11:30:44.242: INFO: Deleting pod "simpletest.rc-vjvkf" in namespace "gc-7829"
Aug 24 11:30:44.334: INFO: Deleting pod "simpletest.rc-vw928" in namespace "gc-7829"
Aug 24 11:30:44.395: INFO: Deleting pod "simpletest.rc-w5bhg" in namespace "gc-7829"
Aug 24 11:30:44.424: INFO: Deleting pod "simpletest.rc-w6lp7" in namespace "gc-7829"
Aug 24 11:30:44.458: INFO: Deleting pod "simpletest.rc-wd79s" in namespace "gc-7829"
Aug 24 11:30:44.500: INFO: Deleting pod "simpletest.rc-wdrlm" in namespace "gc-7829"
Aug 24 11:30:44.554: INFO: Deleting pod "simpletest.rc-wdt9r" in namespace "gc-7829"
Aug 24 11:30:44.640: INFO: Deleting pod "simpletest.rc-wjh2c" in namespace "gc-7829"
Aug 24 11:30:44.697: INFO: Deleting pod "simpletest.rc-wl2n6" in namespace "gc-7829"
Aug 24 11:30:44.738: INFO: Deleting pod "simpletest.rc-wlbp8" in namespace "gc-7829"
Aug 24 11:30:44.774: INFO: Deleting pod "simpletest.rc-wsnjh" in namespace "gc-7829"
Aug 24 11:30:44.842: INFO: Deleting pod "simpletest.rc-x8876" in namespace "gc-7829"
Aug 24 11:30:44.882: INFO: Deleting pod "simpletest.rc-x8bdb" in namespace "gc-7829"
Aug 24 11:30:44.937: INFO: Deleting pod "simpletest.rc-xk8vm" in namespace "gc-7829"
Aug 24 11:30:45.010: INFO: Deleting pod "simpletest.rc-xt4hp" in namespace "gc-7829"
Aug 24 11:30:45.094: INFO: Deleting pod "simpletest.rc-xtnnt" in namespace "gc-7829"
Aug 24 11:30:45.216: INFO: Deleting pod "simpletest.rc-xvqcg" in namespace "gc-7829"
Aug 24 11:30:45.270: INFO: Deleting pod "simpletest.rc-z57pq" in namespace "gc-7829"
Aug 24 11:30:45.309: INFO: Deleting pod "simpletest.rc-z6c9k" in namespace "gc-7829"
Aug 24 11:30:45.359: INFO: Deleting pod "simpletest.rc-zgbt5" in namespace "gc-7829"
Aug 24 11:30:45.423: INFO: Deleting pod "simpletest.rc-zmffb" in namespace "gc-7829"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 11:30:45.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7829" for this suite. 08/24/23 11:30:45.595
------------------------------
â€¢ [SLOW TEST] [46.864 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:29:58.759
    Aug 24 11:29:58.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename gc 08/24/23 11:29:58.761
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:29:58.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:29:58.809
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/24/23 11:29:58.825
    STEP: delete the rc 08/24/23 11:30:03.983
    STEP: wait for the rc to be deleted 08/24/23 11:30:04.093
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/24/23 11:30:09.113
    STEP: Gathering metrics 08/24/23 11:30:39.136
    W0824 11:30:39.152580      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 24 11:30:39.152: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 24 11:30:39.152: INFO: Deleting pod "simpletest.rc-26d6d" in namespace "gc-7829"
    Aug 24 11:30:39.181: INFO: Deleting pod "simpletest.rc-28b2b" in namespace "gc-7829"
    Aug 24 11:30:39.211: INFO: Deleting pod "simpletest.rc-4bxqt" in namespace "gc-7829"
    Aug 24 11:30:39.243: INFO: Deleting pod "simpletest.rc-5fsp5" in namespace "gc-7829"
    Aug 24 11:30:39.270: INFO: Deleting pod "simpletest.rc-5lgm5" in namespace "gc-7829"
    Aug 24 11:30:39.321: INFO: Deleting pod "simpletest.rc-66s59" in namespace "gc-7829"
    Aug 24 11:30:39.356: INFO: Deleting pod "simpletest.rc-6c66z" in namespace "gc-7829"
    Aug 24 11:30:39.431: INFO: Deleting pod "simpletest.rc-6dhv9" in namespace "gc-7829"
    Aug 24 11:30:39.475: INFO: Deleting pod "simpletest.rc-6gsl9" in namespace "gc-7829"
    Aug 24 11:30:39.520: INFO: Deleting pod "simpletest.rc-6p8xz" in namespace "gc-7829"
    Aug 24 11:30:39.604: INFO: Deleting pod "simpletest.rc-7dn9z" in namespace "gc-7829"
    Aug 24 11:30:39.721: INFO: Deleting pod "simpletest.rc-7mnqd" in namespace "gc-7829"
    Aug 24 11:30:39.769: INFO: Deleting pod "simpletest.rc-7q4kq" in namespace "gc-7829"
    Aug 24 11:30:39.807: INFO: Deleting pod "simpletest.rc-7r9st" in namespace "gc-7829"
    Aug 24 11:30:39.837: INFO: Deleting pod "simpletest.rc-8p7pk" in namespace "gc-7829"
    Aug 24 11:30:39.876: INFO: Deleting pod "simpletest.rc-8pxnp" in namespace "gc-7829"
    Aug 24 11:30:39.928: INFO: Deleting pod "simpletest.rc-8qjpc" in namespace "gc-7829"
    Aug 24 11:30:39.985: INFO: Deleting pod "simpletest.rc-8xdzs" in namespace "gc-7829"
    Aug 24 11:30:40.024: INFO: Deleting pod "simpletest.rc-b497p" in namespace "gc-7829"
    Aug 24 11:30:40.090: INFO: Deleting pod "simpletest.rc-b9p88" in namespace "gc-7829"
    Aug 24 11:30:40.123: INFO: Deleting pod "simpletest.rc-bc5l5" in namespace "gc-7829"
    Aug 24 11:30:40.188: INFO: Deleting pod "simpletest.rc-bqprr" in namespace "gc-7829"
    Aug 24 11:30:40.244: INFO: Deleting pod "simpletest.rc-bvpfl" in namespace "gc-7829"
    Aug 24 11:30:40.357: INFO: Deleting pod "simpletest.rc-bvxjz" in namespace "gc-7829"
    Aug 24 11:30:40.425: INFO: Deleting pod "simpletest.rc-c7kcm" in namespace "gc-7829"
    Aug 24 11:30:40.606: INFO: Deleting pod "simpletest.rc-cdgfv" in namespace "gc-7829"
    Aug 24 11:30:40.674: INFO: Deleting pod "simpletest.rc-cpk4v" in namespace "gc-7829"
    Aug 24 11:30:40.756: INFO: Deleting pod "simpletest.rc-d78qp" in namespace "gc-7829"
    Aug 24 11:30:40.852: INFO: Deleting pod "simpletest.rc-dzh54" in namespace "gc-7829"
    Aug 24 11:30:41.011: INFO: Deleting pod "simpletest.rc-fq2sh" in namespace "gc-7829"
    Aug 24 11:30:41.074: INFO: Deleting pod "simpletest.rc-gdk9n" in namespace "gc-7829"
    Aug 24 11:30:41.152: INFO: Deleting pod "simpletest.rc-gmzl8" in namespace "gc-7829"
    Aug 24 11:30:41.188: INFO: Deleting pod "simpletest.rc-grhp4" in namespace "gc-7829"
    Aug 24 11:30:41.263: INFO: Deleting pod "simpletest.rc-gtmr7" in namespace "gc-7829"
    Aug 24 11:30:41.314: INFO: Deleting pod "simpletest.rc-hgk82" in namespace "gc-7829"
    Aug 24 11:30:41.377: INFO: Deleting pod "simpletest.rc-hh2gn" in namespace "gc-7829"
    Aug 24 11:30:41.469: INFO: Deleting pod "simpletest.rc-hptfr" in namespace "gc-7829"
    Aug 24 11:30:41.529: INFO: Deleting pod "simpletest.rc-hzv2w" in namespace "gc-7829"
    Aug 24 11:30:41.587: INFO: Deleting pod "simpletest.rc-jf2pz" in namespace "gc-7829"
    Aug 24 11:30:41.685: INFO: Deleting pod "simpletest.rc-jjhxp" in namespace "gc-7829"
    Aug 24 11:30:41.729: INFO: Deleting pod "simpletest.rc-jq9l9" in namespace "gc-7829"
    Aug 24 11:30:41.770: INFO: Deleting pod "simpletest.rc-jqqxb" in namespace "gc-7829"
    Aug 24 11:30:41.809: INFO: Deleting pod "simpletest.rc-jwcgs" in namespace "gc-7829"
    Aug 24 11:30:41.860: INFO: Deleting pod "simpletest.rc-k9l27" in namespace "gc-7829"
    Aug 24 11:30:41.944: INFO: Deleting pod "simpletest.rc-km6vc" in namespace "gc-7829"
    Aug 24 11:30:42.011: INFO: Deleting pod "simpletest.rc-kqzvf" in namespace "gc-7829"
    Aug 24 11:30:42.071: INFO: Deleting pod "simpletest.rc-l95nj" in namespace "gc-7829"
    Aug 24 11:30:42.133: INFO: Deleting pod "simpletest.rc-ln7zk" in namespace "gc-7829"
    Aug 24 11:30:42.214: INFO: Deleting pod "simpletest.rc-m82zq" in namespace "gc-7829"
    Aug 24 11:30:42.273: INFO: Deleting pod "simpletest.rc-mrg5m" in namespace "gc-7829"
    Aug 24 11:30:42.321: INFO: Deleting pod "simpletest.rc-mvkqm" in namespace "gc-7829"
    Aug 24 11:30:42.379: INFO: Deleting pod "simpletest.rc-mvp8q" in namespace "gc-7829"
    Aug 24 11:30:42.406: INFO: Deleting pod "simpletest.rc-mxwjv" in namespace "gc-7829"
    Aug 24 11:30:42.500: INFO: Deleting pod "simpletest.rc-nflgp" in namespace "gc-7829"
    Aug 24 11:30:42.572: INFO: Deleting pod "simpletest.rc-nqg9b" in namespace "gc-7829"
    Aug 24 11:30:42.661: INFO: Deleting pod "simpletest.rc-nxzfj" in namespace "gc-7829"
    Aug 24 11:30:42.764: INFO: Deleting pod "simpletest.rc-p655w" in namespace "gc-7829"
    Aug 24 11:30:42.866: INFO: Deleting pod "simpletest.rc-pmbln" in namespace "gc-7829"
    Aug 24 11:30:42.943: INFO: Deleting pod "simpletest.rc-pxvkt" in namespace "gc-7829"
    Aug 24 11:30:43.007: INFO: Deleting pod "simpletest.rc-q4gxt" in namespace "gc-7829"
    Aug 24 11:30:43.104: INFO: Deleting pod "simpletest.rc-q6m52" in namespace "gc-7829"
    Aug 24 11:30:43.159: INFO: Deleting pod "simpletest.rc-qbfrt" in namespace "gc-7829"
    Aug 24 11:30:43.211: INFO: Deleting pod "simpletest.rc-qrb78" in namespace "gc-7829"
    Aug 24 11:30:43.258: INFO: Deleting pod "simpletest.rc-qvt6j" in namespace "gc-7829"
    Aug 24 11:30:43.334: INFO: Deleting pod "simpletest.rc-r4mx6" in namespace "gc-7829"
    Aug 24 11:30:43.393: INFO: Deleting pod "simpletest.rc-r4wn6" in namespace "gc-7829"
    Aug 24 11:30:43.440: INFO: Deleting pod "simpletest.rc-r8422" in namespace "gc-7829"
    Aug 24 11:30:43.486: INFO: Deleting pod "simpletest.rc-rl2hf" in namespace "gc-7829"
    Aug 24 11:30:43.528: INFO: Deleting pod "simpletest.rc-s9j75" in namespace "gc-7829"
    Aug 24 11:30:43.568: INFO: Deleting pod "simpletest.rc-sfrkt" in namespace "gc-7829"
    Aug 24 11:30:43.639: INFO: Deleting pod "simpletest.rc-smwxt" in namespace "gc-7829"
    Aug 24 11:30:43.688: INFO: Deleting pod "simpletest.rc-sxtq7" in namespace "gc-7829"
    Aug 24 11:30:43.761: INFO: Deleting pod "simpletest.rc-t8m4v" in namespace "gc-7829"
    Aug 24 11:30:43.819: INFO: Deleting pod "simpletest.rc-tcf5h" in namespace "gc-7829"
    Aug 24 11:30:43.895: INFO: Deleting pod "simpletest.rc-tcjsp" in namespace "gc-7829"
    Aug 24 11:30:43.961: INFO: Deleting pod "simpletest.rc-tfgqb" in namespace "gc-7829"
    Aug 24 11:30:44.021: INFO: Deleting pod "simpletest.rc-tgghg" in namespace "gc-7829"
    Aug 24 11:30:44.127: INFO: Deleting pod "simpletest.rc-tghb2" in namespace "gc-7829"
    Aug 24 11:30:44.189: INFO: Deleting pod "simpletest.rc-vchv6" in namespace "gc-7829"
    Aug 24 11:30:44.242: INFO: Deleting pod "simpletest.rc-vjvkf" in namespace "gc-7829"
    Aug 24 11:30:44.334: INFO: Deleting pod "simpletest.rc-vw928" in namespace "gc-7829"
    Aug 24 11:30:44.395: INFO: Deleting pod "simpletest.rc-w5bhg" in namespace "gc-7829"
    Aug 24 11:30:44.424: INFO: Deleting pod "simpletest.rc-w6lp7" in namespace "gc-7829"
    Aug 24 11:30:44.458: INFO: Deleting pod "simpletest.rc-wd79s" in namespace "gc-7829"
    Aug 24 11:30:44.500: INFO: Deleting pod "simpletest.rc-wdrlm" in namespace "gc-7829"
    Aug 24 11:30:44.554: INFO: Deleting pod "simpletest.rc-wdt9r" in namespace "gc-7829"
    Aug 24 11:30:44.640: INFO: Deleting pod "simpletest.rc-wjh2c" in namespace "gc-7829"
    Aug 24 11:30:44.697: INFO: Deleting pod "simpletest.rc-wl2n6" in namespace "gc-7829"
    Aug 24 11:30:44.738: INFO: Deleting pod "simpletest.rc-wlbp8" in namespace "gc-7829"
    Aug 24 11:30:44.774: INFO: Deleting pod "simpletest.rc-wsnjh" in namespace "gc-7829"
    Aug 24 11:30:44.842: INFO: Deleting pod "simpletest.rc-x8876" in namespace "gc-7829"
    Aug 24 11:30:44.882: INFO: Deleting pod "simpletest.rc-x8bdb" in namespace "gc-7829"
    Aug 24 11:30:44.937: INFO: Deleting pod "simpletest.rc-xk8vm" in namespace "gc-7829"
    Aug 24 11:30:45.010: INFO: Deleting pod "simpletest.rc-xt4hp" in namespace "gc-7829"
    Aug 24 11:30:45.094: INFO: Deleting pod "simpletest.rc-xtnnt" in namespace "gc-7829"
    Aug 24 11:30:45.216: INFO: Deleting pod "simpletest.rc-xvqcg" in namespace "gc-7829"
    Aug 24 11:30:45.270: INFO: Deleting pod "simpletest.rc-z57pq" in namespace "gc-7829"
    Aug 24 11:30:45.309: INFO: Deleting pod "simpletest.rc-z6c9k" in namespace "gc-7829"
    Aug 24 11:30:45.359: INFO: Deleting pod "simpletest.rc-zgbt5" in namespace "gc-7829"
    Aug 24 11:30:45.423: INFO: Deleting pod "simpletest.rc-zmffb" in namespace "gc-7829"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:30:45.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7829" for this suite. 08/24/23 11:30:45.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:30:45.623
Aug 24 11:30:45.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 11:30:45.624
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:30:45.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:30:45.707
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 11:30:45.725
Aug 24 11:30:45.765: INFO: Waiting up to 5m0s for pod "pod-e23d330b-987d-45f5-a07d-7587213db697" in namespace "emptydir-1482" to be "Succeeded or Failed"
Aug 24 11:30:45.782: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Pending", Reason="", readiness=false. Elapsed: 17.050368ms
Aug 24 11:30:47.789: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023863893s
Aug 24 11:30:49.792: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026713222s
Aug 24 11:30:51.786: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021055429s
STEP: Saw pod success 08/24/23 11:30:51.786
Aug 24 11:30:51.786: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697" satisfied condition "Succeeded or Failed"
Aug 24 11:30:51.793: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-e23d330b-987d-45f5-a07d-7587213db697 container test-container: <nil>
STEP: delete the pod 08/24/23 11:30:51.856
Aug 24 11:30:51.887: INFO: Waiting for pod pod-e23d330b-987d-45f5-a07d-7587213db697 to disappear
Aug 24 11:30:51.892: INFO: Pod pod-e23d330b-987d-45f5-a07d-7587213db697 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:30:51.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1482" for this suite. 08/24/23 11:30:51.9
------------------------------
â€¢ [SLOW TEST] [6.290 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:30:45.623
    Aug 24 11:30:45.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:30:45.624
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:30:45.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:30:45.707
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 11:30:45.725
    Aug 24 11:30:45.765: INFO: Waiting up to 5m0s for pod "pod-e23d330b-987d-45f5-a07d-7587213db697" in namespace "emptydir-1482" to be "Succeeded or Failed"
    Aug 24 11:30:45.782: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Pending", Reason="", readiness=false. Elapsed: 17.050368ms
    Aug 24 11:30:47.789: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023863893s
    Aug 24 11:30:49.792: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026713222s
    Aug 24 11:30:51.786: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021055429s
    STEP: Saw pod success 08/24/23 11:30:51.786
    Aug 24 11:30:51.786: INFO: Pod "pod-e23d330b-987d-45f5-a07d-7587213db697" satisfied condition "Succeeded or Failed"
    Aug 24 11:30:51.793: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-e23d330b-987d-45f5-a07d-7587213db697 container test-container: <nil>
    STEP: delete the pod 08/24/23 11:30:51.856
    Aug 24 11:30:51.887: INFO: Waiting for pod pod-e23d330b-987d-45f5-a07d-7587213db697 to disappear
    Aug 24 11:30:51.892: INFO: Pod pod-e23d330b-987d-45f5-a07d-7587213db697 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:30:51.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1482" for this suite. 08/24/23 11:30:51.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:30:51.916
Aug 24 11:30:51.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 11:30:51.917
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:30:51.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:30:51.969
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1097 08/24/23 11:30:51.981
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Aug 24 11:30:52.026: INFO: Found 0 stateful pods, waiting for 1
Aug 24 11:31:02.041: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/24/23 11:31:02.049
W0824 11:31:02.061305      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 24 11:31:02.069: INFO: Found 1 stateful pods, waiting for 2
Aug 24 11:31:12.083: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:31:12.083: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/24/23 11:31:12.089
STEP: Delete all of the StatefulSets 08/24/23 11:31:12.094
STEP: Verify that StatefulSets have been deleted 08/24/23 11:31:12.105
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 11:31:12.111: INFO: Deleting all statefulset in ns statefulset-1097
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:31:12.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1097" for this suite. 08/24/23 11:31:12.158
------------------------------
â€¢ [SLOW TEST] [20.273 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:30:51.916
    Aug 24 11:30:51.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 11:30:51.917
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:30:51.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:30:51.969
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1097 08/24/23 11:30:51.981
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Aug 24 11:30:52.026: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 11:31:02.041: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/24/23 11:31:02.049
    W0824 11:31:02.061305      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 24 11:31:02.069: INFO: Found 1 stateful pods, waiting for 2
    Aug 24 11:31:12.083: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:31:12.083: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/24/23 11:31:12.089
    STEP: Delete all of the StatefulSets 08/24/23 11:31:12.094
    STEP: Verify that StatefulSets have been deleted 08/24/23 11:31:12.105
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 11:31:12.111: INFO: Deleting all statefulset in ns statefulset-1097
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:31:12.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1097" for this suite. 08/24/23 11:31:12.158
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:31:12.19
Aug 24 11:31:12.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-runtime 08/24/23 11:31:12.191
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:12.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:12.233
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/24/23 11:31:12.241
STEP: wait for the container to reach Succeeded 08/24/23 11:31:12.261
STEP: get the container status 08/24/23 11:31:16.287
STEP: the container should be terminated 08/24/23 11:31:16.29
STEP: the termination message should be set 08/24/23 11:31:16.29
Aug 24 11:31:16.291: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/24/23 11:31:16.291
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 11:31:16.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-51" for this suite. 08/24/23 11:31:16.324
------------------------------
â€¢ [4.145 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:31:12.19
    Aug 24 11:31:12.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-runtime 08/24/23 11:31:12.191
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:12.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:12.233
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/24/23 11:31:12.241
    STEP: wait for the container to reach Succeeded 08/24/23 11:31:12.261
    STEP: get the container status 08/24/23 11:31:16.287
    STEP: the container should be terminated 08/24/23 11:31:16.29
    STEP: the termination message should be set 08/24/23 11:31:16.29
    Aug 24 11:31:16.291: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/24/23 11:31:16.291
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:31:16.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-51" for this suite. 08/24/23 11:31:16.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:31:16.336
Aug 24 11:31:16.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-runtime 08/24/23 11:31:16.338
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:16.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:16.364
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/24/23 11:31:16.384
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/24/23 11:31:32.513
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/24/23 11:31:32.517
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/24/23 11:31:32.522
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/24/23 11:31:32.523
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/24/23 11:31:32.597
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/24/23 11:31:35.617
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/24/23 11:31:37.629
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/24/23 11:31:37.635
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/24/23 11:31:37.635
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/24/23 11:31:37.677
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/24/23 11:31:38.691
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/24/23 11:31:41.71
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/24/23 11:31:41.716
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/24/23 11:31:41.716
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 11:31:41.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5160" for this suite. 08/24/23 11:31:41.753
------------------------------
â€¢ [SLOW TEST] [25.437 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:31:16.336
    Aug 24 11:31:16.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-runtime 08/24/23 11:31:16.338
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:16.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:16.364
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/24/23 11:31:16.384
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/24/23 11:31:32.513
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/24/23 11:31:32.517
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/24/23 11:31:32.522
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/24/23 11:31:32.523
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/24/23 11:31:32.597
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/24/23 11:31:35.617
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/24/23 11:31:37.629
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/24/23 11:31:37.635
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/24/23 11:31:37.635
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/24/23 11:31:37.677
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/24/23 11:31:38.691
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/24/23 11:31:41.71
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/24/23 11:31:41.716
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/24/23 11:31:41.716
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:31:41.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5160" for this suite. 08/24/23 11:31:41.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:31:41.774
Aug 24 11:31:41.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 11:31:41.775
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:41.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:41.817
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-beda2301-c75b-4c1f-b193-9016db00aad3 08/24/23 11:31:41.823
STEP: Creating a pod to test consume secrets 08/24/23 11:31:41.833
Aug 24 11:31:41.849: INFO: Waiting up to 5m0s for pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d" in namespace "secrets-9276" to be "Succeeded or Failed"
Aug 24 11:31:41.867: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.249806ms
Aug 24 11:31:43.876: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026236998s
Aug 24 11:31:45.876: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026746933s
STEP: Saw pod success 08/24/23 11:31:45.876
Aug 24 11:31:45.876: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d" satisfied condition "Succeeded or Failed"
Aug 24 11:31:45.880: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 11:31:45.891
Aug 24 11:31:45.908: INFO: Waiting for pod pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d to disappear
Aug 24 11:31:45.916: INFO: Pod pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 11:31:45.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9276" for this suite. 08/24/23 11:31:45.92
------------------------------
â€¢ [4.170 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:31:41.774
    Aug 24 11:31:41.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 11:31:41.775
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:41.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:41.817
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-beda2301-c75b-4c1f-b193-9016db00aad3 08/24/23 11:31:41.823
    STEP: Creating a pod to test consume secrets 08/24/23 11:31:41.833
    Aug 24 11:31:41.849: INFO: Waiting up to 5m0s for pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d" in namespace "secrets-9276" to be "Succeeded or Failed"
    Aug 24 11:31:41.867: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.249806ms
    Aug 24 11:31:43.876: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026236998s
    Aug 24 11:31:45.876: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026746933s
    STEP: Saw pod success 08/24/23 11:31:45.876
    Aug 24 11:31:45.876: INFO: Pod "pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d" satisfied condition "Succeeded or Failed"
    Aug 24 11:31:45.880: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:31:45.891
    Aug 24 11:31:45.908: INFO: Waiting for pod pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d to disappear
    Aug 24 11:31:45.916: INFO: Pod pod-secrets-5bc4b5c7-b84c-4db7-8ff9-6a4db545d76d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:31:45.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9276" for this suite. 08/24/23 11:31:45.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:31:45.947
Aug 24 11:31:45.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:31:45.949
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:45.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:45.981
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 24 11:31:45.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 11:31:48.286
Aug 24 11:31:48.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 create -f -'
Aug 24 11:31:49.468: INFO: stderr: ""
Aug 24 11:31:49.468: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 24 11:31:49.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 delete e2e-test-crd-publish-openapi-5753-crds test-cr'
Aug 24 11:31:49.601: INFO: stderr: ""
Aug 24 11:31:49.601: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 24 11:31:49.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 apply -f -'
Aug 24 11:31:50.078: INFO: stderr: ""
Aug 24 11:31:50.078: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 24 11:31:50.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 delete e2e-test-crd-publish-openapi-5753-crds test-cr'
Aug 24 11:31:50.242: INFO: stderr: ""
Aug 24 11:31:50.242: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/24/23 11:31:50.242
Aug 24 11:31:50.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 explain e2e-test-crd-publish-openapi-5753-crds'
Aug 24 11:31:50.719: INFO: stderr: ""
Aug 24 11:31:50.719: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5753-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:31:52.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8906" for this suite. 08/24/23 11:31:52.834
------------------------------
â€¢ [SLOW TEST] [6.899 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:31:45.947
    Aug 24 11:31:45.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:31:45.949
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:45.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:45.981
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 24 11:31:45.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 11:31:48.286
    Aug 24 11:31:48.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 create -f -'
    Aug 24 11:31:49.468: INFO: stderr: ""
    Aug 24 11:31:49.468: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 24 11:31:49.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 delete e2e-test-crd-publish-openapi-5753-crds test-cr'
    Aug 24 11:31:49.601: INFO: stderr: ""
    Aug 24 11:31:49.601: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 24 11:31:49.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 apply -f -'
    Aug 24 11:31:50.078: INFO: stderr: ""
    Aug 24 11:31:50.078: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 24 11:31:50.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 --namespace=crd-publish-openapi-8906 delete e2e-test-crd-publish-openapi-5753-crds test-cr'
    Aug 24 11:31:50.242: INFO: stderr: ""
    Aug 24 11:31:50.242: INFO: stdout: "e2e-test-crd-publish-openapi-5753-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/24/23 11:31:50.242
    Aug 24 11:31:50.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8906 explain e2e-test-crd-publish-openapi-5753-crds'
    Aug 24 11:31:50.719: INFO: stderr: ""
    Aug 24 11:31:50.719: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5753-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:31:52.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8906" for this suite. 08/24/23 11:31:52.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:31:52.847
Aug 24 11:31:52.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 11:31:52.849
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:52.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:52.88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-ea58d0cb-3f89-4e78-89ef-7cb94e8d33d6 08/24/23 11:31:52.891
STEP: Creating a pod to test consume configMaps 08/24/23 11:31:52.902
Aug 24 11:31:52.924: INFO: Waiting up to 5m0s for pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282" in namespace "configmap-8770" to be "Succeeded or Failed"
Aug 24 11:31:52.938: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282": Phase="Pending", Reason="", readiness=false. Elapsed: 14.342247ms
Aug 24 11:31:54.943: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01856764s
Aug 24 11:31:56.943: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019415332s
STEP: Saw pod success 08/24/23 11:31:56.944
Aug 24 11:31:56.944: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282" satisfied condition "Succeeded or Failed"
Aug 24 11:31:56.948: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:31:56.955
Aug 24 11:31:56.975: INFO: Waiting for pod pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282 to disappear
Aug 24 11:31:56.979: INFO: Pod pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:31:56.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8770" for this suite. 08/24/23 11:31:56.984
------------------------------
â€¢ [4.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:31:52.847
    Aug 24 11:31:52.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 11:31:52.849
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:52.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:52.88
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-ea58d0cb-3f89-4e78-89ef-7cb94e8d33d6 08/24/23 11:31:52.891
    STEP: Creating a pod to test consume configMaps 08/24/23 11:31:52.902
    Aug 24 11:31:52.924: INFO: Waiting up to 5m0s for pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282" in namespace "configmap-8770" to be "Succeeded or Failed"
    Aug 24 11:31:52.938: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282": Phase="Pending", Reason="", readiness=false. Elapsed: 14.342247ms
    Aug 24 11:31:54.943: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01856764s
    Aug 24 11:31:56.943: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019415332s
    STEP: Saw pod success 08/24/23 11:31:56.944
    Aug 24 11:31:56.944: INFO: Pod "pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282" satisfied condition "Succeeded or Failed"
    Aug 24 11:31:56.948: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:31:56.955
    Aug 24 11:31:56.975: INFO: Waiting for pod pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282 to disappear
    Aug 24 11:31:56.979: INFO: Pod pod-configmaps-82616386-1b9f-475e-84d4-778d8a121282 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:31:56.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8770" for this suite. 08/24/23 11:31:56.984
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:31:57.002
Aug 24 11:31:57.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replicaset 08/24/23 11:31:57.004
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:57.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:57.032
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 24 11:31:57.039: INFO: Creating ReplicaSet my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785
Aug 24 11:31:57.054: INFO: Pod name my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785: Found 0 pods out of 1
Aug 24 11:32:02.063: INFO: Pod name my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785: Found 1 pods out of 1
Aug 24 11:32:02.063: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785" is running
Aug 24 11:32:02.063: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2" in namespace "replicaset-6880" to be "running"
Aug 24 11:32:02.069: INFO: Pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2": Phase="Running", Reason="", readiness=true. Elapsed: 6.078802ms
Aug 24 11:32:02.069: INFO: Pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2" satisfied condition "running"
Aug 24 11:32:02.069: INFO: Pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:}])
Aug 24 11:32:02.069: INFO: Trying to dial the pod
Aug 24 11:32:07.082: INFO: Controller my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785: Got expected result from replica 1 [my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2]: "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:32:07.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6880" for this suite. 08/24/23 11:32:07.087
------------------------------
â€¢ [SLOW TEST] [10.101 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:31:57.002
    Aug 24 11:31:57.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replicaset 08/24/23 11:31:57.004
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:31:57.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:31:57.032
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 24 11:31:57.039: INFO: Creating ReplicaSet my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785
    Aug 24 11:31:57.054: INFO: Pod name my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785: Found 0 pods out of 1
    Aug 24 11:32:02.063: INFO: Pod name my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785: Found 1 pods out of 1
    Aug 24 11:32:02.063: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785" is running
    Aug 24 11:32:02.063: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2" in namespace "replicaset-6880" to be "running"
    Aug 24 11:32:02.069: INFO: Pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2": Phase="Running", Reason="", readiness=true. Elapsed: 6.078802ms
    Aug 24 11:32:02.069: INFO: Pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2" satisfied condition "running"
    Aug 24 11:32:02.069: INFO: Pod "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:31:57 +0000 UTC Reason: Message:}])
    Aug 24 11:32:02.069: INFO: Trying to dial the pod
    Aug 24 11:32:07.082: INFO: Controller my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785: Got expected result from replica 1 [my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2]: "my-hostname-basic-4e19cc2f-67a9-423e-b4d5-e5119bedf785-cc9x2", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:32:07.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6880" for this suite. 08/24/23 11:32:07.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:32:07.111
Aug 24 11:32:07.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:32:07.113
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:07.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:07.143
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/24/23 11:32:07.15
Aug 24 11:32:07.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 24 11:32:07.285: INFO: stderr: ""
Aug 24 11:32:07.285: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/24/23 11:32:07.285
Aug 24 11:32:07.285: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 24 11:32:07.285: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5895" to be "running and ready, or succeeded"
Aug 24 11:32:07.296: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.403459ms
Aug 24 11:32:07.296: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'gitlab-1-26-36460-guscsyka22xa-node-2' to be 'Running' but was 'Pending'
Aug 24 11:32:09.302: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.016705385s
Aug 24 11:32:09.302: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 24 11:32:09.302: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/24/23 11:32:09.302
Aug 24 11:32:09.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator'
Aug 24 11:32:09.440: INFO: stderr: ""
Aug 24 11:32:09.440: INFO: stdout: "I0824 11:32:08.180166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/qgqw 416\nI0824 11:32:08.380643       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/xg96 434\nI0824 11:32:08.581121       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nq8k 214\nI0824 11:32:08.780537       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/95n 513\nI0824 11:32:08.981056       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/gb5n 211\nI0824 11:32:09.180326       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l75 491\nI0824 11:32:09.380834       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/g79 388\n"
STEP: limiting log lines 08/24/23 11:32:09.44
Aug 24 11:32:09.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --tail=1'
Aug 24 11:32:09.600: INFO: stderr: ""
Aug 24 11:32:09.600: INFO: stdout: "I0824 11:32:09.581187       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xm2f 433\n"
Aug 24 11:32:09.600: INFO: got output "I0824 11:32:09.581187       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xm2f 433\n"
STEP: limiting log bytes 08/24/23 11:32:09.6
Aug 24 11:32:09.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --limit-bytes=1'
Aug 24 11:32:09.721: INFO: stderr: ""
Aug 24 11:32:09.721: INFO: stdout: "I"
Aug 24 11:32:09.721: INFO: got output "I"
STEP: exposing timestamps 08/24/23 11:32:09.721
Aug 24 11:32:09.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 24 11:32:09.853: INFO: stderr: ""
Aug 24 11:32:09.853: INFO: stdout: "2023-08-24T11:32:09.780826711Z I0824 11:32:09.780605       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/bm4 318\n"
Aug 24 11:32:09.853: INFO: got output "2023-08-24T11:32:09.780826711Z I0824 11:32:09.780605       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/bm4 318\n"
STEP: restricting to a time range 08/24/23 11:32:09.853
Aug 24 11:32:12.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --since=1s'
Aug 24 11:32:12.503: INFO: stderr: ""
Aug 24 11:32:12.503: INFO: stdout: "I0824 11:32:11.580201       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/5tr6 452\nI0824 11:32:11.780606       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/d4p 437\nI0824 11:32:11.980977       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lf7 434\nI0824 11:32:12.180425       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/89b 484\nI0824 11:32:12.380849       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/jmm 394\n"
Aug 24 11:32:12.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --since=24h'
Aug 24 11:32:12.639: INFO: stderr: ""
Aug 24 11:32:12.639: INFO: stdout: "I0824 11:32:08.180166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/qgqw 416\nI0824 11:32:08.380643       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/xg96 434\nI0824 11:32:08.581121       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nq8k 214\nI0824 11:32:08.780537       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/95n 513\nI0824 11:32:08.981056       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/gb5n 211\nI0824 11:32:09.180326       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l75 491\nI0824 11:32:09.380834       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/g79 388\nI0824 11:32:09.581187       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xm2f 433\nI0824 11:32:09.780605       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/bm4 318\nI0824 11:32:09.981059       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/bhc 491\nI0824 11:32:10.180421       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/5v9 512\nI0824 11:32:10.380863       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/wfsj 515\nI0824 11:32:10.581228       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/wd6x 492\nI0824 11:32:10.780740       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/m4c 594\nI0824 11:32:10.981128       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/62g 589\nI0824 11:32:11.180530       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/x46p 297\nI0824 11:32:11.380910       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/lvg 407\nI0824 11:32:11.580201       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/5tr6 452\nI0824 11:32:11.780606       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/d4p 437\nI0824 11:32:11.980977       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lf7 434\nI0824 11:32:12.180425       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/89b 484\nI0824 11:32:12.380849       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/jmm 394\nI0824 11:32:12.581277       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/qbr 210\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 24 11:32:12.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 delete pod logs-generator'
Aug 24 11:32:14.090: INFO: stderr: ""
Aug 24 11:32:14.090: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:32:14.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5895" for this suite. 08/24/23 11:32:14.099
------------------------------
â€¢ [SLOW TEST] [7.002 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:32:07.111
    Aug 24 11:32:07.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:32:07.113
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:07.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:07.143
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/24/23 11:32:07.15
    Aug 24 11:32:07.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 24 11:32:07.285: INFO: stderr: ""
    Aug 24 11:32:07.285: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/24/23 11:32:07.285
    Aug 24 11:32:07.285: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 24 11:32:07.285: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5895" to be "running and ready, or succeeded"
    Aug 24 11:32:07.296: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.403459ms
    Aug 24 11:32:07.296: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'gitlab-1-26-36460-guscsyka22xa-node-2' to be 'Running' but was 'Pending'
    Aug 24 11:32:09.302: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.016705385s
    Aug 24 11:32:09.302: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 24 11:32:09.302: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/24/23 11:32:09.302
    Aug 24 11:32:09.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator'
    Aug 24 11:32:09.440: INFO: stderr: ""
    Aug 24 11:32:09.440: INFO: stdout: "I0824 11:32:08.180166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/qgqw 416\nI0824 11:32:08.380643       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/xg96 434\nI0824 11:32:08.581121       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nq8k 214\nI0824 11:32:08.780537       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/95n 513\nI0824 11:32:08.981056       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/gb5n 211\nI0824 11:32:09.180326       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l75 491\nI0824 11:32:09.380834       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/g79 388\n"
    STEP: limiting log lines 08/24/23 11:32:09.44
    Aug 24 11:32:09.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --tail=1'
    Aug 24 11:32:09.600: INFO: stderr: ""
    Aug 24 11:32:09.600: INFO: stdout: "I0824 11:32:09.581187       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xm2f 433\n"
    Aug 24 11:32:09.600: INFO: got output "I0824 11:32:09.581187       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xm2f 433\n"
    STEP: limiting log bytes 08/24/23 11:32:09.6
    Aug 24 11:32:09.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --limit-bytes=1'
    Aug 24 11:32:09.721: INFO: stderr: ""
    Aug 24 11:32:09.721: INFO: stdout: "I"
    Aug 24 11:32:09.721: INFO: got output "I"
    STEP: exposing timestamps 08/24/23 11:32:09.721
    Aug 24 11:32:09.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 24 11:32:09.853: INFO: stderr: ""
    Aug 24 11:32:09.853: INFO: stdout: "2023-08-24T11:32:09.780826711Z I0824 11:32:09.780605       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/bm4 318\n"
    Aug 24 11:32:09.853: INFO: got output "2023-08-24T11:32:09.780826711Z I0824 11:32:09.780605       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/bm4 318\n"
    STEP: restricting to a time range 08/24/23 11:32:09.853
    Aug 24 11:32:12.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --since=1s'
    Aug 24 11:32:12.503: INFO: stderr: ""
    Aug 24 11:32:12.503: INFO: stdout: "I0824 11:32:11.580201       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/5tr6 452\nI0824 11:32:11.780606       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/d4p 437\nI0824 11:32:11.980977       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lf7 434\nI0824 11:32:12.180425       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/89b 484\nI0824 11:32:12.380849       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/jmm 394\n"
    Aug 24 11:32:12.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 logs logs-generator logs-generator --since=24h'
    Aug 24 11:32:12.639: INFO: stderr: ""
    Aug 24 11:32:12.639: INFO: stdout: "I0824 11:32:08.180166       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/qgqw 416\nI0824 11:32:08.380643       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/xg96 434\nI0824 11:32:08.581121       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/nq8k 214\nI0824 11:32:08.780537       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/95n 513\nI0824 11:32:08.981056       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/gb5n 211\nI0824 11:32:09.180326       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l75 491\nI0824 11:32:09.380834       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/g79 388\nI0824 11:32:09.581187       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/xm2f 433\nI0824 11:32:09.780605       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/bm4 318\nI0824 11:32:09.981059       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/bhc 491\nI0824 11:32:10.180421       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/5v9 512\nI0824 11:32:10.380863       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/wfsj 515\nI0824 11:32:10.581228       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/wd6x 492\nI0824 11:32:10.780740       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/m4c 594\nI0824 11:32:10.981128       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/62g 589\nI0824 11:32:11.180530       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/x46p 297\nI0824 11:32:11.380910       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/lvg 407\nI0824 11:32:11.580201       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/5tr6 452\nI0824 11:32:11.780606       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/d4p 437\nI0824 11:32:11.980977       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/lf7 434\nI0824 11:32:12.180425       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/89b 484\nI0824 11:32:12.380849       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/jmm 394\nI0824 11:32:12.581277       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/qbr 210\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 24 11:32:12.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5895 delete pod logs-generator'
    Aug 24 11:32:14.090: INFO: stderr: ""
    Aug 24 11:32:14.090: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:32:14.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5895" for this suite. 08/24/23 11:32:14.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:32:14.117
Aug 24 11:32:14.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 11:32:14.118
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:14.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:14.157
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/24/23 11:32:14.166
STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:32:14.173
STEP: Creating a ResourceQuota with not terminating scope 08/24/23 11:32:16.179
STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:32:16.187
STEP: Creating a long running pod 08/24/23 11:32:18.196
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/24/23 11:32:18.218
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/24/23 11:32:20.222
STEP: Deleting the pod 08/24/23 11:32:22.227
STEP: Ensuring resource quota status released the pod usage 08/24/23 11:32:22.25
STEP: Creating a terminating pod 08/24/23 11:32:24.254
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/24/23 11:32:24.271
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/24/23 11:32:26.275
STEP: Deleting the pod 08/24/23 11:32:28.28
STEP: Ensuring resource quota status released the pod usage 08/24/23 11:32:28.315
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 11:32:30.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6606" for this suite. 08/24/23 11:32:30.328
------------------------------
â€¢ [SLOW TEST] [16.222 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:32:14.117
    Aug 24 11:32:14.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 11:32:14.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:14.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:14.157
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/24/23 11:32:14.166
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:32:14.173
    STEP: Creating a ResourceQuota with not terminating scope 08/24/23 11:32:16.179
    STEP: Ensuring ResourceQuota status is calculated 08/24/23 11:32:16.187
    STEP: Creating a long running pod 08/24/23 11:32:18.196
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/24/23 11:32:18.218
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/24/23 11:32:20.222
    STEP: Deleting the pod 08/24/23 11:32:22.227
    STEP: Ensuring resource quota status released the pod usage 08/24/23 11:32:22.25
    STEP: Creating a terminating pod 08/24/23 11:32:24.254
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/24/23 11:32:24.271
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/24/23 11:32:26.275
    STEP: Deleting the pod 08/24/23 11:32:28.28
    STEP: Ensuring resource quota status released the pod usage 08/24/23 11:32:28.315
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:32:30.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6606" for this suite. 08/24/23 11:32:30.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:32:30.34
Aug 24 11:32:30.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 11:32:30.341
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:30.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:30.382
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 11:32:30.392
Aug 24 11:32:30.413: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3119" to be "running and ready"
Aug 24 11:32:30.428: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.669598ms
Aug 24 11:32:30.428: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:32:32.433: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.020587728s
Aug 24 11:32:32.433: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 11:32:32.433: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/24/23 11:32:32.437
Aug 24 11:32:32.450: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3119" to be "running and ready"
Aug 24 11:32:32.459: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.237862ms
Aug 24 11:32:32.459: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:32:34.464: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014218912s
Aug 24 11:32:34.464: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 24 11:32:34.464: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/24/23 11:32:34.466
STEP: delete the pod with lifecycle hook 08/24/23 11:32:34.555
Aug 24 11:32:34.575: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 24 11:32:34.579: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 24 11:32:36.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 24 11:32:36.584: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 24 11:32:38.581: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 24 11:32:38.585: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 11:32:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3119" for this suite. 08/24/23 11:32:38.589
------------------------------
â€¢ [SLOW TEST] [8.257 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:32:30.34
    Aug 24 11:32:30.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 11:32:30.341
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:30.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:30.382
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 11:32:30.392
    Aug 24 11:32:30.413: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3119" to be "running and ready"
    Aug 24 11:32:30.428: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 15.669598ms
    Aug 24 11:32:30.428: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:32:32.433: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.020587728s
    Aug 24 11:32:32.433: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 11:32:32.433: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/24/23 11:32:32.437
    Aug 24 11:32:32.450: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3119" to be "running and ready"
    Aug 24 11:32:32.459: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.237862ms
    Aug 24 11:32:32.459: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:32:34.464: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.014218912s
    Aug 24 11:32:34.464: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 24 11:32:34.464: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/24/23 11:32:34.466
    STEP: delete the pod with lifecycle hook 08/24/23 11:32:34.555
    Aug 24 11:32:34.575: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 24 11:32:34.579: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 24 11:32:36.580: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 24 11:32:36.584: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 24 11:32:38.581: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 24 11:32:38.585: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:32:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3119" for this suite. 08/24/23 11:32:38.589
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:32:38.598
Aug 24 11:32:38.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename security-context 08/24/23 11:32:38.599
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:38.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:38.631
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 11:32:38.637
Aug 24 11:32:38.653: INFO: Waiting up to 5m0s for pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745" in namespace "security-context-4024" to be "Succeeded or Failed"
Aug 24 11:32:38.657: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725021ms
Aug 24 11:32:40.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007753884s
Aug 24 11:32:42.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008288146s
Aug 24 11:32:44.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008103765s
STEP: Saw pod success 08/24/23 11:32:44.661
Aug 24 11:32:44.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745" satisfied condition "Succeeded or Failed"
Aug 24 11:32:44.664: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745 container test-container: <nil>
STEP: delete the pod 08/24/23 11:32:44.671
Aug 24 11:32:44.707: INFO: Waiting for pod security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745 to disappear
Aug 24 11:32:44.712: INFO: Pod security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 11:32:44.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4024" for this suite. 08/24/23 11:32:44.717
------------------------------
â€¢ [SLOW TEST] [6.131 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:32:38.598
    Aug 24 11:32:38.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename security-context 08/24/23 11:32:38.599
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:38.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:38.631
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 11:32:38.637
    Aug 24 11:32:38.653: INFO: Waiting up to 5m0s for pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745" in namespace "security-context-4024" to be "Succeeded or Failed"
    Aug 24 11:32:38.657: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725021ms
    Aug 24 11:32:40.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007753884s
    Aug 24 11:32:42.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008288146s
    Aug 24 11:32:44.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008103765s
    STEP: Saw pod success 08/24/23 11:32:44.661
    Aug 24 11:32:44.661: INFO: Pod "security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745" satisfied condition "Succeeded or Failed"
    Aug 24 11:32:44.664: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745 container test-container: <nil>
    STEP: delete the pod 08/24/23 11:32:44.671
    Aug 24 11:32:44.707: INFO: Waiting for pod security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745 to disappear
    Aug 24 11:32:44.712: INFO: Pod security-context-d03d1966-d6b7-4315-9c1e-0664d9ea7745 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:32:44.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4024" for this suite. 08/24/23 11:32:44.717
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:32:44.73
Aug 24 11:32:44.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 11:32:44.732
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:44.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:44.764
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/24/23 11:32:44.777
Aug 24 11:32:44.778: INFO: Creating simple deployment test-deployment-5cl6h
Aug 24 11:32:44.801: INFO: deployment "test-deployment-5cl6h" doesn't have the required revision set
STEP: Getting /status 08/24/23 11:32:46.813
Aug 24 11:32:46.818: INFO: Deployment test-deployment-5cl6h has Conditions: [{Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/24/23 11:32:46.818
Aug 24 11:32:46.835: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 32, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 32, 44, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5cl6h-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/24/23 11:32:46.835
Aug 24 11:32:46.840: INFO: Observed &Deployment event: ADDED
Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
Aug 24 11:32:46.840: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 11:32:46.840: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5cl6h-54bc444df" is progressing.}
Aug 24 11:32:46.841: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
Aug 24 11:32:46.841: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
Aug 24 11:32:46.841: INFO: Found Deployment test-deployment-5cl6h in namespace deployment-1252 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 11:32:46.841: INFO: Deployment test-deployment-5cl6h has an updated status
STEP: patching the Statefulset Status 08/24/23 11:32:46.841
Aug 24 11:32:46.841: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 24 11:32:46.855: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/24/23 11:32:46.855
Aug 24 11:32:46.858: INFO: Observed &Deployment event: ADDED
Aug 24 11:32:46.858: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
Aug 24 11:32:46.858: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.858: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
Aug 24 11:32:46.858: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 11:32:46.859: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5cl6h-54bc444df" is progressing.}
Aug 24 11:32:46.859: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
Aug 24 11:32:46.859: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 24 11:32:46.860: INFO: Observed &Deployment event: MODIFIED
Aug 24 11:32:46.860: INFO: Found deployment test-deployment-5cl6h in namespace deployment-1252 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 24 11:32:46.860: INFO: Deployment test-deployment-5cl6h has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 11:32:46.867: INFO: Deployment "test-deployment-5cl6h":
&Deployment{ObjectMeta:{test-deployment-5cl6h  deployment-1252  a03ab6c3-2cdb-4dbf-89d2-6637e4663723 25438 1 2023-08-24 11:32:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-24 11:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d2df08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-5cl6h-54bc444df",LastUpdateTime:2023-08-24 11:32:46 +0000 UTC,LastTransitionTime:2023-08-24 11:32:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 11:32:46.871: INFO: New ReplicaSet "test-deployment-5cl6h-54bc444df" of Deployment "test-deployment-5cl6h":
&ReplicaSet{ObjectMeta:{test-deployment-5cl6h-54bc444df  deployment-1252  4aeed972-3339-4a80-8022-8d9e311d9e29 25434 1 2023-08-24 11:32:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5cl6h a03ab6c3-2cdb-4dbf-89d2-6637e4663723 0xc00500c9e0 0xc00500c9e1}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a03ab6c3-2cdb-4dbf-89d2-6637e4663723\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00500ca88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 11:32:46.876: INFO: Pod "test-deployment-5cl6h-54bc444df-cgj4m" is available:
&Pod{ObjectMeta:{test-deployment-5cl6h-54bc444df-cgj4m test-deployment-5cl6h-54bc444df- deployment-1252  97e31985-b05c-4379-82aa-296e48088403 25433 0 2023-08-24 11:32:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:cac0bb19baf9c6b90da1d1ae0b140f7057bbe8dd30b5dab9331b9cec407e3bbe cni.projectcalico.org/podIP:10.100.45.186/32 cni.projectcalico.org/podIPs:10.100.45.186/32] [{apps/v1 ReplicaSet test-deployment-5cl6h-54bc444df 4aeed972-3339-4a80-8022-8d9e311d9e29 0xc004861130 0xc004861131}] [] [{kube-controller-manager Update v1 2023-08-24 11:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4aeed972-3339-4a80-8022-8d9e311d9e29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkl4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkl4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.186,StartTime:2023-08-24 11:32:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:32:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6efc6dc5904c1605c33b0b454f9a438ca7e692e1eb2e00eba56633765d998535,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 11:32:46.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1252" for this suite. 08/24/23 11:32:46.885
------------------------------
â€¢ [2.169 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:32:44.73
    Aug 24 11:32:44.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 11:32:44.732
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:44.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:44.764
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/24/23 11:32:44.777
    Aug 24 11:32:44.778: INFO: Creating simple deployment test-deployment-5cl6h
    Aug 24 11:32:44.801: INFO: deployment "test-deployment-5cl6h" doesn't have the required revision set
    STEP: Getting /status 08/24/23 11:32:46.813
    Aug 24 11:32:46.818: INFO: Deployment test-deployment-5cl6h has Conditions: [{Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/24/23 11:32:46.818
    Aug 24 11:32:46.835: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 32, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 32, 44, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5cl6h-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/24/23 11:32:46.835
    Aug 24 11:32:46.840: INFO: Observed &Deployment event: ADDED
    Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
    Aug 24 11:32:46.840: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
    Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 11:32:46.840: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 11:32:46.840: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5cl6h-54bc444df" is progressing.}
    Aug 24 11:32:46.841: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
    Aug 24 11:32:46.841: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 11:32:46.841: INFO: Observed Deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
    Aug 24 11:32:46.841: INFO: Found Deployment test-deployment-5cl6h in namespace deployment-1252 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 11:32:46.841: INFO: Deployment test-deployment-5cl6h has an updated status
    STEP: patching the Statefulset Status 08/24/23 11:32:46.841
    Aug 24 11:32:46.841: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 24 11:32:46.855: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/24/23 11:32:46.855
    Aug 24 11:32:46.858: INFO: Observed &Deployment event: ADDED
    Aug 24 11:32:46.858: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
    Aug 24 11:32:46.858: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.858: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5cl6h-54bc444df"}
    Aug 24 11:32:46.858: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 11:32:46.859: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:44 +0000 UTC 2023-08-24 11:32:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5cl6h-54bc444df" is progressing.}
    Aug 24 11:32:46.859: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
    Aug 24 11:32:46.859: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 11:32:46 +0000 UTC 2023-08-24 11:32:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5cl6h-54bc444df" has successfully progressed.}
    Aug 24 11:32:46.859: INFO: Observed deployment test-deployment-5cl6h in namespace deployment-1252 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 24 11:32:46.860: INFO: Observed &Deployment event: MODIFIED
    Aug 24 11:32:46.860: INFO: Found deployment test-deployment-5cl6h in namespace deployment-1252 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 24 11:32:46.860: INFO: Deployment test-deployment-5cl6h has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 11:32:46.867: INFO: Deployment "test-deployment-5cl6h":
    &Deployment{ObjectMeta:{test-deployment-5cl6h  deployment-1252  a03ab6c3-2cdb-4dbf-89d2-6637e4663723 25438 1 2023-08-24 11:32:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-24 11:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d2df08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-5cl6h-54bc444df",LastUpdateTime:2023-08-24 11:32:46 +0000 UTC,LastTransitionTime:2023-08-24 11:32:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 11:32:46.871: INFO: New ReplicaSet "test-deployment-5cl6h-54bc444df" of Deployment "test-deployment-5cl6h":
    &ReplicaSet{ObjectMeta:{test-deployment-5cl6h-54bc444df  deployment-1252  4aeed972-3339-4a80-8022-8d9e311d9e29 25434 1 2023-08-24 11:32:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5cl6h a03ab6c3-2cdb-4dbf-89d2-6637e4663723 0xc00500c9e0 0xc00500c9e1}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a03ab6c3-2cdb-4dbf-89d2-6637e4663723\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00500ca88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 11:32:46.876: INFO: Pod "test-deployment-5cl6h-54bc444df-cgj4m" is available:
    &Pod{ObjectMeta:{test-deployment-5cl6h-54bc444df-cgj4m test-deployment-5cl6h-54bc444df- deployment-1252  97e31985-b05c-4379-82aa-296e48088403 25433 0 2023-08-24 11:32:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:cac0bb19baf9c6b90da1d1ae0b140f7057bbe8dd30b5dab9331b9cec407e3bbe cni.projectcalico.org/podIP:10.100.45.186/32 cni.projectcalico.org/podIPs:10.100.45.186/32] [{apps/v1 ReplicaSet test-deployment-5cl6h-54bc444df 4aeed972-3339-4a80-8022-8d9e311d9e29 0xc004861130 0xc004861131}] [] [{kube-controller-manager Update v1 2023-08-24 11:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4aeed972-3339-4a80-8022-8d9e311d9e29\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:32:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkl4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkl4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:32:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.186,StartTime:2023-08-24 11:32:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:32:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6efc6dc5904c1605c33b0b454f9a438ca7e692e1eb2e00eba56633765d998535,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:32:46.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1252" for this suite. 08/24/23 11:32:46.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:32:46.9
Aug 24 11:32:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:32:46.901
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:46.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:46.978
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 24 11:32:46.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 11:32:49.28
Aug 24 11:32:49.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 create -f -'
Aug 24 11:32:50.594: INFO: stderr: ""
Aug 24 11:32:50.594: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 24 11:32:50.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 delete e2e-test-crd-publish-openapi-2088-crds test-cr'
Aug 24 11:32:50.728: INFO: stderr: ""
Aug 24 11:32:50.728: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 24 11:32:50.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 apply -f -'
Aug 24 11:32:51.178: INFO: stderr: ""
Aug 24 11:32:51.178: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 24 11:32:51.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 delete e2e-test-crd-publish-openapi-2088-crds test-cr'
Aug 24 11:32:51.313: INFO: stderr: ""
Aug 24 11:32:51.313: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/24/23 11:32:51.313
Aug 24 11:32:51.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 explain e2e-test-crd-publish-openapi-2088-crds'
Aug 24 11:32:51.738: INFO: stderr: ""
Aug 24 11:32:51.738: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2088-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:32:54.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1954" for this suite. 08/24/23 11:32:54.423
------------------------------
â€¢ [SLOW TEST] [7.533 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:32:46.9
    Aug 24 11:32:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:32:46.901
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:46.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:46.978
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 24 11:32:46.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 11:32:49.28
    Aug 24 11:32:49.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 create -f -'
    Aug 24 11:32:50.594: INFO: stderr: ""
    Aug 24 11:32:50.594: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 24 11:32:50.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 delete e2e-test-crd-publish-openapi-2088-crds test-cr'
    Aug 24 11:32:50.728: INFO: stderr: ""
    Aug 24 11:32:50.728: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 24 11:32:50.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 apply -f -'
    Aug 24 11:32:51.178: INFO: stderr: ""
    Aug 24 11:32:51.178: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 24 11:32:51.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 --namespace=crd-publish-openapi-1954 delete e2e-test-crd-publish-openapi-2088-crds test-cr'
    Aug 24 11:32:51.313: INFO: stderr: ""
    Aug 24 11:32:51.313: INFO: stdout: "e2e-test-crd-publish-openapi-2088-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/24/23 11:32:51.313
    Aug 24 11:32:51.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-1954 explain e2e-test-crd-publish-openapi-2088-crds'
    Aug 24 11:32:51.738: INFO: stderr: ""
    Aug 24 11:32:51.738: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2088-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:32:54.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1954" for this suite. 08/24/23 11:32:54.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:32:54.434
Aug 24 11:32:54.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-preemption 08/24/23 11:32:54.435
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:54.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:54.461
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 11:32:54.488: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 11:33:54.546: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:33:54.551
Aug 24 11:33:54.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 11:33:54.552
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.583
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 24 11:33:54.606: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 24 11:33:54.610: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 24 11:33:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:33:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5236" for this suite. 08/24/23 11:33:54.715
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6477" for this suite. 08/24/23 11:33:54.728
------------------------------
â€¢ [SLOW TEST] [60.304 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:32:54.434
    Aug 24 11:32:54.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 11:32:54.435
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:32:54.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:32:54.461
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 11:32:54.488: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 11:33:54.546: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:33:54.551
    Aug 24 11:33:54.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-preemption-path 08/24/23 11:33:54.552
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.583
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 24 11:33:54.606: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 24 11:33:54.610: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:33:54.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:33:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5236" for this suite. 08/24/23 11:33:54.715
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6477" for this suite. 08/24/23 11:33:54.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:33:54.739
Aug 24 11:33:54.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 11:33:54.741
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.768
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-31558bf7-2268-4b3c-8613-fb45de5105a1 08/24/23 11:33:54.774
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:33:54.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-125" for this suite. 08/24/23 11:33:54.792
------------------------------
â€¢ [0.062 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:33:54.739
    Aug 24 11:33:54.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 11:33:54.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.768
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-31558bf7-2268-4b3c-8613-fb45de5105a1 08/24/23 11:33:54.774
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:33:54.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-125" for this suite. 08/24/23 11:33:54.792
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:33:54.802
Aug 24 11:33:54.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename namespaces 08/24/23 11:33:54.805
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.84
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/24/23 11:33:54.847
Aug 24 11:33:54.851: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/24/23 11:33:54.852
Aug 24 11:33:54.860: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/24/23 11:33:54.86
Aug 24 11:33:54.874: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:33:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7190" for this suite. 08/24/23 11:33:54.879
------------------------------
â€¢ [0.087 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:33:54.802
    Aug 24 11:33:54.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename namespaces 08/24/23 11:33:54.805
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.84
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/24/23 11:33:54.847
    Aug 24 11:33:54.851: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/24/23 11:33:54.852
    Aug 24 11:33:54.860: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/24/23 11:33:54.86
    Aug 24 11:33:54.874: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:33:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7190" for this suite. 08/24/23 11:33:54.879
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:33:54.89
Aug 24 11:33:54.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename limitrange 08/24/23 11:33:54.892
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.926
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/24/23 11:33:54.932
STEP: Setting up watch 08/24/23 11:33:54.932
STEP: Submitting a LimitRange 08/24/23 11:33:55.038
STEP: Verifying LimitRange creation was observed 08/24/23 11:33:55.048
STEP: Fetching the LimitRange to ensure it has proper values 08/24/23 11:33:55.048
Aug 24 11:33:55.056: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 24 11:33:55.056: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/24/23 11:33:55.056
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/24/23 11:33:55.072
Aug 24 11:33:55.080: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 24 11:33:55.080: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/24/23 11:33:55.08
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/24/23 11:33:55.089
Aug 24 11:33:55.100: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 24 11:33:55.100: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/24/23 11:33:55.101
STEP: Failing to create a Pod with more than max resources 08/24/23 11:33:55.104
STEP: Updating a LimitRange 08/24/23 11:33:55.109
STEP: Verifying LimitRange updating is effective 08/24/23 11:33:55.118
STEP: Creating a Pod with less than former min resources 08/24/23 11:33:57.122
STEP: Failing to create a Pod with more than max resources 08/24/23 11:33:57.129
STEP: Deleting a LimitRange 08/24/23 11:33:57.132
STEP: Verifying the LimitRange was deleted 08/24/23 11:33:57.143
Aug 24 11:34:02.148: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/24/23 11:34:02.148
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 24 11:34:02.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2349" for this suite. 08/24/23 11:34:02.166
------------------------------
â€¢ [SLOW TEST] [7.289 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:33:54.89
    Aug 24 11:33:54.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename limitrange 08/24/23 11:33:54.892
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:33:54.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:33:54.926
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/24/23 11:33:54.932
    STEP: Setting up watch 08/24/23 11:33:54.932
    STEP: Submitting a LimitRange 08/24/23 11:33:55.038
    STEP: Verifying LimitRange creation was observed 08/24/23 11:33:55.048
    STEP: Fetching the LimitRange to ensure it has proper values 08/24/23 11:33:55.048
    Aug 24 11:33:55.056: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 24 11:33:55.056: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/24/23 11:33:55.056
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/24/23 11:33:55.072
    Aug 24 11:33:55.080: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 24 11:33:55.080: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/24/23 11:33:55.08
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/24/23 11:33:55.089
    Aug 24 11:33:55.100: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 24 11:33:55.100: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/24/23 11:33:55.101
    STEP: Failing to create a Pod with more than max resources 08/24/23 11:33:55.104
    STEP: Updating a LimitRange 08/24/23 11:33:55.109
    STEP: Verifying LimitRange updating is effective 08/24/23 11:33:55.118
    STEP: Creating a Pod with less than former min resources 08/24/23 11:33:57.122
    STEP: Failing to create a Pod with more than max resources 08/24/23 11:33:57.129
    STEP: Deleting a LimitRange 08/24/23 11:33:57.132
    STEP: Verifying the LimitRange was deleted 08/24/23 11:33:57.143
    Aug 24 11:34:02.148: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/24/23 11:34:02.148
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:34:02.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2349" for this suite. 08/24/23 11:34:02.166
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:34:02.192
Aug 24 11:34:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:34:02.193
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:34:02.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:34:02.226
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/24/23 11:34:02.236
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:34:02.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3599" for this suite. 08/24/23 11:34:02.254
------------------------------
â€¢ [0.072 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:34:02.192
    Aug 24 11:34:02.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:34:02.193
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:34:02.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:34:02.226
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/24/23 11:34:02.236
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:34:02.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3599" for this suite. 08/24/23 11:34:02.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:34:02.266
Aug 24 11:34:02.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:34:02.267
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:34:02.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:34:02.299
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/24/23 11:34:02.313
STEP: waiting for available Endpoint 08/24/23 11:34:02.322
STEP: listing all Endpoints 08/24/23 11:34:02.325
STEP: updating the Endpoint 08/24/23 11:34:02.33
STEP: fetching the Endpoint 08/24/23 11:34:02.341
STEP: patching the Endpoint 08/24/23 11:34:02.362
STEP: fetching the Endpoint 08/24/23 11:34:02.375
STEP: deleting the Endpoint by Collection 08/24/23 11:34:02.378
STEP: waiting for Endpoint deletion 08/24/23 11:34:02.387
STEP: fetching the Endpoint 08/24/23 11:34:02.39
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:34:02.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6103" for this suite. 08/24/23 11:34:02.395
------------------------------
â€¢ [0.137 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:34:02.266
    Aug 24 11:34:02.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:34:02.267
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:34:02.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:34:02.299
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/24/23 11:34:02.313
    STEP: waiting for available Endpoint 08/24/23 11:34:02.322
    STEP: listing all Endpoints 08/24/23 11:34:02.325
    STEP: updating the Endpoint 08/24/23 11:34:02.33
    STEP: fetching the Endpoint 08/24/23 11:34:02.341
    STEP: patching the Endpoint 08/24/23 11:34:02.362
    STEP: fetching the Endpoint 08/24/23 11:34:02.375
    STEP: deleting the Endpoint by Collection 08/24/23 11:34:02.378
    STEP: waiting for Endpoint deletion 08/24/23 11:34:02.387
    STEP: fetching the Endpoint 08/24/23 11:34:02.39
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:34:02.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6103" for this suite. 08/24/23 11:34:02.395
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:34:02.403
Aug 24 11:34:02.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 11:34:02.405
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:34:02.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:34:02.433
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-414 08/24/23 11:34:02.44
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/24/23 11:34:02.448
Aug 24 11:34:02.477: INFO: Found 0 stateful pods, waiting for 3
Aug 24 11:34:12.483: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:34:12.483: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:34:12.483: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 11:34:12.494
Aug 24 11:34:12.520: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/24/23 11:34:12.52
STEP: Not applying an update when the partition is greater than the number of replicas 08/24/23 11:34:22.534
STEP: Performing a canary update 08/24/23 11:34:22.534
Aug 24 11:34:22.563: INFO: Updating stateful set ss2
Aug 24 11:34:22.586: INFO: Waiting for Pod statefulset-414/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/24/23 11:34:32.597
Aug 24 11:34:32.722: INFO: Found 2 stateful pods, waiting for 3
Aug 24 11:34:42.729: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:34:42.729: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 11:34:42.729: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/24/23 11:34:42.737
Aug 24 11:34:42.764: INFO: Updating stateful set ss2
Aug 24 11:34:42.770: INFO: Waiting for Pod statefulset-414/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 24 11:34:52.814: INFO: Updating stateful set ss2
Aug 24 11:34:52.839: INFO: Waiting for StatefulSet statefulset-414/ss2 to complete update
Aug 24 11:34:52.839: INFO: Waiting for Pod statefulset-414/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 11:35:02.864: INFO: Deleting all statefulset in ns statefulset-414
Aug 24 11:35:02.874: INFO: Scaling statefulset ss2 to 0
Aug 24 11:35:12.903: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 11:35:12.910: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:12.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-414" for this suite. 08/24/23 11:35:12.975
------------------------------
â€¢ [SLOW TEST] [70.586 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:34:02.403
    Aug 24 11:34:02.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 11:34:02.405
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:34:02.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:34:02.433
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-414 08/24/23 11:34:02.44
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/24/23 11:34:02.448
    Aug 24 11:34:02.477: INFO: Found 0 stateful pods, waiting for 3
    Aug 24 11:34:12.483: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:34:12.483: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:34:12.483: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/24/23 11:34:12.494
    Aug 24 11:34:12.520: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/24/23 11:34:12.52
    STEP: Not applying an update when the partition is greater than the number of replicas 08/24/23 11:34:22.534
    STEP: Performing a canary update 08/24/23 11:34:22.534
    Aug 24 11:34:22.563: INFO: Updating stateful set ss2
    Aug 24 11:34:22.586: INFO: Waiting for Pod statefulset-414/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/24/23 11:34:32.597
    Aug 24 11:34:32.722: INFO: Found 2 stateful pods, waiting for 3
    Aug 24 11:34:42.729: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:34:42.729: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 11:34:42.729: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/24/23 11:34:42.737
    Aug 24 11:34:42.764: INFO: Updating stateful set ss2
    Aug 24 11:34:42.770: INFO: Waiting for Pod statefulset-414/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 24 11:34:52.814: INFO: Updating stateful set ss2
    Aug 24 11:34:52.839: INFO: Waiting for StatefulSet statefulset-414/ss2 to complete update
    Aug 24 11:34:52.839: INFO: Waiting for Pod statefulset-414/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 11:35:02.864: INFO: Deleting all statefulset in ns statefulset-414
    Aug 24 11:35:02.874: INFO: Scaling statefulset ss2 to 0
    Aug 24 11:35:12.903: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 11:35:12.910: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:12.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-414" for this suite. 08/24/23 11:35:12.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:12.991
Aug 24 11:35:12.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:35:12.994
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:13.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:13.025
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/24/23 11:35:13.032
Aug 24 11:35:13.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8582 create -f -'
Aug 24 11:35:14.157: INFO: stderr: ""
Aug 24 11:35:14.157: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/24/23 11:35:14.157
Aug 24 11:35:15.163: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 11:35:15.163: INFO: Found 0 / 1
Aug 24 11:35:16.162: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 11:35:16.162: INFO: Found 1 / 1
Aug 24 11:35:16.162: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/24/23 11:35:16.162
Aug 24 11:35:16.166: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 11:35:16.166: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 24 11:35:16.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8582 patch pod agnhost-primary-x9m2d -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 24 11:35:16.305: INFO: stderr: ""
Aug 24 11:35:16.305: INFO: stdout: "pod/agnhost-primary-x9m2d patched\n"
STEP: checking annotations 08/24/23 11:35:16.305
Aug 24 11:35:16.309: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 11:35:16.309: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:16.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8582" for this suite. 08/24/23 11:35:16.322
------------------------------
â€¢ [3.341 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:12.991
    Aug 24 11:35:12.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:35:12.994
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:13.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:13.025
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/24/23 11:35:13.032
    Aug 24 11:35:13.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8582 create -f -'
    Aug 24 11:35:14.157: INFO: stderr: ""
    Aug 24 11:35:14.157: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/24/23 11:35:14.157
    Aug 24 11:35:15.163: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 11:35:15.163: INFO: Found 0 / 1
    Aug 24 11:35:16.162: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 11:35:16.162: INFO: Found 1 / 1
    Aug 24 11:35:16.162: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/24/23 11:35:16.162
    Aug 24 11:35:16.166: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 11:35:16.166: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 24 11:35:16.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8582 patch pod agnhost-primary-x9m2d -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 24 11:35:16.305: INFO: stderr: ""
    Aug 24 11:35:16.305: INFO: stdout: "pod/agnhost-primary-x9m2d patched\n"
    STEP: checking annotations 08/24/23 11:35:16.305
    Aug 24 11:35:16.309: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 11:35:16.309: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:16.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8582" for this suite. 08/24/23 11:35:16.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:16.339
Aug 24 11:35:16.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename events 08/24/23 11:35:16.34
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:16.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:16.37
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/24/23 11:35:16.378
Aug 24 11:35:16.386: INFO: created test-event-1
Aug 24 11:35:16.396: INFO: created test-event-2
Aug 24 11:35:16.404: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/24/23 11:35:16.404
STEP: delete collection of events 08/24/23 11:35:16.411
Aug 24 11:35:16.411: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/24/23 11:35:16.457
Aug 24 11:35:16.457: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:16.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6262" for this suite. 08/24/23 11:35:16.466
------------------------------
â€¢ [0.138 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:16.339
    Aug 24 11:35:16.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename events 08/24/23 11:35:16.34
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:16.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:16.37
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/24/23 11:35:16.378
    Aug 24 11:35:16.386: INFO: created test-event-1
    Aug 24 11:35:16.396: INFO: created test-event-2
    Aug 24 11:35:16.404: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/24/23 11:35:16.404
    STEP: delete collection of events 08/24/23 11:35:16.411
    Aug 24 11:35:16.411: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/24/23 11:35:16.457
    Aug 24 11:35:16.457: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:16.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6262" for this suite. 08/24/23 11:35:16.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:16.484
Aug 24 11:35:16.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:35:16.486
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:16.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:16.516
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 24 11:35:16.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:23.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2676" for this suite. 08/24/23 11:35:23.227
------------------------------
â€¢ [SLOW TEST] [6.753 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:16.484
    Aug 24 11:35:16.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:35:16.486
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:16.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:16.516
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 24 11:35:16.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:23.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2676" for this suite. 08/24/23 11:35:23.227
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:23.238
Aug 24 11:35:23.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 11:35:23.239
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:23.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:23.311
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 24 11:35:23.319: INFO: Creating simple deployment test-new-deployment
Aug 24 11:35:23.341: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 08/24/23 11:35:25.353
STEP: updating a scale subresource 08/24/23 11:35:25.358
STEP: verifying the deployment Spec.Replicas was modified 08/24/23 11:35:25.371
STEP: Patch a scale subresource 08/24/23 11:35:25.375
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 11:35:25.427: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9193  ef39330f-65f5-4954-8c79-c9d109da6aa4 26434 3 2023-08-24 11:35:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-24 11:35:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bc5b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 11:35:24 +0000 UTC,LastTransitionTime:2023-08-24 11:35:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-24 11:35:24 +0000 UTC,LastTransitionTime:2023-08-24 11:35:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 11:35:25.433: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9193  27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb 26441 2 2023-08-24 11:35:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ef39330f-65f5-4954-8c79-c9d109da6aa4 0xc004a96c77 0xc004a96c78}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef39330f-65f5-4954-8c79-c9d109da6aa4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a96d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 11:35:25.459: INFO: Pod "test-new-deployment-7f5969cbc7-6rkmm" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-6rkmm test-new-deployment-7f5969cbc7- deployment-9193  1aa9ff90-2fb8-41a5-af98-f68e669a9399 26427 0 2023-08-24 11:35:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a3f84f43ca0795370729a4e8bca63e697f6f3179be66374e387f9eda96ceee08 cni.projectcalico.org/podIP:10.100.45.189/32 cni.projectcalico.org/podIPs:10.100.45.189/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb 0xc004a97117 0xc004a97118}] [] [{Go-http-client Update v1 2023-08-24 11:35:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:35:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:35:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tmqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tmqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.189,StartTime:2023-08-24 11:35:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:35:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fb8bfb8cca417cd64ad265a5811612ef796e29edbe50e24154838d5b713c763e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:35:25.460: INFO: Pod "test-new-deployment-7f5969cbc7-txrws" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-txrws test-new-deployment-7f5969cbc7- deployment-9193  4a0c8228-8172-4a97-b6a1-800f710a2175 26442 0 2023-08-24 11:35:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb 0xc004a97310 0xc004a97311}] [] [{kube-controller-manager Update v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9672g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9672g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 11:35:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:25.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9193" for this suite. 08/24/23 11:35:25.467
------------------------------
â€¢ [2.253 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:23.238
    Aug 24 11:35:23.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 11:35:23.239
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:23.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:23.311
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 24 11:35:23.319: INFO: Creating simple deployment test-new-deployment
    Aug 24 11:35:23.341: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 08/24/23 11:35:25.353
    STEP: updating a scale subresource 08/24/23 11:35:25.358
    STEP: verifying the deployment Spec.Replicas was modified 08/24/23 11:35:25.371
    STEP: Patch a scale subresource 08/24/23 11:35:25.375
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 11:35:25.427: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9193  ef39330f-65f5-4954-8c79-c9d109da6aa4 26434 3 2023-08-24 11:35:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-24 11:35:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:35:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bc5b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 11:35:24 +0000 UTC,LastTransitionTime:2023-08-24 11:35:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-24 11:35:24 +0000 UTC,LastTransitionTime:2023-08-24 11:35:23 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 11:35:25.433: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9193  27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb 26441 2 2023-08-24 11:35:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment ef39330f-65f5-4954-8c79-c9d109da6aa4 0xc004a96c77 0xc004a96c78}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef39330f-65f5-4954-8c79-c9d109da6aa4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a96d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 11:35:25.459: INFO: Pod "test-new-deployment-7f5969cbc7-6rkmm" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-6rkmm test-new-deployment-7f5969cbc7- deployment-9193  1aa9ff90-2fb8-41a5-af98-f68e669a9399 26427 0 2023-08-24 11:35:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a3f84f43ca0795370729a4e8bca63e697f6f3179be66374e387f9eda96ceee08 cni.projectcalico.org/podIP:10.100.45.189/32 cni.projectcalico.org/podIPs:10.100.45.189/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb 0xc004a97117 0xc004a97118}] [] [{Go-http-client Update v1 2023-08-24 11:35:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:35:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:35:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tmqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tmqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.189,StartTime:2023-08-24 11:35:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:35:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fb8bfb8cca417cd64ad265a5811612ef796e29edbe50e24154838d5b713c763e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:35:25.460: INFO: Pod "test-new-deployment-7f5969cbc7-txrws" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-txrws test-new-deployment-7f5969cbc7- deployment-9193  4a0c8228-8172-4a97-b6a1-800f710a2175 26442 0 2023-08-24 11:35:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb 0xc004a97310 0xc004a97311}] [] [{kube-controller-manager Update v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27ea2d0f-70f4-450a-8bc5-1c238fc8ffcb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:35:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9672g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9672g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 11:35:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:25.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9193" for this suite. 08/24/23 11:35:25.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:25.5
Aug 24 11:35:25.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 11:35:25.502
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:25.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:25.557
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 24 11:35:25.598: INFO: Waiting up to 5m0s for pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d" in namespace "emptydir-wrapper-8828" to be "running and ready"
Aug 24 11:35:25.607: INFO: Pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.320253ms
Aug 24 11:35:25.607: INFO: The phase of Pod pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:35:27.611: INFO: Pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013724707s
Aug 24 11:35:27.611: INFO: The phase of Pod pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d is Running (Ready = true)
Aug 24 11:35:27.611: INFO: Pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/24/23 11:35:27.615
STEP: Cleaning up the configmap 08/24/23 11:35:27.624
STEP: Cleaning up the pod 08/24/23 11:35:27.633
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:27.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8828" for this suite. 08/24/23 11:35:27.665
------------------------------
â€¢ [2.186 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:25.5
    Aug 24 11:35:25.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 11:35:25.502
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:25.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:25.557
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 24 11:35:25.598: INFO: Waiting up to 5m0s for pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d" in namespace "emptydir-wrapper-8828" to be "running and ready"
    Aug 24 11:35:25.607: INFO: Pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.320253ms
    Aug 24 11:35:25.607: INFO: The phase of Pod pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:35:27.611: INFO: Pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d": Phase="Running", Reason="", readiness=true. Elapsed: 2.013724707s
    Aug 24 11:35:27.611: INFO: The phase of Pod pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d is Running (Ready = true)
    Aug 24 11:35:27.611: INFO: Pod "pod-secrets-9b145cc1-8e92-4358-8ddd-c7c38168b71d" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/24/23 11:35:27.615
    STEP: Cleaning up the configmap 08/24/23 11:35:27.624
    STEP: Cleaning up the pod 08/24/23 11:35:27.633
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:27.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8828" for this suite. 08/24/23 11:35:27.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:27.689
Aug 24 11:35:27.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename namespaces 08/24/23 11:35:27.692
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:27.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:27.717
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/24/23 11:35:27.724
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:27.755
STEP: Creating a service in the namespace 08/24/23 11:35:27.76
STEP: Deleting the namespace 08/24/23 11:35:27.784
STEP: Waiting for the namespace to be removed. 08/24/23 11:35:27.809
STEP: Recreating the namespace 08/24/23 11:35:33.82
STEP: Verifying there is no service in the namespace 08/24/23 11:35:33.865
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:33.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7784" for this suite. 08/24/23 11:35:33.881
STEP: Destroying namespace "nsdeletetest-4186" for this suite. 08/24/23 11:35:33.892
Aug 24 11:35:33.894: INFO: Namespace nsdeletetest-4186 was already deleted
STEP: Destroying namespace "nsdeletetest-2584" for this suite. 08/24/23 11:35:33.894
------------------------------
â€¢ [SLOW TEST] [6.221 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:27.689
    Aug 24 11:35:27.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename namespaces 08/24/23 11:35:27.692
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:27.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:27.717
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/24/23 11:35:27.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:27.755
    STEP: Creating a service in the namespace 08/24/23 11:35:27.76
    STEP: Deleting the namespace 08/24/23 11:35:27.784
    STEP: Waiting for the namespace to be removed. 08/24/23 11:35:27.809
    STEP: Recreating the namespace 08/24/23 11:35:33.82
    STEP: Verifying there is no service in the namespace 08/24/23 11:35:33.865
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:33.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7784" for this suite. 08/24/23 11:35:33.881
    STEP: Destroying namespace "nsdeletetest-4186" for this suite. 08/24/23 11:35:33.892
    Aug 24 11:35:33.894: INFO: Namespace nsdeletetest-4186 was already deleted
    STEP: Destroying namespace "nsdeletetest-2584" for this suite. 08/24/23 11:35:33.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:33.912
Aug 24 11:35:33.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 11:35:33.914
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:33.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:33.943
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/24/23 11:35:33.948
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/24/23 11:35:33.956
STEP: patching the secret 08/24/23 11:35:33.963
STEP: deleting the secret using a LabelSelector 08/24/23 11:35:33.978
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/24/23 11:35:33.987
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:33.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-901" for this suite. 08/24/23 11:35:33.996
------------------------------
â€¢ [0.093 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:33.912
    Aug 24 11:35:33.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 11:35:33.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:33.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:33.943
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/24/23 11:35:33.948
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/24/23 11:35:33.956
    STEP: patching the secret 08/24/23 11:35:33.963
    STEP: deleting the secret using a LabelSelector 08/24/23 11:35:33.978
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/24/23 11:35:33.987
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:33.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-901" for this suite. 08/24/23 11:35:33.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:34.017
Aug 24 11:35:34.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:35:34.018
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:34.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:34.068
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-35a2f6e7-030a-4846-af77-7977aea376f5 08/24/23 11:35:34.082
STEP: Creating a pod to test consume configMaps 08/24/23 11:35:34.098
Aug 24 11:35:34.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce" in namespace "projected-9736" to be "Succeeded or Failed"
Aug 24 11:35:34.184: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce": Phase="Pending", Reason="", readiness=false. Elapsed: 21.159569ms
Aug 24 11:35:36.189: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026450034s
Aug 24 11:35:38.189: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025995785s
STEP: Saw pod success 08/24/23 11:35:38.189
Aug 24 11:35:38.189: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce" satisfied condition "Succeeded or Failed"
Aug 24 11:35:38.192: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:35:38.243
Aug 24 11:35:38.274: INFO: Waiting for pod pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce to disappear
Aug 24 11:35:38.278: INFO: Pod pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:38.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9736" for this suite. 08/24/23 11:35:38.282
------------------------------
â€¢ [4.273 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:34.017
    Aug 24 11:35:34.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:35:34.018
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:34.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:34.068
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-35a2f6e7-030a-4846-af77-7977aea376f5 08/24/23 11:35:34.082
    STEP: Creating a pod to test consume configMaps 08/24/23 11:35:34.098
    Aug 24 11:35:34.163: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce" in namespace "projected-9736" to be "Succeeded or Failed"
    Aug 24 11:35:34.184: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce": Phase="Pending", Reason="", readiness=false. Elapsed: 21.159569ms
    Aug 24 11:35:36.189: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026450034s
    Aug 24 11:35:38.189: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025995785s
    STEP: Saw pod success 08/24/23 11:35:38.189
    Aug 24 11:35:38.189: INFO: Pod "pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce" satisfied condition "Succeeded or Failed"
    Aug 24 11:35:38.192: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:35:38.243
    Aug 24 11:35:38.274: INFO: Waiting for pod pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce to disappear
    Aug 24 11:35:38.278: INFO: Pod pod-projected-configmaps-d1a29cf8-7d1c-495d-9f90-5e80abe6b6ce no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:38.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9736" for this suite. 08/24/23 11:35:38.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:38.292
Aug 24 11:35:38.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:35:38.293
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:38.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:38.315
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 24 11:35:38.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:41.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9308" for this suite. 08/24/23 11:35:41.477
------------------------------
â€¢ [3.195 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:38.292
    Aug 24 11:35:38.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 11:35:38.293
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:38.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:38.315
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 24 11:35:38.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:41.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9308" for this suite. 08/24/23 11:35:41.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:41.491
Aug 24 11:35:41.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 11:35:41.493
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:41.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:41.519
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 24 11:35:41.544: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 24 11:35:46.548: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 11:35:46.549
Aug 24 11:35:46.549: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/24/23 11:35:46.565
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 11:35:46.579: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-614  4b925ee5-d7ff-4a87-82fc-fabfe909310a 26684 1 2023-08-24 11:35:46 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-24 11:35:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057fec18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 24 11:35:46.589: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Aug 24 11:35:46.589: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 24 11:35:46.589: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-614  d7517abb-a964-4455-adba-43d81a9593f8 26685 1 2023-08-24 11:35:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4b925ee5-d7ff-4a87-82fc-fabfe909310a 0xc0057fef77 0xc0057fef78}] [] [{e2e.test Update apps/v1 2023-08-24 11:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:35:46 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4b925ee5-d7ff-4a87-82fc-fabfe909310a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0057ff038 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 11:35:46.593: INFO: Pod "test-cleanup-controller-xxp9j" is available:
&Pod{ObjectMeta:{test-cleanup-controller-xxp9j test-cleanup-controller- deployment-614  4152bd65-307e-453f-8c72-010a774f17b3 26672 0 2023-08-24 11:35:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:3e8c7c309779577e3cbbd1aac290513aaf4baa6ebbd936d74331ef3896876222 cni.projectcalico.org/podIP:10.100.45.132/32 cni.projectcalico.org/podIPs:10.100.45.132/32] [{apps/v1 ReplicaSet test-cleanup-controller d7517abb-a964-4455-adba-43d81a9593f8 0xc0057ff337 0xc0057ff338}] [] [{kube-controller-manager Update v1 2023-08-24 11:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7517abb-a964-4455-adba-43d81a9593f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:35:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l49xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l49xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.132,StartTime:2023-08-24 11:35:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:35:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://98fffa24cca819c4ba71accb2a4dce73d377345621b2033a4517937da0b1b8a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:46.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-614" for this suite. 08/24/23 11:35:46.601
------------------------------
â€¢ [SLOW TEST] [5.144 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:41.491
    Aug 24 11:35:41.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 11:35:41.493
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:41.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:41.519
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 24 11:35:41.544: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 24 11:35:46.548: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 11:35:46.549
    Aug 24 11:35:46.549: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/24/23 11:35:46.565
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 11:35:46.579: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-614  4b925ee5-d7ff-4a87-82fc-fabfe909310a 26684 1 2023-08-24 11:35:46 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-24 11:35:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0057fec18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 24 11:35:46.589: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Aug 24 11:35:46.589: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Aug 24 11:35:46.589: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-614  d7517abb-a964-4455-adba-43d81a9593f8 26685 1 2023-08-24 11:35:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4b925ee5-d7ff-4a87-82fc-fabfe909310a 0xc0057fef77 0xc0057fef78}] [] [{e2e.test Update apps/v1 2023-08-24 11:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:35:46 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4b925ee5-d7ff-4a87-82fc-fabfe909310a\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0057ff038 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 11:35:46.593: INFO: Pod "test-cleanup-controller-xxp9j" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-xxp9j test-cleanup-controller- deployment-614  4152bd65-307e-453f-8c72-010a774f17b3 26672 0 2023-08-24 11:35:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:3e8c7c309779577e3cbbd1aac290513aaf4baa6ebbd936d74331ef3896876222 cni.projectcalico.org/podIP:10.100.45.132/32 cni.projectcalico.org/podIPs:10.100.45.132/32] [{apps/v1 ReplicaSet test-cleanup-controller d7517abb-a964-4455-adba-43d81a9593f8 0xc0057ff337 0xc0057ff338}] [] [{kube-controller-manager Update v1 2023-08-24 11:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7517abb-a964-4455-adba-43d81a9593f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:35:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:35:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l49xz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l49xz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:35:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.132,StartTime:2023-08-24 11:35:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:35:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://98fffa24cca819c4ba71accb2a4dce73d377345621b2033a4517937da0b1b8a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.132,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:46.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-614" for this suite. 08/24/23 11:35:46.601
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:46.636
Aug 24 11:35:46.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename runtimeclass 08/24/23 11:35:46.638
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:46.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:46.699
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 24 11:35:46.736: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2242 to be scheduled
Aug 24 11:35:46.755: INFO: 1 pods are not scheduled: [runtimeclass-2242/test-runtimeclass-runtimeclass-2242-preconfigured-handler-dprxw(da1ea3ce-f362-4490-981f-3fd8e0fc827c)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2242" for this suite. 08/24/23 11:35:48.771
------------------------------
â€¢ [2.146 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:46.636
    Aug 24 11:35:46.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 11:35:46.638
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:46.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:46.699
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 24 11:35:46.736: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2242 to be scheduled
    Aug 24 11:35:46.755: INFO: 1 pods are not scheduled: [runtimeclass-2242/test-runtimeclass-runtimeclass-2242-preconfigured-handler-dprxw(da1ea3ce-f362-4490-981f-3fd8e0fc827c)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2242" for this suite. 08/24/23 11:35:48.771
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:48.782
Aug 24 11:35:48.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replicaset 08/24/23 11:35:48.783
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:48.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:48.806
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/24/23 11:35:48.814
Aug 24 11:35:48.827: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4547" to be "running and ready"
Aug 24 11:35:48.832: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126424ms
Aug 24 11:35:48.832: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:35:50.837: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009982631s
Aug 24 11:35:50.837: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 24 11:35:50.837: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/24/23 11:35:50.84
STEP: Then the orphan pod is adopted 08/24/23 11:35:50.848
STEP: When the matched label of one of its pods change 08/24/23 11:35:51.865
Aug 24 11:35:51.874: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/24/23 11:35:51.895
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:52.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4547" for this suite. 08/24/23 11:35:52.91
------------------------------
â€¢ [4.135 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:48.782
    Aug 24 11:35:48.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replicaset 08/24/23 11:35:48.783
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:48.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:48.806
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/24/23 11:35:48.814
    Aug 24 11:35:48.827: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4547" to be "running and ready"
    Aug 24 11:35:48.832: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126424ms
    Aug 24 11:35:48.832: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:35:50.837: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009982631s
    Aug 24 11:35:50.837: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 24 11:35:50.837: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/24/23 11:35:50.84
    STEP: Then the orphan pod is adopted 08/24/23 11:35:50.848
    STEP: When the matched label of one of its pods change 08/24/23 11:35:51.865
    Aug 24 11:35:51.874: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/24/23 11:35:51.895
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:52.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4547" for this suite. 08/24/23 11:35:52.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:52.919
Aug 24 11:35:52.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:35:52.92
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:52.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:52.949
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-29649475-2905-4c7a-8fde-48c45ed396d4 08/24/23 11:35:52.955
STEP: Creating a pod to test consume secrets 08/24/23 11:35:52.962
Aug 24 11:35:52.975: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98" in namespace "projected-4597" to be "Succeeded or Failed"
Aug 24 11:35:52.985: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98": Phase="Pending", Reason="", readiness=false. Elapsed: 9.374447ms
Aug 24 11:35:54.989: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013845812s
Aug 24 11:35:56.989: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014049047s
STEP: Saw pod success 08/24/23 11:35:56.989
Aug 24 11:35:56.990: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98" satisfied condition "Succeeded or Failed"
Aug 24 11:35:56.994: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 11:35:57.069
Aug 24 11:35:57.117: INFO: Waiting for pod pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98 to disappear
Aug 24 11:35:57.124: INFO: Pod pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 11:35:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4597" for this suite. 08/24/23 11:35:57.129
------------------------------
â€¢ [4.219 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:52.919
    Aug 24 11:35:52.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:35:52.92
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:52.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:52.949
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-29649475-2905-4c7a-8fde-48c45ed396d4 08/24/23 11:35:52.955
    STEP: Creating a pod to test consume secrets 08/24/23 11:35:52.962
    Aug 24 11:35:52.975: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98" in namespace "projected-4597" to be "Succeeded or Failed"
    Aug 24 11:35:52.985: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98": Phase="Pending", Reason="", readiness=false. Elapsed: 9.374447ms
    Aug 24 11:35:54.989: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013845812s
    Aug 24 11:35:56.989: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014049047s
    STEP: Saw pod success 08/24/23 11:35:56.989
    Aug 24 11:35:56.990: INFO: Pod "pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98" satisfied condition "Succeeded or Failed"
    Aug 24 11:35:56.994: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:35:57.069
    Aug 24 11:35:57.117: INFO: Waiting for pod pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98 to disappear
    Aug 24 11:35:57.124: INFO: Pod pod-projected-secrets-03526f85-3f21-4884-8948-f2676d803b98 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:35:57.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4597" for this suite. 08/24/23 11:35:57.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:35:57.144
Aug 24 11:35:57.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:35:57.145
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:57.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:57.178
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-e0d610c1-01ca-4f48-a4ec-d63ae70cc388 08/24/23 11:35:57.186
STEP: Creating a pod to test consume configMaps 08/24/23 11:35:57.195
Aug 24 11:35:57.207: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f" in namespace "projected-8516" to be "Succeeded or Failed"
Aug 24 11:35:57.213: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132099ms
Aug 24 11:35:59.228: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021143643s
Aug 24 11:36:01.219: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011843781s
STEP: Saw pod success 08/24/23 11:36:01.219
Aug 24 11:36:01.219: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f" satisfied condition "Succeeded or Failed"
Aug 24 11:36:01.223: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:36:01.234
Aug 24 11:36:01.260: INFO: Waiting for pod pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f to disappear
Aug 24 11:36:01.264: INFO: Pod pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:36:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8516" for this suite. 08/24/23 11:36:01.268
------------------------------
â€¢ [4.131 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:35:57.144
    Aug 24 11:35:57.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:35:57.145
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:35:57.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:35:57.178
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-e0d610c1-01ca-4f48-a4ec-d63ae70cc388 08/24/23 11:35:57.186
    STEP: Creating a pod to test consume configMaps 08/24/23 11:35:57.195
    Aug 24 11:35:57.207: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f" in namespace "projected-8516" to be "Succeeded or Failed"
    Aug 24 11:35:57.213: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.132099ms
    Aug 24 11:35:59.228: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021143643s
    Aug 24 11:36:01.219: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011843781s
    STEP: Saw pod success 08/24/23 11:36:01.219
    Aug 24 11:36:01.219: INFO: Pod "pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f" satisfied condition "Succeeded or Failed"
    Aug 24 11:36:01.223: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:36:01.234
    Aug 24 11:36:01.260: INFO: Waiting for pod pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f to disappear
    Aug 24 11:36:01.264: INFO: Pod pod-projected-configmaps-7bd3e1d5-95d5-4d82-afa5-856b5826c47f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:36:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8516" for this suite. 08/24/23 11:36:01.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:36:01.277
Aug 24 11:36:01.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 11:36:01.278
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:01.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:01.308
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-e40f3776-7830-45f2-9103-c1bee8c04481 08/24/23 11:36:01.315
STEP: Creating a pod to test consume configMaps 08/24/23 11:36:01.322
Aug 24 11:36:01.344: INFO: Waiting up to 5m0s for pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a" in namespace "configmap-3736" to be "Succeeded or Failed"
Aug 24 11:36:01.350: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225703ms
Aug 24 11:36:03.354: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010424358s
Aug 24 11:36:05.357: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013470099s
STEP: Saw pod success 08/24/23 11:36:05.357
Aug 24 11:36:05.357: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a" satisfied condition "Succeeded or Failed"
Aug 24 11:36:05.360: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:36:05.37
Aug 24 11:36:05.393: INFO: Waiting for pod pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a to disappear
Aug 24 11:36:05.398: INFO: Pod pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:36:05.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3736" for this suite. 08/24/23 11:36:05.403
------------------------------
â€¢ [4.135 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:36:01.277
    Aug 24 11:36:01.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 11:36:01.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:01.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:01.308
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-e40f3776-7830-45f2-9103-c1bee8c04481 08/24/23 11:36:01.315
    STEP: Creating a pod to test consume configMaps 08/24/23 11:36:01.322
    Aug 24 11:36:01.344: INFO: Waiting up to 5m0s for pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a" in namespace "configmap-3736" to be "Succeeded or Failed"
    Aug 24 11:36:01.350: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225703ms
    Aug 24 11:36:03.354: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010424358s
    Aug 24 11:36:05.357: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013470099s
    STEP: Saw pod success 08/24/23 11:36:05.357
    Aug 24 11:36:05.357: INFO: Pod "pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a" satisfied condition "Succeeded or Failed"
    Aug 24 11:36:05.360: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:36:05.37
    Aug 24 11:36:05.393: INFO: Waiting for pod pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a to disappear
    Aug 24 11:36:05.398: INFO: Pod pod-configmaps-c39fe1b6-673e-4ce2-a16e-0a6b86c20b2a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:36:05.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3736" for this suite. 08/24/23 11:36:05.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:36:05.413
Aug 24 11:36:05.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 11:36:05.418
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:05.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:05.449
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 11:36:05.456
Aug 24 11:36:05.470: INFO: Waiting up to 5m0s for pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e" in namespace "emptydir-9367" to be "Succeeded or Failed"
Aug 24 11:36:05.488: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.473715ms
Aug 24 11:36:07.493: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023238283s
Aug 24 11:36:09.492: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021891546s
STEP: Saw pod success 08/24/23 11:36:09.492
Aug 24 11:36:09.492: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e" satisfied condition "Succeeded or Failed"
Aug 24 11:36:09.494: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-69f2c603-dea9-4444-9303-15ce0118c99e container test-container: <nil>
STEP: delete the pod 08/24/23 11:36:09.502
Aug 24 11:36:09.530: INFO: Waiting for pod pod-69f2c603-dea9-4444-9303-15ce0118c99e to disappear
Aug 24 11:36:09.538: INFO: Pod pod-69f2c603-dea9-4444-9303-15ce0118c99e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:36:09.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9367" for this suite. 08/24/23 11:36:09.543
------------------------------
â€¢ [4.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:36:05.413
    Aug 24 11:36:05.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:36:05.418
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:05.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:05.449
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 11:36:05.456
    Aug 24 11:36:05.470: INFO: Waiting up to 5m0s for pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e" in namespace "emptydir-9367" to be "Succeeded or Failed"
    Aug 24 11:36:05.488: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.473715ms
    Aug 24 11:36:07.493: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023238283s
    Aug 24 11:36:09.492: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021891546s
    STEP: Saw pod success 08/24/23 11:36:09.492
    Aug 24 11:36:09.492: INFO: Pod "pod-69f2c603-dea9-4444-9303-15ce0118c99e" satisfied condition "Succeeded or Failed"
    Aug 24 11:36:09.494: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-69f2c603-dea9-4444-9303-15ce0118c99e container test-container: <nil>
    STEP: delete the pod 08/24/23 11:36:09.502
    Aug 24 11:36:09.530: INFO: Waiting for pod pod-69f2c603-dea9-4444-9303-15ce0118c99e to disappear
    Aug 24 11:36:09.538: INFO: Pod pod-69f2c603-dea9-4444-9303-15ce0118c99e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:36:09.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9367" for this suite. 08/24/23 11:36:09.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:36:09.552
Aug 24 11:36:09.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:36:09.553
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:09.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:09.591
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-2921 08/24/23 11:36:09.6
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[] 08/24/23 11:36:09.617
Aug 24 11:36:09.648: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2921 08/24/23 11:36:09.648
Aug 24 11:36:09.662: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2921" to be "running and ready"
Aug 24 11:36:09.670: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.853404ms
Aug 24 11:36:09.671: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:36:11.675: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013510474s
Aug 24 11:36:11.675: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 24 11:36:11.675: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[pod1:[80]] 08/24/23 11:36:11.678
Aug 24 11:36:11.687: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/24/23 11:36:11.687
Aug 24 11:36:11.687: INFO: Creating new exec pod
Aug 24 11:36:11.699: INFO: Waiting up to 5m0s for pod "execpod45pdx" in namespace "services-2921" to be "running"
Aug 24 11:36:11.710: INFO: Pod "execpod45pdx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.303009ms
Aug 24 11:36:13.714: INFO: Pod "execpod45pdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.014228104s
Aug 24 11:36:13.714: INFO: Pod "execpod45pdx" satisfied condition "running"
Aug 24 11:36:14.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 24 11:36:14.992: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 24 11:36:14.992: INFO: stdout: ""
Aug 24 11:36:14.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 10.254.30.111 80'
Aug 24 11:36:15.238: INFO: stderr: "+ nc -v -z -w 2 10.254.30.111 80\nConnection to 10.254.30.111 80 port [tcp/http] succeeded!\n"
Aug 24 11:36:15.238: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-2921 08/24/23 11:36:15.238
Aug 24 11:36:15.250: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2921" to be "running and ready"
Aug 24 11:36:15.259: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.54524ms
Aug 24 11:36:15.259: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:36:17.263: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013333305s
Aug 24 11:36:17.263: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 24 11:36:17.263: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[pod1:[80] pod2:[80]] 08/24/23 11:36:17.266
Aug 24 11:36:17.277: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/24/23 11:36:17.277
Aug 24 11:36:18.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 24 11:36:18.539: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 24 11:36:18.539: INFO: stdout: ""
Aug 24 11:36:18.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 10.254.30.111 80'
Aug 24 11:36:18.790: INFO: stderr: "+ nc -v -z -w 2 10.254.30.111 80\nConnection to 10.254.30.111 80 port [tcp/http] succeeded!\n"
Aug 24 11:36:18.790: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2921 08/24/23 11:36:18.79
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[pod2:[80]] 08/24/23 11:36:18.837
Aug 24 11:36:18.863: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/24/23 11:36:18.863
Aug 24 11:36:19.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 24 11:36:20.107: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 24 11:36:20.107: INFO: stdout: ""
Aug 24 11:36:20.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 10.254.30.111 80'
Aug 24 11:36:20.353: INFO: stderr: "+ nc -v -z -w 2 10.254.30.111 80\nConnection to 10.254.30.111 80 port [tcp/http] succeeded!\n"
Aug 24 11:36:20.354: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-2921 08/24/23 11:36:20.354
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[] 08/24/23 11:36:20.379
Aug 24 11:36:21.406: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:36:21.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2921" for this suite. 08/24/23 11:36:21.476
------------------------------
â€¢ [SLOW TEST] [11.942 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:36:09.552
    Aug 24 11:36:09.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:36:09.553
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:09.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:09.591
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-2921 08/24/23 11:36:09.6
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[] 08/24/23 11:36:09.617
    Aug 24 11:36:09.648: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2921 08/24/23 11:36:09.648
    Aug 24 11:36:09.662: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2921" to be "running and ready"
    Aug 24 11:36:09.670: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.853404ms
    Aug 24 11:36:09.671: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:36:11.675: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013510474s
    Aug 24 11:36:11.675: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 24 11:36:11.675: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[pod1:[80]] 08/24/23 11:36:11.678
    Aug 24 11:36:11.687: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/24/23 11:36:11.687
    Aug 24 11:36:11.687: INFO: Creating new exec pod
    Aug 24 11:36:11.699: INFO: Waiting up to 5m0s for pod "execpod45pdx" in namespace "services-2921" to be "running"
    Aug 24 11:36:11.710: INFO: Pod "execpod45pdx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.303009ms
    Aug 24 11:36:13.714: INFO: Pod "execpod45pdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.014228104s
    Aug 24 11:36:13.714: INFO: Pod "execpod45pdx" satisfied condition "running"
    Aug 24 11:36:14.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 24 11:36:14.992: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 24 11:36:14.992: INFO: stdout: ""
    Aug 24 11:36:14.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 10.254.30.111 80'
    Aug 24 11:36:15.238: INFO: stderr: "+ nc -v -z -w 2 10.254.30.111 80\nConnection to 10.254.30.111 80 port [tcp/http] succeeded!\n"
    Aug 24 11:36:15.238: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-2921 08/24/23 11:36:15.238
    Aug 24 11:36:15.250: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2921" to be "running and ready"
    Aug 24 11:36:15.259: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.54524ms
    Aug 24 11:36:15.259: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:36:17.263: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013333305s
    Aug 24 11:36:17.263: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 24 11:36:17.263: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[pod1:[80] pod2:[80]] 08/24/23 11:36:17.266
    Aug 24 11:36:17.277: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/24/23 11:36:17.277
    Aug 24 11:36:18.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 24 11:36:18.539: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 24 11:36:18.539: INFO: stdout: ""
    Aug 24 11:36:18.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 10.254.30.111 80'
    Aug 24 11:36:18.790: INFO: stderr: "+ nc -v -z -w 2 10.254.30.111 80\nConnection to 10.254.30.111 80 port [tcp/http] succeeded!\n"
    Aug 24 11:36:18.790: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2921 08/24/23 11:36:18.79
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[pod2:[80]] 08/24/23 11:36:18.837
    Aug 24 11:36:18.863: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/24/23 11:36:18.863
    Aug 24 11:36:19.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 24 11:36:20.107: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 24 11:36:20.107: INFO: stdout: ""
    Aug 24 11:36:20.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-2921 exec execpod45pdx -- /bin/sh -x -c nc -v -z -w 2 10.254.30.111 80'
    Aug 24 11:36:20.353: INFO: stderr: "+ nc -v -z -w 2 10.254.30.111 80\nConnection to 10.254.30.111 80 port [tcp/http] succeeded!\n"
    Aug 24 11:36:20.354: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-2921 08/24/23 11:36:20.354
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2921 to expose endpoints map[] 08/24/23 11:36:20.379
    Aug 24 11:36:21.406: INFO: successfully validated that service endpoint-test2 in namespace services-2921 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:36:21.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2921" for this suite. 08/24/23 11:36:21.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:36:21.496
Aug 24 11:36:21.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename security-context 08/24/23 11:36:21.497
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:21.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:21.562
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 11:36:21.567
Aug 24 11:36:21.579: INFO: Waiting up to 5m0s for pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee" in namespace "security-context-9446" to be "Succeeded or Failed"
Aug 24 11:36:21.587: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.753002ms
Aug 24 11:36:23.591: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011604167s
Aug 24 11:36:25.593: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01343486s
STEP: Saw pod success 08/24/23 11:36:25.593
Aug 24 11:36:25.593: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee" satisfied condition "Succeeded or Failed"
Aug 24 11:36:25.597: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod security-context-cdee7721-6274-414f-ae26-d802f70f9eee container test-container: <nil>
STEP: delete the pod 08/24/23 11:36:25.605
Aug 24 11:36:25.637: INFO: Waiting for pod security-context-cdee7721-6274-414f-ae26-d802f70f9eee to disappear
Aug 24 11:36:25.642: INFO: Pod security-context-cdee7721-6274-414f-ae26-d802f70f9eee no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 11:36:25.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-9446" for this suite. 08/24/23 11:36:25.647
------------------------------
â€¢ [4.164 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:36:21.496
    Aug 24 11:36:21.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename security-context 08/24/23 11:36:21.497
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:21.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:21.562
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/24/23 11:36:21.567
    Aug 24 11:36:21.579: INFO: Waiting up to 5m0s for pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee" in namespace "security-context-9446" to be "Succeeded or Failed"
    Aug 24 11:36:21.587: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.753002ms
    Aug 24 11:36:23.591: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011604167s
    Aug 24 11:36:25.593: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01343486s
    STEP: Saw pod success 08/24/23 11:36:25.593
    Aug 24 11:36:25.593: INFO: Pod "security-context-cdee7721-6274-414f-ae26-d802f70f9eee" satisfied condition "Succeeded or Failed"
    Aug 24 11:36:25.597: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod security-context-cdee7721-6274-414f-ae26-d802f70f9eee container test-container: <nil>
    STEP: delete the pod 08/24/23 11:36:25.605
    Aug 24 11:36:25.637: INFO: Waiting for pod security-context-cdee7721-6274-414f-ae26-d802f70f9eee to disappear
    Aug 24 11:36:25.642: INFO: Pod security-context-cdee7721-6274-414f-ae26-d802f70f9eee no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:36:25.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-9446" for this suite. 08/24/23 11:36:25.647
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:36:25.662
Aug 24 11:36:25.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 11:36:25.663
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:25.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:25.7
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-5337ba86-c4da-4f92-b86a-778beb1b7959 in namespace container-probe-1029 08/24/23 11:36:25.706
Aug 24 11:36:25.722: INFO: Waiting up to 5m0s for pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959" in namespace "container-probe-1029" to be "not pending"
Aug 24 11:36:25.741: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959": Phase="Pending", Reason="", readiness=false. Elapsed: 19.5691ms
Aug 24 11:36:27.749: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026873473s
Aug 24 11:36:29.749: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959": Phase="Running", Reason="", readiness=true. Elapsed: 4.026716123s
Aug 24 11:36:29.749: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959" satisfied condition "not pending"
Aug 24 11:36:29.749: INFO: Started pod liveness-5337ba86-c4da-4f92-b86a-778beb1b7959 in namespace container-probe-1029
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:36:29.749
Aug 24 11:36:29.752: INFO: Initial restart count of pod liveness-5337ba86-c4da-4f92-b86a-778beb1b7959 is 0
STEP: deleting the pod 08/24/23 11:40:30.424
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:30.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1029" for this suite. 08/24/23 11:40:30.465
------------------------------
â€¢ [SLOW TEST] [244.813 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:36:25.662
    Aug 24 11:36:25.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 11:36:25.663
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:36:25.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:36:25.7
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-5337ba86-c4da-4f92-b86a-778beb1b7959 in namespace container-probe-1029 08/24/23 11:36:25.706
    Aug 24 11:36:25.722: INFO: Waiting up to 5m0s for pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959" in namespace "container-probe-1029" to be "not pending"
    Aug 24 11:36:25.741: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959": Phase="Pending", Reason="", readiness=false. Elapsed: 19.5691ms
    Aug 24 11:36:27.749: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026873473s
    Aug 24 11:36:29.749: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959": Phase="Running", Reason="", readiness=true. Elapsed: 4.026716123s
    Aug 24 11:36:29.749: INFO: Pod "liveness-5337ba86-c4da-4f92-b86a-778beb1b7959" satisfied condition "not pending"
    Aug 24 11:36:29.749: INFO: Started pod liveness-5337ba86-c4da-4f92-b86a-778beb1b7959 in namespace container-probe-1029
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 11:36:29.749
    Aug 24 11:36:29.752: INFO: Initial restart count of pod liveness-5337ba86-c4da-4f92-b86a-778beb1b7959 is 0
    STEP: deleting the pod 08/24/23 11:40:30.424
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:30.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1029" for this suite. 08/24/23 11:40:30.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:30.48
Aug 24 11:40:30.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 11:40:30.482
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:30.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:30.515
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:40:30.558
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:40:31.268
STEP: Deploying the webhook pod 08/24/23 11:40:31.28
STEP: Wait for the deployment to be ready 08/24/23 11:40:31.315
Aug 24 11:40:31.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 24 11:40:33.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/24/23 11:40:35.364
STEP: Verifying the service has paired with the endpoint 08/24/23 11:40:35.383
Aug 24 11:40:36.384: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/24/23 11:40:36.389
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:36.39
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/24/23 11:40:36.415
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/24/23 11:40:37.431
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:37.431
STEP: Having no error when timeout is longer than webhook latency 08/24/23 11:40:38.482
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:38.482
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/24/23 11:40:43.539
STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:43.539
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:48.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6974" for this suite. 08/24/23 11:40:48.682
STEP: Destroying namespace "webhook-6974-markers" for this suite. 08/24/23 11:40:48.7
------------------------------
â€¢ [SLOW TEST] [18.240 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:30.48
    Aug 24 11:40:30.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 11:40:30.482
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:30.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:30.515
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:40:30.558
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:40:31.268
    STEP: Deploying the webhook pod 08/24/23 11:40:31.28
    STEP: Wait for the deployment to be ready 08/24/23 11:40:31.315
    Aug 24 11:40:31.339: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 24 11:40:33.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 40, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/24/23 11:40:35.364
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:40:35.383
    Aug 24 11:40:36.384: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/24/23 11:40:36.389
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:36.39
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/24/23 11:40:36.415
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/24/23 11:40:37.431
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:37.431
    STEP: Having no error when timeout is longer than webhook latency 08/24/23 11:40:38.482
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:38.482
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/24/23 11:40:43.539
    STEP: Registering slow webhook via the AdmissionRegistration API 08/24/23 11:40:43.539
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:48.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6974" for this suite. 08/24/23 11:40:48.682
    STEP: Destroying namespace "webhook-6974-markers" for this suite. 08/24/23 11:40:48.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:48.721
Aug 24 11:40:48.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename gc 08/24/23 11:40:48.723
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:48.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:48.824
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/24/23 11:40:48.974
STEP: delete the rc 08/24/23 11:40:54.108
STEP: wait for all pods to be garbage collected 08/24/23 11:40:54.118
STEP: Gathering metrics 08/24/23 11:40:59.128
W0824 11:40:59.143717      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 24 11:40:59.143: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 11:40:59.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8049" for this suite. 08/24/23 11:40:59.148
------------------------------
â€¢ [SLOW TEST] [10.439 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:48.721
    Aug 24 11:40:48.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename gc 08/24/23 11:40:48.723
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:48.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:48.824
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/24/23 11:40:48.974
    STEP: delete the rc 08/24/23 11:40:54.108
    STEP: wait for all pods to be garbage collected 08/24/23 11:40:54.118
    STEP: Gathering metrics 08/24/23 11:40:59.128
    W0824 11:40:59.143717      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 24 11:40:59.143: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:40:59.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8049" for this suite. 08/24/23 11:40:59.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:40:59.162
Aug 24 11:40:59.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 11:40:59.164
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:59.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:59.195
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-6173/secret-test-56dd190f-1b61-4f40-976f-fb017c519f0d 08/24/23 11:40:59.202
STEP: Creating a pod to test consume secrets 08/24/23 11:40:59.209
Aug 24 11:40:59.230: INFO: Waiting up to 5m0s for pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e" in namespace "secrets-6173" to be "Succeeded or Failed"
Aug 24 11:40:59.248: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.102003ms
Aug 24 11:41:01.252: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022480314s
Aug 24 11:41:03.256: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025617357s
STEP: Saw pod success 08/24/23 11:41:03.256
Aug 24 11:41:03.256: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e" satisfied condition "Succeeded or Failed"
Aug 24 11:41:03.259: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e container env-test: <nil>
STEP: delete the pod 08/24/23 11:41:03.317
Aug 24 11:41:03.340: INFO: Waiting for pod pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e to disappear
Aug 24 11:41:03.352: INFO: Pod pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:03.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6173" for this suite. 08/24/23 11:41:03.367
------------------------------
â€¢ [4.219 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:40:59.162
    Aug 24 11:40:59.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 11:40:59.164
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:40:59.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:40:59.195
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-6173/secret-test-56dd190f-1b61-4f40-976f-fb017c519f0d 08/24/23 11:40:59.202
    STEP: Creating a pod to test consume secrets 08/24/23 11:40:59.209
    Aug 24 11:40:59.230: INFO: Waiting up to 5m0s for pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e" in namespace "secrets-6173" to be "Succeeded or Failed"
    Aug 24 11:40:59.248: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.102003ms
    Aug 24 11:41:01.252: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022480314s
    Aug 24 11:41:03.256: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025617357s
    STEP: Saw pod success 08/24/23 11:41:03.256
    Aug 24 11:41:03.256: INFO: Pod "pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e" satisfied condition "Succeeded or Failed"
    Aug 24 11:41:03.259: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e container env-test: <nil>
    STEP: delete the pod 08/24/23 11:41:03.317
    Aug 24 11:41:03.340: INFO: Waiting for pod pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e to disappear
    Aug 24 11:41:03.352: INFO: Pod pod-configmaps-f32e41e7-208d-4ff5-8941-52e67f3bad2e no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:03.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6173" for this suite. 08/24/23 11:41:03.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:03.386
Aug 24 11:41:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename containers 08/24/23 11:41:03.387
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:03.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:03.417
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 24 11:41:03.439: INFO: Waiting up to 5m0s for pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c" in namespace "containers-9407" to be "running"
Aug 24 11:41:03.444: INFO: Pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096618ms
Aug 24 11:41:05.449: INFO: Pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010243697s
Aug 24 11:41:05.449: INFO: Pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:05.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9407" for this suite. 08/24/23 11:41:05.462
------------------------------
â€¢ [2.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:03.386
    Aug 24 11:41:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename containers 08/24/23 11:41:03.387
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:03.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:03.417
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 24 11:41:03.439: INFO: Waiting up to 5m0s for pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c" in namespace "containers-9407" to be "running"
    Aug 24 11:41:03.444: INFO: Pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096618ms
    Aug 24 11:41:05.449: INFO: Pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010243697s
    Aug 24 11:41:05.449: INFO: Pod "client-containers-17afc8f4-c8d7-449b-9a92-b60d3542828c" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:05.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9407" for this suite. 08/24/23 11:41:05.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:05.474
Aug 24 11:41:05.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sysctl 08/24/23 11:41:05.475
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.506
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/24/23 11:41:05.514
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:05.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8854" for this suite. 08/24/23 11:41:05.527
------------------------------
â€¢ [0.064 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:05.474
    Aug 24 11:41:05.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sysctl 08/24/23 11:41:05.475
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.506
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/24/23 11:41:05.514
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:05.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8854" for this suite. 08/24/23 11:41:05.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:05.541
Aug 24 11:41:05.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename tables 08/24/23 11:41:05.543
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.576
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:05.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3469" for this suite. 08/24/23 11:41:05.597
------------------------------
â€¢ [0.067 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:05.541
    Aug 24 11:41:05.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename tables 08/24/23 11:41:05.543
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.576
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:05.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3469" for this suite. 08/24/23 11:41:05.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:05.61
Aug 24 11:41:05.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 11:41:05.612
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.639
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:05.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6764" for this suite. 08/24/23 11:41:05.714
------------------------------
â€¢ [0.111 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:05.61
    Aug 24 11:41:05.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 11:41:05.612
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.639
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:05.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6764" for this suite. 08/24/23 11:41:05.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:05.724
Aug 24 11:41:05.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pod-network-test 08/24/23 11:41:05.726
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.759
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3607 08/24/23 11:41:05.768
STEP: creating a selector 08/24/23 11:41:05.768
STEP: Creating the service pods in kubernetes 08/24/23 11:41:05.768
Aug 24 11:41:05.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 11:41:05.826: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3607" to be "running and ready"
Aug 24 11:41:05.838: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.13024ms
Aug 24 11:41:05.838: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:41:07.843: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01739528s
Aug 24 11:41:07.843: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:41:09.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018037107s
Aug 24 11:41:09.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:11.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017532778s
Aug 24 11:41:11.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:13.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016216722s
Aug 24 11:41:13.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:15.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016961361s
Aug 24 11:41:15.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:17.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01654288s
Aug 24 11:41:17.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:19.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018029994s
Aug 24 11:41:19.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:21.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016298736s
Aug 24 11:41:21.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:23.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019663388s
Aug 24 11:41:23.846: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:25.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019569634s
Aug 24 11:41:25.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 11:41:27.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016385516s
Aug 24 11:41:27.842: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 11:41:27.842: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 11:41:27.845: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3607" to be "running and ready"
Aug 24 11:41:27.849: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.814177ms
Aug 24 11:41:27.849: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 11:41:27.849: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 11:41:27.851: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3607" to be "running and ready"
Aug 24 11:41:27.855: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.545447ms
Aug 24 11:41:27.855: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 11:41:27.855: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 11:41:27.858
Aug 24 11:41:27.867: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3607" to be "running"
Aug 24 11:41:27.882: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.454125ms
Aug 24 11:41:29.888: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020131218s
Aug 24 11:41:29.888: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 11:41:29.892: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 11:41:29.892: INFO: Breadth first check of 10.100.148.227 on host 10.0.0.4...
Aug 24 11:41:29.895: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.152:9080/dial?request=hostname&protocol=http&host=10.100.148.227&port=8083&tries=1'] Namespace:pod-network-test-3607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:41:29.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:41:29.897: INFO: ExecWithOptions: Clientset creation
Aug 24 11:41:29.897: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-3607/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.148.227%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 11:41:30.040: INFO: Waiting for responses: map[]
Aug 24 11:41:30.040: INFO: reached 10.100.148.227 after 0/1 tries
Aug 24 11:41:30.040: INFO: Breadth first check of 10.100.181.185 on host 10.0.0.18...
Aug 24 11:41:30.044: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.152:9080/dial?request=hostname&protocol=http&host=10.100.181.185&port=8083&tries=1'] Namespace:pod-network-test-3607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:41:30.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:41:30.045: INFO: ExecWithOptions: Clientset creation
Aug 24 11:41:30.045: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-3607/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.181.185%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 11:41:30.177: INFO: Waiting for responses: map[]
Aug 24 11:41:30.177: INFO: reached 10.100.181.185 after 0/1 tries
Aug 24 11:41:30.177: INFO: Breadth first check of 10.100.45.141 on host 10.0.0.17...
Aug 24 11:41:30.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.152:9080/dial?request=hostname&protocol=http&host=10.100.45.141&port=8083&tries=1'] Namespace:pod-network-test-3607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:41:30.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:41:30.182: INFO: ExecWithOptions: Clientset creation
Aug 24 11:41:30.183: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-3607/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.45.141%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 11:41:30.339: INFO: Waiting for responses: map[]
Aug 24 11:41:30.339: INFO: reached 10.100.45.141 after 0/1 tries
Aug 24 11:41:30.339: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:30.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3607" for this suite. 08/24/23 11:41:30.345
------------------------------
â€¢ [SLOW TEST] [24.636 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:05.724
    Aug 24 11:41:05.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 11:41:05.726
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:05.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:05.759
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3607 08/24/23 11:41:05.768
    STEP: creating a selector 08/24/23 11:41:05.768
    STEP: Creating the service pods in kubernetes 08/24/23 11:41:05.768
    Aug 24 11:41:05.768: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 11:41:05.826: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3607" to be "running and ready"
    Aug 24 11:41:05.838: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.13024ms
    Aug 24 11:41:05.838: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:41:07.843: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01739528s
    Aug 24 11:41:07.843: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:41:09.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.018037107s
    Aug 24 11:41:09.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:11.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017532778s
    Aug 24 11:41:11.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:13.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016216722s
    Aug 24 11:41:13.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:15.843: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016961361s
    Aug 24 11:41:15.843: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:17.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01654288s
    Aug 24 11:41:17.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:19.844: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.018029994s
    Aug 24 11:41:19.844: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:21.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016298736s
    Aug 24 11:41:21.842: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:23.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019663388s
    Aug 24 11:41:23.846: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:25.845: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019569634s
    Aug 24 11:41:25.845: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 11:41:27.842: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.016385516s
    Aug 24 11:41:27.842: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 11:41:27.842: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 11:41:27.845: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3607" to be "running and ready"
    Aug 24 11:41:27.849: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.814177ms
    Aug 24 11:41:27.849: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 11:41:27.849: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 11:41:27.851: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3607" to be "running and ready"
    Aug 24 11:41:27.855: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.545447ms
    Aug 24 11:41:27.855: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 11:41:27.855: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 11:41:27.858
    Aug 24 11:41:27.867: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3607" to be "running"
    Aug 24 11:41:27.882: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.454125ms
    Aug 24 11:41:29.888: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020131218s
    Aug 24 11:41:29.888: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 11:41:29.892: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 11:41:29.892: INFO: Breadth first check of 10.100.148.227 on host 10.0.0.4...
    Aug 24 11:41:29.895: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.152:9080/dial?request=hostname&protocol=http&host=10.100.148.227&port=8083&tries=1'] Namespace:pod-network-test-3607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:41:29.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:41:29.897: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:41:29.897: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-3607/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.148.227%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 11:41:30.040: INFO: Waiting for responses: map[]
    Aug 24 11:41:30.040: INFO: reached 10.100.148.227 after 0/1 tries
    Aug 24 11:41:30.040: INFO: Breadth first check of 10.100.181.185 on host 10.0.0.18...
    Aug 24 11:41:30.044: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.152:9080/dial?request=hostname&protocol=http&host=10.100.181.185&port=8083&tries=1'] Namespace:pod-network-test-3607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:41:30.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:41:30.045: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:41:30.045: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-3607/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.181.185%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 11:41:30.177: INFO: Waiting for responses: map[]
    Aug 24 11:41:30.177: INFO: reached 10.100.181.185 after 0/1 tries
    Aug 24 11:41:30.177: INFO: Breadth first check of 10.100.45.141 on host 10.0.0.17...
    Aug 24 11:41:30.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.152:9080/dial?request=hostname&protocol=http&host=10.100.45.141&port=8083&tries=1'] Namespace:pod-network-test-3607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:41:30.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:41:30.182: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:41:30.183: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-3607/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.45.141%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 11:41:30.339: INFO: Waiting for responses: map[]
    Aug 24 11:41:30.339: INFO: reached 10.100.45.141 after 0/1 tries
    Aug 24 11:41:30.339: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:30.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3607" for this suite. 08/24/23 11:41:30.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:30.362
Aug 24 11:41:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 11:41:30.364
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:30.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:30.392
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/24/23 11:41:30.399
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/24/23 11:41:30.411
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/24/23 11:41:30.411
STEP: creating a pod to probe DNS 08/24/23 11:41:30.411
STEP: submitting the pod to kubernetes 08/24/23 11:41:30.411
Aug 24 11:41:30.437: INFO: Waiting up to 15m0s for pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133" in namespace "dns-7480" to be "running"
Aug 24 11:41:30.449: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133": Phase="Pending", Reason="", readiness=false. Elapsed: 11.422319ms
Aug 24 11:41:32.460: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022569843s
Aug 24 11:41:34.460: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133": Phase="Running", Reason="", readiness=true. Elapsed: 4.023340491s
Aug 24 11:41:34.461: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133" satisfied condition "running"
STEP: retrieving the pod 08/24/23 11:41:34.461
STEP: looking for the results for each expected name from probers 08/24/23 11:41:34.467
Aug 24 11:41:34.485: INFO: DNS probes using dns-7480/dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133 succeeded

STEP: deleting the pod 08/24/23 11:41:34.485
STEP: deleting the test headless service 08/24/23 11:41:34.532
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:34.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7480" for this suite. 08/24/23 11:41:34.633
------------------------------
â€¢ [4.295 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:30.362
    Aug 24 11:41:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 11:41:30.364
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:30.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:30.392
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/24/23 11:41:30.399
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/24/23 11:41:30.411
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7480.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/24/23 11:41:30.411
    STEP: creating a pod to probe DNS 08/24/23 11:41:30.411
    STEP: submitting the pod to kubernetes 08/24/23 11:41:30.411
    Aug 24 11:41:30.437: INFO: Waiting up to 15m0s for pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133" in namespace "dns-7480" to be "running"
    Aug 24 11:41:30.449: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133": Phase="Pending", Reason="", readiness=false. Elapsed: 11.422319ms
    Aug 24 11:41:32.460: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022569843s
    Aug 24 11:41:34.460: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133": Phase="Running", Reason="", readiness=true. Elapsed: 4.023340491s
    Aug 24 11:41:34.461: INFO: Pod "dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 11:41:34.461
    STEP: looking for the results for each expected name from probers 08/24/23 11:41:34.467
    Aug 24 11:41:34.485: INFO: DNS probes using dns-7480/dns-test-5f1723e5-96ba-4dfc-b0a7-11b7d7b79133 succeeded

    STEP: deleting the pod 08/24/23 11:41:34.485
    STEP: deleting the test headless service 08/24/23 11:41:34.532
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:34.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7480" for this suite. 08/24/23 11:41:34.633
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:34.658
Aug 24 11:41:34.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename init-container 08/24/23 11:41:34.66
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:34.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:34.69
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/24/23 11:41:34.698
Aug 24 11:41:34.698: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6949" for this suite. 08/24/23 11:41:37.809
------------------------------
â€¢ [3.160 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:34.658
    Aug 24 11:41:34.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename init-container 08/24/23 11:41:34.66
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:34.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:34.69
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/24/23 11:41:34.698
    Aug 24 11:41:34.698: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6949" for this suite. 08/24/23 11:41:37.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:37.828
Aug 24 11:41:37.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 11:41:37.829
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:37.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:37.851
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 24 11:41:37.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: creating the pod 08/24/23 11:41:37.858
STEP: submitting the pod to kubernetes 08/24/23 11:41:37.859
Aug 24 11:41:37.875: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81" in namespace "pods-8830" to be "running and ready"
Aug 24 11:41:37.885: INFO: Pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81": Phase="Pending", Reason="", readiness=false. Elapsed: 9.354594ms
Aug 24 11:41:37.885: INFO: The phase of Pod pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:41:39.891: INFO: Pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81": Phase="Running", Reason="", readiness=true. Elapsed: 2.016269689s
Aug 24 11:41:39.892: INFO: The phase of Pod pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81 is Running (Ready = true)
Aug 24 11:41:39.892: INFO: Pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:39.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8830" for this suite. 08/24/23 11:41:39.944
------------------------------
â€¢ [2.125 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:37.828
    Aug 24 11:41:37.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 11:41:37.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:37.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:37.851
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 24 11:41:37.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: creating the pod 08/24/23 11:41:37.858
    STEP: submitting the pod to kubernetes 08/24/23 11:41:37.859
    Aug 24 11:41:37.875: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81" in namespace "pods-8830" to be "running and ready"
    Aug 24 11:41:37.885: INFO: Pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81": Phase="Pending", Reason="", readiness=false. Elapsed: 9.354594ms
    Aug 24 11:41:37.885: INFO: The phase of Pod pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:41:39.891: INFO: Pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81": Phase="Running", Reason="", readiness=true. Elapsed: 2.016269689s
    Aug 24 11:41:39.892: INFO: The phase of Pod pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81 is Running (Ready = true)
    Aug 24 11:41:39.892: INFO: Pod "pod-logs-websocket-ed28804e-1e1e-41b6-bd65-920ca6529a81" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:39.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8830" for this suite. 08/24/23 11:41:39.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:39.954
Aug 24 11:41:39.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:41:39.956
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:39.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:39.996
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:41:40.008
Aug 24 11:41:40.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed" in namespace "downward-api-5458" to be "Succeeded or Failed"
Aug 24 11:41:40.042: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.672877ms
Aug 24 11:41:42.046: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015583101s
Aug 24 11:41:44.046: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015348072s
STEP: Saw pod success 08/24/23 11:41:44.046
Aug 24 11:41:44.047: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed" satisfied condition "Succeeded or Failed"
Aug 24 11:41:44.049: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed container client-container: <nil>
STEP: delete the pod 08/24/23 11:41:44.057
Aug 24 11:41:44.075: INFO: Waiting for pod downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed to disappear
Aug 24 11:41:44.080: INFO: Pod downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 11:41:44.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5458" for this suite. 08/24/23 11:41:44.084
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:39.954
    Aug 24 11:41:39.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:41:39.956
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:39.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:39.996
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:41:40.008
    Aug 24 11:41:40.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed" in namespace "downward-api-5458" to be "Succeeded or Failed"
    Aug 24 11:41:40.042: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.672877ms
    Aug 24 11:41:42.046: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015583101s
    Aug 24 11:41:44.046: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015348072s
    STEP: Saw pod success 08/24/23 11:41:44.046
    Aug 24 11:41:44.047: INFO: Pod "downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed" satisfied condition "Succeeded or Failed"
    Aug 24 11:41:44.049: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed container client-container: <nil>
    STEP: delete the pod 08/24/23 11:41:44.057
    Aug 24 11:41:44.075: INFO: Waiting for pod downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed to disappear
    Aug 24 11:41:44.080: INFO: Pod downwardapi-volume-816ae518-45e4-4c6d-ac0d-23f9e777f5ed no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:41:44.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5458" for this suite. 08/24/23 11:41:44.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:41:44.1
Aug 24 11:41:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename subpath 08/24/23 11:41:44.102
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:44.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:44.13
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 11:41:44.138
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-l8lk 08/24/23 11:41:44.164
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:41:44.164
Aug 24 11:41:44.179: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l8lk" in namespace "subpath-7999" to be "Succeeded or Failed"
Aug 24 11:41:44.191: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Pending", Reason="", readiness=false. Elapsed: 12.305273ms
Aug 24 11:41:46.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01695947s
Aug 24 11:41:48.198: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 4.018920695s
Aug 24 11:41:50.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 6.017373s
Aug 24 11:41:52.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 8.016468161s
Aug 24 11:41:54.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 10.016438122s
Aug 24 11:41:56.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 12.016261775s
Aug 24 11:41:58.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 14.01677905s
Aug 24 11:42:00.197: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 16.017729147s
Aug 24 11:42:02.198: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 18.018907442s
Aug 24 11:42:04.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 20.017389615s
Aug 24 11:42:06.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=false. Elapsed: 22.016416093s
Aug 24 11:42:08.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016681419s
STEP: Saw pod success 08/24/23 11:42:08.196
Aug 24 11:42:08.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk" satisfied condition "Succeeded or Failed"
Aug 24 11:42:08.199: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-subpath-test-downwardapi-l8lk container test-container-subpath-downwardapi-l8lk: <nil>
STEP: delete the pod 08/24/23 11:42:08.266
Aug 24 11:42:08.289: INFO: Waiting for pod pod-subpath-test-downwardapi-l8lk to disappear
Aug 24 11:42:08.294: INFO: Pod pod-subpath-test-downwardapi-l8lk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-l8lk 08/24/23 11:42:08.294
Aug 24 11:42:08.294: INFO: Deleting pod "pod-subpath-test-downwardapi-l8lk" in namespace "subpath-7999"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:08.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7999" for this suite. 08/24/23 11:42:08.304
------------------------------
â€¢ [SLOW TEST] [24.214 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:41:44.1
    Aug 24 11:41:44.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename subpath 08/24/23 11:41:44.102
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:41:44.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:41:44.13
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 11:41:44.138
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-l8lk 08/24/23 11:41:44.164
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:41:44.164
    Aug 24 11:41:44.179: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l8lk" in namespace "subpath-7999" to be "Succeeded or Failed"
    Aug 24 11:41:44.191: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Pending", Reason="", readiness=false. Elapsed: 12.305273ms
    Aug 24 11:41:46.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01695947s
    Aug 24 11:41:48.198: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 4.018920695s
    Aug 24 11:41:50.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 6.017373s
    Aug 24 11:41:52.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 8.016468161s
    Aug 24 11:41:54.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 10.016438122s
    Aug 24 11:41:56.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 12.016261775s
    Aug 24 11:41:58.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 14.01677905s
    Aug 24 11:42:00.197: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 16.017729147s
    Aug 24 11:42:02.198: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 18.018907442s
    Aug 24 11:42:04.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=true. Elapsed: 20.017389615s
    Aug 24 11:42:06.195: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Running", Reason="", readiness=false. Elapsed: 22.016416093s
    Aug 24 11:42:08.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016681419s
    STEP: Saw pod success 08/24/23 11:42:08.196
    Aug 24 11:42:08.196: INFO: Pod "pod-subpath-test-downwardapi-l8lk" satisfied condition "Succeeded or Failed"
    Aug 24 11:42:08.199: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-subpath-test-downwardapi-l8lk container test-container-subpath-downwardapi-l8lk: <nil>
    STEP: delete the pod 08/24/23 11:42:08.266
    Aug 24 11:42:08.289: INFO: Waiting for pod pod-subpath-test-downwardapi-l8lk to disappear
    Aug 24 11:42:08.294: INFO: Pod pod-subpath-test-downwardapi-l8lk no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-l8lk 08/24/23 11:42:08.294
    Aug 24 11:42:08.294: INFO: Deleting pod "pod-subpath-test-downwardapi-l8lk" in namespace "subpath-7999"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:08.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7999" for this suite. 08/24/23 11:42:08.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:08.315
Aug 24 11:42:08.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 11:42:08.316
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:08.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:08.346
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-30fdd1b8-d8f4-43c4-a1a6-d068436eb45d 08/24/23 11:42:08.358
STEP: Creating the pod 08/24/23 11:42:08.365
Aug 24 11:42:08.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb" in namespace "configmap-6673" to be "running"
Aug 24 11:42:08.400: INFO: Pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.46222ms
Aug 24 11:42:10.405: INFO: Pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb": Phase="Running", Reason="", readiness=false. Elapsed: 2.024508963s
Aug 24 11:42:10.405: INFO: Pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb" satisfied condition "running"
STEP: Waiting for pod with text data 08/24/23 11:42:10.405
STEP: Waiting for pod with binary data 08/24/23 11:42:10.413
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6673" for this suite. 08/24/23 11:42:10.424
------------------------------
â€¢ [2.121 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:08.315
    Aug 24 11:42:08.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 11:42:08.316
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:08.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:08.346
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-30fdd1b8-d8f4-43c4-a1a6-d068436eb45d 08/24/23 11:42:08.358
    STEP: Creating the pod 08/24/23 11:42:08.365
    Aug 24 11:42:08.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb" in namespace "configmap-6673" to be "running"
    Aug 24 11:42:08.400: INFO: Pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.46222ms
    Aug 24 11:42:10.405: INFO: Pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb": Phase="Running", Reason="", readiness=false. Elapsed: 2.024508963s
    Aug 24 11:42:10.405: INFO: Pod "pod-configmaps-0c46f641-74e0-4dd8-b00c-c437fa1a6deb" satisfied condition "running"
    STEP: Waiting for pod with text data 08/24/23 11:42:10.405
    STEP: Waiting for pod with binary data 08/24/23 11:42:10.413
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:10.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6673" for this suite. 08/24/23 11:42:10.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:10.438
Aug 24 11:42:10.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 11:42:10.44
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:10.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:10.459
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:42:10.484
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:42:11.669
STEP: Deploying the webhook pod 08/24/23 11:42:11.68
STEP: Wait for the deployment to be ready 08/24/23 11:42:11.702
Aug 24 11:42:11.716: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 11:42:13.733
STEP: Verifying the service has paired with the endpoint 08/24/23 11:42:13.754
Aug 24 11:42:14.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 24 11:42:14.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-534-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 11:42:15.282
STEP: Creating a custom resource while v1 is storage version 08/24/23 11:42:15.308
STEP: Patching Custom Resource Definition to set v2 as storage 08/24/23 11:42:17.371
STEP: Patching the custom resource while v2 is storage version 08/24/23 11:42:17.579
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:18.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-598" for this suite. 08/24/23 11:42:18.373
STEP: Destroying namespace "webhook-598-markers" for this suite. 08/24/23 11:42:18.461
------------------------------
â€¢ [SLOW TEST] [8.054 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:10.438
    Aug 24 11:42:10.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 11:42:10.44
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:10.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:10.459
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:42:10.484
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:42:11.669
    STEP: Deploying the webhook pod 08/24/23 11:42:11.68
    STEP: Wait for the deployment to be ready 08/24/23 11:42:11.702
    Aug 24 11:42:11.716: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 11:42:13.733
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:42:13.754
    Aug 24 11:42:14.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 24 11:42:14.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-534-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 11:42:15.282
    STEP: Creating a custom resource while v1 is storage version 08/24/23 11:42:15.308
    STEP: Patching Custom Resource Definition to set v2 as storage 08/24/23 11:42:17.371
    STEP: Patching the custom resource while v2 is storage version 08/24/23 11:42:17.579
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:18.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-598" for this suite. 08/24/23 11:42:18.373
    STEP: Destroying namespace "webhook-598-markers" for this suite. 08/24/23 11:42:18.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:18.494
Aug 24 11:42:18.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:42:18.496
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:18.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:18.546
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:42:18.555
Aug 24 11:42:18.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935" in namespace "projected-5609" to be "Succeeded or Failed"
Aug 24 11:42:18.587: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935": Phase="Pending", Reason="", readiness=false. Elapsed: 8.726584ms
Aug 24 11:42:20.591: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012905375s
Aug 24 11:42:22.592: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013395299s
STEP: Saw pod success 08/24/23 11:42:22.592
Aug 24 11:42:22.592: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935" satisfied condition "Succeeded or Failed"
Aug 24 11:42:22.595: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935 container client-container: <nil>
STEP: delete the pod 08/24/23 11:42:22.602
Aug 24 11:42:22.620: INFO: Waiting for pod downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935 to disappear
Aug 24 11:42:22.624: INFO: Pod downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:22.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5609" for this suite. 08/24/23 11:42:22.629
------------------------------
â€¢ [4.141 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:18.494
    Aug 24 11:42:18.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:42:18.496
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:18.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:18.546
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:42:18.555
    Aug 24 11:42:18.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935" in namespace "projected-5609" to be "Succeeded or Failed"
    Aug 24 11:42:18.587: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935": Phase="Pending", Reason="", readiness=false. Elapsed: 8.726584ms
    Aug 24 11:42:20.591: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012905375s
    Aug 24 11:42:22.592: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013395299s
    STEP: Saw pod success 08/24/23 11:42:22.592
    Aug 24 11:42:22.592: INFO: Pod "downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935" satisfied condition "Succeeded or Failed"
    Aug 24 11:42:22.595: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:42:22.602
    Aug 24 11:42:22.620: INFO: Waiting for pod downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935 to disappear
    Aug 24 11:42:22.624: INFO: Pod downwardapi-volume-e08db467-464d-43fe-a7f9-2fb10f2dc935 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:22.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5609" for this suite. 08/24/23 11:42:22.629
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:22.636
Aug 24 11:42:22.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 11:42:22.638
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:22.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:22.668
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 24 11:42:22.674: INFO: Creating deployment "webserver-deployment"
Aug 24 11:42:22.684: INFO: Waiting for observed generation 1
Aug 24 11:42:24.707: INFO: Waiting for all required pods to come up
Aug 24 11:42:24.735: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/24/23 11:42:24.735
Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vnpjk" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-84mmn" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8v6rg" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-96clb" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-f5krg" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k62fp" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lc4mf" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-r7chm" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-scwtf" in namespace "deployment-8515" to be "running"
Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg": Phase="Pending", Reason="", readiness=false. Elapsed: 27.83622ms
Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm": Phase="Pending", Reason="", readiness=false. Elapsed: 27.385162ms
Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk": Phase="Pending", Reason="", readiness=false. Elapsed: 28.477767ms
Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn": Phase="Pending", Reason="", readiness=false. Elapsed: 28.296477ms
Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-96clb": Phase="Pending", Reason="", readiness=false. Elapsed: 27.80362ms
Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf": Phase="Pending", Reason="", readiness=false. Elapsed: 27.816912ms
Aug 24 11:42:24.767: INFO: Pod "webserver-deployment-7f5969cbc7-scwtf": Phase="Pending", Reason="", readiness=false. Elapsed: 29.769737ms
Aug 24 11:42:24.767: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg": Phase="Pending", Reason="", readiness=false. Elapsed: 30.522542ms
Aug 24 11:42:24.767: INFO: Pod "webserver-deployment-7f5969cbc7-k62fp": Phase="Pending", Reason="", readiness=false. Elapsed: 30.465437ms
Aug 24 11:42:26.772: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk": Phase="Running", Reason="", readiness=true. Elapsed: 2.036468929s
Aug 24 11:42:26.772: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk" satisfied condition "running"
Aug 24 11:42:26.773: INFO: Pod "webserver-deployment-7f5969cbc7-96clb": Phase="Running", Reason="", readiness=true. Elapsed: 2.036658516s
Aug 24 11:42:26.773: INFO: Pod "webserver-deployment-7f5969cbc7-96clb" satisfied condition "running"
Aug 24 11:42:26.774: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm": Phase="Running", Reason="", readiness=true. Elapsed: 2.037333734s
Aug 24 11:42:26.774: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm" satisfied condition "running"
Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn": Phase="Running", Reason="", readiness=true. Elapsed: 2.04031439s
Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn" satisfied condition "running"
Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-k62fp": Phase="Running", Reason="", readiness=true. Elapsed: 2.040030889s
Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-k62fp" satisfied condition "running"
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg": Phase="Running", Reason="", readiness=true. Elapsed: 2.040760624s
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg" satisfied condition "running"
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf": Phase="Running", Reason="", readiness=true. Elapsed: 2.040445308s
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf" satisfied condition "running"
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-scwtf": Phase="Running", Reason="", readiness=true. Elapsed: 2.040327052s
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-scwtf" satisfied condition "running"
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg": Phase="Running", Reason="", readiness=true. Elapsed: 2.041103128s
Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg" satisfied condition "running"
Aug 24 11:42:26.777: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 24 11:42:26.784: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 24 11:42:26.800: INFO: Updating deployment webserver-deployment
Aug 24 11:42:26.800: INFO: Waiting for observed generation 2
Aug 24 11:42:28.807: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 24 11:42:28.811: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 24 11:42:28.815: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 24 11:42:28.822: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 24 11:42:28.822: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 24 11:42:28.824: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 24 11:42:28.828: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 24 11:42:28.828: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 24 11:42:28.843: INFO: Updating deployment webserver-deployment
Aug 24 11:42:28.843: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 24 11:42:28.854: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 24 11:42:28.860: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 11:42:28.910: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8515  7180bbfb-ad0c-4706-83d9-8da48b074a9a 28891 3 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00499b798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-24 11:42:27 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 11:42:28 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 24 11:42:28.964: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8515  94409e4e-8f23-4d0e-a188-5164e0434aed 28888 3 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7180bbfb-ad0c-4706-83d9-8da48b074a9a 0xc00499bcd7 0xc00499bcd8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7180bbfb-ad0c-4706-83d9-8da48b074a9a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00499bd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 11:42:28.964: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 24 11:42:28.965: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8515  48512246-edbe-49e6-8884-b3b86301ef99 28886 3 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7180bbfb-ad0c-4706-83d9-8da48b074a9a 0xc00499bbe7 0xc00499bbe8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7180bbfb-ad0c-4706-83d9-8da48b074a9a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00499bc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 24 11:42:29.000: INFO: Pod "webserver-deployment-7f5969cbc7-2jz5p" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2jz5p webserver-deployment-7f5969cbc7- deployment-8515  82f530d8-3139-4b2c-b54e-3e06a27c19c7 28738 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d0deb55bab29665af83eb0dcc555222be3d09027bad318566bf0d538604351a4 cni.projectcalico.org/podIP:10.100.45.160/32 cni.projectcalico.org/podIPs:10.100.45.160/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6297 0xc001ac6298}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lffcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lffcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.160,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1d5b87c725aba4f65bcaf53b1d37a9fb1e51d0f4bb6e181a7f285d8d25f97220,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.000: INFO: Pod "webserver-deployment-7f5969cbc7-6mvkj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6mvkj webserver-deployment-7f5969cbc7- deployment-8515  6f9beb26-3d67-4b23-b8db-9819d659ea48 28895 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6490 0xc001ac6491}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfg5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfg5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.001: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-84mmn webserver-deployment-7f5969cbc7- deployment-8515  bfb02e78-1dea-4113-8b19-b823783dae0a 28781 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f1caa9f5f41ff4c3ba576f1869c33b227bc86eb4fefa08aff18d332911495015 cni.projectcalico.org/podIP:10.100.181.189/32 cni.projectcalico.org/podIPs:10.100.181.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6a30 0xc001ac6a31}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tw4dc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tw4dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.189,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9fb65e00f22e8bd68068a0b1ec9856ca1ccb4a00c96760e0b1f46a5995f3799c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.002: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8v6rg webserver-deployment-7f5969cbc7- deployment-8515  fcc0f9f8-6421-4c37-bfef-7947404932d8 28783 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:939e0888e08a976c7f3e9c0b2a486898a6fa24f9a9baead4b8a0662857a42ebf cni.projectcalico.org/podIP:10.100.181.191/32 cni.projectcalico.org/podIPs:10.100.181.191/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6c30 0xc001ac6c31}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2wf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2wf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.191,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3da70ee515995fef9c7a70cf8701fa72168dfd34f7307e684add828a8914b950,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.003: INFO: Pod "webserver-deployment-7f5969cbc7-96clb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-96clb webserver-deployment-7f5969cbc7- deployment-8515  91f4ce83-a2e2-49cb-a311-299f8098337a 28744 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:81a2f18ccb239faef025df8897ac1dd2f67de16f0d3cd2f0d5e5c27d083c45a4 cni.projectcalico.org/podIP:10.100.45.159/32 cni.projectcalico.org/podIPs:10.100.45.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6e50 0xc001ac6e51}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b79ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b79ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.159,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d534b9f57830c55d58af12dab7a6b340e34026e96f6eb09448f6bb0d7eb5f71b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.003: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f5krg webserver-deployment-7f5969cbc7- deployment-8515  1e580886-a4b3-442a-9bf8-53a76cca8342 28789 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7e61891b7dd9be08b368b83765b9abcabcd39eb38876af1c339b1752c0194170 cni.projectcalico.org/podIP:10.100.181.131/32 cni.projectcalico.org/podIPs:10.100.181.131/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac7050 0xc001ac7051}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lc5w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lc5w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.131,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6bd9d6d39183304b97bed0209f9b0128bba0b8022ab6d98d372ecccfe32cde04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.004: INFO: Pod "webserver-deployment-7f5969cbc7-kvx8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kvx8p webserver-deployment-7f5969cbc7- deployment-8515  6e8c1928-ec2e-466f-a200-0e4984a528f8 28902 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac7250 0xc001ac7251}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68wcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68wcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.004: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lc4mf webserver-deployment-7f5969cbc7- deployment-8515  adbc643f-ece0-4f84-9b9b-ea41815bf432 28764 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3365471770728440797de132938d318434b763ba212f917269dca2318bb60873 cni.projectcalico.org/podIP:10.100.148.230/32 cni.projectcalico.org/podIPs:10.100.148.230/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170a6a0 0xc00170a6a1}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.148.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvq47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvq47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.100.148.230,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6543a793811dc06c10e3e1655ff526e5ee8a70b8b30056a01a26d406fe8de25d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.148.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.005: INFO: Pod "webserver-deployment-7f5969cbc7-q7ddz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q7ddz webserver-deployment-7f5969cbc7- deployment-8515  9a43e6a3-3dc6-47b1-8e99-f9533a367dd2 28906 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170ad50 0xc00170ad51}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjrlf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjrlf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.005: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r7chm webserver-deployment-7f5969cbc7- deployment-8515  d8ba6b73-74e4-4f9c-a373-ef429440e35a 28768 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:acaeda944ef184e21858f53ffb7722bb08f92ed19d581423acefa3d8b4391187 cni.projectcalico.org/podIP:10.100.148.228/32 cni.projectcalico.org/podIPs:10.100.148.228/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170b340 0xc00170b341}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.148.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xc69l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xc69l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.100.148.228,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3acd6aaaded2c02935613f946af8703f5117fed9064d12fd7cca627eb17d44a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.148.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.006: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vnpjk webserver-deployment-7f5969cbc7- deployment-8515  ce699928-6e3c-480a-8356-62be2f14d79d 28786 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d8398e8c145bd21b88eb10320e04e8b1e9276ffc59e85233b593ef03e2a79ddd cni.projectcalico.org/podIP:10.100.181.190/32 cni.projectcalico.org/podIPs:10.100.181.190/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170b540 0xc00170b541}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrnvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrnvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.190,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://587ff68b68826d0877baad75d5b6593d2aae174b80018c81752d9d021dd455a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.006: INFO: Pod "webserver-deployment-d9f79cb5-2d4cg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2d4cg webserver-deployment-d9f79cb5- deployment-8515  26483979-4656-4d17-9c06-a7c3fc87369c 28894 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc00170baff 0xc00170bb10}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49662,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49662,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.006: INFO: Pod "webserver-deployment-d9f79cb5-7c2hq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7c2hq webserver-deployment-d9f79cb5- deployment-8515  ca390ccc-3a9f-475e-983e-180dd5c9072c 28857 0 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f3a685ff50a2003d70fedaa409891152b852529771180cb661b57db68edd77b5 cni.projectcalico.org/podIP:10.100.45.145/32 cni.projectcalico.org/podIPs:10.100.45.145/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc00170be2f 0xc00170bf30}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rr75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rr75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:,StartTime:2023-08-24 11:42:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.007: INFO: Pod "webserver-deployment-d9f79cb5-ckmlg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ckmlg webserver-deployment-d9f79cb5- deployment-8515  679127cd-1f4e-4a9d-ae37-a935aeace8bc 28905 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e617f 0xc0047e6190}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8t5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8t5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.007: INFO: Pod "webserver-deployment-d9f79cb5-mp6g8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mp6g8 webserver-deployment-d9f79cb5- deployment-8515  228a44d1-a52f-4b19-b93c-c63a1318b748 28871 0 2023-08-24 11:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:06420551c3ee049e637c85b99e54bab3760fe995b251bb9a63591284c1e1946d cni.projectcalico.org/podIP:10.100.181.130/32 cni.projectcalico.org/podIPs:10.100.181.130/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e62df 0xc0047e62f0}] [] [{Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzkpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzkpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 11:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.008: INFO: Pod "webserver-deployment-d9f79cb5-pd82z" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pd82z webserver-deployment-d9f79cb5- deployment-8515  295314e0-0a30-4cac-8f39-4faae439cda8 28865 0 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:93b0664689e1b4d40c5a39de384a2f11232713db85e57ece5af39d5377219d85 cni.projectcalico.org/podIP:10.100.148.231/32 cni.projectcalico.org/podIPs:10.100.148.231/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e64cf 0xc0047e64e0}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prfgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prfgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-08-24 11:42:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.008: INFO: Pod "webserver-deployment-d9f79cb5-r4vs7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r4vs7 webserver-deployment-d9f79cb5- deployment-8515  c27b004c-7d9a-467a-8cfe-042dc5b873e1 28863 0 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e346cc02c1caac3902d4852981544561c2f2d92975a36361c0a001768bdb87a9 cni.projectcalico.org/podIP:10.100.181.132/32 cni.projectcalico.org/podIPs:10.100.181.132/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e66cf 0xc0047e66e0}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbbhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbbhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 11:42:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.009: INFO: Pod "webserver-deployment-d9f79cb5-rgljd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rgljd webserver-deployment-d9f79cb5- deployment-8515  26eacf67-1678-4ab9-a732-28fdf7e3877d 28869 0 2023-08-24 11:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cc7b13029a7938402d3e1a6fcc5eb3219fd5dbae4a7bdd03256241862f4b5e26 cni.projectcalico.org/podIP:10.100.45.147/32 cni.projectcalico.org/podIPs:10.100.45.147/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e68cf 0xc0047e6900}] [] [{Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-297tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-297tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:,StartTime:2023-08-24 11:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:42:29.009: INFO: Pod "webserver-deployment-d9f79cb5-vvpml" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vvpml webserver-deployment-d9f79cb5- deployment-8515  5a193ca2-fa09-412f-95f1-f3163c4e7dc5 28903 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e6adf 0xc0047e6af0}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk2m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk2m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:29.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8515" for this suite. 08/24/23 11:42:29.055
------------------------------
â€¢ [SLOW TEST] [6.503 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:22.636
    Aug 24 11:42:22.636: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 11:42:22.638
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:22.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:22.668
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 24 11:42:22.674: INFO: Creating deployment "webserver-deployment"
    Aug 24 11:42:22.684: INFO: Waiting for observed generation 1
    Aug 24 11:42:24.707: INFO: Waiting for all required pods to come up
    Aug 24 11:42:24.735: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/24/23 11:42:24.735
    Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vnpjk" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-84mmn" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-8v6rg" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-96clb" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-f5krg" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.736: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k62fp" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-lc4mf" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-r7chm" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.737: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-scwtf" in namespace "deployment-8515" to be "running"
    Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg": Phase="Pending", Reason="", readiness=false. Elapsed: 27.83622ms
    Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm": Phase="Pending", Reason="", readiness=false. Elapsed: 27.385162ms
    Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk": Phase="Pending", Reason="", readiness=false. Elapsed: 28.477767ms
    Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn": Phase="Pending", Reason="", readiness=false. Elapsed: 28.296477ms
    Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-96clb": Phase="Pending", Reason="", readiness=false. Elapsed: 27.80362ms
    Aug 24 11:42:24.764: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf": Phase="Pending", Reason="", readiness=false. Elapsed: 27.816912ms
    Aug 24 11:42:24.767: INFO: Pod "webserver-deployment-7f5969cbc7-scwtf": Phase="Pending", Reason="", readiness=false. Elapsed: 29.769737ms
    Aug 24 11:42:24.767: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg": Phase="Pending", Reason="", readiness=false. Elapsed: 30.522542ms
    Aug 24 11:42:24.767: INFO: Pod "webserver-deployment-7f5969cbc7-k62fp": Phase="Pending", Reason="", readiness=false. Elapsed: 30.465437ms
    Aug 24 11:42:26.772: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk": Phase="Running", Reason="", readiness=true. Elapsed: 2.036468929s
    Aug 24 11:42:26.772: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk" satisfied condition "running"
    Aug 24 11:42:26.773: INFO: Pod "webserver-deployment-7f5969cbc7-96clb": Phase="Running", Reason="", readiness=true. Elapsed: 2.036658516s
    Aug 24 11:42:26.773: INFO: Pod "webserver-deployment-7f5969cbc7-96clb" satisfied condition "running"
    Aug 24 11:42:26.774: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm": Phase="Running", Reason="", readiness=true. Elapsed: 2.037333734s
    Aug 24 11:42:26.774: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm" satisfied condition "running"
    Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn": Phase="Running", Reason="", readiness=true. Elapsed: 2.04031439s
    Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn" satisfied condition "running"
    Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-k62fp": Phase="Running", Reason="", readiness=true. Elapsed: 2.040030889s
    Aug 24 11:42:26.776: INFO: Pod "webserver-deployment-7f5969cbc7-k62fp" satisfied condition "running"
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg": Phase="Running", Reason="", readiness=true. Elapsed: 2.040760624s
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg" satisfied condition "running"
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf": Phase="Running", Reason="", readiness=true. Elapsed: 2.040445308s
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf" satisfied condition "running"
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-scwtf": Phase="Running", Reason="", readiness=true. Elapsed: 2.040327052s
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-scwtf" satisfied condition "running"
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg": Phase="Running", Reason="", readiness=true. Elapsed: 2.041103128s
    Aug 24 11:42:26.777: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg" satisfied condition "running"
    Aug 24 11:42:26.777: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 24 11:42:26.784: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 24 11:42:26.800: INFO: Updating deployment webserver-deployment
    Aug 24 11:42:26.800: INFO: Waiting for observed generation 2
    Aug 24 11:42:28.807: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 24 11:42:28.811: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 24 11:42:28.815: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 24 11:42:28.822: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 24 11:42:28.822: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 24 11:42:28.824: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 24 11:42:28.828: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 24 11:42:28.828: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 24 11:42:28.843: INFO: Updating deployment webserver-deployment
    Aug 24 11:42:28.843: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 24 11:42:28.854: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 24 11:42:28.860: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 11:42:28.910: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-8515  7180bbfb-ad0c-4706-83d9-8da48b074a9a 28891 3 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00499b798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-24 11:42:27 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 11:42:28 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 24 11:42:28.964: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-8515  94409e4e-8f23-4d0e-a188-5164e0434aed 28888 3 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7180bbfb-ad0c-4706-83d9-8da48b074a9a 0xc00499bcd7 0xc00499bcd8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7180bbfb-ad0c-4706-83d9-8da48b074a9a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00499bd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 11:42:28.964: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 24 11:42:28.965: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-8515  48512246-edbe-49e6-8884-b3b86301ef99 28886 3 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7180bbfb-ad0c-4706-83d9-8da48b074a9a 0xc00499bbe7 0xc00499bbe8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7180bbfb-ad0c-4706-83d9-8da48b074a9a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00499bc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 11:42:29.000: INFO: Pod "webserver-deployment-7f5969cbc7-2jz5p" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2jz5p webserver-deployment-7f5969cbc7- deployment-8515  82f530d8-3139-4b2c-b54e-3e06a27c19c7 28738 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d0deb55bab29665af83eb0dcc555222be3d09027bad318566bf0d538604351a4 cni.projectcalico.org/podIP:10.100.45.160/32 cni.projectcalico.org/podIPs:10.100.45.160/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6297 0xc001ac6298}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lffcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lffcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.160,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1d5b87c725aba4f65bcaf53b1d37a9fb1e51d0f4bb6e181a7f285d8d25f97220,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.000: INFO: Pod "webserver-deployment-7f5969cbc7-6mvkj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6mvkj webserver-deployment-7f5969cbc7- deployment-8515  6f9beb26-3d67-4b23-b8db-9819d659ea48 28895 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6490 0xc001ac6491}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfg5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfg5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.001: INFO: Pod "webserver-deployment-7f5969cbc7-84mmn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-84mmn webserver-deployment-7f5969cbc7- deployment-8515  bfb02e78-1dea-4113-8b19-b823783dae0a 28781 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f1caa9f5f41ff4c3ba576f1869c33b227bc86eb4fefa08aff18d332911495015 cni.projectcalico.org/podIP:10.100.181.189/32 cni.projectcalico.org/podIPs:10.100.181.189/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6a30 0xc001ac6a31}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tw4dc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tw4dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.189,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9fb65e00f22e8bd68068a0b1ec9856ca1ccb4a00c96760e0b1f46a5995f3799c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.002: INFO: Pod "webserver-deployment-7f5969cbc7-8v6rg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8v6rg webserver-deployment-7f5969cbc7- deployment-8515  fcc0f9f8-6421-4c37-bfef-7947404932d8 28783 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:939e0888e08a976c7f3e9c0b2a486898a6fa24f9a9baead4b8a0662857a42ebf cni.projectcalico.org/podIP:10.100.181.191/32 cni.projectcalico.org/podIPs:10.100.181.191/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6c30 0xc001ac6c31}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2wf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2wf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.191,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3da70ee515995fef9c7a70cf8701fa72168dfd34f7307e684add828a8914b950,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.003: INFO: Pod "webserver-deployment-7f5969cbc7-96clb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-96clb webserver-deployment-7f5969cbc7- deployment-8515  91f4ce83-a2e2-49cb-a311-299f8098337a 28744 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:81a2f18ccb239faef025df8897ac1dd2f67de16f0d3cd2f0d5e5c27d083c45a4 cni.projectcalico.org/podIP:10.100.45.159/32 cni.projectcalico.org/podIPs:10.100.45.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac6e50 0xc001ac6e51}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b79ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b79ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:10.100.45.159,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d534b9f57830c55d58af12dab7a6b340e34026e96f6eb09448f6bb0d7eb5f71b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.45.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.003: INFO: Pod "webserver-deployment-7f5969cbc7-f5krg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f5krg webserver-deployment-7f5969cbc7- deployment-8515  1e580886-a4b3-442a-9bf8-53a76cca8342 28789 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7e61891b7dd9be08b368b83765b9abcabcd39eb38876af1c339b1752c0194170 cni.projectcalico.org/podIP:10.100.181.131/32 cni.projectcalico.org/podIPs:10.100.181.131/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac7050 0xc001ac7051}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lc5w8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lc5w8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.131,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6bd9d6d39183304b97bed0209f9b0128bba0b8022ab6d98d372ecccfe32cde04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.004: INFO: Pod "webserver-deployment-7f5969cbc7-kvx8p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kvx8p webserver-deployment-7f5969cbc7- deployment-8515  6e8c1928-ec2e-466f-a200-0e4984a528f8 28902 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc001ac7250 0xc001ac7251}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68wcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68wcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.004: INFO: Pod "webserver-deployment-7f5969cbc7-lc4mf" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lc4mf webserver-deployment-7f5969cbc7- deployment-8515  adbc643f-ece0-4f84-9b9b-ea41815bf432 28764 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3365471770728440797de132938d318434b763ba212f917269dca2318bb60873 cni.projectcalico.org/podIP:10.100.148.230/32 cni.projectcalico.org/podIPs:10.100.148.230/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170a6a0 0xc00170a6a1}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.148.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nvq47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nvq47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.100.148.230,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6543a793811dc06c10e3e1655ff526e5ee8a70b8b30056a01a26d406fe8de25d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.148.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.005: INFO: Pod "webserver-deployment-7f5969cbc7-q7ddz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q7ddz webserver-deployment-7f5969cbc7- deployment-8515  9a43e6a3-3dc6-47b1-8e99-f9533a367dd2 28906 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170ad50 0xc00170ad51}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjrlf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjrlf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.005: INFO: Pod "webserver-deployment-7f5969cbc7-r7chm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-r7chm webserver-deployment-7f5969cbc7- deployment-8515  d8ba6b73-74e4-4f9c-a373-ef429440e35a 28768 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:acaeda944ef184e21858f53ffb7722bb08f92ed19d581423acefa3d8b4391187 cni.projectcalico.org/podIP:10.100.148.228/32 cni.projectcalico.org/podIPs:10.100.148.228/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170b340 0xc00170b341}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.148.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xc69l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xc69l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.100.148.228,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3acd6aaaded2c02935613f946af8703f5117fed9064d12fd7cca627eb17d44a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.148.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.006: INFO: Pod "webserver-deployment-7f5969cbc7-vnpjk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vnpjk webserver-deployment-7f5969cbc7- deployment-8515  ce699928-6e3c-480a-8356-62be2f14d79d 28786 0 2023-08-24 11:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d8398e8c145bd21b88eb10320e04e8b1e9276ffc59e85233b593ef03e2a79ddd cni.projectcalico.org/podIP:10.100.181.190/32 cni.projectcalico.org/podIPs:10.100.181.190/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 48512246-edbe-49e6-8884-b3b86301ef99 0xc00170b540 0xc00170b541}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48512246-edbe-49e6-8884-b3b86301ef99\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrnvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrnvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.190,StartTime:2023-08-24 11:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 11:42:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://587ff68b68826d0877baad75d5b6593d2aae174b80018c81752d9d021dd455a8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.006: INFO: Pod "webserver-deployment-d9f79cb5-2d4cg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2d4cg webserver-deployment-d9f79cb5- deployment-8515  26483979-4656-4d17-9c06-a7c3fc87369c 28894 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc00170baff 0xc00170bb10}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49662,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49662,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.006: INFO: Pod "webserver-deployment-d9f79cb5-7c2hq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7c2hq webserver-deployment-d9f79cb5- deployment-8515  ca390ccc-3a9f-475e-983e-180dd5c9072c 28857 0 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f3a685ff50a2003d70fedaa409891152b852529771180cb661b57db68edd77b5 cni.projectcalico.org/podIP:10.100.45.145/32 cni.projectcalico.org/podIPs:10.100.45.145/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc00170be2f 0xc00170bf30}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4rr75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rr75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:,StartTime:2023-08-24 11:42:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.007: INFO: Pod "webserver-deployment-d9f79cb5-ckmlg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ckmlg webserver-deployment-d9f79cb5- deployment-8515  679127cd-1f4e-4a9d-ae37-a935aeace8bc 28905 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e617f 0xc0047e6190}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8t5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8t5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.007: INFO: Pod "webserver-deployment-d9f79cb5-mp6g8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mp6g8 webserver-deployment-d9f79cb5- deployment-8515  228a44d1-a52f-4b19-b93c-c63a1318b748 28871 0 2023-08-24 11:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:06420551c3ee049e637c85b99e54bab3760fe995b251bb9a63591284c1e1946d cni.projectcalico.org/podIP:10.100.181.130/32 cni.projectcalico.org/podIPs:10.100.181.130/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e62df 0xc0047e62f0}] [] [{Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzkpx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzkpx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 11:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.008: INFO: Pod "webserver-deployment-d9f79cb5-pd82z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pd82z webserver-deployment-d9f79cb5- deployment-8515  295314e0-0a30-4cac-8f39-4faae439cda8 28865 0 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:93b0664689e1b4d40c5a39de384a2f11232713db85e57ece5af39d5377219d85 cni.projectcalico.org/podIP:10.100.148.231/32 cni.projectcalico.org/podIPs:10.100.148.231/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e64cf 0xc0047e64e0}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prfgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prfgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-08-24 11:42:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.008: INFO: Pod "webserver-deployment-d9f79cb5-r4vs7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r4vs7 webserver-deployment-d9f79cb5- deployment-8515  c27b004c-7d9a-467a-8cfe-042dc5b873e1 28863 0 2023-08-24 11:42:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e346cc02c1caac3902d4852981544561c2f2d92975a36361c0a001768bdb87a9 cni.projectcalico.org/podIP:10.100.181.132/32 cni.projectcalico.org/podIPs:10.100.181.132/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e66cf 0xc0047e66e0}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbbhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbbhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 11:42:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.009: INFO: Pod "webserver-deployment-d9f79cb5-rgljd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rgljd webserver-deployment-d9f79cb5- deployment-8515  26eacf67-1678-4ab9-a732-28fdf7e3877d 28869 0 2023-08-24 11:42:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:cc7b13029a7938402d3e1a6fcc5eb3219fd5dbae4a7bdd03256241862f4b5e26 cni.projectcalico.org/podIP:10.100.45.147/32 cni.projectcalico.org/podIPs:10.100.45.147/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e68cf 0xc0047e6900}] [] [{Go-http-client Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 11:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-297tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-297tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.17,PodIP:,StartTime:2023-08-24 11:42:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:42:29.009: INFO: Pod "webserver-deployment-d9f79cb5-vvpml" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vvpml webserver-deployment-d9f79cb5- deployment-8515  5a193ca2-fa09-412f-95f1-f3163c4e7dc5 28903 0 2023-08-24 11:42:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 94409e4e-8f23-4d0e-a188-5164e0434aed 0xc0047e6adf 0xc0047e6af0}] [] [{kube-controller-manager Update v1 2023-08-24 11:42:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94409e4e-8f23-4d0e-a188-5164e0434aed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rk2m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rk2m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 11:42:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:29.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8515" for this suite. 08/24/23 11:42:29.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:29.145
Aug 24 11:42:29.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:42:29.146
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:29.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:29.439
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-462d7617-acda-4190-8d90-ad8117e1e84b 08/24/23 11:42:29.489
STEP: Creating a pod to test consume configMaps 08/24/23 11:42:29.503
Aug 24 11:42:29.529: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3" in namespace "projected-7730" to be "Succeeded or Failed"
Aug 24 11:42:29.540: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.589695ms
Aug 24 11:42:31.545: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015579516s
Aug 24 11:42:33.543: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014006061s
STEP: Saw pod success 08/24/23 11:42:33.543
Aug 24 11:42:33.544: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3" satisfied condition "Succeeded or Failed"
Aug 24 11:42:33.563: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:42:33.575
Aug 24 11:42:33.597: INFO: Waiting for pod pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3 to disappear
Aug 24 11:42:33.605: INFO: Pod pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:33.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7730" for this suite. 08/24/23 11:42:33.609
------------------------------
â€¢ [4.472 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:29.145
    Aug 24 11:42:29.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:42:29.146
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:29.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:29.439
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-462d7617-acda-4190-8d90-ad8117e1e84b 08/24/23 11:42:29.489
    STEP: Creating a pod to test consume configMaps 08/24/23 11:42:29.503
    Aug 24 11:42:29.529: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3" in namespace "projected-7730" to be "Succeeded or Failed"
    Aug 24 11:42:29.540: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.589695ms
    Aug 24 11:42:31.545: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015579516s
    Aug 24 11:42:33.543: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014006061s
    STEP: Saw pod success 08/24/23 11:42:33.543
    Aug 24 11:42:33.544: INFO: Pod "pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3" satisfied condition "Succeeded or Failed"
    Aug 24 11:42:33.563: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:42:33.575
    Aug 24 11:42:33.597: INFO: Waiting for pod pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3 to disappear
    Aug 24 11:42:33.605: INFO: Pod pod-projected-configmaps-8aafcb9a-5554-4c90-bcf5-633de997aef3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:33.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7730" for this suite. 08/24/23 11:42:33.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:33.619
Aug 24 11:42:33.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:42:33.621
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:33.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:33.654
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 24 11:42:33.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/24/23 11:42:37.875
Aug 24 11:42:37.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
Aug 24 11:42:39.278: INFO: stderr: ""
Aug 24 11:42:39.278: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 24 11:42:39.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 delete e2e-test-crd-publish-openapi-2297-crds test-foo'
Aug 24 11:42:39.426: INFO: stderr: ""
Aug 24 11:42:39.426: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 24 11:42:39.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 apply -f -'
Aug 24 11:42:39.880: INFO: stderr: ""
Aug 24 11:42:39.880: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 24 11:42:39.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 delete e2e-test-crd-publish-openapi-2297-crds test-foo'
Aug 24 11:42:40.008: INFO: stderr: ""
Aug 24 11:42:40.008: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/24/23 11:42:40.008
Aug 24 11:42:40.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
Aug 24 11:42:40.479: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/24/23 11:42:40.479
Aug 24 11:42:40.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
Aug 24 11:42:40.932: INFO: rc: 1
Aug 24 11:42:40.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 apply -f -'
Aug 24 11:42:41.417: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/24/23 11:42:41.418
Aug 24 11:42:41.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
Aug 24 11:42:41.877: INFO: rc: 1
Aug 24 11:42:41.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 apply -f -'
Aug 24 11:42:42.325: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/24/23 11:42:42.325
Aug 24 11:42:42.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds'
Aug 24 11:42:42.742: INFO: stderr: ""
Aug 24 11:42:42.742: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/24/23 11:42:42.743
Aug 24 11:42:42.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.metadata'
Aug 24 11:42:43.170: INFO: stderr: ""
Aug 24 11:42:43.171: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 24 11:42:43.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.spec'
Aug 24 11:42:43.671: INFO: stderr: ""
Aug 24 11:42:43.671: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 24 11:42:43.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.spec.bars'
Aug 24 11:42:44.158: INFO: stderr: ""
Aug 24 11:42:44.158: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/24/23 11:42:44.158
Aug 24 11:42:44.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.spec.bars2'
Aug 24 11:42:44.611: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:46.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7623" for this suite. 08/24/23 11:42:46.8
------------------------------
â€¢ [SLOW TEST] [13.191 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:33.619
    Aug 24 11:42:33.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:42:33.621
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:33.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:33.654
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 24 11:42:33.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/24/23 11:42:37.875
    Aug 24 11:42:37.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
    Aug 24 11:42:39.278: INFO: stderr: ""
    Aug 24 11:42:39.278: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 24 11:42:39.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 delete e2e-test-crd-publish-openapi-2297-crds test-foo'
    Aug 24 11:42:39.426: INFO: stderr: ""
    Aug 24 11:42:39.426: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 24 11:42:39.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 apply -f -'
    Aug 24 11:42:39.880: INFO: stderr: ""
    Aug 24 11:42:39.880: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 24 11:42:39.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 delete e2e-test-crd-publish-openapi-2297-crds test-foo'
    Aug 24 11:42:40.008: INFO: stderr: ""
    Aug 24 11:42:40.008: INFO: stdout: "e2e-test-crd-publish-openapi-2297-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/24/23 11:42:40.008
    Aug 24 11:42:40.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
    Aug 24 11:42:40.479: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/24/23 11:42:40.479
    Aug 24 11:42:40.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
    Aug 24 11:42:40.932: INFO: rc: 1
    Aug 24 11:42:40.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 apply -f -'
    Aug 24 11:42:41.417: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/24/23 11:42:41.418
    Aug 24 11:42:41.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 create -f -'
    Aug 24 11:42:41.877: INFO: rc: 1
    Aug 24 11:42:41.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 --namespace=crd-publish-openapi-7623 apply -f -'
    Aug 24 11:42:42.325: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/24/23 11:42:42.325
    Aug 24 11:42:42.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds'
    Aug 24 11:42:42.742: INFO: stderr: ""
    Aug 24 11:42:42.742: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/24/23 11:42:42.743
    Aug 24 11:42:42.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.metadata'
    Aug 24 11:42:43.170: INFO: stderr: ""
    Aug 24 11:42:43.171: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 24 11:42:43.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.spec'
    Aug 24 11:42:43.671: INFO: stderr: ""
    Aug 24 11:42:43.671: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 24 11:42:43.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.spec.bars'
    Aug 24 11:42:44.158: INFO: stderr: ""
    Aug 24 11:42:44.158: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2297-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/24/23 11:42:44.158
    Aug 24 11:42:44.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-7623 explain e2e-test-crd-publish-openapi-2297-crds.spec.bars2'
    Aug 24 11:42:44.611: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:46.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7623" for this suite. 08/24/23 11:42:46.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:46.812
Aug 24 11:42:46.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:42:46.813
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:46.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:46.843
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-689e7b59-0ac9-4607-9cdf-232f5708257b 08/24/23 11:42:46.863
STEP: Creating secret with name s-test-opt-upd-101f480d-e0af-46ea-8596-c73550ccdf75 08/24/23 11:42:46.871
STEP: Creating the pod 08/24/23 11:42:46.88
Aug 24 11:42:46.902: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599" in namespace "projected-6701" to be "running and ready"
Aug 24 11:42:46.924: INFO: Pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599": Phase="Pending", Reason="", readiness=false. Elapsed: 22.578844ms
Aug 24 11:42:46.924: INFO: The phase of Pod pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:42:48.928: INFO: Pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599": Phase="Running", Reason="", readiness=true. Elapsed: 2.026285891s
Aug 24 11:42:48.928: INFO: The phase of Pod pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599 is Running (Ready = true)
Aug 24 11:42:48.928: INFO: Pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-689e7b59-0ac9-4607-9cdf-232f5708257b 08/24/23 11:42:48.952
STEP: Updating secret s-test-opt-upd-101f480d-e0af-46ea-8596-c73550ccdf75 08/24/23 11:42:48.964
STEP: Creating secret with name s-test-opt-create-263d5764-aad9-40d1-9f17-ee9b3e736873 08/24/23 11:42:48.973
STEP: waiting to observe update in volume 08/24/23 11:42:48.979
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 11:42:53.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6701" for this suite. 08/24/23 11:42:53.02
------------------------------
â€¢ [SLOW TEST] [6.217 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:46.812
    Aug 24 11:42:46.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:42:46.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:46.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:46.843
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-689e7b59-0ac9-4607-9cdf-232f5708257b 08/24/23 11:42:46.863
    STEP: Creating secret with name s-test-opt-upd-101f480d-e0af-46ea-8596-c73550ccdf75 08/24/23 11:42:46.871
    STEP: Creating the pod 08/24/23 11:42:46.88
    Aug 24 11:42:46.902: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599" in namespace "projected-6701" to be "running and ready"
    Aug 24 11:42:46.924: INFO: Pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599": Phase="Pending", Reason="", readiness=false. Elapsed: 22.578844ms
    Aug 24 11:42:46.924: INFO: The phase of Pod pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:42:48.928: INFO: Pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599": Phase="Running", Reason="", readiness=true. Elapsed: 2.026285891s
    Aug 24 11:42:48.928: INFO: The phase of Pod pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599 is Running (Ready = true)
    Aug 24 11:42:48.928: INFO: Pod "pod-projected-secrets-d633fc52-0374-4d61-b44d-15135f072599" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-689e7b59-0ac9-4607-9cdf-232f5708257b 08/24/23 11:42:48.952
    STEP: Updating secret s-test-opt-upd-101f480d-e0af-46ea-8596-c73550ccdf75 08/24/23 11:42:48.964
    STEP: Creating secret with name s-test-opt-create-263d5764-aad9-40d1-9f17-ee9b3e736873 08/24/23 11:42:48.973
    STEP: waiting to observe update in volume 08/24/23 11:42:48.979
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:42:53.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6701" for this suite. 08/24/23 11:42:53.02
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:42:53.028
Aug 24 11:42:53.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:42:53.029
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:53.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:53.062
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-412 08/24/23 11:42:53.071
STEP: creating service affinity-clusterip-transition in namespace services-412 08/24/23 11:42:53.071
STEP: creating replication controller affinity-clusterip-transition in namespace services-412 08/24/23 11:42:53.086
I0824 11:42:53.112584      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-412, replica count: 3
I0824 11:42:56.164265      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0824 11:42:59.165797      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 11:42:59.173: INFO: Creating new exec pod
Aug 24 11:42:59.185: INFO: Waiting up to 5m0s for pod "execpod-affinityj4wrf" in namespace "services-412" to be "running"
Aug 24 11:42:59.195: INFO: Pod "execpod-affinityj4wrf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.553474ms
Aug 24 11:43:01.200: INFO: Pod "execpod-affinityj4wrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.014764055s
Aug 24 11:43:01.200: INFO: Pod "execpod-affinityj4wrf" satisfied condition "running"
Aug 24 11:43:02.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 24 11:43:02.467: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 24 11:43:02.467: INFO: stdout: ""
Aug 24 11:43:02.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c nc -v -z -w 2 10.254.189.244 80'
Aug 24 11:43:02.707: INFO: stderr: "+ nc -v -z -w 2 10.254.189.244 80\nConnection to 10.254.189.244 80 port [tcp/http] succeeded!\n"
Aug 24 11:43:02.707: INFO: stdout: ""
Aug 24 11:43:02.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.189.244:80/ ; done'
Aug 24 11:43:03.068: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n"
Aug 24 11:43:03.068: INFO: stdout: "\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-wtzs9"
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
Aug 24 11:43:03.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.189.244:80/ ; done'
Aug 24 11:43:03.454: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n"
Aug 24 11:43:03.454: INFO: stdout: "\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns"
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
Aug 24 11:43:03.454: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-412, will wait for the garbage collector to delete the pods 08/24/23 11:43:03.476
Aug 24 11:43:03.573: INFO: Deleting ReplicationController affinity-clusterip-transition took: 36.006419ms
Aug 24 11:43:03.774: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.773282ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:05.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-412" for this suite. 08/24/23 11:43:05.937
------------------------------
â€¢ [SLOW TEST] [12.926 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:42:53.028
    Aug 24 11:42:53.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:42:53.029
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:42:53.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:42:53.062
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-412 08/24/23 11:42:53.071
    STEP: creating service affinity-clusterip-transition in namespace services-412 08/24/23 11:42:53.071
    STEP: creating replication controller affinity-clusterip-transition in namespace services-412 08/24/23 11:42:53.086
    I0824 11:42:53.112584      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-412, replica count: 3
    I0824 11:42:56.164265      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0824 11:42:59.165797      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 11:42:59.173: INFO: Creating new exec pod
    Aug 24 11:42:59.185: INFO: Waiting up to 5m0s for pod "execpod-affinityj4wrf" in namespace "services-412" to be "running"
    Aug 24 11:42:59.195: INFO: Pod "execpod-affinityj4wrf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.553474ms
    Aug 24 11:43:01.200: INFO: Pod "execpod-affinityj4wrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.014764055s
    Aug 24 11:43:01.200: INFO: Pod "execpod-affinityj4wrf" satisfied condition "running"
    Aug 24 11:43:02.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 24 11:43:02.467: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 24 11:43:02.467: INFO: stdout: ""
    Aug 24 11:43:02.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c nc -v -z -w 2 10.254.189.244 80'
    Aug 24 11:43:02.707: INFO: stderr: "+ nc -v -z -w 2 10.254.189.244 80\nConnection to 10.254.189.244 80 port [tcp/http] succeeded!\n"
    Aug 24 11:43:02.707: INFO: stdout: ""
    Aug 24 11:43:02.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.189.244:80/ ; done'
    Aug 24 11:43:03.068: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n"
    Aug 24 11:43:03.068: INFO: stdout: "\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-wtzs9\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-mr8vk\naffinity-clusterip-transition-wtzs9"
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-mr8vk
    Aug 24 11:43:03.068: INFO: Received response from host: affinity-clusterip-transition-wtzs9
    Aug 24 11:43:03.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-412 exec execpod-affinityj4wrf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.189.244:80/ ; done'
    Aug 24 11:43:03.454: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.189.244:80/\n"
    Aug 24 11:43:03.454: INFO: stdout: "\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns\naffinity-clusterip-transition-w5cns"
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Received response from host: affinity-clusterip-transition-w5cns
    Aug 24 11:43:03.454: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-412, will wait for the garbage collector to delete the pods 08/24/23 11:43:03.476
    Aug 24 11:43:03.573: INFO: Deleting ReplicationController affinity-clusterip-transition took: 36.006419ms
    Aug 24 11:43:03.774: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.773282ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:05.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-412" for this suite. 08/24/23 11:43:05.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:05.956
Aug 24 11:43:05.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:43:05.958
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:06.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:06.027
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-76thw"  08/24/23 11:43:06.045
Aug 24 11:43:06.056: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-76thw"  08/24/23 11:43:06.056
Aug 24 11:43:06.103: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:06.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2999" for this suite. 08/24/23 11:43:06.108
------------------------------
â€¢ [0.161 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:05.956
    Aug 24 11:43:05.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:43:05.958
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:06.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:06.027
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-76thw"  08/24/23 11:43:06.045
    Aug 24 11:43:06.056: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-76thw"  08/24/23 11:43:06.056
    Aug 24 11:43:06.103: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:06.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2999" for this suite. 08/24/23 11:43:06.108
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:06.118
Aug 24 11:43:06.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:43:06.119
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:06.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:06.16
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/24/23 11:43:06.167
Aug 24 11:43:06.168: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 24 11:43:06.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
Aug 24 11:43:07.646: INFO: stderr: ""
Aug 24 11:43:07.646: INFO: stdout: "service/agnhost-replica created\n"
Aug 24 11:43:07.647: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 24 11:43:07.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
Aug 24 11:43:08.107: INFO: stderr: ""
Aug 24 11:43:08.107: INFO: stdout: "service/agnhost-primary created\n"
Aug 24 11:43:08.107: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 24 11:43:08.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
Aug 24 11:43:08.580: INFO: stderr: ""
Aug 24 11:43:08.580: INFO: stdout: "service/frontend created\n"
Aug 24 11:43:08.580: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 24 11:43:08.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
Aug 24 11:43:09.036: INFO: stderr: ""
Aug 24 11:43:09.036: INFO: stdout: "deployment.apps/frontend created\n"
Aug 24 11:43:09.036: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 24 11:43:09.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
Aug 24 11:43:09.526: INFO: stderr: ""
Aug 24 11:43:09.526: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 24 11:43:09.526: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 24 11:43:09.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
Aug 24 11:43:10.034: INFO: stderr: ""
Aug 24 11:43:10.034: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/24/23 11:43:10.034
Aug 24 11:43:10.035: INFO: Waiting for all frontend pods to be Running.
Aug 24 11:43:15.087: INFO: Waiting for frontend to serve content.
Aug 24 11:43:15.107: INFO: Trying to add a new entry to the guestbook.
Aug 24 11:43:15.122: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/24/23 11:43:15.133
Aug 24 11:43:15.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
Aug 24 11:43:15.282: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:43:15.282: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 11:43:15.282
Aug 24 11:43:15.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
Aug 24 11:43:15.442: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:43:15.442: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 11:43:15.442
Aug 24 11:43:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
Aug 24 11:43:15.593: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:43:15.593: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 11:43:15.593
Aug 24 11:43:15.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
Aug 24 11:43:15.738: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:43:15.738: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 11:43:15.738
Aug 24 11:43:15.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
Aug 24 11:43:15.886: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:43:15.887: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/24/23 11:43:15.887
Aug 24 11:43:15.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
Aug 24 11:43:16.048: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:43:16.048: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4745" for this suite. 08/24/23 11:43:16.058
------------------------------
â€¢ [SLOW TEST] [9.964 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:06.118
    Aug 24 11:43:06.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:43:06.119
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:06.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:06.16
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/24/23 11:43:06.167
    Aug 24 11:43:06.168: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 24 11:43:06.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
    Aug 24 11:43:07.646: INFO: stderr: ""
    Aug 24 11:43:07.646: INFO: stdout: "service/agnhost-replica created\n"
    Aug 24 11:43:07.647: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 24 11:43:07.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
    Aug 24 11:43:08.107: INFO: stderr: ""
    Aug 24 11:43:08.107: INFO: stdout: "service/agnhost-primary created\n"
    Aug 24 11:43:08.107: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 24 11:43:08.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
    Aug 24 11:43:08.580: INFO: stderr: ""
    Aug 24 11:43:08.580: INFO: stdout: "service/frontend created\n"
    Aug 24 11:43:08.580: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 24 11:43:08.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
    Aug 24 11:43:09.036: INFO: stderr: ""
    Aug 24 11:43:09.036: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 24 11:43:09.036: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 24 11:43:09.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
    Aug 24 11:43:09.526: INFO: stderr: ""
    Aug 24 11:43:09.526: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 24 11:43:09.526: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 24 11:43:09.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 create -f -'
    Aug 24 11:43:10.034: INFO: stderr: ""
    Aug 24 11:43:10.034: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/24/23 11:43:10.034
    Aug 24 11:43:10.035: INFO: Waiting for all frontend pods to be Running.
    Aug 24 11:43:15.087: INFO: Waiting for frontend to serve content.
    Aug 24 11:43:15.107: INFO: Trying to add a new entry to the guestbook.
    Aug 24 11:43:15.122: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/24/23 11:43:15.133
    Aug 24 11:43:15.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
    Aug 24 11:43:15.282: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:43:15.282: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 11:43:15.282
    Aug 24 11:43:15.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
    Aug 24 11:43:15.442: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:43:15.442: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 11:43:15.442
    Aug 24 11:43:15.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
    Aug 24 11:43:15.593: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:43:15.593: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 11:43:15.593
    Aug 24 11:43:15.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
    Aug 24 11:43:15.738: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:43:15.738: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 11:43:15.738
    Aug 24 11:43:15.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
    Aug 24 11:43:15.886: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:43:15.887: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/24/23 11:43:15.887
    Aug 24 11:43:15.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-4745 delete --grace-period=0 --force -f -'
    Aug 24 11:43:16.048: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:43:16.048: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:16.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4745" for this suite. 08/24/23 11:43:16.058
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:16.082
Aug 24 11:43:16.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename job 08/24/23 11:43:16.092
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:16.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:16.19
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/24/23 11:43:16.211
STEP: Ensure pods equal to parallelism count is attached to the job 08/24/23 11:43:16.236
STEP: patching /status 08/24/23 11:43:18.241
STEP: updating /status 08/24/23 11:43:18.253
STEP: get /status 08/24/23 11:43:18.29
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 11:43:18.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6588" for this suite. 08/24/23 11:43:18.302
------------------------------
â€¢ [2.230 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:16.082
    Aug 24 11:43:16.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename job 08/24/23 11:43:16.092
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:16.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:16.19
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/24/23 11:43:16.211
    STEP: Ensure pods equal to parallelism count is attached to the job 08/24/23 11:43:16.236
    STEP: patching /status 08/24/23 11:43:18.241
    STEP: updating /status 08/24/23 11:43:18.253
    STEP: get /status 08/24/23 11:43:18.29
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:43:18.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6588" for this suite. 08/24/23 11:43:18.302
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:43:18.312
Aug 24 11:43:18.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:43:18.313
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:18.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:18.395
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-69dee128-2f55-4807-8681-584f61674a8e 08/24/23 11:43:18.403
STEP: Creating configMap with name cm-test-opt-upd-d67ee0da-b8ed-468a-ba3f-25e82bbb2391 08/24/23 11:43:18.411
STEP: Creating the pod 08/24/23 11:43:18.423
Aug 24 11:43:18.436: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0" in namespace "projected-2635" to be "running and ready"
Aug 24 11:43:18.457: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.96867ms
Aug 24 11:43:18.457: INFO: The phase of Pod pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:43:20.465: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028693803s
Aug 24 11:43:20.465: INFO: The phase of Pod pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:43:22.464: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0": Phase="Running", Reason="", readiness=true. Elapsed: 4.028165223s
Aug 24 11:43:22.465: INFO: The phase of Pod pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0 is Running (Ready = true)
Aug 24 11:43:22.465: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-69dee128-2f55-4807-8681-584f61674a8e 08/24/23 11:43:22.495
STEP: Updating configmap cm-test-opt-upd-d67ee0da-b8ed-468a-ba3f-25e82bbb2391 08/24/23 11:43:22.503
STEP: Creating configMap with name cm-test-opt-create-04501352-f998-4d27-b288-8fa68147a6d1 08/24/23 11:43:22.512
STEP: waiting to observe update in volume 08/24/23 11:43:22.524
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:44:40.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2635" for this suite. 08/24/23 11:44:40.943
------------------------------
â€¢ [SLOW TEST] [82.641 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:43:18.312
    Aug 24 11:43:18.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:43:18.313
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:43:18.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:43:18.395
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-69dee128-2f55-4807-8681-584f61674a8e 08/24/23 11:43:18.403
    STEP: Creating configMap with name cm-test-opt-upd-d67ee0da-b8ed-468a-ba3f-25e82bbb2391 08/24/23 11:43:18.411
    STEP: Creating the pod 08/24/23 11:43:18.423
    Aug 24 11:43:18.436: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0" in namespace "projected-2635" to be "running and ready"
    Aug 24 11:43:18.457: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.96867ms
    Aug 24 11:43:18.457: INFO: The phase of Pod pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:43:20.465: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028693803s
    Aug 24 11:43:20.465: INFO: The phase of Pod pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:43:22.464: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0": Phase="Running", Reason="", readiness=true. Elapsed: 4.028165223s
    Aug 24 11:43:22.465: INFO: The phase of Pod pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0 is Running (Ready = true)
    Aug 24 11:43:22.465: INFO: Pod "pod-projected-configmaps-a33d7552-f747-4878-8864-8d77b53527d0" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-69dee128-2f55-4807-8681-584f61674a8e 08/24/23 11:43:22.495
    STEP: Updating configmap cm-test-opt-upd-d67ee0da-b8ed-468a-ba3f-25e82bbb2391 08/24/23 11:43:22.503
    STEP: Creating configMap with name cm-test-opt-create-04501352-f998-4d27-b288-8fa68147a6d1 08/24/23 11:43:22.512
    STEP: waiting to observe update in volume 08/24/23 11:43:22.524
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:44:40.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2635" for this suite. 08/24/23 11:44:40.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:44:40.955
Aug 24 11:44:40.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename events 08/24/23 11:44:40.958
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:40.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:40.994
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/24/23 11:44:41.001
STEP: listing all events in all namespaces 08/24/23 11:44:41.011
STEP: patching the test event 08/24/23 11:44:41.017
STEP: fetching the test event 08/24/23 11:44:41.026
STEP: updating the test event 08/24/23 11:44:41.03
STEP: getting the test event 08/24/23 11:44:41.042
STEP: deleting the test event 08/24/23 11:44:41.045
STEP: listing all events in all namespaces 08/24/23 11:44:41.058
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 24 11:44:41.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5091" for this suite. 08/24/23 11:44:41.066
------------------------------
â€¢ [0.120 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:44:40.955
    Aug 24 11:44:40.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename events 08/24/23 11:44:40.958
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:40.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:40.994
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/24/23 11:44:41.001
    STEP: listing all events in all namespaces 08/24/23 11:44:41.011
    STEP: patching the test event 08/24/23 11:44:41.017
    STEP: fetching the test event 08/24/23 11:44:41.026
    STEP: updating the test event 08/24/23 11:44:41.03
    STEP: getting the test event 08/24/23 11:44:41.042
    STEP: deleting the test event 08/24/23 11:44:41.045
    STEP: listing all events in all namespaces 08/24/23 11:44:41.058
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:44:41.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5091" for this suite. 08/24/23 11:44:41.066
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:44:41.076
Aug 24 11:44:41.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:44:41.077
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:41.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:41.104
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-12725007-412f-44b5-aaee-824dfde64a16 08/24/23 11:44:41.109
STEP: Creating secret with name secret-projected-all-test-volume-f08d4577-def7-4677-81b5-aa2c07076529 08/24/23 11:44:41.116
STEP: Creating a pod to test Check all projections for projected volume plugin 08/24/23 11:44:41.127
Aug 24 11:44:41.143: INFO: Waiting up to 5m0s for pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed" in namespace "projected-5850" to be "Succeeded or Failed"
Aug 24 11:44:41.165: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed": Phase="Pending", Reason="", readiness=false. Elapsed: 21.685457ms
Aug 24 11:44:43.169: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026398442s
Aug 24 11:44:45.171: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027909038s
STEP: Saw pod success 08/24/23 11:44:45.171
Aug 24 11:44:45.171: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed" satisfied condition "Succeeded or Failed"
Aug 24 11:44:45.174: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed container projected-all-volume-test: <nil>
STEP: delete the pod 08/24/23 11:44:45.233
Aug 24 11:44:45.258: INFO: Waiting for pod projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed to disappear
Aug 24 11:44:45.263: INFO: Pod projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 24 11:44:45.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5850" for this suite. 08/24/23 11:44:45.268
------------------------------
â€¢ [4.202 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:44:41.076
    Aug 24 11:44:41.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:44:41.077
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:41.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:41.104
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-12725007-412f-44b5-aaee-824dfde64a16 08/24/23 11:44:41.109
    STEP: Creating secret with name secret-projected-all-test-volume-f08d4577-def7-4677-81b5-aa2c07076529 08/24/23 11:44:41.116
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/24/23 11:44:41.127
    Aug 24 11:44:41.143: INFO: Waiting up to 5m0s for pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed" in namespace "projected-5850" to be "Succeeded or Failed"
    Aug 24 11:44:41.165: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed": Phase="Pending", Reason="", readiness=false. Elapsed: 21.685457ms
    Aug 24 11:44:43.169: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026398442s
    Aug 24 11:44:45.171: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027909038s
    STEP: Saw pod success 08/24/23 11:44:45.171
    Aug 24 11:44:45.171: INFO: Pod "projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed" satisfied condition "Succeeded or Failed"
    Aug 24 11:44:45.174: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed container projected-all-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:44:45.233
    Aug 24 11:44:45.258: INFO: Waiting for pod projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed to disappear
    Aug 24 11:44:45.263: INFO: Pod projected-volume-192eec9c-dbf4-4097-acaf-e5204e5febed no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:44:45.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5850" for this suite. 08/24/23 11:44:45.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:44:45.278
Aug 24 11:44:45.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 11:44:45.279
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:45.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:45.315
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:44:45.343
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:44:46.856
STEP: Deploying the webhook pod 08/24/23 11:44:46.866
STEP: Wait for the deployment to be ready 08/24/23 11:44:46.897
Aug 24 11:44:46.917: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 11:44:48.929
STEP: Verifying the service has paired with the endpoint 08/24/23 11:44:48.946
Aug 24 11:44:49.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/24/23 11:44:49.953
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/24/23 11:44:49.988
STEP: Creating a configMap that should not be mutated 08/24/23 11:44:50.002
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/24/23 11:44:50.034
STEP: Creating a configMap that should be mutated 08/24/23 11:44:50.048
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:44:50.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7836" for this suite. 08/24/23 11:44:50.267
STEP: Destroying namespace "webhook-7836-markers" for this suite. 08/24/23 11:44:50.329
------------------------------
â€¢ [SLOW TEST] [5.126 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:44:45.278
    Aug 24 11:44:45.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 11:44:45.279
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:45.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:45.315
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:44:45.343
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:44:46.856
    STEP: Deploying the webhook pod 08/24/23 11:44:46.866
    STEP: Wait for the deployment to be ready 08/24/23 11:44:46.897
    Aug 24 11:44:46.917: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 11:44:48.929
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:44:48.946
    Aug 24 11:44:49.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/24/23 11:44:49.953
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/24/23 11:44:49.988
    STEP: Creating a configMap that should not be mutated 08/24/23 11:44:50.002
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/24/23 11:44:50.034
    STEP: Creating a configMap that should be mutated 08/24/23 11:44:50.048
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:44:50.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7836" for this suite. 08/24/23 11:44:50.267
    STEP: Destroying namespace "webhook-7836-markers" for this suite. 08/24/23 11:44:50.329
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:44:50.404
Aug 24 11:44:50.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:44:50.405
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:50.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:50.535
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6b040602-ef6b-4877-b731-691aff1580e1 08/24/23 11:44:50.56
STEP: Creating the pod 08/24/23 11:44:50.582
Aug 24 11:44:50.602: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40" in namespace "projected-1370" to be "running and ready"
Aug 24 11:44:50.628: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40": Phase="Pending", Reason="", readiness=false. Elapsed: 25.703203ms
Aug 24 11:44:50.628: INFO: The phase of Pod pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:44:52.634: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031914589s
Aug 24 11:44:52.634: INFO: The phase of Pod pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:44:54.634: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40": Phase="Running", Reason="", readiness=true. Elapsed: 4.031588461s
Aug 24 11:44:54.634: INFO: The phase of Pod pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40 is Running (Ready = true)
Aug 24 11:44:54.634: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-6b040602-ef6b-4877-b731-691aff1580e1 08/24/23 11:44:54.647
STEP: waiting to observe update in volume 08/24/23 11:44:54.656
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:46:27.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1370" for this suite. 08/24/23 11:46:27.122
------------------------------
â€¢ [SLOW TEST] [96.727 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:44:50.404
    Aug 24 11:44:50.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:44:50.405
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:44:50.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:44:50.535
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-6b040602-ef6b-4877-b731-691aff1580e1 08/24/23 11:44:50.56
    STEP: Creating the pod 08/24/23 11:44:50.582
    Aug 24 11:44:50.602: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40" in namespace "projected-1370" to be "running and ready"
    Aug 24 11:44:50.628: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40": Phase="Pending", Reason="", readiness=false. Elapsed: 25.703203ms
    Aug 24 11:44:50.628: INFO: The phase of Pod pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:44:52.634: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031914589s
    Aug 24 11:44:52.634: INFO: The phase of Pod pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:44:54.634: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40": Phase="Running", Reason="", readiness=true. Elapsed: 4.031588461s
    Aug 24 11:44:54.634: INFO: The phase of Pod pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40 is Running (Ready = true)
    Aug 24 11:44:54.634: INFO: Pod "pod-projected-configmaps-f94ffe2d-876a-47c0-b17e-f2311083ca40" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-6b040602-ef6b-4877-b731-691aff1580e1 08/24/23 11:44:54.647
    STEP: waiting to observe update in volume 08/24/23 11:44:54.656
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:46:27.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1370" for this suite. 08/24/23 11:46:27.122
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:46:27.132
Aug 24 11:46:27.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename init-container 08/24/23 11:46:27.133
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:27.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:27.164
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/24/23 11:46:27.171
Aug 24 11:46:27.171: INFO: PodSpec: initContainers in spec.initContainers
Aug 24 11:47:12.480: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-95ebc227-d967-4632-9984-4da1785cc5e8", GenerateName:"", Namespace:"init-container-4069", SelfLink:"", UID:"0772d713-015a-4d41-8d0d-f3a8df2ce265", ResourceVersion:"30601", Generation:0, CreationTimestamp:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"171657693"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"1de433b2e17c47a98bab7694a86caada6fe5e577c5e8ec0af44fd4925444030b", "cni.projectcalico.org/podIP":"10.100.45.154/32", "cni.projectcalico.org/podIPs":"10.100.45.154/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058ea0d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058ea120), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 47, 12, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058ea150), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-fgz58", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004b000e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fgz58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fgz58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fgz58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00416e460), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"gitlab-1-26-36460-guscsyka22xa-node-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004dea000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416e4e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416e500)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00416e508), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00416e50c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000df6270), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.17", PodIP:"10.100.45.154", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.45.154"}}, StartTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004dea0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004dea150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://50910e0fd684a7d032f8b9809e7ffc940d147d2b8fca54144ea3efce1fa811bb", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b00280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b001e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00416e58f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:12.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4069" for this suite. 08/24/23 11:47:12.497
------------------------------
â€¢ [SLOW TEST] [45.375 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:46:27.132
    Aug 24 11:46:27.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename init-container 08/24/23 11:46:27.133
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:46:27.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:46:27.164
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/24/23 11:46:27.171
    Aug 24 11:46:27.171: INFO: PodSpec: initContainers in spec.initContainers
    Aug 24 11:47:12.480: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-95ebc227-d967-4632-9984-4da1785cc5e8", GenerateName:"", Namespace:"init-container-4069", SelfLink:"", UID:"0772d713-015a-4d41-8d0d-f3a8df2ce265", ResourceVersion:"30601", Generation:0, CreationTimestamp:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"171657693"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"1de433b2e17c47a98bab7694a86caada6fe5e577c5e8ec0af44fd4925444030b", "cni.projectcalico.org/podIP":"10.100.45.154/32", "cni.projectcalico.org/podIPs":"10.100.45.154/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058ea0d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058ea120), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 47, 12, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0058ea150), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-fgz58", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004b000e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fgz58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fgz58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-fgz58", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00416e460), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"gitlab-1-26-36460-guscsyka22xa-node-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004dea000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416e4e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00416e500)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00416e508), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00416e50c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000df6270), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.17", PodIP:"10.100.45.154", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.45.154"}}, StartTime:time.Date(2023, time.August, 24, 11, 46, 27, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004dea0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004dea150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://50910e0fd684a7d032f8b9809e7ffc940d147d2b8fca54144ea3efce1fa811bb", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b00280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b001e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00416e58f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:12.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4069" for this suite. 08/24/23 11:47:12.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:12.509
Aug 24 11:47:12.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 11:47:12.511
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:12.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:12.543
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 11:47:12.55
Aug 24 11:47:12.566: INFO: Waiting up to 5m0s for pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565" in namespace "emptydir-4220" to be "Succeeded or Failed"
Aug 24 11:47:12.584: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565": Phase="Pending", Reason="", readiness=false. Elapsed: 18.099297ms
Aug 24 11:47:14.589: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02328556s
Aug 24 11:47:16.590: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024476289s
STEP: Saw pod success 08/24/23 11:47:16.591
Aug 24 11:47:16.591: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565" satisfied condition "Succeeded or Failed"
Aug 24 11:47:16.594: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-c951542e-f48a-4489-88e1-aa2fc7501565 container test-container: <nil>
STEP: delete the pod 08/24/23 11:47:16.667
Aug 24 11:47:16.685: INFO: Waiting for pod pod-c951542e-f48a-4489-88e1-aa2fc7501565 to disappear
Aug 24 11:47:16.692: INFO: Pod pod-c951542e-f48a-4489-88e1-aa2fc7501565 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:16.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4220" for this suite. 08/24/23 11:47:16.698
------------------------------
â€¢ [4.197 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:12.509
    Aug 24 11:47:12.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:47:12.511
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:12.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:12.543
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 11:47:12.55
    Aug 24 11:47:12.566: INFO: Waiting up to 5m0s for pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565" in namespace "emptydir-4220" to be "Succeeded or Failed"
    Aug 24 11:47:12.584: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565": Phase="Pending", Reason="", readiness=false. Elapsed: 18.099297ms
    Aug 24 11:47:14.589: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02328556s
    Aug 24 11:47:16.590: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024476289s
    STEP: Saw pod success 08/24/23 11:47:16.591
    Aug 24 11:47:16.591: INFO: Pod "pod-c951542e-f48a-4489-88e1-aa2fc7501565" satisfied condition "Succeeded or Failed"
    Aug 24 11:47:16.594: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-c951542e-f48a-4489-88e1-aa2fc7501565 container test-container: <nil>
    STEP: delete the pod 08/24/23 11:47:16.667
    Aug 24 11:47:16.685: INFO: Waiting for pod pod-c951542e-f48a-4489-88e1-aa2fc7501565 to disappear
    Aug 24 11:47:16.692: INFO: Pod pod-c951542e-f48a-4489-88e1-aa2fc7501565 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:16.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4220" for this suite. 08/24/23 11:47:16.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:16.709
Aug 24 11:47:16.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-webhook 08/24/23 11:47:16.715
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:16.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:16.741
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/24/23 11:47:16.755
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 11:47:17.401
STEP: Deploying the custom resource conversion webhook pod 08/24/23 11:47:17.412
STEP: Wait for the deployment to be ready 08/24/23 11:47:17.448
Aug 24 11:47:17.465: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 11:47:19.477
STEP: Verifying the service has paired with the endpoint 08/24/23 11:47:19.498
Aug 24 11:47:20.498: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 24 11:47:20.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Creating a v1 custom resource 08/24/23 11:47:23.11
STEP: Create a v2 custom resource 08/24/23 11:47:23.165
STEP: List CRs in v1 08/24/23 11:47:23.25
STEP: List CRs in v2 08/24/23 11:47:23.267
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:23.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6629" for this suite. 08/24/23 11:47:23.934
------------------------------
â€¢ [SLOW TEST] [7.273 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:16.709
    Aug 24 11:47:16.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-webhook 08/24/23 11:47:16.715
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:16.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:16.741
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/24/23 11:47:16.755
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/24/23 11:47:17.401
    STEP: Deploying the custom resource conversion webhook pod 08/24/23 11:47:17.412
    STEP: Wait for the deployment to be ready 08/24/23 11:47:17.448
    Aug 24 11:47:17.465: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 11:47:19.477
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:47:19.498
    Aug 24 11:47:20.498: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 24 11:47:20.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Creating a v1 custom resource 08/24/23 11:47:23.11
    STEP: Create a v2 custom resource 08/24/23 11:47:23.165
    STEP: List CRs in v1 08/24/23 11:47:23.25
    STEP: List CRs in v2 08/24/23 11:47:23.267
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:23.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6629" for this suite. 08/24/23 11:47:23.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:23.986
Aug 24 11:47:23.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:47:23.987
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:24.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:24.383
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:47:24.392
Aug 24 11:47:24.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b" in namespace "downward-api-4852" to be "Succeeded or Failed"
Aug 24 11:47:24.444: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.412206ms
Aug 24 11:47:26.448: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033576942s
Aug 24 11:47:28.449: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034416005s
STEP: Saw pod success 08/24/23 11:47:28.449
Aug 24 11:47:28.449: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b" satisfied condition "Succeeded or Failed"
Aug 24 11:47:28.453: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b container client-container: <nil>
STEP: delete the pod 08/24/23 11:47:28.461
Aug 24 11:47:28.485: INFO: Waiting for pod downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b to disappear
Aug 24 11:47:28.490: INFO: Pod downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:28.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4852" for this suite. 08/24/23 11:47:28.494
------------------------------
â€¢ [4.515 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:23.986
    Aug 24 11:47:23.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:47:23.987
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:24.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:24.383
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:47:24.392
    Aug 24 11:47:24.415: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b" in namespace "downward-api-4852" to be "Succeeded or Failed"
    Aug 24 11:47:24.444: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 29.412206ms
    Aug 24 11:47:26.448: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033576942s
    Aug 24 11:47:28.449: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034416005s
    STEP: Saw pod success 08/24/23 11:47:28.449
    Aug 24 11:47:28.449: INFO: Pod "downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b" satisfied condition "Succeeded or Failed"
    Aug 24 11:47:28.453: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b container client-container: <nil>
    STEP: delete the pod 08/24/23 11:47:28.461
    Aug 24 11:47:28.485: INFO: Waiting for pod downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b to disappear
    Aug 24 11:47:28.490: INFO: Pod downwardapi-volume-4b92005a-6141-42c8-a43e-0cf58f886b0b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:28.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4852" for this suite. 08/24/23 11:47:28.494
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:28.502
Aug 24 11:47:28.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 11:47:28.503
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:28.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:28.533
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-25577ce7-ec9e-44c8-ae23-d441ec2bd5c2 08/24/23 11:47:28.538
STEP: Creating a pod to test consume secrets 08/24/23 11:47:28.552
Aug 24 11:47:28.569: INFO: Waiting up to 5m0s for pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d" in namespace "secrets-3598" to be "Succeeded or Failed"
Aug 24 11:47:28.578: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465105ms
Aug 24 11:47:30.583: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d": Phase="Running", Reason="", readiness=false. Elapsed: 2.014319611s
Aug 24 11:47:32.583: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014310575s
STEP: Saw pod success 08/24/23 11:47:32.583
Aug 24 11:47:32.584: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d" satisfied condition "Succeeded or Failed"
Aug 24 11:47:32.587: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 11:47:32.595
Aug 24 11:47:32.623: INFO: Waiting for pod pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d to disappear
Aug 24 11:47:32.629: INFO: Pod pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 11:47:32.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3598" for this suite. 08/24/23 11:47:32.634
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:28.502
    Aug 24 11:47:28.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 11:47:28.503
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:28.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:28.533
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-25577ce7-ec9e-44c8-ae23-d441ec2bd5c2 08/24/23 11:47:28.538
    STEP: Creating a pod to test consume secrets 08/24/23 11:47:28.552
    Aug 24 11:47:28.569: INFO: Waiting up to 5m0s for pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d" in namespace "secrets-3598" to be "Succeeded or Failed"
    Aug 24 11:47:28.578: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.465105ms
    Aug 24 11:47:30.583: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d": Phase="Running", Reason="", readiness=false. Elapsed: 2.014319611s
    Aug 24 11:47:32.583: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014310575s
    STEP: Saw pod success 08/24/23 11:47:32.583
    Aug 24 11:47:32.584: INFO: Pod "pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d" satisfied condition "Succeeded or Failed"
    Aug 24 11:47:32.587: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:47:32.595
    Aug 24 11:47:32.623: INFO: Waiting for pod pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d to disappear
    Aug 24 11:47:32.629: INFO: Pod pod-secrets-3462640e-02b6-4d06-8d37-8ef4987d103d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:47:32.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3598" for this suite. 08/24/23 11:47:32.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:47:32.652
Aug 24 11:47:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 11:47:32.654
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:32.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:32.683
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-ttwkw" 08/24/23 11:47:32.696
Aug 24 11:47:32.713: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard cpu limit of 500m
Aug 24 11:47:32.713: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-ttwkw" /status 08/24/23 11:47:32.713
STEP: Confirm /status for "e2e-rq-status-ttwkw" resourceQuota via watch 08/24/23 11:47:32.735
Aug 24 11:47:32.739: INFO: observed resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList(nil)
Aug 24 11:47:32.739: INFO: Found resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 24 11:47:32.740: INFO: ResourceQuota "e2e-rq-status-ttwkw" /status was updated
STEP: Patching hard spec values for cpu & memory 08/24/23 11:47:32.743
Aug 24 11:47:32.754: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard cpu limit of 1
Aug 24 11:47:32.754: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-ttwkw" /status 08/24/23 11:47:32.754
STEP: Confirm /status for "e2e-rq-status-ttwkw" resourceQuota via watch 08/24/23 11:47:32.77
Aug 24 11:47:32.773: INFO: observed resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 24 11:47:32.773: INFO: Found resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 24 11:47:32.773: INFO: ResourceQuota "e2e-rq-status-ttwkw" /status was patched
STEP: Get "e2e-rq-status-ttwkw" /status 08/24/23 11:47:32.773
Aug 24 11:47:32.780: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard cpu of 1
Aug 24 11:47:32.780: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-ttwkw" /status before checking Spec is unchanged 08/24/23 11:47:32.783
Aug 24 11:47:32.792: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard cpu of 2
Aug 24 11:47:32.792: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard memory of 2Gi
Aug 24 11:47:32.795: INFO: Found resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 24 11:49:12.804: INFO: ResourceQuota "e2e-rq-status-ttwkw" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 11:49:12.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7662" for this suite. 08/24/23 11:49:12.81
------------------------------
â€¢ [SLOW TEST] [100.167 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:47:32.652
    Aug 24 11:47:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 11:47:32.654
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:47:32.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:47:32.683
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-ttwkw" 08/24/23 11:47:32.696
    Aug 24 11:47:32.713: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard cpu limit of 500m
    Aug 24 11:47:32.713: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-ttwkw" /status 08/24/23 11:47:32.713
    STEP: Confirm /status for "e2e-rq-status-ttwkw" resourceQuota via watch 08/24/23 11:47:32.735
    Aug 24 11:47:32.739: INFO: observed resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList(nil)
    Aug 24 11:47:32.739: INFO: Found resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 24 11:47:32.740: INFO: ResourceQuota "e2e-rq-status-ttwkw" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/24/23 11:47:32.743
    Aug 24 11:47:32.754: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard cpu limit of 1
    Aug 24 11:47:32.754: INFO: Resource quota "e2e-rq-status-ttwkw" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-ttwkw" /status 08/24/23 11:47:32.754
    STEP: Confirm /status for "e2e-rq-status-ttwkw" resourceQuota via watch 08/24/23 11:47:32.77
    Aug 24 11:47:32.773: INFO: observed resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 24 11:47:32.773: INFO: Found resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 24 11:47:32.773: INFO: ResourceQuota "e2e-rq-status-ttwkw" /status was patched
    STEP: Get "e2e-rq-status-ttwkw" /status 08/24/23 11:47:32.773
    Aug 24 11:47:32.780: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard cpu of 1
    Aug 24 11:47:32.780: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-ttwkw" /status before checking Spec is unchanged 08/24/23 11:47:32.783
    Aug 24 11:47:32.792: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard cpu of 2
    Aug 24 11:47:32.792: INFO: Resourcequota "e2e-rq-status-ttwkw" reports status: hard memory of 2Gi
    Aug 24 11:47:32.795: INFO: Found resourceQuota "e2e-rq-status-ttwkw" in namespace "resourcequota-7662" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 24 11:49:12.804: INFO: ResourceQuota "e2e-rq-status-ttwkw" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:49:12.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7662" for this suite. 08/24/23 11:49:12.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:49:12.82
Aug 24 11:49:12.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:49:12.822
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:12.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:12.851
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 24 11:49:12.864: INFO: Got root ca configmap in namespace "svcaccounts-8687"
Aug 24 11:49:12.873: INFO: Deleted root ca configmap in namespace "svcaccounts-8687"
STEP: waiting for a new root ca configmap created 08/24/23 11:49:13.374
Aug 24 11:49:13.378: INFO: Recreated root ca configmap in namespace "svcaccounts-8687"
Aug 24 11:49:13.390: INFO: Updated root ca configmap in namespace "svcaccounts-8687"
STEP: waiting for the root ca configmap reconciled 08/24/23 11:49:13.891
Aug 24 11:49:13.895: INFO: Reconciled root ca configmap in namespace "svcaccounts-8687"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 11:49:13.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8687" for this suite. 08/24/23 11:49:13.901
------------------------------
â€¢ [1.092 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:49:12.82
    Aug 24 11:49:12.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 11:49:12.822
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:12.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:12.851
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 24 11:49:12.864: INFO: Got root ca configmap in namespace "svcaccounts-8687"
    Aug 24 11:49:12.873: INFO: Deleted root ca configmap in namespace "svcaccounts-8687"
    STEP: waiting for a new root ca configmap created 08/24/23 11:49:13.374
    Aug 24 11:49:13.378: INFO: Recreated root ca configmap in namespace "svcaccounts-8687"
    Aug 24 11:49:13.390: INFO: Updated root ca configmap in namespace "svcaccounts-8687"
    STEP: waiting for the root ca configmap reconciled 08/24/23 11:49:13.891
    Aug 24 11:49:13.895: INFO: Reconciled root ca configmap in namespace "svcaccounts-8687"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:49:13.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8687" for this suite. 08/24/23 11:49:13.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:49:13.916
Aug 24 11:49:13.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename gc 08/24/23 11:49:13.917
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:13.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:13.94
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/24/23 11:49:13.952
STEP: create the rc2 08/24/23 11:49:13.96
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/24/23 11:49:19.03
STEP: delete the rc simpletest-rc-to-be-deleted 08/24/23 11:49:21.847
STEP: wait for the rc to be deleted 08/24/23 11:49:21.871
Aug 24 11:49:27.032: INFO: 68 pods remaining
Aug 24 11:49:27.032: INFO: 68 pods has nil DeletionTimestamp
Aug 24 11:49:27.032: INFO: 
STEP: Gathering metrics 08/24/23 11:49:31.891
W0824 11:49:31.924570      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 24 11:49:31.924: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 24 11:49:31.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-24cgd" in namespace "gc-5260"
Aug 24 11:49:31.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-25rq6" in namespace "gc-5260"
Aug 24 11:49:32.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vwkn" in namespace "gc-5260"
Aug 24 11:49:32.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-42mnd" in namespace "gc-5260"
Aug 24 11:49:32.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-45s8x" in namespace "gc-5260"
Aug 24 11:49:32.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-56s4h" in namespace "gc-5260"
Aug 24 11:49:32.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-59kj5" in namespace "gc-5260"
Aug 24 11:49:32.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-5djhh" in namespace "gc-5260"
Aug 24 11:49:32.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m9ns" in namespace "gc-5260"
Aug 24 11:49:32.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qw4c" in namespace "gc-5260"
Aug 24 11:49:32.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-66rfn" in namespace "gc-5260"
Aug 24 11:49:32.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dk7v" in namespace "gc-5260"
Aug 24 11:49:32.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sq58" in namespace "gc-5260"
Aug 24 11:49:32.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-7l7h6" in namespace "gc-5260"
Aug 24 11:49:32.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pglm" in namespace "gc-5260"
Aug 24 11:49:32.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lngb" in namespace "gc-5260"
Aug 24 11:49:32.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-988l5" in namespace "gc-5260"
Aug 24 11:49:32.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fcv6" in namespace "gc-5260"
Aug 24 11:49:32.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbh55" in namespace "gc-5260"
Aug 24 11:49:32.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbjgt" in namespace "gc-5260"
Aug 24 11:49:32.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdjlc" in namespace "gc-5260"
Aug 24 11:49:32.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf97t" in namespace "gc-5260"
Aug 24 11:49:32.944: INFO: Deleting pod "simpletest-rc-to-be-deleted-blpfr" in namespace "gc-5260"
Aug 24 11:49:33.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5ghs" in namespace "gc-5260"
Aug 24 11:49:33.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7dd5" in namespace "gc-5260"
Aug 24 11:49:33.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnnfw" in namespace "gc-5260"
Aug 24 11:49:33.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpdzm" in namespace "gc-5260"
Aug 24 11:49:33.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4cc9" in namespace "gc-5260"
Aug 24 11:49:33.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6s22" in namespace "gc-5260"
Aug 24 11:49:33.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgztq" in namespace "gc-5260"
Aug 24 11:49:33.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-dt9wd" in namespace "gc-5260"
Aug 24 11:49:33.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvt6s" in namespace "gc-5260"
Aug 24 11:49:33.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-f972t" in namespace "gc-5260"
Aug 24 11:49:33.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6qfw" in namespace "gc-5260"
Aug 24 11:49:33.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb67h" in namespace "gc-5260"
Aug 24 11:49:33.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-ght5l" in namespace "gc-5260"
Aug 24 11:49:33.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-grqng" in namespace "gc-5260"
Aug 24 11:49:33.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsfm9" in namespace "gc-5260"
Aug 24 11:49:33.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxf5w" in namespace "gc-5260"
Aug 24 11:49:33.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2r6k" in namespace "gc-5260"
Aug 24 11:49:33.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgnm5" in namespace "gc-5260"
Aug 24 11:49:34.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhb6c" in namespace "gc-5260"
Aug 24 11:49:34.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjqq8" in namespace "gc-5260"
Aug 24 11:49:34.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpxrf" in namespace "gc-5260"
Aug 24 11:49:34.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv9lg" in namespace "gc-5260"
Aug 24 11:49:34.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2pj7" in namespace "gc-5260"
Aug 24 11:49:34.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2rcl" in namespace "gc-5260"
Aug 24 11:49:34.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9mzg" in namespace "gc-5260"
Aug 24 11:49:34.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-km2h4" in namespace "gc-5260"
Aug 24 11:49:34.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-kn52w" in namespace "gc-5260"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 11:49:34.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5260" for this suite. 08/24/23 11:49:34.689
------------------------------
â€¢ [SLOW TEST] [20.789 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:49:13.916
    Aug 24 11:49:13.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename gc 08/24/23 11:49:13.917
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:13.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:13.94
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/24/23 11:49:13.952
    STEP: create the rc2 08/24/23 11:49:13.96
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/24/23 11:49:19.03
    STEP: delete the rc simpletest-rc-to-be-deleted 08/24/23 11:49:21.847
    STEP: wait for the rc to be deleted 08/24/23 11:49:21.871
    Aug 24 11:49:27.032: INFO: 68 pods remaining
    Aug 24 11:49:27.032: INFO: 68 pods has nil DeletionTimestamp
    Aug 24 11:49:27.032: INFO: 
    STEP: Gathering metrics 08/24/23 11:49:31.891
    W0824 11:49:31.924570      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 24 11:49:31.924: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 24 11:49:31.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-24cgd" in namespace "gc-5260"
    Aug 24 11:49:31.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-25rq6" in namespace "gc-5260"
    Aug 24 11:49:32.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vwkn" in namespace "gc-5260"
    Aug 24 11:49:32.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-42mnd" in namespace "gc-5260"
    Aug 24 11:49:32.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-45s8x" in namespace "gc-5260"
    Aug 24 11:49:32.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-56s4h" in namespace "gc-5260"
    Aug 24 11:49:32.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-59kj5" in namespace "gc-5260"
    Aug 24 11:49:32.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-5djhh" in namespace "gc-5260"
    Aug 24 11:49:32.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m9ns" in namespace "gc-5260"
    Aug 24 11:49:32.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qw4c" in namespace "gc-5260"
    Aug 24 11:49:32.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-66rfn" in namespace "gc-5260"
    Aug 24 11:49:32.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dk7v" in namespace "gc-5260"
    Aug 24 11:49:32.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sq58" in namespace "gc-5260"
    Aug 24 11:49:32.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-7l7h6" in namespace "gc-5260"
    Aug 24 11:49:32.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pglm" in namespace "gc-5260"
    Aug 24 11:49:32.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lngb" in namespace "gc-5260"
    Aug 24 11:49:32.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-988l5" in namespace "gc-5260"
    Aug 24 11:49:32.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fcv6" in namespace "gc-5260"
    Aug 24 11:49:32.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbh55" in namespace "gc-5260"
    Aug 24 11:49:32.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbjgt" in namespace "gc-5260"
    Aug 24 11:49:32.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdjlc" in namespace "gc-5260"
    Aug 24 11:49:32.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf97t" in namespace "gc-5260"
    Aug 24 11:49:32.944: INFO: Deleting pod "simpletest-rc-to-be-deleted-blpfr" in namespace "gc-5260"
    Aug 24 11:49:33.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5ghs" in namespace "gc-5260"
    Aug 24 11:49:33.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7dd5" in namespace "gc-5260"
    Aug 24 11:49:33.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnnfw" in namespace "gc-5260"
    Aug 24 11:49:33.151: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpdzm" in namespace "gc-5260"
    Aug 24 11:49:33.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4cc9" in namespace "gc-5260"
    Aug 24 11:49:33.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6s22" in namespace "gc-5260"
    Aug 24 11:49:33.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgztq" in namespace "gc-5260"
    Aug 24 11:49:33.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-dt9wd" in namespace "gc-5260"
    Aug 24 11:49:33.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvt6s" in namespace "gc-5260"
    Aug 24 11:49:33.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-f972t" in namespace "gc-5260"
    Aug 24 11:49:33.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6qfw" in namespace "gc-5260"
    Aug 24 11:49:33.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-gb67h" in namespace "gc-5260"
    Aug 24 11:49:33.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-ght5l" in namespace "gc-5260"
    Aug 24 11:49:33.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-grqng" in namespace "gc-5260"
    Aug 24 11:49:33.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsfm9" in namespace "gc-5260"
    Aug 24 11:49:33.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxf5w" in namespace "gc-5260"
    Aug 24 11:49:33.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2r6k" in namespace "gc-5260"
    Aug 24 11:49:33.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgnm5" in namespace "gc-5260"
    Aug 24 11:49:34.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhb6c" in namespace "gc-5260"
    Aug 24 11:49:34.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjqq8" in namespace "gc-5260"
    Aug 24 11:49:34.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-hpxrf" in namespace "gc-5260"
    Aug 24 11:49:34.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-hv9lg" in namespace "gc-5260"
    Aug 24 11:49:34.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2pj7" in namespace "gc-5260"
    Aug 24 11:49:34.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2rcl" in namespace "gc-5260"
    Aug 24 11:49:34.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9mzg" in namespace "gc-5260"
    Aug 24 11:49:34.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-km2h4" in namespace "gc-5260"
    Aug 24 11:49:34.646: INFO: Deleting pod "simpletest-rc-to-be-deleted-kn52w" in namespace "gc-5260"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:49:34.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5260" for this suite. 08/24/23 11:49:34.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:49:34.711
Aug 24 11:49:34.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:49:34.712
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:34.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:34.845
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:49:34.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7555" for this suite. 08/24/23 11:49:34.921
------------------------------
â€¢ [0.256 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:49:34.711
    Aug 24 11:49:34.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:49:34.712
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:34.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:34.845
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:49:34.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7555" for this suite. 08/24/23 11:49:34.921
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:49:34.968
Aug 24 11:49:34.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename disruption 08/24/23 11:49:34.969
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:35.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:35.03
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/24/23 11:49:35.057
STEP: Waiting for the pdb to be processed 08/24/23 11:49:35.076
STEP: First trying to evict a pod which shouldn't be evictable 08/24/23 11:49:35.109
STEP: Waiting for all pods to be running 08/24/23 11:49:35.109
Aug 24 11:49:35.128: INFO: pods: 0 < 3
Aug 24 11:49:37.134: INFO: running pods: 0 < 3
Aug 24 11:49:39.133: INFO: running pods: 1 < 3
STEP: locating a running pod 08/24/23 11:49:41.132
STEP: Updating the pdb to allow a pod to be evicted 08/24/23 11:49:41.144
STEP: Waiting for the pdb to be processed 08/24/23 11:49:41.158
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:49:43.167
STEP: Waiting for all pods to be running 08/24/23 11:49:43.167
STEP: Waiting for the pdb to observed all healthy pods 08/24/23 11:49:43.171
STEP: Patching the pdb to disallow a pod to be evicted 08/24/23 11:49:43.22
STEP: Waiting for the pdb to be processed 08/24/23 11:49:43.285
STEP: Waiting for all pods to be running 08/24/23 11:49:45.304
STEP: locating a running pod 08/24/23 11:49:45.312
STEP: Deleting the pdb to allow a pod to be evicted 08/24/23 11:49:45.328
STEP: Waiting for the pdb to be deleted 08/24/23 11:49:45.339
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:49:45.342
STEP: Waiting for all pods to be running 08/24/23 11:49:45.342
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 11:49:45.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-3501" for this suite. 08/24/23 11:49:45.396
------------------------------
â€¢ [SLOW TEST] [10.459 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:49:34.968
    Aug 24 11:49:34.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename disruption 08/24/23 11:49:34.969
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:35.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:35.03
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/24/23 11:49:35.057
    STEP: Waiting for the pdb to be processed 08/24/23 11:49:35.076
    STEP: First trying to evict a pod which shouldn't be evictable 08/24/23 11:49:35.109
    STEP: Waiting for all pods to be running 08/24/23 11:49:35.109
    Aug 24 11:49:35.128: INFO: pods: 0 < 3
    Aug 24 11:49:37.134: INFO: running pods: 0 < 3
    Aug 24 11:49:39.133: INFO: running pods: 1 < 3
    STEP: locating a running pod 08/24/23 11:49:41.132
    STEP: Updating the pdb to allow a pod to be evicted 08/24/23 11:49:41.144
    STEP: Waiting for the pdb to be processed 08/24/23 11:49:41.158
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:49:43.167
    STEP: Waiting for all pods to be running 08/24/23 11:49:43.167
    STEP: Waiting for the pdb to observed all healthy pods 08/24/23 11:49:43.171
    STEP: Patching the pdb to disallow a pod to be evicted 08/24/23 11:49:43.22
    STEP: Waiting for the pdb to be processed 08/24/23 11:49:43.285
    STEP: Waiting for all pods to be running 08/24/23 11:49:45.304
    STEP: locating a running pod 08/24/23 11:49:45.312
    STEP: Deleting the pdb to allow a pod to be evicted 08/24/23 11:49:45.328
    STEP: Waiting for the pdb to be deleted 08/24/23 11:49:45.339
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/24/23 11:49:45.342
    STEP: Waiting for all pods to be running 08/24/23 11:49:45.342
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:49:45.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-3501" for this suite. 08/24/23 11:49:45.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:49:45.429
Aug 24 11:49:45.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename cronjob 08/24/23 11:49:45.431
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:45.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:45.474
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/24/23 11:49:45.479
STEP: Ensuring no jobs are scheduled 08/24/23 11:49:45.488
STEP: Ensuring no job exists by listing jobs explicitly 08/24/23 11:54:45.495
STEP: Removing cronjob 08/24/23 11:54:45.499
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 11:54:45.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1119" for this suite. 08/24/23 11:54:45.522
------------------------------
â€¢ [SLOW TEST] [300.104 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:49:45.429
    Aug 24 11:49:45.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename cronjob 08/24/23 11:49:45.431
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:49:45.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:49:45.474
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/24/23 11:49:45.479
    STEP: Ensuring no jobs are scheduled 08/24/23 11:49:45.488
    STEP: Ensuring no job exists by listing jobs explicitly 08/24/23 11:54:45.495
    STEP: Removing cronjob 08/24/23 11:54:45.499
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:54:45.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1119" for this suite. 08/24/23 11:54:45.522
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:54:45.533
Aug 24 11:54:45.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:54:45.535
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:54:45.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:54:45.563
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/24/23 11:54:45.57
Aug 24 11:54:45.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 create -f -'
Aug 24 11:54:47.175: INFO: stderr: ""
Aug 24 11:54:47.175: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:54:47.175
Aug 24 11:54:47.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:54:47.301: INFO: stderr: ""
Aug 24 11:54:47.301: INFO: stdout: "update-demo-nautilus-42n2g update-demo-nautilus-qgpp8 "
Aug 24 11:54:47.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-42n2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:54:47.436: INFO: stderr: ""
Aug 24 11:54:47.436: INFO: stdout: ""
Aug 24 11:54:47.436: INFO: update-demo-nautilus-42n2g is created but not running
Aug 24 11:54:52.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 24 11:54:52.549: INFO: stderr: ""
Aug 24 11:54:52.549: INFO: stdout: "update-demo-nautilus-42n2g update-demo-nautilus-qgpp8 "
Aug 24 11:54:52.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-42n2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:54:52.659: INFO: stderr: ""
Aug 24 11:54:52.659: INFO: stdout: "true"
Aug 24 11:54:52.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-42n2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:54:52.770: INFO: stderr: ""
Aug 24 11:54:52.770: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:54:52.770: INFO: validating pod update-demo-nautilus-42n2g
Aug 24 11:54:52.777: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:54:52.777: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:54:52.777: INFO: update-demo-nautilus-42n2g is verified up and running
Aug 24 11:54:52.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-qgpp8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 24 11:54:52.887: INFO: stderr: ""
Aug 24 11:54:52.887: INFO: stdout: "true"
Aug 24 11:54:52.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-qgpp8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 24 11:54:53.026: INFO: stderr: ""
Aug 24 11:54:53.026: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 24 11:54:53.026: INFO: validating pod update-demo-nautilus-qgpp8
Aug 24 11:54:53.032: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 24 11:54:53.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 24 11:54:53.032: INFO: update-demo-nautilus-qgpp8 is verified up and running
STEP: using delete to clean up resources 08/24/23 11:54:53.032
Aug 24 11:54:53.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 delete --grace-period=0 --force -f -'
Aug 24 11:54:53.169: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 11:54:53.169: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 24 11:54:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get rc,svc -l name=update-demo --no-headers'
Aug 24 11:54:53.313: INFO: stderr: "No resources found in kubectl-8825 namespace.\n"
Aug 24 11:54:53.313: INFO: stdout: ""
Aug 24 11:54:53.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 24 11:54:53.450: INFO: stderr: ""
Aug 24 11:54:53.450: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:54:53.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8825" for this suite. 08/24/23 11:54:53.455
------------------------------
â€¢ [SLOW TEST] [7.933 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:54:45.533
    Aug 24 11:54:45.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:54:45.535
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:54:45.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:54:45.563
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/24/23 11:54:45.57
    Aug 24 11:54:45.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 create -f -'
    Aug 24 11:54:47.175: INFO: stderr: ""
    Aug 24 11:54:47.175: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/24/23 11:54:47.175
    Aug 24 11:54:47.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:54:47.301: INFO: stderr: ""
    Aug 24 11:54:47.301: INFO: stdout: "update-demo-nautilus-42n2g update-demo-nautilus-qgpp8 "
    Aug 24 11:54:47.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-42n2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:54:47.436: INFO: stderr: ""
    Aug 24 11:54:47.436: INFO: stdout: ""
    Aug 24 11:54:47.436: INFO: update-demo-nautilus-42n2g is created but not running
    Aug 24 11:54:52.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 24 11:54:52.549: INFO: stderr: ""
    Aug 24 11:54:52.549: INFO: stdout: "update-demo-nautilus-42n2g update-demo-nautilus-qgpp8 "
    Aug 24 11:54:52.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-42n2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:54:52.659: INFO: stderr: ""
    Aug 24 11:54:52.659: INFO: stdout: "true"
    Aug 24 11:54:52.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-42n2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:54:52.770: INFO: stderr: ""
    Aug 24 11:54:52.770: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:54:52.770: INFO: validating pod update-demo-nautilus-42n2g
    Aug 24 11:54:52.777: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:54:52.777: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:54:52.777: INFO: update-demo-nautilus-42n2g is verified up and running
    Aug 24 11:54:52.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-qgpp8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 24 11:54:52.887: INFO: stderr: ""
    Aug 24 11:54:52.887: INFO: stdout: "true"
    Aug 24 11:54:52.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods update-demo-nautilus-qgpp8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 24 11:54:53.026: INFO: stderr: ""
    Aug 24 11:54:53.026: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 24 11:54:53.026: INFO: validating pod update-demo-nautilus-qgpp8
    Aug 24 11:54:53.032: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 24 11:54:53.032: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 24 11:54:53.032: INFO: update-demo-nautilus-qgpp8 is verified up and running
    STEP: using delete to clean up resources 08/24/23 11:54:53.032
    Aug 24 11:54:53.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 delete --grace-period=0 --force -f -'
    Aug 24 11:54:53.169: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 11:54:53.169: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 24 11:54:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get rc,svc -l name=update-demo --no-headers'
    Aug 24 11:54:53.313: INFO: stderr: "No resources found in kubectl-8825 namespace.\n"
    Aug 24 11:54:53.313: INFO: stdout: ""
    Aug 24 11:54:53.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8825 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 24 11:54:53.450: INFO: stderr: ""
    Aug 24 11:54:53.450: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:54:53.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8825" for this suite. 08/24/23 11:54:53.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:54:53.468
Aug 24 11:54:53.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 11:54:53.469
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:54:53.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:54:53.584
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 24 11:54:53.618: INFO: Waiting up to 5m0s for pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61" in namespace "pods-1359" to be "running and ready"
Aug 24 11:54:53.631: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 12.919941ms
Aug 24 11:54:53.631: INFO: The phase of Pod server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:54:55.636: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017769117s
Aug 24 11:54:55.636: INFO: The phase of Pod server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:54:57.636: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61": Phase="Running", Reason="", readiness=true. Elapsed: 4.017640262s
Aug 24 11:54:57.636: INFO: The phase of Pod server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61 is Running (Ready = true)
Aug 24 11:54:57.636: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61" satisfied condition "running and ready"
Aug 24 11:54:57.683: INFO: Waiting up to 5m0s for pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20" in namespace "pods-1359" to be "Succeeded or Failed"
Aug 24 11:54:57.697: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20": Phase="Pending", Reason="", readiness=false. Elapsed: 13.537035ms
Aug 24 11:54:59.702: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018339762s
Aug 24 11:55:01.701: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017766781s
STEP: Saw pod success 08/24/23 11:55:01.701
Aug 24 11:55:01.702: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20" satisfied condition "Succeeded or Failed"
Aug 24 11:55:01.705: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20 container env3cont: <nil>
STEP: delete the pod 08/24/23 11:55:01.762
Aug 24 11:55:01.780: INFO: Waiting for pod client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20 to disappear
Aug 24 11:55:01.786: INFO: Pod client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:01.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1359" for this suite. 08/24/23 11:55:01.79
------------------------------
â€¢ [SLOW TEST] [8.339 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:54:53.468
    Aug 24 11:54:53.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 11:54:53.469
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:54:53.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:54:53.584
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 24 11:54:53.618: INFO: Waiting up to 5m0s for pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61" in namespace "pods-1359" to be "running and ready"
    Aug 24 11:54:53.631: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 12.919941ms
    Aug 24 11:54:53.631: INFO: The phase of Pod server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:54:55.636: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017769117s
    Aug 24 11:54:55.636: INFO: The phase of Pod server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:54:57.636: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61": Phase="Running", Reason="", readiness=true. Elapsed: 4.017640262s
    Aug 24 11:54:57.636: INFO: The phase of Pod server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61 is Running (Ready = true)
    Aug 24 11:54:57.636: INFO: Pod "server-envvars-0ca9b136-c905-49b4-a9e7-eba9e8e7fb61" satisfied condition "running and ready"
    Aug 24 11:54:57.683: INFO: Waiting up to 5m0s for pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20" in namespace "pods-1359" to be "Succeeded or Failed"
    Aug 24 11:54:57.697: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20": Phase="Pending", Reason="", readiness=false. Elapsed: 13.537035ms
    Aug 24 11:54:59.702: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018339762s
    Aug 24 11:55:01.701: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017766781s
    STEP: Saw pod success 08/24/23 11:55:01.701
    Aug 24 11:55:01.702: INFO: Pod "client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20" satisfied condition "Succeeded or Failed"
    Aug 24 11:55:01.705: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20 container env3cont: <nil>
    STEP: delete the pod 08/24/23 11:55:01.762
    Aug 24 11:55:01.780: INFO: Waiting for pod client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20 to disappear
    Aug 24 11:55:01.786: INFO: Pod client-envvars-f9b2a00c-890c-4fdf-9d9e-432e7eb8cd20 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:01.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1359" for this suite. 08/24/23 11:55:01.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:01.813
Aug 24 11:55:01.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename endpointslice 08/24/23 11:55:01.814
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:01.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:01.849
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/24/23 11:55:01.855
STEP: getting /apis/discovery.k8s.io 08/24/23 11:55:01.86
STEP: getting /apis/discovery.k8s.iov1 08/24/23 11:55:01.862
STEP: creating 08/24/23 11:55:01.865
STEP: getting 08/24/23 11:55:01.894
STEP: listing 08/24/23 11:55:01.898
STEP: watching 08/24/23 11:55:01.902
Aug 24 11:55:01.902: INFO: starting watch
STEP: cluster-wide listing 08/24/23 11:55:01.905
STEP: cluster-wide watching 08/24/23 11:55:01.915
Aug 24 11:55:01.915: INFO: starting watch
STEP: patching 08/24/23 11:55:01.919
STEP: updating 08/24/23 11:55:01.93
Aug 24 11:55:01.943: INFO: waiting for watch events with expected annotations
Aug 24 11:55:01.943: INFO: saw patched and updated annotations
STEP: deleting 08/24/23 11:55:01.943
STEP: deleting a collection 08/24/23 11:55:01.959
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:01.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4339" for this suite. 08/24/23 11:55:01.983
------------------------------
â€¢ [0.184 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:01.813
    Aug 24 11:55:01.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename endpointslice 08/24/23 11:55:01.814
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:01.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:01.849
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/24/23 11:55:01.855
    STEP: getting /apis/discovery.k8s.io 08/24/23 11:55:01.86
    STEP: getting /apis/discovery.k8s.iov1 08/24/23 11:55:01.862
    STEP: creating 08/24/23 11:55:01.865
    STEP: getting 08/24/23 11:55:01.894
    STEP: listing 08/24/23 11:55:01.898
    STEP: watching 08/24/23 11:55:01.902
    Aug 24 11:55:01.902: INFO: starting watch
    STEP: cluster-wide listing 08/24/23 11:55:01.905
    STEP: cluster-wide watching 08/24/23 11:55:01.915
    Aug 24 11:55:01.915: INFO: starting watch
    STEP: patching 08/24/23 11:55:01.919
    STEP: updating 08/24/23 11:55:01.93
    Aug 24 11:55:01.943: INFO: waiting for watch events with expected annotations
    Aug 24 11:55:01.943: INFO: saw patched and updated annotations
    STEP: deleting 08/24/23 11:55:01.943
    STEP: deleting a collection 08/24/23 11:55:01.959
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:01.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4339" for this suite. 08/24/23 11:55:01.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:02.001
Aug 24 11:55:02.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename conformance-tests 08/24/23 11:55:02.002
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:02.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:02.031
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/24/23 11:55:02.038
Aug 24 11:55:02.038: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:02.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-7123" for this suite. 08/24/23 11:55:02.048
------------------------------
â€¢ [0.057 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:02.001
    Aug 24 11:55:02.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename conformance-tests 08/24/23 11:55:02.002
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:02.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:02.031
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/24/23 11:55:02.038
    Aug 24 11:55:02.038: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:02.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-7123" for this suite. 08/24/23 11:55:02.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:02.059
Aug 24 11:55:02.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 11:55:02.06
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:02.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:02.091
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 11:55:02.096
Aug 24 11:55:02.113: INFO: Waiting up to 5m0s for pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf" in namespace "emptydir-6339" to be "Succeeded or Failed"
Aug 24 11:55:02.118: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830453ms
Aug 24 11:55:04.124: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010490892s
Aug 24 11:55:06.122: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008942769s
STEP: Saw pod success 08/24/23 11:55:06.122
Aug 24 11:55:06.123: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf" satisfied condition "Succeeded or Failed"
Aug 24 11:55:06.129: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf container test-container: <nil>
STEP: delete the pod 08/24/23 11:55:06.196
Aug 24 11:55:06.217: INFO: Waiting for pod pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf to disappear
Aug 24 11:55:06.221: INFO: Pod pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:06.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6339" for this suite. 08/24/23 11:55:06.226
------------------------------
â€¢ [4.177 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:02.059
    Aug 24 11:55:02.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:55:02.06
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:02.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:02.091
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 11:55:02.096
    Aug 24 11:55:02.113: INFO: Waiting up to 5m0s for pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf" in namespace "emptydir-6339" to be "Succeeded or Failed"
    Aug 24 11:55:02.118: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.830453ms
    Aug 24 11:55:04.124: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010490892s
    Aug 24 11:55:06.122: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008942769s
    STEP: Saw pod success 08/24/23 11:55:06.122
    Aug 24 11:55:06.123: INFO: Pod "pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf" satisfied condition "Succeeded or Failed"
    Aug 24 11:55:06.129: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf container test-container: <nil>
    STEP: delete the pod 08/24/23 11:55:06.196
    Aug 24 11:55:06.217: INFO: Waiting for pod pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf to disappear
    Aug 24 11:55:06.221: INFO: Pod pod-74f3fe1c-b274-4c93-bb24-1fa3c56f4ebf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:06.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6339" for this suite. 08/24/23 11:55:06.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:06.242
Aug 24 11:55:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:55:06.243
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:06.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:06.275
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-309 08/24/23 11:55:06.288
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 11:55:06.314
STEP: creating service externalsvc in namespace services-309 08/24/23 11:55:06.315
STEP: creating replication controller externalsvc in namespace services-309 08/24/23 11:55:06.347
I0824 11:55:06.366357      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-309, replica count: 2
I0824 11:55:09.417428      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/24/23 11:55:09.433
Aug 24 11:55:09.496: INFO: Creating new exec pod
Aug 24 11:55:09.523: INFO: Waiting up to 5m0s for pod "execpodsthqk" in namespace "services-309" to be "running"
Aug 24 11:55:09.537: INFO: Pod "execpodsthqk": Phase="Pending", Reason="", readiness=false. Elapsed: 13.958386ms
Aug 24 11:55:11.543: INFO: Pod "execpodsthqk": Phase="Running", Reason="", readiness=true. Elapsed: 2.020010157s
Aug 24 11:55:11.543: INFO: Pod "execpodsthqk" satisfied condition "running"
Aug 24 11:55:11.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-309 exec execpodsthqk -- /bin/sh -x -c nslookup nodeport-service.services-309.svc.cluster.local'
Aug 24 11:55:11.824: INFO: stderr: "+ nslookup nodeport-service.services-309.svc.cluster.local\n"
Aug 24 11:55:11.824: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-309.svc.cluster.local\tcanonical name = externalsvc.services-309.svc.cluster.local.\nName:\texternalsvc.services-309.svc.cluster.local\nAddress: 10.254.226.102\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-309, will wait for the garbage collector to delete the pods 08/24/23 11:55:11.824
Aug 24 11:55:11.892: INFO: Deleting ReplicationController externalsvc took: 11.354241ms
Aug 24 11:55:11.992: INFO: Terminating ReplicationController externalsvc pods took: 100.456418ms
Aug 24 11:55:14.530: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:14.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-309" for this suite. 08/24/23 11:55:14.579
------------------------------
â€¢ [SLOW TEST] [8.350 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:06.242
    Aug 24 11:55:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:55:06.243
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:06.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:06.275
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-309 08/24/23 11:55:06.288
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 11:55:06.314
    STEP: creating service externalsvc in namespace services-309 08/24/23 11:55:06.315
    STEP: creating replication controller externalsvc in namespace services-309 08/24/23 11:55:06.347
    I0824 11:55:06.366357      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-309, replica count: 2
    I0824 11:55:09.417428      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/24/23 11:55:09.433
    Aug 24 11:55:09.496: INFO: Creating new exec pod
    Aug 24 11:55:09.523: INFO: Waiting up to 5m0s for pod "execpodsthqk" in namespace "services-309" to be "running"
    Aug 24 11:55:09.537: INFO: Pod "execpodsthqk": Phase="Pending", Reason="", readiness=false. Elapsed: 13.958386ms
    Aug 24 11:55:11.543: INFO: Pod "execpodsthqk": Phase="Running", Reason="", readiness=true. Elapsed: 2.020010157s
    Aug 24 11:55:11.543: INFO: Pod "execpodsthqk" satisfied condition "running"
    Aug 24 11:55:11.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-309 exec execpodsthqk -- /bin/sh -x -c nslookup nodeport-service.services-309.svc.cluster.local'
    Aug 24 11:55:11.824: INFO: stderr: "+ nslookup nodeport-service.services-309.svc.cluster.local\n"
    Aug 24 11:55:11.824: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-309.svc.cluster.local\tcanonical name = externalsvc.services-309.svc.cluster.local.\nName:\texternalsvc.services-309.svc.cluster.local\nAddress: 10.254.226.102\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-309, will wait for the garbage collector to delete the pods 08/24/23 11:55:11.824
    Aug 24 11:55:11.892: INFO: Deleting ReplicationController externalsvc took: 11.354241ms
    Aug 24 11:55:11.992: INFO: Terminating ReplicationController externalsvc pods took: 100.456418ms
    Aug 24 11:55:14.530: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:14.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-309" for this suite. 08/24/23 11:55:14.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:14.593
Aug 24 11:55:14.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-runtime 08/24/23 11:55:14.595
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:14.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:14.626
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/24/23 11:55:14.632
STEP: wait for the container to reach Failed 08/24/23 11:55:14.65
STEP: get the container status 08/24/23 11:55:18.673
STEP: the container should be terminated 08/24/23 11:55:18.675
STEP: the termination message should be set 08/24/23 11:55:18.675
Aug 24 11:55:18.676: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/24/23 11:55:18.676
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:18.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6825" for this suite. 08/24/23 11:55:18.708
------------------------------
â€¢ [4.142 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:14.593
    Aug 24 11:55:14.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-runtime 08/24/23 11:55:14.595
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:14.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:14.626
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/24/23 11:55:14.632
    STEP: wait for the container to reach Failed 08/24/23 11:55:14.65
    STEP: get the container status 08/24/23 11:55:18.673
    STEP: the container should be terminated 08/24/23 11:55:18.675
    STEP: the termination message should be set 08/24/23 11:55:18.675
    Aug 24 11:55:18.676: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/24/23 11:55:18.676
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:18.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6825" for this suite. 08/24/23 11:55:18.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:18.738
Aug 24 11:55:18.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename runtimeclass 08/24/23 11:55:18.739
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:18.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:18.769
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/24/23 11:55:18.776
STEP: getting /apis/node.k8s.io 08/24/23 11:55:18.779
STEP: getting /apis/node.k8s.io/v1 08/24/23 11:55:18.781
STEP: creating 08/24/23 11:55:18.784
STEP: watching 08/24/23 11:55:18.806
Aug 24 11:55:18.806: INFO: starting watch
STEP: getting 08/24/23 11:55:18.815
STEP: listing 08/24/23 11:55:18.818
STEP: patching 08/24/23 11:55:18.829
STEP: updating 08/24/23 11:55:18.838
Aug 24 11:55:18.851: INFO: waiting for watch events with expected annotations
STEP: deleting 08/24/23 11:55:18.851
STEP: deleting a collection 08/24/23 11:55:18.865
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:18.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5616" for this suite. 08/24/23 11:55:18.886
------------------------------
â€¢ [0.157 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:18.738
    Aug 24 11:55:18.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 11:55:18.739
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:18.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:18.769
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/24/23 11:55:18.776
    STEP: getting /apis/node.k8s.io 08/24/23 11:55:18.779
    STEP: getting /apis/node.k8s.io/v1 08/24/23 11:55:18.781
    STEP: creating 08/24/23 11:55:18.784
    STEP: watching 08/24/23 11:55:18.806
    Aug 24 11:55:18.806: INFO: starting watch
    STEP: getting 08/24/23 11:55:18.815
    STEP: listing 08/24/23 11:55:18.818
    STEP: patching 08/24/23 11:55:18.829
    STEP: updating 08/24/23 11:55:18.838
    Aug 24 11:55:18.851: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/24/23 11:55:18.851
    STEP: deleting a collection 08/24/23 11:55:18.865
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:18.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5616" for this suite. 08/24/23 11:55:18.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:18.895
Aug 24 11:55:18.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 11:55:18.897
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:18.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:18.935
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/24/23 11:55:18.945
STEP: Counting existing ResourceQuota 08/24/23 11:55:23.967
STEP: Creating a ResourceQuota 08/24/23 11:55:28.971
STEP: Ensuring resource quota status is calculated 08/24/23 11:55:28.98
STEP: Creating a Secret 08/24/23 11:55:30.985
STEP: Ensuring resource quota status captures secret creation 08/24/23 11:55:31.004
STEP: Deleting a secret 08/24/23 11:55:33.01
STEP: Ensuring resource quota status released usage 08/24/23 11:55:33.018
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:35.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7161" for this suite. 08/24/23 11:55:35.029
------------------------------
â€¢ [SLOW TEST] [16.143 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:18.895
    Aug 24 11:55:18.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 11:55:18.897
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:18.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:18.935
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/24/23 11:55:18.945
    STEP: Counting existing ResourceQuota 08/24/23 11:55:23.967
    STEP: Creating a ResourceQuota 08/24/23 11:55:28.971
    STEP: Ensuring resource quota status is calculated 08/24/23 11:55:28.98
    STEP: Creating a Secret 08/24/23 11:55:30.985
    STEP: Ensuring resource quota status captures secret creation 08/24/23 11:55:31.004
    STEP: Deleting a secret 08/24/23 11:55:33.01
    STEP: Ensuring resource quota status released usage 08/24/23 11:55:33.018
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:35.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7161" for this suite. 08/24/23 11:55:35.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:35.039
Aug 24 11:55:35.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-pred 08/24/23 11:55:35.041
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:35.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:35.073
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 11:55:35.080: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 11:55:35.089: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 11:55:35.093: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
Aug 24 11:55:35.105: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.105: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 11:55:35.105: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.106: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 11:55:35.106: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 11:55:35.106: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.106: INFO: 	Container autoscaler ready: true, restart count 0
Aug 24 11:55:35.106: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
Aug 24 11:55:35.106: INFO: 	Container grafana ready: true, restart count 0
Aug 24 11:55:35.106: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Aug 24 11:55:35.106: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Aug 24 11:55:35.106: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.106: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Aug 24 11:55:35.106: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.106: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 24 11:55:35.106: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.107: INFO: 	Container metrics-server ready: true, restart count 0
Aug 24 11:55:35.107: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.107: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 11:55:35.107: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.107: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 11:55:35.107: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.107: INFO: 	Container config-reloader ready: true, restart count 0
Aug 24 11:55:35.107: INFO: 	Container prometheus ready: true, restart count 0
Aug 24 11:55:35.107: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.107: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 11:55:35.108: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 11:55:35.108: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.108: INFO: 	Container busybox ready: true, restart count 1
Aug 24 11:55:35.108: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.108: INFO: 	Container nginx ready: true, restart count 0
Aug 24 11:55:35.108: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.108: INFO: 	Container nginx ready: true, restart count 0
Aug 24 11:55:35.108: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
Aug 24 11:55:35.118: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.119: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 11:55:35.119: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.119: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 11:55:35.119: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 11:55:35.119: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.119: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 11:55:35.119: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.119: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 11:55:35.119: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.119: INFO: 	Container e2e ready: true, restart count 0
Aug 24 11:55:35.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 11:55:35.120: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 11:55:35.120: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 11:55:35.120: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
Aug 24 11:55:35.128: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.128: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 11:55:35.128: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.128: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 11:55:35.128: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 11:55:35.128: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.128: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 11:55:35.128: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.128: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 11:55:35.128: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
Aug 24 11:55:35.128: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 11:55:35.128: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 11:55:35.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 11:55:35.129: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/24/23 11:55:35.129
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177e4fb1d680dd1e], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 08/24/23 11:55:35.179
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:36.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1436" for this suite. 08/24/23 11:55:36.181
------------------------------
â€¢ [1.155 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:35.039
    Aug 24 11:55:35.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-pred 08/24/23 11:55:35.041
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:35.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:35.073
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 11:55:35.080: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 11:55:35.089: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 11:55:35.093: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
    Aug 24 11:55:35.105: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.105: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 11:55:35.105: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.106: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.106: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
    Aug 24 11:55:35.106: INFO: 	Container grafana ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.106: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.106: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 24 11:55:35.106: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.107: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 24 11:55:35.107: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.107: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 11:55:35.107: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.107: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 11:55:35.107: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.107: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 24 11:55:35.107: INFO: 	Container prometheus ready: true, restart count 0
    Aug 24 11:55:35.107: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.107: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 11:55:35.108: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 11:55:35.108: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.108: INFO: 	Container busybox ready: true, restart count 1
    Aug 24 11:55:35.108: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.108: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 11:55:35.108: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.108: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 11:55:35.108: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
    Aug 24 11:55:35.118: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.119: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 11:55:35.119: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.119: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 11:55:35.119: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 11:55:35.119: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.119: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 11:55:35.119: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.119: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 11:55:35.119: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.119: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 11:55:35.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 11:55:35.120: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.120: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 11:55:35.120: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 11:55:35.120: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
    Aug 24 11:55:35.128: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.128: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 11:55:35.128: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.128: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 11:55:35.128: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 11:55:35.128: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.128: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 11:55:35.128: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.128: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 11:55:35.128: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
    Aug 24 11:55:35.128: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 11:55:35.128: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 11:55:35.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 11:55:35.129: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/24/23 11:55:35.129
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177e4fb1d680dd1e], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] 08/24/23 11:55:35.179
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:36.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1436" for this suite. 08/24/23 11:55:36.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:36.196
Aug 24 11:55:36.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:55:36.197
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:36.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:36.225
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-2ba86804-9ebb-486e-b211-79bc417c2751 08/24/23 11:55:36.231
STEP: Creating a pod to test consume configMaps 08/24/23 11:55:36.239
Aug 24 11:55:36.257: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9" in namespace "projected-2918" to be "Succeeded or Failed"
Aug 24 11:55:36.264: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.246942ms
Aug 24 11:55:38.268: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011554043s
Aug 24 11:55:40.268: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011320936s
STEP: Saw pod success 08/24/23 11:55:40.268
Aug 24 11:55:40.268: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9" satisfied condition "Succeeded or Failed"
Aug 24 11:55:40.272: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 11:55:40.281
Aug 24 11:55:40.307: INFO: Waiting for pod pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9 to disappear
Aug 24 11:55:40.315: INFO: Pod pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:40.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2918" for this suite. 08/24/23 11:55:40.32
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:36.196
    Aug 24 11:55:36.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:55:36.197
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:36.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:36.225
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-2ba86804-9ebb-486e-b211-79bc417c2751 08/24/23 11:55:36.231
    STEP: Creating a pod to test consume configMaps 08/24/23 11:55:36.239
    Aug 24 11:55:36.257: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9" in namespace "projected-2918" to be "Succeeded or Failed"
    Aug 24 11:55:36.264: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.246942ms
    Aug 24 11:55:38.268: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011554043s
    Aug 24 11:55:40.268: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011320936s
    STEP: Saw pod success 08/24/23 11:55:40.268
    Aug 24 11:55:40.268: INFO: Pod "pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9" satisfied condition "Succeeded or Failed"
    Aug 24 11:55:40.272: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 11:55:40.281
    Aug 24 11:55:40.307: INFO: Waiting for pod pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9 to disappear
    Aug 24 11:55:40.315: INFO: Pod pod-projected-configmaps-e14b9241-e240-41df-9d20-5d049fa8f9d9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:40.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2918" for this suite. 08/24/23 11:55:40.32
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:40.336
Aug 24 11:55:40.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename limitrange 08/24/23 11:55:40.338
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:40.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:40.372
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-k5lrt" in namespace "limitrange-6497" 08/24/23 11:55:40.384
STEP: Creating another limitRange in another namespace 08/24/23 11:55:40.393
Aug 24 11:55:40.422: INFO: Namespace "e2e-limitrange-k5lrt-3442" created
Aug 24 11:55:40.422: INFO: Creating LimitRange "e2e-limitrange-k5lrt" in namespace "e2e-limitrange-k5lrt-3442"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-k5lrt" 08/24/23 11:55:40.432
Aug 24 11:55:40.436: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-k5lrt" in "limitrange-6497" namespace 08/24/23 11:55:40.436
Aug 24 11:55:40.445: INFO: LimitRange "e2e-limitrange-k5lrt" has been patched
STEP: Delete LimitRange "e2e-limitrange-k5lrt" by Collection with labelSelector: "e2e-limitrange-k5lrt=patched" 08/24/23 11:55:40.445
STEP: Confirm that the limitRange "e2e-limitrange-k5lrt" has been deleted 08/24/23 11:55:40.456
Aug 24 11:55:40.456: INFO: Requesting list of LimitRange to confirm quantity
Aug 24 11:55:40.463: INFO: Found 0 LimitRange with label "e2e-limitrange-k5lrt=patched"
Aug 24 11:55:40.463: INFO: LimitRange "e2e-limitrange-k5lrt" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-k5lrt" 08/24/23 11:55:40.463
Aug 24 11:55:40.471: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:40.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6497" for this suite. 08/24/23 11:55:40.476
STEP: Destroying namespace "e2e-limitrange-k5lrt-3442" for this suite. 08/24/23 11:55:40.49
------------------------------
â€¢ [0.163 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:40.336
    Aug 24 11:55:40.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename limitrange 08/24/23 11:55:40.338
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:40.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:40.372
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-k5lrt" in namespace "limitrange-6497" 08/24/23 11:55:40.384
    STEP: Creating another limitRange in another namespace 08/24/23 11:55:40.393
    Aug 24 11:55:40.422: INFO: Namespace "e2e-limitrange-k5lrt-3442" created
    Aug 24 11:55:40.422: INFO: Creating LimitRange "e2e-limitrange-k5lrt" in namespace "e2e-limitrange-k5lrt-3442"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-k5lrt" 08/24/23 11:55:40.432
    Aug 24 11:55:40.436: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-k5lrt" in "limitrange-6497" namespace 08/24/23 11:55:40.436
    Aug 24 11:55:40.445: INFO: LimitRange "e2e-limitrange-k5lrt" has been patched
    STEP: Delete LimitRange "e2e-limitrange-k5lrt" by Collection with labelSelector: "e2e-limitrange-k5lrt=patched" 08/24/23 11:55:40.445
    STEP: Confirm that the limitRange "e2e-limitrange-k5lrt" has been deleted 08/24/23 11:55:40.456
    Aug 24 11:55:40.456: INFO: Requesting list of LimitRange to confirm quantity
    Aug 24 11:55:40.463: INFO: Found 0 LimitRange with label "e2e-limitrange-k5lrt=patched"
    Aug 24 11:55:40.463: INFO: LimitRange "e2e-limitrange-k5lrt" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-k5lrt" 08/24/23 11:55:40.463
    Aug 24 11:55:40.471: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:40.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6497" for this suite. 08/24/23 11:55:40.476
    STEP: Destroying namespace "e2e-limitrange-k5lrt-3442" for this suite. 08/24/23 11:55:40.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:40.501
Aug 24 11:55:40.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:55:40.503
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:40.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:40.536
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/24/23 11:55:40.542
Aug 24 11:55:40.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:55:43.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4961" for this suite. 08/24/23 11:55:52.74
------------------------------
â€¢ [SLOW TEST] [12.248 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:40.501
    Aug 24 11:55:40.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:55:40.503
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:40.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:40.536
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/24/23 11:55:40.542
    Aug 24 11:55:40.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:55:43.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4961" for this suite. 08/24/23 11:55:52.74
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:52.753
Aug 24 11:55:52.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 11:55:52.754
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:52.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:52.779
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:55:52.785
Aug 24 11:55:52.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55" in namespace "downward-api-8910" to be "Succeeded or Failed"
Aug 24 11:55:52.816: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Pending", Reason="", readiness=false. Elapsed: 17.280003ms
Aug 24 11:55:54.828: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028409795s
Aug 24 11:55:56.823: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023481316s
Aug 24 11:55:58.821: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022162301s
STEP: Saw pod success 08/24/23 11:55:58.821
Aug 24 11:55:58.821: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55" satisfied condition "Succeeded or Failed"
Aug 24 11:55:58.824: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55 container client-container: <nil>
STEP: delete the pod 08/24/23 11:55:58.831
Aug 24 11:55:58.857: INFO: Waiting for pod downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55 to disappear
Aug 24 11:55:58.863: INFO: Pod downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 11:55:58.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8910" for this suite. 08/24/23 11:55:58.87
------------------------------
â€¢ [SLOW TEST] [6.136 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:52.753
    Aug 24 11:55:52.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 11:55:52.754
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:52.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:52.779
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:55:52.785
    Aug 24 11:55:52.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55" in namespace "downward-api-8910" to be "Succeeded or Failed"
    Aug 24 11:55:52.816: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Pending", Reason="", readiness=false. Elapsed: 17.280003ms
    Aug 24 11:55:54.828: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028409795s
    Aug 24 11:55:56.823: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023481316s
    Aug 24 11:55:58.821: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022162301s
    STEP: Saw pod success 08/24/23 11:55:58.821
    Aug 24 11:55:58.821: INFO: Pod "downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55" satisfied condition "Succeeded or Failed"
    Aug 24 11:55:58.824: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:55:58.831
    Aug 24 11:55:58.857: INFO: Waiting for pod downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55 to disappear
    Aug 24 11:55:58.863: INFO: Pod downwardapi-volume-3611d818-9b86-4f0d-bd77-89253ba23c55 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:55:58.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8910" for this suite. 08/24/23 11:55:58.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:55:58.895
Aug 24 11:55:58.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename endpointslice 08/24/23 11:55:58.897
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:58.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:58.92
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 11:56:01.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9990" for this suite. 08/24/23 11:56:01.065
------------------------------
â€¢ [2.178 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:55:58.895
    Aug 24 11:55:58.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename endpointslice 08/24/23 11:55:58.897
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:55:58.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:55:58.92
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:56:01.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9990" for this suite. 08/24/23 11:56:01.065
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:56:01.074
Aug 24 11:56:01.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 11:56:01.075
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:01.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:01.107
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 11:56:01.116
Aug 24 11:56:01.131: INFO: Waiting up to 5m0s for pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b" in namespace "emptydir-1791" to be "Succeeded or Failed"
Aug 24 11:56:01.152: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.577303ms
Aug 24 11:56:03.158: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b": Phase="Running", Reason="", readiness=false. Elapsed: 2.026559522s
Aug 24 11:56:05.170: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039044495s
STEP: Saw pod success 08/24/23 11:56:05.17
Aug 24 11:56:05.170: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b" satisfied condition "Succeeded or Failed"
Aug 24 11:56:05.175: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-02d265c0-3f97-421e-b94b-98069e5d306b container test-container: <nil>
STEP: delete the pod 08/24/23 11:56:05.185
Aug 24 11:56:05.210: INFO: Waiting for pod pod-02d265c0-3f97-421e-b94b-98069e5d306b to disappear
Aug 24 11:56:05.214: INFO: Pod pod-02d265c0-3f97-421e-b94b-98069e5d306b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:56:05.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1791" for this suite. 08/24/23 11:56:05.218
------------------------------
â€¢ [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:56:01.074
    Aug 24 11:56:01.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:56:01.075
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:01.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:01.107
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/24/23 11:56:01.116
    Aug 24 11:56:01.131: INFO: Waiting up to 5m0s for pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b" in namespace "emptydir-1791" to be "Succeeded or Failed"
    Aug 24 11:56:01.152: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.577303ms
    Aug 24 11:56:03.158: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b": Phase="Running", Reason="", readiness=false. Elapsed: 2.026559522s
    Aug 24 11:56:05.170: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039044495s
    STEP: Saw pod success 08/24/23 11:56:05.17
    Aug 24 11:56:05.170: INFO: Pod "pod-02d265c0-3f97-421e-b94b-98069e5d306b" satisfied condition "Succeeded or Failed"
    Aug 24 11:56:05.175: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-02d265c0-3f97-421e-b94b-98069e5d306b container test-container: <nil>
    STEP: delete the pod 08/24/23 11:56:05.185
    Aug 24 11:56:05.210: INFO: Waiting for pod pod-02d265c0-3f97-421e-b94b-98069e5d306b to disappear
    Aug 24 11:56:05.214: INFO: Pod pod-02d265c0-3f97-421e-b94b-98069e5d306b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:56:05.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1791" for this suite. 08/24/23 11:56:05.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:56:05.228
Aug 24 11:56:05.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 11:56:05.229
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:05.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:05.259
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:05.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9068" for this suite. 08/24/23 11:57:05.286
------------------------------
â€¢ [SLOW TEST] [60.073 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:56:05.228
    Aug 24 11:56:05.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 11:56:05.229
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:56:05.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:56:05.259
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:05.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9068" for this suite. 08/24/23 11:57:05.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:05.303
Aug 24 11:57:05.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:57:05.304
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:05.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:05.344
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/24/23 11:57:05.352
Aug 24 11:57:05.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85" in namespace "projected-6673" to be "Succeeded or Failed"
Aug 24 11:57:05.382: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85": Phase="Pending", Reason="", readiness=false. Elapsed: 10.393452ms
Aug 24 11:57:07.390: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01800299s
Aug 24 11:57:09.386: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014864592s
STEP: Saw pod success 08/24/23 11:57:09.386
Aug 24 11:57:09.387: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85" satisfied condition "Succeeded or Failed"
Aug 24 11:57:09.389: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85 container client-container: <nil>
STEP: delete the pod 08/24/23 11:57:09.454
Aug 24 11:57:09.478: INFO: Waiting for pod downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85 to disappear
Aug 24 11:57:09.491: INFO: Pod downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6673" for this suite. 08/24/23 11:57:09.5
------------------------------
â€¢ [4.222 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:05.303
    Aug 24 11:57:05.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:57:05.304
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:05.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:05.344
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/24/23 11:57:05.352
    Aug 24 11:57:05.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85" in namespace "projected-6673" to be "Succeeded or Failed"
    Aug 24 11:57:05.382: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85": Phase="Pending", Reason="", readiness=false. Elapsed: 10.393452ms
    Aug 24 11:57:07.390: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01800299s
    Aug 24 11:57:09.386: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014864592s
    STEP: Saw pod success 08/24/23 11:57:09.386
    Aug 24 11:57:09.387: INFO: Pod "downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85" satisfied condition "Succeeded or Failed"
    Aug 24 11:57:09.389: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85 container client-container: <nil>
    STEP: delete the pod 08/24/23 11:57:09.454
    Aug 24 11:57:09.478: INFO: Waiting for pod downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85 to disappear
    Aug 24 11:57:09.491: INFO: Pod downwardapi-volume-473fb351-d1bb-43d8-98e1-950948a78a85 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6673" for this suite. 08/24/23 11:57:09.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:09.525
Aug 24 11:57:09.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename certificates 08/24/23 11:57:09.527
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:09.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:09.572
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/24/23 11:57:10.482
STEP: getting /apis/certificates.k8s.io 08/24/23 11:57:10.489
STEP: getting /apis/certificates.k8s.io/v1 08/24/23 11:57:10.494
STEP: creating 08/24/23 11:57:10.498
STEP: getting 08/24/23 11:57:10.538
STEP: listing 08/24/23 11:57:10.542
STEP: watching 08/24/23 11:57:10.551
Aug 24 11:57:10.551: INFO: starting watch
STEP: patching 08/24/23 11:57:10.555
STEP: updating 08/24/23 11:57:10.574
Aug 24 11:57:10.584: INFO: waiting for watch events with expected annotations
Aug 24 11:57:10.584: INFO: saw patched and updated annotations
STEP: getting /approval 08/24/23 11:57:10.584
STEP: patching /approval 08/24/23 11:57:10.591
STEP: updating /approval 08/24/23 11:57:10.615
STEP: getting /status 08/24/23 11:57:10.628
STEP: patching /status 08/24/23 11:57:10.632
STEP: updating /status 08/24/23 11:57:10.658
STEP: deleting 08/24/23 11:57:10.674
STEP: deleting a collection 08/24/23 11:57:10.69
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:10.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-4976" for this suite. 08/24/23 11:57:10.716
------------------------------
â€¢ [1.205 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:09.525
    Aug 24 11:57:09.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename certificates 08/24/23 11:57:09.527
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:09.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:09.572
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/24/23 11:57:10.482
    STEP: getting /apis/certificates.k8s.io 08/24/23 11:57:10.489
    STEP: getting /apis/certificates.k8s.io/v1 08/24/23 11:57:10.494
    STEP: creating 08/24/23 11:57:10.498
    STEP: getting 08/24/23 11:57:10.538
    STEP: listing 08/24/23 11:57:10.542
    STEP: watching 08/24/23 11:57:10.551
    Aug 24 11:57:10.551: INFO: starting watch
    STEP: patching 08/24/23 11:57:10.555
    STEP: updating 08/24/23 11:57:10.574
    Aug 24 11:57:10.584: INFO: waiting for watch events with expected annotations
    Aug 24 11:57:10.584: INFO: saw patched and updated annotations
    STEP: getting /approval 08/24/23 11:57:10.584
    STEP: patching /approval 08/24/23 11:57:10.591
    STEP: updating /approval 08/24/23 11:57:10.615
    STEP: getting /status 08/24/23 11:57:10.628
    STEP: patching /status 08/24/23 11:57:10.632
    STEP: updating /status 08/24/23 11:57:10.658
    STEP: deleting 08/24/23 11:57:10.674
    STEP: deleting a collection 08/24/23 11:57:10.69
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:10.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-4976" for this suite. 08/24/23 11:57:10.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:10.733
Aug 24 11:57:10.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 11:57:10.735
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:10.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:10.776
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-647e5677-d74d-4ed0-ad28-3875ff98af6a 08/24/23 11:57:10.782
STEP: Creating a pod to test consume secrets 08/24/23 11:57:10.792
Aug 24 11:57:10.807: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe" in namespace "projected-3950" to be "Succeeded or Failed"
Aug 24 11:57:10.816: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.581323ms
Aug 24 11:57:12.821: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013720549s
Aug 24 11:57:14.821: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014023089s
STEP: Saw pod success 08/24/23 11:57:14.822
Aug 24 11:57:14.822: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe" satisfied condition "Succeeded or Failed"
Aug 24 11:57:14.826: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 11:57:14.834
Aug 24 11:57:14.859: INFO: Waiting for pod pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe to disappear
Aug 24 11:57:14.864: INFO: Pod pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:14.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3950" for this suite. 08/24/23 11:57:14.87
------------------------------
â€¢ [4.149 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:10.733
    Aug 24 11:57:10.733: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 11:57:10.735
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:10.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:10.776
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-647e5677-d74d-4ed0-ad28-3875ff98af6a 08/24/23 11:57:10.782
    STEP: Creating a pod to test consume secrets 08/24/23 11:57:10.792
    Aug 24 11:57:10.807: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe" in namespace "projected-3950" to be "Succeeded or Failed"
    Aug 24 11:57:10.816: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.581323ms
    Aug 24 11:57:12.821: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013720549s
    Aug 24 11:57:14.821: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014023089s
    STEP: Saw pod success 08/24/23 11:57:14.822
    Aug 24 11:57:14.822: INFO: Pod "pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe" satisfied condition "Succeeded or Failed"
    Aug 24 11:57:14.826: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:57:14.834
    Aug 24 11:57:14.859: INFO: Waiting for pod pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe to disappear
    Aug 24 11:57:14.864: INFO: Pod pod-projected-secrets-8d727403-3ef1-401b-bdf0-aa2cb1bc3bfe no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:14.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3950" for this suite. 08/24/23 11:57:14.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:14.885
Aug 24 11:57:14.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename prestop 08/24/23 11:57:14.886
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:14.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:14.929
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1707 08/24/23 11:57:14.943
STEP: Waiting for pods to come up. 08/24/23 11:57:14.97
Aug 24 11:57:14.970: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1707" to be "running"
Aug 24 11:57:14.995: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 25.421495ms
Aug 24 11:57:17.001: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.030922312s
Aug 24 11:57:17.002: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1707 08/24/23 11:57:17.005
Aug 24 11:57:17.013: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1707" to be "running"
Aug 24 11:57:17.020: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.898885ms
Aug 24 11:57:19.025: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.012336808s
Aug 24 11:57:19.025: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/24/23 11:57:19.025
Aug 24 11:57:24.049: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/24/23 11:57:24.049
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:24.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-1707" for this suite. 08/24/23 11:57:24.087
------------------------------
â€¢ [SLOW TEST] [9.213 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:14.885
    Aug 24 11:57:14.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename prestop 08/24/23 11:57:14.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:14.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:14.929
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1707 08/24/23 11:57:14.943
    STEP: Waiting for pods to come up. 08/24/23 11:57:14.97
    Aug 24 11:57:14.970: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1707" to be "running"
    Aug 24 11:57:14.995: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 25.421495ms
    Aug 24 11:57:17.001: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.030922312s
    Aug 24 11:57:17.002: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1707 08/24/23 11:57:17.005
    Aug 24 11:57:17.013: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1707" to be "running"
    Aug 24 11:57:17.020: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.898885ms
    Aug 24 11:57:19.025: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.012336808s
    Aug 24 11:57:19.025: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/24/23 11:57:19.025
    Aug 24 11:57:24.049: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/24/23 11:57:24.049
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:24.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-1707" for this suite. 08/24/23 11:57:24.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:24.098
Aug 24 11:57:24.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 11:57:24.1
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:24.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:24.134
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/24/23 11:57:24.151
Aug 24 11:57:24.164: INFO: Waiting up to 5m0s for pod "pod-sph8n" in namespace "pods-7163" to be "running"
Aug 24 11:57:24.172: INFO: Pod "pod-sph8n": Phase="Pending", Reason="", readiness=false. Elapsed: 7.24552ms
Aug 24 11:57:26.178: INFO: Pod "pod-sph8n": Phase="Running", Reason="", readiness=true. Elapsed: 2.01362201s
Aug 24 11:57:26.178: INFO: Pod "pod-sph8n" satisfied condition "running"
STEP: patching /status 08/24/23 11:57:26.178
Aug 24 11:57:26.200: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:26.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7163" for this suite. 08/24/23 11:57:26.206
------------------------------
â€¢ [2.119 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:24.098
    Aug 24 11:57:24.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 11:57:24.1
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:24.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:24.134
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/24/23 11:57:24.151
    Aug 24 11:57:24.164: INFO: Waiting up to 5m0s for pod "pod-sph8n" in namespace "pods-7163" to be "running"
    Aug 24 11:57:24.172: INFO: Pod "pod-sph8n": Phase="Pending", Reason="", readiness=false. Elapsed: 7.24552ms
    Aug 24 11:57:26.178: INFO: Pod "pod-sph8n": Phase="Running", Reason="", readiness=true. Elapsed: 2.01362201s
    Aug 24 11:57:26.178: INFO: Pod "pod-sph8n" satisfied condition "running"
    STEP: patching /status 08/24/23 11:57:26.178
    Aug 24 11:57:26.200: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:26.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7163" for this suite. 08/24/23 11:57:26.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:26.22
Aug 24 11:57:26.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 11:57:26.221
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:26.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:26.258
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/24/23 11:57:26.264
STEP: submitting the pod to kubernetes 08/24/23 11:57:26.264
Aug 24 11:57:26.281: INFO: Waiting up to 5m0s for pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" in namespace "pods-3633" to be "running and ready"
Aug 24 11:57:26.295: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366": Phase="Pending", Reason="", readiness=false. Elapsed: 13.330482ms
Aug 24 11:57:26.295: INFO: The phase of Pod pod-update-026e851d-6267-4d17-b27d-bf929c5da366 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:57:28.298: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366": Phase="Running", Reason="", readiness=true. Elapsed: 2.017034973s
Aug 24 11:57:28.298: INFO: The phase of Pod pod-update-026e851d-6267-4d17-b27d-bf929c5da366 is Running (Ready = true)
Aug 24 11:57:28.298: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/24/23 11:57:28.301
STEP: updating the pod 08/24/23 11:57:28.305
Aug 24 11:57:28.825: INFO: Successfully updated pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366"
Aug 24 11:57:28.825: INFO: Waiting up to 5m0s for pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" in namespace "pods-3633" to be "running"
Aug 24 11:57:28.831: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366": Phase="Running", Reason="", readiness=true. Elapsed: 5.694894ms
Aug 24 11:57:28.831: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/24/23 11:57:28.831
Aug 24 11:57:28.834: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:28.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3633" for this suite. 08/24/23 11:57:28.84
------------------------------
â€¢ [2.644 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:26.22
    Aug 24 11:57:26.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 11:57:26.221
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:26.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:26.258
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/24/23 11:57:26.264
    STEP: submitting the pod to kubernetes 08/24/23 11:57:26.264
    Aug 24 11:57:26.281: INFO: Waiting up to 5m0s for pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" in namespace "pods-3633" to be "running and ready"
    Aug 24 11:57:26.295: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366": Phase="Pending", Reason="", readiness=false. Elapsed: 13.330482ms
    Aug 24 11:57:26.295: INFO: The phase of Pod pod-update-026e851d-6267-4d17-b27d-bf929c5da366 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:57:28.298: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366": Phase="Running", Reason="", readiness=true. Elapsed: 2.017034973s
    Aug 24 11:57:28.298: INFO: The phase of Pod pod-update-026e851d-6267-4d17-b27d-bf929c5da366 is Running (Ready = true)
    Aug 24 11:57:28.298: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/24/23 11:57:28.301
    STEP: updating the pod 08/24/23 11:57:28.305
    Aug 24 11:57:28.825: INFO: Successfully updated pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366"
    Aug 24 11:57:28.825: INFO: Waiting up to 5m0s for pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" in namespace "pods-3633" to be "running"
    Aug 24 11:57:28.831: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366": Phase="Running", Reason="", readiness=true. Elapsed: 5.694894ms
    Aug 24 11:57:28.831: INFO: Pod "pod-update-026e851d-6267-4d17-b27d-bf929c5da366" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/24/23 11:57:28.831
    Aug 24 11:57:28.834: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:28.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3633" for this suite. 08/24/23 11:57:28.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:28.884
Aug 24 11:57:28.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename ingress 08/24/23 11:57:28.886
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:28.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:28.92
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/24/23 11:57:28.927
STEP: getting /apis/networking.k8s.io 08/24/23 11:57:28.93
STEP: getting /apis/networking.k8s.iov1 08/24/23 11:57:28.933
STEP: creating 08/24/23 11:57:28.937
STEP: getting 08/24/23 11:57:28.972
STEP: listing 08/24/23 11:57:28.975
STEP: watching 08/24/23 11:57:28.98
Aug 24 11:57:28.980: INFO: starting watch
STEP: cluster-wide listing 08/24/23 11:57:28.983
STEP: cluster-wide watching 08/24/23 11:57:28.986
Aug 24 11:57:28.987: INFO: starting watch
STEP: patching 08/24/23 11:57:28.989
STEP: updating 08/24/23 11:57:28.999
Aug 24 11:57:29.012: INFO: waiting for watch events with expected annotations
Aug 24 11:57:29.012: INFO: saw patched and updated annotations
STEP: patching /status 08/24/23 11:57:29.013
STEP: updating /status 08/24/23 11:57:29.034
STEP: get /status 08/24/23 11:57:29.045
STEP: deleting 08/24/23 11:57:29.049
STEP: deleting a collection 08/24/23 11:57:29.062
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:29.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-9634" for this suite. 08/24/23 11:57:29.167
------------------------------
â€¢ [0.316 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:28.884
    Aug 24 11:57:28.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename ingress 08/24/23 11:57:28.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:28.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:28.92
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/24/23 11:57:28.927
    STEP: getting /apis/networking.k8s.io 08/24/23 11:57:28.93
    STEP: getting /apis/networking.k8s.iov1 08/24/23 11:57:28.933
    STEP: creating 08/24/23 11:57:28.937
    STEP: getting 08/24/23 11:57:28.972
    STEP: listing 08/24/23 11:57:28.975
    STEP: watching 08/24/23 11:57:28.98
    Aug 24 11:57:28.980: INFO: starting watch
    STEP: cluster-wide listing 08/24/23 11:57:28.983
    STEP: cluster-wide watching 08/24/23 11:57:28.986
    Aug 24 11:57:28.987: INFO: starting watch
    STEP: patching 08/24/23 11:57:28.989
    STEP: updating 08/24/23 11:57:28.999
    Aug 24 11:57:29.012: INFO: waiting for watch events with expected annotations
    Aug 24 11:57:29.012: INFO: saw patched and updated annotations
    STEP: patching /status 08/24/23 11:57:29.013
    STEP: updating /status 08/24/23 11:57:29.034
    STEP: get /status 08/24/23 11:57:29.045
    STEP: deleting 08/24/23 11:57:29.049
    STEP: deleting a collection 08/24/23 11:57:29.062
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:29.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-9634" for this suite. 08/24/23 11:57:29.167
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:29.201
Aug 24 11:57:29.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename subpath 08/24/23 11:57:29.203
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:29.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:29.251
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 11:57:29.261
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-qkb4 08/24/23 11:57:29.279
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:57:29.28
Aug 24 11:57:29.317: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qkb4" in namespace "subpath-8349" to be "Succeeded or Failed"
Aug 24 11:57:29.351: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Pending", Reason="", readiness=false. Elapsed: 34.219846ms
Aug 24 11:57:31.361: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.043621476s
Aug 24 11:57:33.356: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.038627747s
Aug 24 11:57:35.357: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 6.039696355s
Aug 24 11:57:37.356: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 8.039278041s
Aug 24 11:57:39.397: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 10.079618811s
Aug 24 11:57:41.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 12.038248766s
Aug 24 11:57:43.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 14.038176686s
Aug 24 11:57:45.358: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 16.040755503s
Aug 24 11:57:47.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 18.038236501s
Aug 24 11:57:49.361: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 20.044106009s
Aug 24 11:57:51.362: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=false. Elapsed: 22.045094341s
Aug 24 11:57:53.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.037863542s
STEP: Saw pod success 08/24/23 11:57:53.355
Aug 24 11:57:53.355: INFO: Pod "pod-subpath-test-secret-qkb4" satisfied condition "Succeeded or Failed"
Aug 24 11:57:53.358: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-subpath-test-secret-qkb4 container test-container-subpath-secret-qkb4: <nil>
STEP: delete the pod 08/24/23 11:57:53.416
Aug 24 11:57:53.443: INFO: Waiting for pod pod-subpath-test-secret-qkb4 to disappear
Aug 24 11:57:53.447: INFO: Pod pod-subpath-test-secret-qkb4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-qkb4 08/24/23 11:57:53.447
Aug 24 11:57:53.448: INFO: Deleting pod "pod-subpath-test-secret-qkb4" in namespace "subpath-8349"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:53.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8349" for this suite. 08/24/23 11:57:53.458
------------------------------
â€¢ [SLOW TEST] [24.270 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:29.201
    Aug 24 11:57:29.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename subpath 08/24/23 11:57:29.203
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:29.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:29.251
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 11:57:29.261
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-qkb4 08/24/23 11:57:29.279
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 11:57:29.28
    Aug 24 11:57:29.317: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qkb4" in namespace "subpath-8349" to be "Succeeded or Failed"
    Aug 24 11:57:29.351: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Pending", Reason="", readiness=false. Elapsed: 34.219846ms
    Aug 24 11:57:31.361: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.043621476s
    Aug 24 11:57:33.356: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 4.038627747s
    Aug 24 11:57:35.357: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 6.039696355s
    Aug 24 11:57:37.356: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 8.039278041s
    Aug 24 11:57:39.397: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 10.079618811s
    Aug 24 11:57:41.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 12.038248766s
    Aug 24 11:57:43.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 14.038176686s
    Aug 24 11:57:45.358: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 16.040755503s
    Aug 24 11:57:47.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 18.038236501s
    Aug 24 11:57:49.361: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=true. Elapsed: 20.044106009s
    Aug 24 11:57:51.362: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Running", Reason="", readiness=false. Elapsed: 22.045094341s
    Aug 24 11:57:53.355: INFO: Pod "pod-subpath-test-secret-qkb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.037863542s
    STEP: Saw pod success 08/24/23 11:57:53.355
    Aug 24 11:57:53.355: INFO: Pod "pod-subpath-test-secret-qkb4" satisfied condition "Succeeded or Failed"
    Aug 24 11:57:53.358: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-subpath-test-secret-qkb4 container test-container-subpath-secret-qkb4: <nil>
    STEP: delete the pod 08/24/23 11:57:53.416
    Aug 24 11:57:53.443: INFO: Waiting for pod pod-subpath-test-secret-qkb4 to disappear
    Aug 24 11:57:53.447: INFO: Pod pod-subpath-test-secret-qkb4 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-qkb4 08/24/23 11:57:53.447
    Aug 24 11:57:53.448: INFO: Deleting pod "pod-subpath-test-secret-qkb4" in namespace "subpath-8349"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:53.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8349" for this suite. 08/24/23 11:57:53.458
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:53.471
Aug 24 11:57:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:57:53.473
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:53.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:53.529
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/24/23 11:57:53.536
Aug 24 11:57:53.536: INFO: Creating e2e-svc-a-frjmc
Aug 24 11:57:53.556: INFO: Creating e2e-svc-b-j9vxz
Aug 24 11:57:53.579: INFO: Creating e2e-svc-c-c969t
STEP: deleting service collection 08/24/23 11:57:53.604
Aug 24 11:57:53.653: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:53.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3406" for this suite. 08/24/23 11:57:53.658
------------------------------
â€¢ [0.201 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:53.471
    Aug 24 11:57:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:57:53.473
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:53.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:53.529
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/24/23 11:57:53.536
    Aug 24 11:57:53.536: INFO: Creating e2e-svc-a-frjmc
    Aug 24 11:57:53.556: INFO: Creating e2e-svc-b-j9vxz
    Aug 24 11:57:53.579: INFO: Creating e2e-svc-c-c969t
    STEP: deleting service collection 08/24/23 11:57:53.604
    Aug 24 11:57:53.653: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:53.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3406" for this suite. 08/24/23 11:57:53.658
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:53.674
Aug 24 11:57:53.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 11:57:53.675
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:53.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:53.713
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-a26e65d5-4bdf-4c2f-ad83-319f631bc124 08/24/23 11:57:53.718
STEP: Creating a pod to test consume secrets 08/24/23 11:57:53.732
Aug 24 11:57:53.749: INFO: Waiting up to 5m0s for pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b" in namespace "secrets-2033" to be "Succeeded or Failed"
Aug 24 11:57:53.767: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.724365ms
Aug 24 11:57:55.772: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022931092s
Aug 24 11:57:57.772: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022780133s
STEP: Saw pod success 08/24/23 11:57:57.772
Aug 24 11:57:57.772: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b" satisfied condition "Succeeded or Failed"
Aug 24 11:57:57.775: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 11:57:57.782
Aug 24 11:57:57.809: INFO: Waiting for pod pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b to disappear
Aug 24 11:57:57.813: INFO: Pod pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 11:57:57.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2033" for this suite. 08/24/23 11:57:57.817
------------------------------
â€¢ [4.152 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:53.674
    Aug 24 11:57:53.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 11:57:53.675
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:53.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:53.713
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-a26e65d5-4bdf-4c2f-ad83-319f631bc124 08/24/23 11:57:53.718
    STEP: Creating a pod to test consume secrets 08/24/23 11:57:53.732
    Aug 24 11:57:53.749: INFO: Waiting up to 5m0s for pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b" in namespace "secrets-2033" to be "Succeeded or Failed"
    Aug 24 11:57:53.767: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.724365ms
    Aug 24 11:57:55.772: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022931092s
    Aug 24 11:57:57.772: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022780133s
    STEP: Saw pod success 08/24/23 11:57:57.772
    Aug 24 11:57:57.772: INFO: Pod "pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b" satisfied condition "Succeeded or Failed"
    Aug 24 11:57:57.775: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 11:57:57.782
    Aug 24 11:57:57.809: INFO: Waiting for pod pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b to disappear
    Aug 24 11:57:57.813: INFO: Pod pod-secrets-a8e3200d-82a3-4c17-95a8-393b21ba8e8b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:57:57.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2033" for this suite. 08/24/23 11:57:57.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:57:57.831
Aug 24 11:57:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename hostport 08/24/23 11:57:57.832
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:57.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:57.856
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/24/23 11:57:57.867
Aug 24 11:57:57.887: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2947" to be "running and ready"
Aug 24 11:57:57.892: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.738181ms
Aug 24 11:57:57.892: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:57:59.904: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017430324s
Aug 24 11:57:59.904: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 24 11:57:59.904: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.18 on the node which pod1 resides and expect scheduled 08/24/23 11:57:59.904
Aug 24 11:57:59.915: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2947" to be "running and ready"
Aug 24 11:57:59.919: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935504ms
Aug 24 11:57:59.919: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:58:01.925: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010164361s
Aug 24 11:58:01.925: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 24 11:58:01.925: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.18 but use UDP protocol on the node which pod2 resides 08/24/23 11:58:01.926
Aug 24 11:58:01.935: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2947" to be "running and ready"
Aug 24 11:58:01.943: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464598ms
Aug 24 11:58:01.943: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:58:03.947: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012571368s
Aug 24 11:58:03.947: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 24 11:58:03.947: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 24 11:58:03.957: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2947" to be "running and ready"
Aug 24 11:58:03.962: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061974ms
Aug 24 11:58:03.962: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:58:05.980: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.022117694s
Aug 24 11:58:05.980: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 24 11:58:05.980: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/24/23 11:58:05.983
Aug 24 11:58:05.983: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.18 http://127.0.0.1:54323/hostname] Namespace:hostport-2947 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:58:05.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:58:05.985: INFO: ExecWithOptions: Clientset creation
Aug 24 11:58:05.985: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-2947/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.18+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.18, port: 54323 08/24/23 11:58:06.141
Aug 24 11:58:06.142: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.18:54323/hostname] Namespace:hostport-2947 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:58:06.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:58:06.142: INFO: ExecWithOptions: Clientset creation
Aug 24 11:58:06.143: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-2947/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.18%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.18, port: 54323 UDP 08/24/23 11:58:06.289
Aug 24 11:58:06.289: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.18 54323] Namespace:hostport-2947 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:58:06.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:58:06.290: INFO: ExecWithOptions: Clientset creation
Aug 24 11:58:06.290: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-2947/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.18+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 24 11:58:11.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2947" for this suite. 08/24/23 11:58:11.428
------------------------------
â€¢ [SLOW TEST] [13.607 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:57:57.831
    Aug 24 11:57:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename hostport 08/24/23 11:57:57.832
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:57:57.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:57:57.856
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/24/23 11:57:57.867
    Aug 24 11:57:57.887: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2947" to be "running and ready"
    Aug 24 11:57:57.892: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.738181ms
    Aug 24 11:57:57.892: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:57:59.904: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017430324s
    Aug 24 11:57:59.904: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 24 11:57:59.904: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.18 on the node which pod1 resides and expect scheduled 08/24/23 11:57:59.904
    Aug 24 11:57:59.915: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2947" to be "running and ready"
    Aug 24 11:57:59.919: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935504ms
    Aug 24 11:57:59.919: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:58:01.925: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010164361s
    Aug 24 11:58:01.925: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 24 11:58:01.925: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.18 but use UDP protocol on the node which pod2 resides 08/24/23 11:58:01.926
    Aug 24 11:58:01.935: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2947" to be "running and ready"
    Aug 24 11:58:01.943: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464598ms
    Aug 24 11:58:01.943: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:58:03.947: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.012571368s
    Aug 24 11:58:03.947: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 24 11:58:03.947: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 24 11:58:03.957: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2947" to be "running and ready"
    Aug 24 11:58:03.962: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061974ms
    Aug 24 11:58:03.962: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:58:05.980: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.022117694s
    Aug 24 11:58:05.980: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 24 11:58:05.980: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/24/23 11:58:05.983
    Aug 24 11:58:05.983: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.18 http://127.0.0.1:54323/hostname] Namespace:hostport-2947 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:58:05.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:58:05.985: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:58:05.985: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-2947/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.18+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.18, port: 54323 08/24/23 11:58:06.141
    Aug 24 11:58:06.142: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.18:54323/hostname] Namespace:hostport-2947 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:58:06.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:58:06.142: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:58:06.143: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-2947/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.18%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.18, port: 54323 UDP 08/24/23 11:58:06.289
    Aug 24 11:58:06.289: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.18 54323] Namespace:hostport-2947 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:58:06.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:58:06.290: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:58:06.290: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-2947/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.18+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:58:11.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2947" for this suite. 08/24/23 11:58:11.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:58:11.448
Aug 24 11:58:11.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:58:11.449
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:11.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:11.483
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 24 11:58:11.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 11:58:13.519
Aug 24 11:58:13.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 create -f -'
Aug 24 11:58:14.979: INFO: stderr: ""
Aug 24 11:58:14.979: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 24 11:58:14.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-6612-crds test-cr'
Aug 24 11:58:15.102: INFO: stderr: ""
Aug 24 11:58:15.102: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 24 11:58:15.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 apply -f -'
Aug 24 11:58:15.549: INFO: stderr: ""
Aug 24 11:58:15.549: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 24 11:58:15.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-6612-crds test-cr'
Aug 24 11:58:15.679: INFO: stderr: ""
Aug 24 11:58:15.679: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/24/23 11:58:15.679
Aug 24 11:58:15.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 explain e2e-test-crd-publish-openapi-6612-crds'
Aug 24 11:58:16.121: INFO: stderr: ""
Aug 24 11:58:16.121: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6612-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:58:18.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8803" for this suite. 08/24/23 11:58:18.887
------------------------------
â€¢ [SLOW TEST] [7.451 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:58:11.448
    Aug 24 11:58:11.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 11:58:11.449
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:11.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:11.483
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 24 11:58:11.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/24/23 11:58:13.519
    Aug 24 11:58:13.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 create -f -'
    Aug 24 11:58:14.979: INFO: stderr: ""
    Aug 24 11:58:14.979: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 24 11:58:14.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-6612-crds test-cr'
    Aug 24 11:58:15.102: INFO: stderr: ""
    Aug 24 11:58:15.102: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 24 11:58:15.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 apply -f -'
    Aug 24 11:58:15.549: INFO: stderr: ""
    Aug 24 11:58:15.549: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 24 11:58:15.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 --namespace=crd-publish-openapi-8803 delete e2e-test-crd-publish-openapi-6612-crds test-cr'
    Aug 24 11:58:15.679: INFO: stderr: ""
    Aug 24 11:58:15.679: INFO: stdout: "e2e-test-crd-publish-openapi-6612-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/24/23 11:58:15.679
    Aug 24 11:58:15.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=crd-publish-openapi-8803 explain e2e-test-crd-publish-openapi-6612-crds'
    Aug 24 11:58:16.121: INFO: stderr: ""
    Aug 24 11:58:16.121: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6612-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:58:18.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8803" for this suite. 08/24/23 11:58:18.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:58:18.903
Aug 24 11:58:18.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 11:58:18.904
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:18.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:18.937
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 24 11:58:18.955: INFO: Waiting up to 5m0s for pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65" in namespace "container-probe-2579" to be "running and ready"
Aug 24 11:58:18.975: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Pending", Reason="", readiness=false. Elapsed: 19.996053ms
Aug 24 11:58:18.975: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:58:20.980: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 2.02488449s
Aug 24 11:58:20.980: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:22.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 4.025527411s
Aug 24 11:58:22.981: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:24.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 6.026203301s
Aug 24 11:58:24.982: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:26.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 8.025939923s
Aug 24 11:58:26.981: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:28.979: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 10.023992841s
Aug 24 11:58:28.979: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:30.993: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 12.037420632s
Aug 24 11:58:30.993: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:32.979: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 14.0238078s
Aug 24 11:58:32.979: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:34.982: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 16.026371112s
Aug 24 11:58:34.982: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:36.986: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 18.030960315s
Aug 24 11:58:36.986: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:38.980: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 20.024900187s
Aug 24 11:58:38.980: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
Aug 24 11:58:40.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=true. Elapsed: 22.025331423s
Aug 24 11:58:40.981: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = true)
Aug 24 11:58:40.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65" satisfied condition "running and ready"
Aug 24 11:58:40.984: INFO: Container started at 2023-08-24 11:58:19 +0000 UTC, pod became ready at 2023-08-24 11:58:39 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 11:58:40.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2579" for this suite. 08/24/23 11:58:40.989
------------------------------
â€¢ [SLOW TEST] [22.098 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:58:18.903
    Aug 24 11:58:18.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 11:58:18.904
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:18.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:18.937
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 24 11:58:18.955: INFO: Waiting up to 5m0s for pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65" in namespace "container-probe-2579" to be "running and ready"
    Aug 24 11:58:18.975: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Pending", Reason="", readiness=false. Elapsed: 19.996053ms
    Aug 24 11:58:18.975: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:58:20.980: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 2.02488449s
    Aug 24 11:58:20.980: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:22.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 4.025527411s
    Aug 24 11:58:22.981: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:24.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 6.026203301s
    Aug 24 11:58:24.982: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:26.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 8.025939923s
    Aug 24 11:58:26.981: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:28.979: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 10.023992841s
    Aug 24 11:58:28.979: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:30.993: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 12.037420632s
    Aug 24 11:58:30.993: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:32.979: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 14.0238078s
    Aug 24 11:58:32.979: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:34.982: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 16.026371112s
    Aug 24 11:58:34.982: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:36.986: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 18.030960315s
    Aug 24 11:58:36.986: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:38.980: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=false. Elapsed: 20.024900187s
    Aug 24 11:58:38.980: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = false)
    Aug 24 11:58:40.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65": Phase="Running", Reason="", readiness=true. Elapsed: 22.025331423s
    Aug 24 11:58:40.981: INFO: The phase of Pod test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65 is Running (Ready = true)
    Aug 24 11:58:40.981: INFO: Pod "test-webserver-76b76e15-e9e0-4ac2-a83d-30e2d9a17d65" satisfied condition "running and ready"
    Aug 24 11:58:40.984: INFO: Container started at 2023-08-24 11:58:19 +0000 UTC, pod became ready at 2023-08-24 11:58:39 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:58:40.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2579" for this suite. 08/24/23 11:58:40.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:58:41.002
Aug 24 11:58:41.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 11:58:41.004
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:41.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:41.033
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5466 08/24/23 11:58:41.04
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 11:58:41.07
STEP: creating service externalsvc in namespace services-5466 08/24/23 11:58:41.07
STEP: creating replication controller externalsvc in namespace services-5466 08/24/23 11:58:41.104
I0824 11:58:41.119552      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5466, replica count: 2
I0824 11:58:44.171004      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/24/23 11:58:44.178
Aug 24 11:58:44.208: INFO: Creating new exec pod
Aug 24 11:58:44.246: INFO: Waiting up to 5m0s for pod "execpodnx8zb" in namespace "services-5466" to be "running"
Aug 24 11:58:44.274: INFO: Pod "execpodnx8zb": Phase="Pending", Reason="", readiness=false. Elapsed: 27.996847ms
Aug 24 11:58:46.284: INFO: Pod "execpodnx8zb": Phase="Running", Reason="", readiness=true. Elapsed: 2.038272917s
Aug 24 11:58:46.285: INFO: Pod "execpodnx8zb" satisfied condition "running"
Aug 24 11:58:46.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5466 exec execpodnx8zb -- /bin/sh -x -c nslookup clusterip-service.services-5466.svc.cluster.local'
Aug 24 11:58:46.660: INFO: stderr: "+ nslookup clusterip-service.services-5466.svc.cluster.local\n"
Aug 24 11:58:46.660: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-5466.svc.cluster.local\tcanonical name = externalsvc.services-5466.svc.cluster.local.\nName:\texternalsvc.services-5466.svc.cluster.local\nAddress: 10.254.7.115\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5466, will wait for the garbage collector to delete the pods 08/24/23 11:58:46.66
Aug 24 11:58:46.729: INFO: Deleting ReplicationController externalsvc took: 11.370008ms
Aug 24 11:58:46.829: INFO: Terminating ReplicationController externalsvc pods took: 100.427176ms
Aug 24 11:58:49.166: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 11:58:49.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5466" for this suite. 08/24/23 11:58:49.207
------------------------------
â€¢ [SLOW TEST] [8.218 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:58:41.002
    Aug 24 11:58:41.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 11:58:41.004
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:41.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:41.033
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5466 08/24/23 11:58:41.04
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/24/23 11:58:41.07
    STEP: creating service externalsvc in namespace services-5466 08/24/23 11:58:41.07
    STEP: creating replication controller externalsvc in namespace services-5466 08/24/23 11:58:41.104
    I0824 11:58:41.119552      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5466, replica count: 2
    I0824 11:58:44.171004      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/24/23 11:58:44.178
    Aug 24 11:58:44.208: INFO: Creating new exec pod
    Aug 24 11:58:44.246: INFO: Waiting up to 5m0s for pod "execpodnx8zb" in namespace "services-5466" to be "running"
    Aug 24 11:58:44.274: INFO: Pod "execpodnx8zb": Phase="Pending", Reason="", readiness=false. Elapsed: 27.996847ms
    Aug 24 11:58:46.284: INFO: Pod "execpodnx8zb": Phase="Running", Reason="", readiness=true. Elapsed: 2.038272917s
    Aug 24 11:58:46.285: INFO: Pod "execpodnx8zb" satisfied condition "running"
    Aug 24 11:58:46.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-5466 exec execpodnx8zb -- /bin/sh -x -c nslookup clusterip-service.services-5466.svc.cluster.local'
    Aug 24 11:58:46.660: INFO: stderr: "+ nslookup clusterip-service.services-5466.svc.cluster.local\n"
    Aug 24 11:58:46.660: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-5466.svc.cluster.local\tcanonical name = externalsvc.services-5466.svc.cluster.local.\nName:\texternalsvc.services-5466.svc.cluster.local\nAddress: 10.254.7.115\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5466, will wait for the garbage collector to delete the pods 08/24/23 11:58:46.66
    Aug 24 11:58:46.729: INFO: Deleting ReplicationController externalsvc took: 11.370008ms
    Aug 24 11:58:46.829: INFO: Terminating ReplicationController externalsvc pods took: 100.427176ms
    Aug 24 11:58:49.166: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:58:49.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5466" for this suite. 08/24/23 11:58:49.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:58:49.221
Aug 24 11:58:49.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 11:58:49.222
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:49.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:49.249
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/24/23 11:58:49.255
Aug 24 11:58:49.270: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8830  7289be0c-da09-424a-a2e1-ee0251e0fe3a 35770 0 2023-08-24 11:58:49 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-24 11:58:49 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbxph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbxph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 24 11:58:49.271: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8830" to be "running and ready"
Aug 24 11:58:49.285: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 14.609238ms
Aug 24 11:58:49.285: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:58:51.291: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020439477s
Aug 24 11:58:51.291: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:58:53.305: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.034323855s
Aug 24 11:58:53.305: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 24 11:58:53.305: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/24/23 11:58:53.305
Aug 24 11:58:53.305: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8830 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:58:53.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:58:53.308: INFO: ExecWithOptions: Clientset creation
Aug 24 11:58:53.308: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-8830/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/24/23 11:58:53.472
Aug 24 11:58:53.472: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8830 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:58:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:58:53.473: INFO: ExecWithOptions: Clientset creation
Aug 24 11:58:53.473: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-8830/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 24 11:58:53.623: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 11:58:53.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8830" for this suite. 08/24/23 11:58:53.657
------------------------------
â€¢ [4.449 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:58:49.221
    Aug 24 11:58:49.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 11:58:49.222
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:49.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:49.249
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/24/23 11:58:49.255
    Aug 24 11:58:49.270: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8830  7289be0c-da09-424a-a2e1-ee0251e0fe3a 35770 0 2023-08-24 11:58:49 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-24 11:58:49 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbxph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbxph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 24 11:58:49.271: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8830" to be "running and ready"
    Aug 24 11:58:49.285: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 14.609238ms
    Aug 24 11:58:49.285: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:58:51.291: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020439477s
    Aug 24 11:58:51.291: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:58:53.305: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.034323855s
    Aug 24 11:58:53.305: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 24 11:58:53.305: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/24/23 11:58:53.305
    Aug 24 11:58:53.305: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8830 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:58:53.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:58:53.308: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:58:53.308: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-8830/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/24/23 11:58:53.472
    Aug 24 11:58:53.472: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8830 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:58:53.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:58:53.473: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:58:53.473: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-8830/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 24 11:58:53.623: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:58:53.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8830" for this suite. 08/24/23 11:58:53.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:58:53.675
Aug 24 11:58:53.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename csistoragecapacity 08/24/23 11:58:53.677
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:53.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:53.715
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/24/23 11:58:53.723
STEP: getting /apis/storage.k8s.io 08/24/23 11:58:53.73
STEP: getting /apis/storage.k8s.io/v1 08/24/23 11:58:53.735
STEP: creating 08/24/23 11:58:53.74
STEP: watching 08/24/23 11:58:53.773
Aug 24 11:58:53.773: INFO: starting watch
STEP: getting 08/24/23 11:58:53.79
STEP: listing in namespace 08/24/23 11:58:53.795
STEP: listing across namespaces 08/24/23 11:58:53.8
STEP: patching 08/24/23 11:58:53.806
STEP: updating 08/24/23 11:58:53.816
Aug 24 11:58:53.828: INFO: waiting for watch events with expected annotations in namespace
Aug 24 11:58:53.828: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/24/23 11:58:53.828
STEP: deleting a collection 08/24/23 11:58:53.845
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 24 11:58:53.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-983" for this suite. 08/24/23 11:58:53.872
------------------------------
â€¢ [0.224 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:58:53.675
    Aug 24 11:58:53.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename csistoragecapacity 08/24/23 11:58:53.677
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:53.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:53.715
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/24/23 11:58:53.723
    STEP: getting /apis/storage.k8s.io 08/24/23 11:58:53.73
    STEP: getting /apis/storage.k8s.io/v1 08/24/23 11:58:53.735
    STEP: creating 08/24/23 11:58:53.74
    STEP: watching 08/24/23 11:58:53.773
    Aug 24 11:58:53.773: INFO: starting watch
    STEP: getting 08/24/23 11:58:53.79
    STEP: listing in namespace 08/24/23 11:58:53.795
    STEP: listing across namespaces 08/24/23 11:58:53.8
    STEP: patching 08/24/23 11:58:53.806
    STEP: updating 08/24/23 11:58:53.816
    Aug 24 11:58:53.828: INFO: waiting for watch events with expected annotations in namespace
    Aug 24 11:58:53.828: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/24/23 11:58:53.828
    STEP: deleting a collection 08/24/23 11:58:53.845
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:58:53.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-983" for this suite. 08/24/23 11:58:53.872
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:58:53.9
Aug 24 11:58:53.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 11:58:53.902
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:53.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:53.944
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/24/23 11:58:53.954
STEP: Creating a ResourceQuota 08/24/23 11:58:58.963
STEP: Ensuring resource quota status is calculated 08/24/23 11:58:58.974
STEP: Creating a Pod that fits quota 08/24/23 11:59:00.98
STEP: Ensuring ResourceQuota status captures the pod usage 08/24/23 11:59:00.999
STEP: Not allowing a pod to be created that exceeds remaining quota 08/24/23 11:59:03.004
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/24/23 11:59:03.006
STEP: Ensuring a pod cannot update its resource requirements 08/24/23 11:59:03.009
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/24/23 11:59:03.014
STEP: Deleting the pod 08/24/23 11:59:05.02
STEP: Ensuring resource quota status released the pod usage 08/24/23 11:59:05.06
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:07.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5766" for this suite. 08/24/23 11:59:07.071
------------------------------
â€¢ [SLOW TEST] [13.185 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:58:53.9
    Aug 24 11:58:53.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 11:58:53.902
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:58:53.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:58:53.944
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/24/23 11:58:53.954
    STEP: Creating a ResourceQuota 08/24/23 11:58:58.963
    STEP: Ensuring resource quota status is calculated 08/24/23 11:58:58.974
    STEP: Creating a Pod that fits quota 08/24/23 11:59:00.98
    STEP: Ensuring ResourceQuota status captures the pod usage 08/24/23 11:59:00.999
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/24/23 11:59:03.004
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/24/23 11:59:03.006
    STEP: Ensuring a pod cannot update its resource requirements 08/24/23 11:59:03.009
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/24/23 11:59:03.014
    STEP: Deleting the pod 08/24/23 11:59:05.02
    STEP: Ensuring resource quota status released the pod usage 08/24/23 11:59:05.06
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:07.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5766" for this suite. 08/24/23 11:59:07.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:07.086
Aug 24 11:59:07.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 11:59:07.088
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:07.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:07.123
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:59:07.154
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:59:07.858
STEP: Deploying the webhook pod 08/24/23 11:59:07.873
STEP: Wait for the deployment to be ready 08/24/23 11:59:07.902
Aug 24 11:59:07.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 24 11:59:09.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 59, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 59, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 59, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 59, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/24/23 11:59:11.964
STEP: Verifying the service has paired with the endpoint 08/24/23 11:59:11.994
Aug 24 11:59:12.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/24/23 11:59:13
STEP: create a pod that should be updated by the webhook 08/24/23 11:59:13.026
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4365" for this suite. 08/24/23 11:59:13.218
STEP: Destroying namespace "webhook-4365-markers" for this suite. 08/24/23 11:59:13.243
------------------------------
â€¢ [SLOW TEST] [6.178 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:07.086
    Aug 24 11:59:07.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 11:59:07.088
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:07.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:07.123
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:59:07.154
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:59:07.858
    STEP: Deploying the webhook pod 08/24/23 11:59:07.873
    STEP: Wait for the deployment to be ready 08/24/23 11:59:07.902
    Aug 24 11:59:07.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 24 11:59:09.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 11, 59, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 59, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 11, 59, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 11, 59, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/24/23 11:59:11.964
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:59:11.994
    Aug 24 11:59:12.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/24/23 11:59:13
    STEP: create a pod that should be updated by the webhook 08/24/23 11:59:13.026
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:13.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4365" for this suite. 08/24/23 11:59:13.218
    STEP: Destroying namespace "webhook-4365-markers" for this suite. 08/24/23 11:59:13.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:13.265
Aug 24 11:59:13.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/24/23 11:59:13.266
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:13.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:13.314
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/24/23 11:59:13.326
STEP: Creating hostNetwork=false pod 08/24/23 11:59:13.327
Aug 24 11:59:13.344: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-379" to be "running and ready"
Aug 24 11:59:13.351: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221459ms
Aug 24 11:59:13.351: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:59:15.355: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010703097s
Aug 24 11:59:15.355: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 24 11:59:15.355: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/24/23 11:59:15.358
Aug 24 11:59:15.369: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-379" to be "running and ready"
Aug 24 11:59:15.375: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.719866ms
Aug 24 11:59:15.375: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:59:17.380: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011446361s
Aug 24 11:59:17.380: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 24 11:59:17.380: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/24/23 11:59:17.384
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/24/23 11:59:17.384
Aug 24 11:59:17.384: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:17.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:17.385: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:17.386: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 11:59:17.531: INFO: Exec stderr: ""
Aug 24 11:59:17.531: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:17.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:17.532: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:17.532: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 11:59:17.671: INFO: Exec stderr: ""
Aug 24 11:59:17.671: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:17.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:17.672: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:17.672: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 11:59:17.805: INFO: Exec stderr: ""
Aug 24 11:59:17.805: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:17.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:17.805: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:17.805: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 11:59:17.954: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/24/23 11:59:17.954
Aug 24 11:59:17.954: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:17.955: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:17.955: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 24 11:59:18.129: INFO: Exec stderr: ""
Aug 24 11:59:18.129: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:18.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:18.130: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:18.130: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 24 11:59:18.278: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/24/23 11:59:18.278
Aug 24 11:59:18.278: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:18.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:18.279: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:18.279: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 11:59:18.440: INFO: Exec stderr: ""
Aug 24 11:59:18.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:18.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:18.441: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:18.441: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 24 11:59:18.581: INFO: Exec stderr: ""
Aug 24 11:59:18.582: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:18.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:18.583: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:18.583: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 11:59:18.731: INFO: Exec stderr: ""
Aug 24 11:59:18.731: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 11:59:18.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 11:59:18.732: INFO: ExecWithOptions: Clientset creation
Aug 24 11:59:18.732: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 24 11:59:18.886: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:18.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-379" for this suite. 08/24/23 11:59:18.892
------------------------------
â€¢ [SLOW TEST] [5.635 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:13.265
    Aug 24 11:59:13.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/24/23 11:59:13.266
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:13.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:13.314
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/24/23 11:59:13.326
    STEP: Creating hostNetwork=false pod 08/24/23 11:59:13.327
    Aug 24 11:59:13.344: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-379" to be "running and ready"
    Aug 24 11:59:13.351: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.221459ms
    Aug 24 11:59:13.351: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:59:15.355: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010703097s
    Aug 24 11:59:15.355: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 24 11:59:15.355: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/24/23 11:59:15.358
    Aug 24 11:59:15.369: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-379" to be "running and ready"
    Aug 24 11:59:15.375: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.719866ms
    Aug 24 11:59:15.375: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:59:17.380: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011446361s
    Aug 24 11:59:17.380: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 24 11:59:17.380: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/24/23 11:59:17.384
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/24/23 11:59:17.384
    Aug 24 11:59:17.384: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:17.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:17.385: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:17.386: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 11:59:17.531: INFO: Exec stderr: ""
    Aug 24 11:59:17.531: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:17.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:17.532: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:17.532: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 11:59:17.671: INFO: Exec stderr: ""
    Aug 24 11:59:17.671: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:17.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:17.672: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:17.672: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 11:59:17.805: INFO: Exec stderr: ""
    Aug 24 11:59:17.805: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:17.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:17.805: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:17.805: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 11:59:17.954: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/24/23 11:59:17.954
    Aug 24 11:59:17.954: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:17.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:17.955: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:17.955: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 24 11:59:18.129: INFO: Exec stderr: ""
    Aug 24 11:59:18.129: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:18.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:18.130: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:18.130: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 24 11:59:18.278: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/24/23 11:59:18.278
    Aug 24 11:59:18.278: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:18.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:18.279: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:18.279: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 11:59:18.440: INFO: Exec stderr: ""
    Aug 24 11:59:18.440: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:18.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:18.441: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:18.441: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 24 11:59:18.581: INFO: Exec stderr: ""
    Aug 24 11:59:18.582: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:18.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:18.583: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:18.583: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 11:59:18.731: INFO: Exec stderr: ""
    Aug 24 11:59:18.731: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-379 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 11:59:18.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 11:59:18.732: INFO: ExecWithOptions: Clientset creation
    Aug 24 11:59:18.732: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-379/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 24 11:59:18.886: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:18.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-379" for this suite. 08/24/23 11:59:18.892
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:18.9
Aug 24 11:59:18.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 11:59:18.902
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:18.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:18.933
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/24/23 11:59:18.938
STEP: setting up watch 08/24/23 11:59:18.939
STEP: submitting the pod to kubernetes 08/24/23 11:59:19.042
STEP: verifying the pod is in kubernetes 08/24/23 11:59:19.061
STEP: verifying pod creation was observed 08/24/23 11:59:19.066
Aug 24 11:59:19.066: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1" in namespace "pods-2272" to be "running"
Aug 24 11:59:19.078: INFO: Pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.154462ms
Aug 24 11:59:21.083: INFO: Pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016919673s
Aug 24 11:59:21.083: INFO: Pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1" satisfied condition "running"
STEP: deleting the pod gracefully 08/24/23 11:59:21.086
STEP: verifying pod deletion was observed 08/24/23 11:59:21.112
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:23.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2272" for this suite. 08/24/23 11:59:23.607
------------------------------
â€¢ [4.721 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:18.9
    Aug 24 11:59:18.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 11:59:18.902
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:18.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:18.933
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/24/23 11:59:18.938
    STEP: setting up watch 08/24/23 11:59:18.939
    STEP: submitting the pod to kubernetes 08/24/23 11:59:19.042
    STEP: verifying the pod is in kubernetes 08/24/23 11:59:19.061
    STEP: verifying pod creation was observed 08/24/23 11:59:19.066
    Aug 24 11:59:19.066: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1" in namespace "pods-2272" to be "running"
    Aug 24 11:59:19.078: INFO: Pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.154462ms
    Aug 24 11:59:21.083: INFO: Pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016919673s
    Aug 24 11:59:21.083: INFO: Pod "pod-submit-remove-5429c9af-190d-4b20-9f9f-a1aba4d8a8b1" satisfied condition "running"
    STEP: deleting the pod gracefully 08/24/23 11:59:21.086
    STEP: verifying pod deletion was observed 08/24/23 11:59:21.112
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:23.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2272" for this suite. 08/24/23 11:59:23.607
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:23.623
Aug 24 11:59:23.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename security-context-test 08/24/23 11:59:23.625
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:23.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:23.662
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 24 11:59:23.682: INFO: Waiting up to 5m0s for pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3" in namespace "security-context-test-6737" to be "Succeeded or Failed"
Aug 24 11:59:23.688: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.571998ms
Aug 24 11:59:25.702: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019203508s
Aug 24 11:59:27.694: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011835295s
Aug 24 11:59:27.694: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:27.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6737" for this suite. 08/24/23 11:59:27.703
------------------------------
â€¢ [4.094 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:23.623
    Aug 24 11:59:23.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename security-context-test 08/24/23 11:59:23.625
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:23.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:23.662
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 24 11:59:23.682: INFO: Waiting up to 5m0s for pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3" in namespace "security-context-test-6737" to be "Succeeded or Failed"
    Aug 24 11:59:23.688: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.571998ms
    Aug 24 11:59:25.702: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019203508s
    Aug 24 11:59:27.694: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011835295s
    Aug 24 11:59:27.694: INFO: Pod "busybox-user-65534-de268275-2c76-4011-9356-021d7fd8dcd3" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:27.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6737" for this suite. 08/24/23 11:59:27.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:27.718
Aug 24 11:59:27.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 11:59:27.719
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:27.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:27.759
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 11:59:27.773
Aug 24 11:59:27.792: INFO: Waiting up to 5m0s for pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b" in namespace "emptydir-677" to be "Succeeded or Failed"
Aug 24 11:59:27.799: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711066ms
Aug 24 11:59:29.803: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011347685s
Aug 24 11:59:31.803: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011669568s
STEP: Saw pod success 08/24/23 11:59:31.804
Aug 24 11:59:31.804: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b" satisfied condition "Succeeded or Failed"
Aug 24 11:59:31.815: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-b969c783-0863-4d36-93a1-9c491f97cb6b container test-container: <nil>
STEP: delete the pod 08/24/23 11:59:31.871
Aug 24 11:59:31.892: INFO: Waiting for pod pod-b969c783-0863-4d36-93a1-9c491f97cb6b to disappear
Aug 24 11:59:31.909: INFO: Pod pod-b969c783-0863-4d36-93a1-9c491f97cb6b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:31.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-677" for this suite. 08/24/23 11:59:31.913
------------------------------
â€¢ [4.208 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:27.718
    Aug 24 11:59:27.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 11:59:27.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:27.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:27.759
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 11:59:27.773
    Aug 24 11:59:27.792: INFO: Waiting up to 5m0s for pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b" in namespace "emptydir-677" to be "Succeeded or Failed"
    Aug 24 11:59:27.799: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711066ms
    Aug 24 11:59:29.803: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011347685s
    Aug 24 11:59:31.803: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011669568s
    STEP: Saw pod success 08/24/23 11:59:31.804
    Aug 24 11:59:31.804: INFO: Pod "pod-b969c783-0863-4d36-93a1-9c491f97cb6b" satisfied condition "Succeeded or Failed"
    Aug 24 11:59:31.815: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-b969c783-0863-4d36-93a1-9c491f97cb6b container test-container: <nil>
    STEP: delete the pod 08/24/23 11:59:31.871
    Aug 24 11:59:31.892: INFO: Waiting for pod pod-b969c783-0863-4d36-93a1-9c491f97cb6b to disappear
    Aug 24 11:59:31.909: INFO: Pod pod-b969c783-0863-4d36-93a1-9c491f97cb6b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:31.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-677" for this suite. 08/24/23 11:59:31.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:31.926
Aug 24 11:59:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename proxy 08/24/23 11:59:31.928
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:31.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:31.992
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 24 11:59:32.002: INFO: Creating pod...
Aug 24 11:59:32.020: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7598" to be "running"
Aug 24 11:59:32.037: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 16.715358ms
Aug 24 11:59:34.041: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.021038188s
Aug 24 11:59:34.041: INFO: Pod "agnhost" satisfied condition "running"
Aug 24 11:59:34.041: INFO: Creating service...
Aug 24 11:59:34.065: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/DELETE
Aug 24 11:59:34.085: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 11:59:34.085: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/GET
Aug 24 11:59:34.108: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 24 11:59:34.108: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/HEAD
Aug 24 11:59:34.113: INFO: http.Client request:HEAD | StatusCode:200
Aug 24 11:59:34.113: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 24 11:59:34.121: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 11:59:34.121: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/PATCH
Aug 24 11:59:34.126: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 11:59:34.126: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/POST
Aug 24 11:59:34.131: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 11:59:34.131: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/PUT
Aug 24 11:59:34.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 24 11:59:34.144: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/DELETE
Aug 24 11:59:34.152: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 24 11:59:34.152: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/GET
Aug 24 11:59:34.160: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 24 11:59:34.160: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/HEAD
Aug 24 11:59:34.168: INFO: http.Client request:HEAD | StatusCode:200
Aug 24 11:59:34.168: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/OPTIONS
Aug 24 11:59:34.180: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 24 11:59:34.180: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/PATCH
Aug 24 11:59:34.189: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 24 11:59:34.189: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/POST
Aug 24 11:59:34.200: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 24 11:59:34.200: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/PUT
Aug 24 11:59:34.218: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:34.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7598" for this suite. 08/24/23 11:59:34.234
------------------------------
â€¢ [2.406 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:31.926
    Aug 24 11:59:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename proxy 08/24/23 11:59:31.928
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:31.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:31.992
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 24 11:59:32.002: INFO: Creating pod...
    Aug 24 11:59:32.020: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7598" to be "running"
    Aug 24 11:59:32.037: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 16.715358ms
    Aug 24 11:59:34.041: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.021038188s
    Aug 24 11:59:34.041: INFO: Pod "agnhost" satisfied condition "running"
    Aug 24 11:59:34.041: INFO: Creating service...
    Aug 24 11:59:34.065: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/DELETE
    Aug 24 11:59:34.085: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 11:59:34.085: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/GET
    Aug 24 11:59:34.108: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 24 11:59:34.108: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/HEAD
    Aug 24 11:59:34.113: INFO: http.Client request:HEAD | StatusCode:200
    Aug 24 11:59:34.113: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 24 11:59:34.121: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 11:59:34.121: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/PATCH
    Aug 24 11:59:34.126: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 11:59:34.126: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/POST
    Aug 24 11:59:34.131: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 11:59:34.131: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/pods/agnhost/proxy/some/path/with/PUT
    Aug 24 11:59:34.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 24 11:59:34.144: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/DELETE
    Aug 24 11:59:34.152: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 24 11:59:34.152: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/GET
    Aug 24 11:59:34.160: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 24 11:59:34.160: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/HEAD
    Aug 24 11:59:34.168: INFO: http.Client request:HEAD | StatusCode:200
    Aug 24 11:59:34.168: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/OPTIONS
    Aug 24 11:59:34.180: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 24 11:59:34.180: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/PATCH
    Aug 24 11:59:34.189: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 24 11:59:34.189: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/POST
    Aug 24 11:59:34.200: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 24 11:59:34.200: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-7598/services/test-service/proxy/some/path/with/PUT
    Aug 24 11:59:34.218: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:34.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7598" for this suite. 08/24/23 11:59:34.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:34.338
Aug 24 11:59:34.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 11:59:34.34
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:34.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:34.415
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:59:34.423
Aug 24 11:59:34.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2266 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 24 11:59:34.541: INFO: stderr: ""
Aug 24 11:59:34.541: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 11:59:34.541
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 24 11:59:34.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2266 delete pods e2e-test-httpd-pod'
Aug 24 11:59:37.183: INFO: stderr: ""
Aug 24 11:59:37.183: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:37.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2266" for this suite. 08/24/23 11:59:37.19
------------------------------
â€¢ [2.863 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:34.338
    Aug 24 11:59:34.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 11:59:34.34
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:34.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:34.415
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/24/23 11:59:34.423
    Aug 24 11:59:34.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2266 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 24 11:59:34.541: INFO: stderr: ""
    Aug 24 11:59:34.541: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/24/23 11:59:34.541
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 24 11:59:34.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2266 delete pods e2e-test-httpd-pod'
    Aug 24 11:59:37.183: INFO: stderr: ""
    Aug 24 11:59:37.183: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:37.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2266" for this suite. 08/24/23 11:59:37.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:37.203
Aug 24 11:59:37.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 11:59:37.204
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:37.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:37.251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 11:59:37.29
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:59:38.631
STEP: Deploying the webhook pod 08/24/23 11:59:38.659
STEP: Wait for the deployment to be ready 08/24/23 11:59:38.688
Aug 24 11:59:38.702: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/24/23 11:59:40.713
STEP: Verifying the service has paired with the endpoint 08/24/23 11:59:40.736
Aug 24 11:59:41.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 11:59:41.742
Aug 24 11:59:41.777: INFO: Waiting for webhook configuration to be ready...
Aug 24 11:59:41.894: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod that should be denied by the webhook 08/24/23 11:59:41.989
STEP: create a pod that causes the webhook to hang 08/24/23 11:59:42.011
STEP: create a configmap that should be denied by the webhook 08/24/23 11:59:52.025
STEP: create a configmap that should be admitted by the webhook 08/24/23 11:59:52.046
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 11:59:52.063
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 11:59:52.074
STEP: create a namespace that bypass the webhook 08/24/23 11:59:52.086
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/24/23 11:59:52.096
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:52.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7955" for this suite. 08/24/23 11:59:52.23
STEP: Destroying namespace "webhook-7955-markers" for this suite. 08/24/23 11:59:52.261
------------------------------
â€¢ [SLOW TEST] [15.088 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:37.203
    Aug 24 11:59:37.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 11:59:37.204
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:37.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:37.251
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 11:59:37.29
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 11:59:38.631
    STEP: Deploying the webhook pod 08/24/23 11:59:38.659
    STEP: Wait for the deployment to be ready 08/24/23 11:59:38.688
    Aug 24 11:59:38.702: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/24/23 11:59:40.713
    STEP: Verifying the service has paired with the endpoint 08/24/23 11:59:40.736
    Aug 24 11:59:41.736: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 11:59:41.742
    Aug 24 11:59:41.777: INFO: Waiting for webhook configuration to be ready...
    Aug 24 11:59:41.894: INFO: Waiting for webhook configuration to be ready...
    STEP: create a pod that should be denied by the webhook 08/24/23 11:59:41.989
    STEP: create a pod that causes the webhook to hang 08/24/23 11:59:42.011
    STEP: create a configmap that should be denied by the webhook 08/24/23 11:59:52.025
    STEP: create a configmap that should be admitted by the webhook 08/24/23 11:59:52.046
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 11:59:52.063
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/24/23 11:59:52.074
    STEP: create a namespace that bypass the webhook 08/24/23 11:59:52.086
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/24/23 11:59:52.096
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:52.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7955" for this suite. 08/24/23 11:59:52.23
    STEP: Destroying namespace "webhook-7955-markers" for this suite. 08/24/23 11:59:52.261
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:52.299
Aug 24 11:59:52.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replication-controller 08/24/23 11:59:52.301
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:52.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:52.407
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/24/23 11:59:52.443
STEP: waiting for RC to be added 08/24/23 11:59:52.461
STEP: waiting for available Replicas 08/24/23 11:59:52.462
STEP: patching ReplicationController 08/24/23 11:59:54.708
STEP: waiting for RC to be modified 08/24/23 11:59:54.72
STEP: patching ReplicationController status 08/24/23 11:59:54.72
STEP: waiting for RC to be modified 08/24/23 11:59:54.732
STEP: waiting for available Replicas 08/24/23 11:59:54.732
STEP: fetching ReplicationController status 08/24/23 11:59:54.741
STEP: patching ReplicationController scale 08/24/23 11:59:54.748
STEP: waiting for RC to be modified 08/24/23 11:59:54.763
STEP: waiting for ReplicationController's scale to be the max amount 08/24/23 11:59:54.763
STEP: fetching ReplicationController; ensuring that it's patched 08/24/23 11:59:56.238
STEP: updating ReplicationController status 08/24/23 11:59:56.245
STEP: waiting for RC to be modified 08/24/23 11:59:56.269
STEP: listing all ReplicationControllers 08/24/23 11:59:56.27
STEP: checking that ReplicationController has expected values 08/24/23 11:59:56.282
STEP: deleting ReplicationControllers by collection 08/24/23 11:59:56.282
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/24/23 11:59:56.3
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 11:59:56.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4381" for this suite. 08/24/23 11:59:56.373
------------------------------
â€¢ [4.091 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:52.299
    Aug 24 11:59:52.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replication-controller 08/24/23 11:59:52.301
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:52.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:52.407
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/24/23 11:59:52.443
    STEP: waiting for RC to be added 08/24/23 11:59:52.461
    STEP: waiting for available Replicas 08/24/23 11:59:52.462
    STEP: patching ReplicationController 08/24/23 11:59:54.708
    STEP: waiting for RC to be modified 08/24/23 11:59:54.72
    STEP: patching ReplicationController status 08/24/23 11:59:54.72
    STEP: waiting for RC to be modified 08/24/23 11:59:54.732
    STEP: waiting for available Replicas 08/24/23 11:59:54.732
    STEP: fetching ReplicationController status 08/24/23 11:59:54.741
    STEP: patching ReplicationController scale 08/24/23 11:59:54.748
    STEP: waiting for RC to be modified 08/24/23 11:59:54.763
    STEP: waiting for ReplicationController's scale to be the max amount 08/24/23 11:59:54.763
    STEP: fetching ReplicationController; ensuring that it's patched 08/24/23 11:59:56.238
    STEP: updating ReplicationController status 08/24/23 11:59:56.245
    STEP: waiting for RC to be modified 08/24/23 11:59:56.269
    STEP: listing all ReplicationControllers 08/24/23 11:59:56.27
    STEP: checking that ReplicationController has expected values 08/24/23 11:59:56.282
    STEP: deleting ReplicationControllers by collection 08/24/23 11:59:56.282
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/24/23 11:59:56.3
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 11:59:56.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4381" for this suite. 08/24/23 11:59:56.373
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 11:59:56.39
Aug 24 11:59:56.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 11:59:56.391
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:56.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:56.423
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-7fffc44b-bdbf-4b5d-89ed-e5b2ad2ed045 08/24/23 11:59:56.434
STEP: Creating configMap with name cm-test-opt-upd-1671a5e7-00cc-4db7-a862-67e6bba75cab 08/24/23 11:59:56.444
STEP: Creating the pod 08/24/23 11:59:56.454
Aug 24 11:59:56.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659" in namespace "configmap-8922" to be "running and ready"
Aug 24 11:59:56.477: INFO: Pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659": Phase="Pending", Reason="", readiness=false. Elapsed: 7.480642ms
Aug 24 11:59:56.477: INFO: The phase of Pod pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 11:59:58.482: INFO: Pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659": Phase="Running", Reason="", readiness=true. Elapsed: 2.012690825s
Aug 24 11:59:58.482: INFO: The phase of Pod pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659 is Running (Ready = true)
Aug 24 11:59:58.482: INFO: Pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-7fffc44b-bdbf-4b5d-89ed-e5b2ad2ed045 08/24/23 11:59:58.579
STEP: Updating configmap cm-test-opt-upd-1671a5e7-00cc-4db7-a862-67e6bba75cab 08/24/23 11:59:58.588
STEP: Creating configMap with name cm-test-opt-create-b6154186-5af6-49c4-bf2b-272340050324 08/24/23 11:59:58.599
STEP: waiting to observe update in volume 08/24/23 11:59:58.605
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:00.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8922" for this suite. 08/24/23 12:00:00.659
------------------------------
â€¢ [4.283 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 11:59:56.39
    Aug 24 11:59:56.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 11:59:56.391
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 11:59:56.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 11:59:56.423
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-7fffc44b-bdbf-4b5d-89ed-e5b2ad2ed045 08/24/23 11:59:56.434
    STEP: Creating configMap with name cm-test-opt-upd-1671a5e7-00cc-4db7-a862-67e6bba75cab 08/24/23 11:59:56.444
    STEP: Creating the pod 08/24/23 11:59:56.454
    Aug 24 11:59:56.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659" in namespace "configmap-8922" to be "running and ready"
    Aug 24 11:59:56.477: INFO: Pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659": Phase="Pending", Reason="", readiness=false. Elapsed: 7.480642ms
    Aug 24 11:59:56.477: INFO: The phase of Pod pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 11:59:58.482: INFO: Pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659": Phase="Running", Reason="", readiness=true. Elapsed: 2.012690825s
    Aug 24 11:59:58.482: INFO: The phase of Pod pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659 is Running (Ready = true)
    Aug 24 11:59:58.482: INFO: Pod "pod-configmaps-5719bdcc-d623-4a81-b6e1-b600756b0659" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-7fffc44b-bdbf-4b5d-89ed-e5b2ad2ed045 08/24/23 11:59:58.579
    STEP: Updating configmap cm-test-opt-upd-1671a5e7-00cc-4db7-a862-67e6bba75cab 08/24/23 11:59:58.588
    STEP: Creating configMap with name cm-test-opt-create-b6154186-5af6-49c4-bf2b-272340050324 08/24/23 11:59:58.599
    STEP: waiting to observe update in volume 08/24/23 11:59:58.605
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:00.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8922" for this suite. 08/24/23 12:00:00.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:00.677
Aug 24 12:00:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 12:00:00.679
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:00.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:00.708
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-2cb83792-9384-4536-a87a-3bb081f43dfd 08/24/23 12:00:00.72
STEP: Creating a pod to test consume secrets 08/24/23 12:00:00.729
Aug 24 12:00:00.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58" in namespace "projected-3848" to be "Succeeded or Failed"
Aug 24 12:00:00.756: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490147ms
Aug 24 12:00:02.761: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016136409s
Aug 24 12:00:04.762: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01705648s
STEP: Saw pod success 08/24/23 12:00:04.762
Aug 24 12:00:04.762: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58" satisfied condition "Succeeded or Failed"
Aug 24 12:00:04.766: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:00:04.779
Aug 24 12:00:04.802: INFO: Waiting for pod pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58 to disappear
Aug 24 12:00:04.806: INFO: Pod pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3848" for this suite. 08/24/23 12:00:04.811
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:00.677
    Aug 24 12:00:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 12:00:00.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:00.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:00.708
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-2cb83792-9384-4536-a87a-3bb081f43dfd 08/24/23 12:00:00.72
    STEP: Creating a pod to test consume secrets 08/24/23 12:00:00.729
    Aug 24 12:00:00.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58" in namespace "projected-3848" to be "Succeeded or Failed"
    Aug 24 12:00:00.756: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58": Phase="Pending", Reason="", readiness=false. Elapsed: 11.490147ms
    Aug 24 12:00:02.761: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016136409s
    Aug 24 12:00:04.762: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01705648s
    STEP: Saw pod success 08/24/23 12:00:04.762
    Aug 24 12:00:04.762: INFO: Pod "pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58" satisfied condition "Succeeded or Failed"
    Aug 24 12:00:04.766: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:00:04.779
    Aug 24 12:00:04.802: INFO: Waiting for pod pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58 to disappear
    Aug 24 12:00:04.806: INFO: Pod pod-projected-secrets-822e2f26-93f3-489c-a8b3-58e1ba288d58 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3848" for this suite. 08/24/23 12:00:04.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:04.822
Aug 24 12:00:04.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 12:00:04.824
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:04.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:04.861
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/24/23 12:00:04.87
Aug 24 12:00:04.870: INFO: namespace kubectl-5911
Aug 24 12:00:04.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 create -f -'
Aug 24 12:00:06.520: INFO: stderr: ""
Aug 24 12:00:06.520: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/24/23 12:00:06.52
Aug 24 12:00:07.527: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:00:07.527: INFO: Found 0 / 1
Aug 24 12:00:08.524: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:00:08.524: INFO: Found 1 / 1
Aug 24 12:00:08.524: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 24 12:00:08.527: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 24 12:00:08.527: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 24 12:00:08.527: INFO: wait on agnhost-primary startup in kubectl-5911 
Aug 24 12:00:08.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 logs agnhost-primary-6z896 agnhost-primary'
Aug 24 12:00:08.652: INFO: stderr: ""
Aug 24 12:00:08.652: INFO: stdout: "Paused\n"
STEP: exposing RC 08/24/23 12:00:08.652
Aug 24 12:00:08.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 24 12:00:08.795: INFO: stderr: ""
Aug 24 12:00:08.795: INFO: stdout: "service/rm2 exposed\n"
Aug 24 12:00:08.818: INFO: Service rm2 in namespace kubectl-5911 found.
STEP: exposing service 08/24/23 12:00:10.832
Aug 24 12:00:10.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 24 12:00:10.991: INFO: stderr: ""
Aug 24 12:00:10.992: INFO: stdout: "service/rm3 exposed\n"
Aug 24 12:00:11.000: INFO: Service rm3 in namespace kubectl-5911 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:13.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5911" for this suite. 08/24/23 12:00:13.015
------------------------------
â€¢ [SLOW TEST] [8.206 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:04.822
    Aug 24 12:00:04.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:00:04.824
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:04.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:04.861
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/24/23 12:00:04.87
    Aug 24 12:00:04.870: INFO: namespace kubectl-5911
    Aug 24 12:00:04.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 create -f -'
    Aug 24 12:00:06.520: INFO: stderr: ""
    Aug 24 12:00:06.520: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/24/23 12:00:06.52
    Aug 24 12:00:07.527: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:00:07.527: INFO: Found 0 / 1
    Aug 24 12:00:08.524: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:00:08.524: INFO: Found 1 / 1
    Aug 24 12:00:08.524: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 24 12:00:08.527: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 24 12:00:08.527: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 24 12:00:08.527: INFO: wait on agnhost-primary startup in kubectl-5911 
    Aug 24 12:00:08.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 logs agnhost-primary-6z896 agnhost-primary'
    Aug 24 12:00:08.652: INFO: stderr: ""
    Aug 24 12:00:08.652: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/24/23 12:00:08.652
    Aug 24 12:00:08.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 24 12:00:08.795: INFO: stderr: ""
    Aug 24 12:00:08.795: INFO: stdout: "service/rm2 exposed\n"
    Aug 24 12:00:08.818: INFO: Service rm2 in namespace kubectl-5911 found.
    STEP: exposing service 08/24/23 12:00:10.832
    Aug 24 12:00:10.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-5911 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 24 12:00:10.991: INFO: stderr: ""
    Aug 24 12:00:10.992: INFO: stdout: "service/rm3 exposed\n"
    Aug 24 12:00:11.000: INFO: Service rm3 in namespace kubectl-5911 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:13.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5911" for this suite. 08/24/23 12:00:13.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:13.03
Aug 24 12:00:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:00:13.031
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:13.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:13.057
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 24 12:00:13.079: INFO: Waiting up to 5m0s for pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060" in namespace "svcaccounts-4737" to be "running"
Aug 24 12:00:13.086: INFO: Pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060": Phase="Pending", Reason="", readiness=false. Elapsed: 6.923535ms
Aug 24 12:00:15.090: INFO: Pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060": Phase="Running", Reason="", readiness=true. Elapsed: 2.011279851s
Aug 24 12:00:15.090: INFO: Pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060" satisfied condition "running"
STEP: reading a file in the container 08/24/23 12:00:15.09
Aug 24 12:00:15.091: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4737 pod-service-account-33630c96-547d-4190-8fb9-46930afbf060 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/24/23 12:00:15.373
Aug 24 12:00:15.373: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4737 pod-service-account-33630c96-547d-4190-8fb9-46930afbf060 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/24/23 12:00:15.631
Aug 24 12:00:15.632: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4737 pod-service-account-33630c96-547d-4190-8fb9-46930afbf060 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 24 12:00:15.931: INFO: Got root ca configmap in namespace "svcaccounts-4737"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:15.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4737" for this suite. 08/24/23 12:00:15.946
------------------------------
â€¢ [2.928 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:13.03
    Aug 24 12:00:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename svcaccounts 08/24/23 12:00:13.031
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:13.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:13.057
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 24 12:00:13.079: INFO: Waiting up to 5m0s for pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060" in namespace "svcaccounts-4737" to be "running"
    Aug 24 12:00:13.086: INFO: Pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060": Phase="Pending", Reason="", readiness=false. Elapsed: 6.923535ms
    Aug 24 12:00:15.090: INFO: Pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060": Phase="Running", Reason="", readiness=true. Elapsed: 2.011279851s
    Aug 24 12:00:15.090: INFO: Pod "pod-service-account-33630c96-547d-4190-8fb9-46930afbf060" satisfied condition "running"
    STEP: reading a file in the container 08/24/23 12:00:15.09
    Aug 24 12:00:15.091: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4737 pod-service-account-33630c96-547d-4190-8fb9-46930afbf060 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/24/23 12:00:15.373
    Aug 24 12:00:15.373: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4737 pod-service-account-33630c96-547d-4190-8fb9-46930afbf060 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/24/23 12:00:15.631
    Aug 24 12:00:15.632: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4737 pod-service-account-33630c96-547d-4190-8fb9-46930afbf060 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 24 12:00:15.931: INFO: Got root ca configmap in namespace "svcaccounts-4737"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:15.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4737" for this suite. 08/24/23 12:00:15.946
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:15.959
Aug 24 12:00:15.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename server-version 08/24/23 12:00:15.961
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:15.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:16.003
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/24/23 12:00:16.014
STEP: Confirm major version 08/24/23 12:00:16.019
Aug 24 12:00:16.019: INFO: Major version: 1
STEP: Confirm minor version 08/24/23 12:00:16.019
Aug 24 12:00:16.019: INFO: cleanMinorVersion: 26
Aug 24 12:00:16.019: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:16.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-9648" for this suite. 08/24/23 12:00:16.025
------------------------------
â€¢ [0.081 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:15.959
    Aug 24 12:00:15.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename server-version 08/24/23 12:00:15.961
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:15.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:16.003
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/24/23 12:00:16.014
    STEP: Confirm major version 08/24/23 12:00:16.019
    Aug 24 12:00:16.019: INFO: Major version: 1
    STEP: Confirm minor version 08/24/23 12:00:16.019
    Aug 24 12:00:16.019: INFO: cleanMinorVersion: 26
    Aug 24 12:00:16.019: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:16.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-9648" for this suite. 08/24/23 12:00:16.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:16.041
Aug 24 12:00:16.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:00:16.043
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:16.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:16.072
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/24/23 12:00:16.079
Aug 24 12:00:16.093: INFO: Waiting up to 5m0s for pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd" in namespace "emptydir-8021" to be "Succeeded or Failed"
Aug 24 12:00:16.100: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.972025ms
Aug 24 12:00:18.104: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010712076s
Aug 24 12:00:20.104: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010630251s
STEP: Saw pod success 08/24/23 12:00:20.104
Aug 24 12:00:20.104: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd" satisfied condition "Succeeded or Failed"
Aug 24 12:00:20.107: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd container test-container: <nil>
STEP: delete the pod 08/24/23 12:00:20.115
Aug 24 12:00:20.137: INFO: Waiting for pod pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd to disappear
Aug 24 12:00:20.142: INFO: Pod pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:20.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8021" for this suite. 08/24/23 12:00:20.147
------------------------------
â€¢ [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:16.041
    Aug 24 12:00:16.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:00:16.043
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:16.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:16.072
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/24/23 12:00:16.079
    Aug 24 12:00:16.093: INFO: Waiting up to 5m0s for pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd" in namespace "emptydir-8021" to be "Succeeded or Failed"
    Aug 24 12:00:16.100: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.972025ms
    Aug 24 12:00:18.104: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010712076s
    Aug 24 12:00:20.104: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010630251s
    STEP: Saw pod success 08/24/23 12:00:20.104
    Aug 24 12:00:20.104: INFO: Pod "pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd" satisfied condition "Succeeded or Failed"
    Aug 24 12:00:20.107: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd container test-container: <nil>
    STEP: delete the pod 08/24/23 12:00:20.115
    Aug 24 12:00:20.137: INFO: Waiting for pod pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd to disappear
    Aug 24 12:00:20.142: INFO: Pod pod-36b6c117-9b4b-48e2-a17f-fa2c20129cbd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:20.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8021" for this suite. 08/24/23 12:00:20.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:20.164
Aug 24 12:00:20.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 12:00:20.165
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:20.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:20.193
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7619 08/24/23 12:00:20.202
STEP: changing the ExternalName service to type=NodePort 08/24/23 12:00:20.217
STEP: creating replication controller externalname-service in namespace services-7619 08/24/23 12:00:20.268
I0824 12:00:20.292643      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7619, replica count: 2
I0824 12:00:23.344292      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:00:23.344: INFO: Creating new exec pod
Aug 24 12:00:23.360: INFO: Waiting up to 5m0s for pod "execpod9g4qs" in namespace "services-7619" to be "running"
Aug 24 12:00:23.366: INFO: Pod "execpod9g4qs": Phase="Pending", Reason="", readiness=false. Elapsed: 6.412854ms
Aug 24 12:00:25.371: INFO: Pod "execpod9g4qs": Phase="Running", Reason="", readiness=true. Elapsed: 2.01179033s
Aug 24 12:00:25.371: INFO: Pod "execpod9g4qs" satisfied condition "running"
Aug 24 12:00:26.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 24 12:00:26.641: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 24 12:00:26.641: INFO: stdout: ""
Aug 24 12:00:26.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 10.254.27.105 80'
Aug 24 12:00:26.888: INFO: stderr: "+ nc -v -z -w 2 10.254.27.105 80\nConnection to 10.254.27.105 80 port [tcp/http] succeeded!\n"
Aug 24 12:00:26.888: INFO: stdout: ""
Aug 24 12:00:26.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 10.0.0.17 31819'
Aug 24 12:00:27.165: INFO: stderr: "+ nc -v -z -w 2 10.0.0.17 31819\nConnection to 10.0.0.17 31819 port [tcp/*] succeeded!\n"
Aug 24 12:00:27.165: INFO: stdout: ""
Aug 24 12:00:27.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 10.0.0.18 31819'
Aug 24 12:00:27.454: INFO: stderr: "+ nc -v -z -w 2 10.0.0.18 31819\nConnection to 10.0.0.18 31819 port [tcp/*] succeeded!\n"
Aug 24 12:00:27.454: INFO: stdout: ""
Aug 24 12:00:27.454: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:27.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7619" for this suite. 08/24/23 12:00:27.505
------------------------------
â€¢ [SLOW TEST] [7.349 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:20.164
    Aug 24 12:00:20.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 12:00:20.165
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:20.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:20.193
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7619 08/24/23 12:00:20.202
    STEP: changing the ExternalName service to type=NodePort 08/24/23 12:00:20.217
    STEP: creating replication controller externalname-service in namespace services-7619 08/24/23 12:00:20.268
    I0824 12:00:20.292643      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7619, replica count: 2
    I0824 12:00:23.344292      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:00:23.344: INFO: Creating new exec pod
    Aug 24 12:00:23.360: INFO: Waiting up to 5m0s for pod "execpod9g4qs" in namespace "services-7619" to be "running"
    Aug 24 12:00:23.366: INFO: Pod "execpod9g4qs": Phase="Pending", Reason="", readiness=false. Elapsed: 6.412854ms
    Aug 24 12:00:25.371: INFO: Pod "execpod9g4qs": Phase="Running", Reason="", readiness=true. Elapsed: 2.01179033s
    Aug 24 12:00:25.371: INFO: Pod "execpod9g4qs" satisfied condition "running"
    Aug 24 12:00:26.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 24 12:00:26.641: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 24 12:00:26.641: INFO: stdout: ""
    Aug 24 12:00:26.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 10.254.27.105 80'
    Aug 24 12:00:26.888: INFO: stderr: "+ nc -v -z -w 2 10.254.27.105 80\nConnection to 10.254.27.105 80 port [tcp/http] succeeded!\n"
    Aug 24 12:00:26.888: INFO: stdout: ""
    Aug 24 12:00:26.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 10.0.0.17 31819'
    Aug 24 12:00:27.165: INFO: stderr: "+ nc -v -z -w 2 10.0.0.17 31819\nConnection to 10.0.0.17 31819 port [tcp/*] succeeded!\n"
    Aug 24 12:00:27.165: INFO: stdout: ""
    Aug 24 12:00:27.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7619 exec execpod9g4qs -- /bin/sh -x -c nc -v -z -w 2 10.0.0.18 31819'
    Aug 24 12:00:27.454: INFO: stderr: "+ nc -v -z -w 2 10.0.0.18 31819\nConnection to 10.0.0.18 31819 port [tcp/*] succeeded!\n"
    Aug 24 12:00:27.454: INFO: stdout: ""
    Aug 24 12:00:27.454: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:27.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7619" for this suite. 08/24/23 12:00:27.505
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:27.514
Aug 24 12:00:27.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:00:27.516
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:27.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:27.553
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Aug 24 12:00:27.579: INFO: Waiting up to 2m0s for pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" in namespace "var-expansion-9740" to be "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:00:27.598: INFO: Pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.833077ms
Aug 24 12:00:29.603: INFO: Pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02440254s
Aug 24 12:00:29.603: INFO: Pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:00:29.603: INFO: Deleting pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" in namespace "var-expansion-9740"
Aug 24 12:00:29.619: INFO: Wait up to 5m0s for pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:33.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9740" for this suite. 08/24/23 12:00:33.632
------------------------------
â€¢ [SLOW TEST] [6.173 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:27.514
    Aug 24 12:00:27.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:00:27.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:27.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:27.553
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Aug 24 12:00:27.579: INFO: Waiting up to 2m0s for pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" in namespace "var-expansion-9740" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:00:27.598: INFO: Pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.833077ms
    Aug 24 12:00:29.603: INFO: Pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02440254s
    Aug 24 12:00:29.603: INFO: Pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:00:29.603: INFO: Deleting pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" in namespace "var-expansion-9740"
    Aug 24 12:00:29.619: INFO: Wait up to 5m0s for pod "var-expansion-0174656c-c07a-44d9-a34b-e774a8484de6" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:33.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9740" for this suite. 08/24/23 12:00:33.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:33.688
Aug 24 12:00:33.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 12:00:33.689
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:33.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:33.738
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-b63ae9a8-7f65-40ed-a2d0-649ab601c3b4 08/24/23 12:00:33.75
STEP: Creating a pod to test consume configMaps 08/24/23 12:00:33.761
Aug 24 12:00:33.778: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854" in namespace "projected-1028" to be "Succeeded or Failed"
Aug 24 12:00:33.828: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Pending", Reason="", readiness=false. Elapsed: 50.614818ms
Aug 24 12:00:35.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055135711s
Aug 24 12:00:37.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055222732s
Aug 24 12:00:39.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055258912s
STEP: Saw pod success 08/24/23 12:00:39.833
Aug 24 12:00:39.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854" satisfied condition "Succeeded or Failed"
Aug 24 12:00:39.835: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:00:39.843
Aug 24 12:00:39.865: INFO: Waiting for pod pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854 to disappear
Aug 24 12:00:39.876: INFO: Pod pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1028" for this suite. 08/24/23 12:00:39.882
------------------------------
â€¢ [SLOW TEST] [6.204 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:33.688
    Aug 24 12:00:33.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 12:00:33.689
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:33.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:33.738
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-b63ae9a8-7f65-40ed-a2d0-649ab601c3b4 08/24/23 12:00:33.75
    STEP: Creating a pod to test consume configMaps 08/24/23 12:00:33.761
    Aug 24 12:00:33.778: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854" in namespace "projected-1028" to be "Succeeded or Failed"
    Aug 24 12:00:33.828: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Pending", Reason="", readiness=false. Elapsed: 50.614818ms
    Aug 24 12:00:35.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055135711s
    Aug 24 12:00:37.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055222732s
    Aug 24 12:00:39.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055258912s
    STEP: Saw pod success 08/24/23 12:00:39.833
    Aug 24 12:00:39.833: INFO: Pod "pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854" satisfied condition "Succeeded or Failed"
    Aug 24 12:00:39.835: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:00:39.843
    Aug 24 12:00:39.865: INFO: Waiting for pod pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854 to disappear
    Aug 24 12:00:39.876: INFO: Pod pod-projected-configmaps-2fce0d0c-bddf-49ee-a0eb-a68f606b7854 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1028" for this suite. 08/24/23 12:00:39.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:39.892
Aug 24 12:00:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 12:00:39.893
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:39.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:39.925
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-58451193-7875-4cbf-a61f-b4aed90df6f9 08/24/23 12:00:39.931
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:39.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8099" for this suite. 08/24/23 12:00:39.941
------------------------------
â€¢ [0.058 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:39.892
    Aug 24 12:00:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 12:00:39.893
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:39.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:39.925
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-58451193-7875-4cbf-a61f-b4aed90df6f9 08/24/23 12:00:39.931
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:39.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8099" for this suite. 08/24/23 12:00:39.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:39.959
Aug 24 12:00:39.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:00:39.961
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:39.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:39.995
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 12:00:40.001
Aug 24 12:00:40.014: INFO: Waiting up to 5m0s for pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0" in namespace "emptydir-782" to be "Succeeded or Failed"
Aug 24 12:00:40.020: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054227ms
Aug 24 12:00:42.025: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011103995s
Aug 24 12:00:44.026: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011971693s
STEP: Saw pod success 08/24/23 12:00:44.026
Aug 24 12:00:44.026: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0" satisfied condition "Succeeded or Failed"
Aug 24 12:00:44.030: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0 container test-container: <nil>
STEP: delete the pod 08/24/23 12:00:44.038
Aug 24 12:00:44.067: INFO: Waiting for pod pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0 to disappear
Aug 24 12:00:44.072: INFO: Pod pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:44.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-782" for this suite. 08/24/23 12:00:44.079
------------------------------
â€¢ [4.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:39.959
    Aug 24 12:00:39.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:00:39.961
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:39.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:39.995
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/24/23 12:00:40.001
    Aug 24 12:00:40.014: INFO: Waiting up to 5m0s for pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0" in namespace "emptydir-782" to be "Succeeded or Failed"
    Aug 24 12:00:40.020: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054227ms
    Aug 24 12:00:42.025: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011103995s
    Aug 24 12:00:44.026: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011971693s
    STEP: Saw pod success 08/24/23 12:00:44.026
    Aug 24 12:00:44.026: INFO: Pod "pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0" satisfied condition "Succeeded or Failed"
    Aug 24 12:00:44.030: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:00:44.038
    Aug 24 12:00:44.067: INFO: Waiting for pod pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0 to disappear
    Aug 24 12:00:44.072: INFO: Pod pod-1d613ea3-e6cc-4faf-8b3b-b9b8adfc83b0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:44.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-782" for this suite. 08/24/23 12:00:44.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:44.094
Aug 24 12:00:44.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 12:00:44.096
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:44.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:44.129
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 24 12:00:44.140: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 24 12:00:44.176: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 24 12:00:49.180: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:00:49.18
Aug 24 12:00:49.181: INFO: Creating deployment "test-rolling-update-deployment"
Aug 24 12:00:49.197: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 24 12:00:49.212: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 24 12:00:51.220: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 24 12:00:51.223: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:00:51.232: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8781  538b5e32-2ed3-40b3-a668-141cef8cec49 37071 1 2023-08-24 12:00:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7dcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:00:49 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-24 12:00:50 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 12:00:51.236: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-8781  55ea60d9-ad82-492e-b517-94a5d99d714e 37061 1 2023-08-24 12:00:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 538b5e32-2ed3-40b3-a668-141cef8cec49 0xc004c322d7 0xc004c322d8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"538b5e32-2ed3-40b3-a668-141cef8cec49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c32558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:00:51.236: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 24 12:00:51.236: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8781  b4bcdcbc-45e1-4892-ad32-f31e0da6e8c3 37070 2 2023-08-24 12:00:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 538b5e32-2ed3-40b3-a668-141cef8cec49 0xc004c32097 0xc004c32098}] [] [{e2e.test Update apps/v1 2023-08-24 12:00:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"538b5e32-2ed3-40b3-a668-141cef8cec49\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c32158 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:00:51.240: INFO: Pod "test-rolling-update-deployment-7549d9f46d-z5sdn" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-z5sdn test-rolling-update-deployment-7549d9f46d- deployment-8781  57183268-4ddc-4d23-a7e7-6091288fb335 37059 0 2023-08-24 12:00:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:2038e099f3d4021f3578a992cdcc469ede194bd4c5bd7cfe7e082afe41b990cb cni.projectcalico.org/podIP:10.100.181.134/32 cni.projectcalico.org/podIPs:10.100.181.134/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 55ea60d9-ad82-492e-b517-94a5d99d714e 0xc004c329c7 0xc004c329c8}] [] [{Go-http-client Update v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55ea60d9-ad82-492e-b517-94a5d99d714e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prrzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prrzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.134,StartTime:2023-08-24 12:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://c93500fa5b7276ff014cf0f33946eac75e4b32c57a25f41a50c61ed5df4ec790,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8781" for this suite. 08/24/23 12:00:51.245
------------------------------
â€¢ [SLOW TEST] [7.161 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:44.094
    Aug 24 12:00:44.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 12:00:44.096
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:44.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:44.129
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 24 12:00:44.140: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 24 12:00:44.176: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 24 12:00:49.180: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:00:49.18
    Aug 24 12:00:49.181: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 24 12:00:49.197: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 24 12:00:49.212: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 24 12:00:51.220: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 24 12:00:51.223: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:00:51.232: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8781  538b5e32-2ed3-40b3-a668-141cef8cec49 37071 1 2023-08-24 12:00:49 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f7dcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:00:49 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-24 12:00:50 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 12:00:51.236: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-8781  55ea60d9-ad82-492e-b517-94a5d99d714e 37061 1 2023-08-24 12:00:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 538b5e32-2ed3-40b3-a668-141cef8cec49 0xc004c322d7 0xc004c322d8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"538b5e32-2ed3-40b3-a668-141cef8cec49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c32558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:00:51.236: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 24 12:00:51.236: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8781  b4bcdcbc-45e1-4892-ad32-f31e0da6e8c3 37070 2 2023-08-24 12:00:44 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 538b5e32-2ed3-40b3-a668-141cef8cec49 0xc004c32097 0xc004c32098}] [] [{e2e.test Update apps/v1 2023-08-24 12:00:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"538b5e32-2ed3-40b3-a668-141cef8cec49\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c32158 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:00:51.240: INFO: Pod "test-rolling-update-deployment-7549d9f46d-z5sdn" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-z5sdn test-rolling-update-deployment-7549d9f46d- deployment-8781  57183268-4ddc-4d23-a7e7-6091288fb335 37059 0 2023-08-24 12:00:49 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:2038e099f3d4021f3578a992cdcc469ede194bd4c5bd7cfe7e082afe41b990cb cni.projectcalico.org/podIP:10.100.181.134/32 cni.projectcalico.org/podIPs:10.100.181.134/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 55ea60d9-ad82-492e-b517-94a5d99d714e 0xc004c329c7 0xc004c329c8}] [] [{Go-http-client Update v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-24 12:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55ea60d9-ad82-492e-b517-94a5d99d714e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:00:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prrzs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prrzs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:00:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.134,StartTime:2023-08-24 12:00:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:00:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://c93500fa5b7276ff014cf0f33946eac75e4b32c57a25f41a50c61ed5df4ec790,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8781" for this suite. 08/24/23 12:00:51.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:51.262
Aug 24 12:00:51.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 12:00:51.264
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:51.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:51.301
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-aa2af4be-3574-46ef-8656-163a111ef60a 08/24/23 12:00:51.36
STEP: Creating a pod to test consume secrets 08/24/23 12:00:51.37
Aug 24 12:00:51.384: INFO: Waiting up to 5m0s for pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0" in namespace "secrets-4841" to be "Succeeded or Failed"
Aug 24 12:00:51.400: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.169138ms
Aug 24 12:00:53.406: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021986547s
Aug 24 12:00:55.405: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020688768s
STEP: Saw pod success 08/24/23 12:00:55.405
Aug 24 12:00:55.405: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0" satisfied condition "Succeeded or Failed"
Aug 24 12:00:55.408: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:00:55.416
Aug 24 12:00:55.433: INFO: Waiting for pod pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0 to disappear
Aug 24 12:00:55.439: INFO: Pod pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:55.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4841" for this suite. 08/24/23 12:00:55.446
STEP: Destroying namespace "secret-namespace-7229" for this suite. 08/24/23 12:00:55.455
------------------------------
â€¢ [4.220 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:51.262
    Aug 24 12:00:51.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 12:00:51.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:51.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:51.301
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-aa2af4be-3574-46ef-8656-163a111ef60a 08/24/23 12:00:51.36
    STEP: Creating a pod to test consume secrets 08/24/23 12:00:51.37
    Aug 24 12:00:51.384: INFO: Waiting up to 5m0s for pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0" in namespace "secrets-4841" to be "Succeeded or Failed"
    Aug 24 12:00:51.400: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.169138ms
    Aug 24 12:00:53.406: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021986547s
    Aug 24 12:00:55.405: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020688768s
    STEP: Saw pod success 08/24/23 12:00:55.405
    Aug 24 12:00:55.405: INFO: Pod "pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0" satisfied condition "Succeeded or Failed"
    Aug 24 12:00:55.408: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:00:55.416
    Aug 24 12:00:55.433: INFO: Waiting for pod pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0 to disappear
    Aug 24 12:00:55.439: INFO: Pod pod-secrets-ea806f92-74e2-455b-a20f-24f836a15da0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:55.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4841" for this suite. 08/24/23 12:00:55.446
    STEP: Destroying namespace "secret-namespace-7229" for this suite. 08/24/23 12:00:55.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:55.483
Aug 24 12:00:55.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:00:55.484
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:55.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:55.524
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:00:55.566
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:00:55.578
Aug 24 12:00:55.583: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:00:55.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:00:55.591: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:00:56.612: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:00:56.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:00:56.616: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:00:57.596: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:00:57.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:00:57.599: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:00:58.598: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:00:58.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:00:58.602: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 08/24/23 12:00:58.607
STEP: DeleteCollection of the DaemonSets 08/24/23 12:00:58.612
STEP: Verify that ReplicaSets have been deleted 08/24/23 12:00:58.622
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Aug 24 12:00:58.647: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37198"},"items":null}

Aug 24 12:00:58.657: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37199"},"items":[{"metadata":{"name":"daemon-set-6bg2l","generateName":"daemon-set-","namespace":"daemonsets-767","uid":"234c931f-e166-4eff-abcf-2db7896296c6","resourceVersion":"37166","creationTimestamp":"2023-08-24T12:00:55Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b63baca33c4e5de2ce8cb1de24e26c588ce2c121588426e9a9ffa42dffc401ca","cni.projectcalico.org/podIP":"10.100.45.180/32","cni.projectcalico.org/podIPs":"10.100.45.180/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8899db45-6bcb-49d2-b36c-c35076ae5090","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8899db45-6bcb-49d2-b36c-c35076ae5090\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-98gvm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-98gvm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gitlab-1-26-36460-guscsyka22xa-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gitlab-1-26-36460-guscsyka22xa-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"}],"hostIP":"10.0.0.17","podIP":"10.100.45.180","podIPs":[{"ip":"10.100.45.180"}],"startTime":"2023-08-24T12:00:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:00:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://47d5310893a32e674df72fa0501ec268577cfba267f30533911e2c90e94b823e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6h4q8","generateName":"daemon-set-","namespace":"daemonsets-767","uid":"2df93116-9a32-4b59-927d-76948d923506","resourceVersion":"37190","creationTimestamp":"2023-08-24T12:00:55Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"400c3d46983c831caf4b4f0c3e563a2e82a9fcf66c9afc3bbfc8a1fba6285ae0","cni.projectcalico.org/podIP":"10.100.181.135/32","cni.projectcalico.org/podIPs":"10.100.181.135/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8899db45-6bcb-49d2-b36c-c35076ae5090","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8899db45-6bcb-49d2-b36c-c35076ae5090\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b8dg8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b8dg8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gitlab-1-26-36460-guscsyka22xa-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gitlab-1-26-36460-guscsyka22xa-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"}],"hostIP":"10.0.0.18","podIP":"10.100.181.135","podIPs":[{"ip":"10.100.181.135"}],"startTime":"2023-08-24T12:00:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:00:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://408cc9853678febedf301b5290c0bd84205b4563102f1c9a50756f588a6d5d25","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rw5f9","generateName":"daemon-set-","namespace":"daemonsets-767","uid":"4ee7edb7-9508-4e1e-a602-fe4fe182d38b","resourceVersion":"37199","creationTimestamp":"2023-08-24T12:00:55Z","deletionTimestamp":"2023-08-24T12:01:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"321acfa398c7c0961f28daa619a299e289366afde52effd4505deeba64415776","cni.projectcalico.org/podIP":"10.100.148.202/32","cni.projectcalico.org/podIPs":"10.100.148.202/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8899db45-6bcb-49d2-b36c-c35076ae5090","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8899db45-6bcb-49d2-b36c-c35076ae5090\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.148.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ml75z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ml75z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gitlab-1-26-36460-guscsyka22xa-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gitlab-1-26-36460-guscsyka22xa-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"}],"hostIP":"10.0.0.4","podIP":"10.100.148.202","podIPs":[{"ip":"10.100.148.202"}],"startTime":"2023-08-24T12:00:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:00:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f3f0d724e5b261be8c6a9aac5856dc203aaab0fe5188fa6302b4723e5fc17493","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:00:58.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-767" for this suite. 08/24/23 12:00:58.699
------------------------------
â€¢ [3.226 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:55.483
    Aug 24 12:00:55.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:00:55.484
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:55.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:55.524
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:00:55.566
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:00:55.578
    Aug 24 12:00:55.583: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:00:55.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:00:55.591: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:00:56.612: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:00:56.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:00:56.616: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:00:57.596: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:00:57.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:00:57.599: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:00:58.598: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:00:58.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:00:58.602: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 08/24/23 12:00:58.607
    STEP: DeleteCollection of the DaemonSets 08/24/23 12:00:58.612
    STEP: Verify that ReplicaSets have been deleted 08/24/23 12:00:58.622
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Aug 24 12:00:58.647: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37198"},"items":null}

    Aug 24 12:00:58.657: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37199"},"items":[{"metadata":{"name":"daemon-set-6bg2l","generateName":"daemon-set-","namespace":"daemonsets-767","uid":"234c931f-e166-4eff-abcf-2db7896296c6","resourceVersion":"37166","creationTimestamp":"2023-08-24T12:00:55Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b63baca33c4e5de2ce8cb1de24e26c588ce2c121588426e9a9ffa42dffc401ca","cni.projectcalico.org/podIP":"10.100.45.180/32","cni.projectcalico.org/podIPs":"10.100.45.180/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8899db45-6bcb-49d2-b36c-c35076ae5090","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8899db45-6bcb-49d2-b36c-c35076ae5090\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.45.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-98gvm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-98gvm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gitlab-1-26-36460-guscsyka22xa-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gitlab-1-26-36460-guscsyka22xa-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:56Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:56Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"}],"hostIP":"10.0.0.17","podIP":"10.100.45.180","podIPs":[{"ip":"10.100.45.180"}],"startTime":"2023-08-24T12:00:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:00:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://47d5310893a32e674df72fa0501ec268577cfba267f30533911e2c90e94b823e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-6h4q8","generateName":"daemon-set-","namespace":"daemonsets-767","uid":"2df93116-9a32-4b59-927d-76948d923506","resourceVersion":"37190","creationTimestamp":"2023-08-24T12:00:55Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"400c3d46983c831caf4b4f0c3e563a2e82a9fcf66c9afc3bbfc8a1fba6285ae0","cni.projectcalico.org/podIP":"10.100.181.135/32","cni.projectcalico.org/podIPs":"10.100.181.135/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8899db45-6bcb-49d2-b36c-c35076ae5090","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8899db45-6bcb-49d2-b36c-c35076ae5090\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b8dg8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b8dg8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gitlab-1-26-36460-guscsyka22xa-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gitlab-1-26-36460-guscsyka22xa-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"}],"hostIP":"10.0.0.18","podIP":"10.100.181.135","podIPs":[{"ip":"10.100.181.135"}],"startTime":"2023-08-24T12:00:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:00:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://408cc9853678febedf301b5290c0bd84205b4563102f1c9a50756f588a6d5d25","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rw5f9","generateName":"daemon-set-","namespace":"daemonsets-767","uid":"4ee7edb7-9508-4e1e-a602-fe4fe182d38b","resourceVersion":"37199","creationTimestamp":"2023-08-24T12:00:55Z","deletionTimestamp":"2023-08-24T12:01:28Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"321acfa398c7c0961f28daa619a299e289366afde52effd4505deeba64415776","cni.projectcalico.org/podIP":"10.100.148.202/32","cni.projectcalico.org/podIPs":"10.100.148.202/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"8899db45-6bcb-49d2-b36c-c35076ae5090","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:55Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8899db45-6bcb-49d2-b36c-c35076ae5090\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T12:00:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.148.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ml75z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ml75z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gitlab-1-26-36460-guscsyka22xa-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gitlab-1-26-36460-guscsyka22xa-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:57Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T12:00:55Z"}],"hostIP":"10.0.0.4","podIP":"10.100.148.202","podIPs":[{"ip":"10.100.148.202"}],"startTime":"2023-08-24T12:00:55Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T12:00:56Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f3f0d724e5b261be8c6a9aac5856dc203aaab0fe5188fa6302b4723e5fc17493","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:00:58.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-767" for this suite. 08/24/23 12:00:58.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:00:58.715
Aug 24 12:00:58.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 12:00:58.719
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:58.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:58.753
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:00:58.761
Aug 24 12:00:58.812: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e" in namespace "projected-6968" to be "Succeeded or Failed"
Aug 24 12:00:58.849: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.499938ms
Aug 24 12:01:00.855: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041912921s
Aug 24 12:01:02.854: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041165882s
STEP: Saw pod success 08/24/23 12:01:02.854
Aug 24 12:01:02.854: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e" satisfied condition "Succeeded or Failed"
Aug 24 12:01:02.857: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e container client-container: <nil>
STEP: delete the pod 08/24/23 12:01:02.864
Aug 24 12:01:02.883: INFO: Waiting for pod downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e to disappear
Aug 24 12:01:02.889: INFO: Pod downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:02.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6968" for this suite. 08/24/23 12:01:02.893
------------------------------
â€¢ [4.193 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:00:58.715
    Aug 24 12:00:58.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 12:00:58.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:00:58.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:00:58.753
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:00:58.761
    Aug 24 12:00:58.812: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e" in namespace "projected-6968" to be "Succeeded or Failed"
    Aug 24 12:00:58.849: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.499938ms
    Aug 24 12:01:00.855: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041912921s
    Aug 24 12:01:02.854: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041165882s
    STEP: Saw pod success 08/24/23 12:01:02.854
    Aug 24 12:01:02.854: INFO: Pod "downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e" satisfied condition "Succeeded or Failed"
    Aug 24 12:01:02.857: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e container client-container: <nil>
    STEP: delete the pod 08/24/23 12:01:02.864
    Aug 24 12:01:02.883: INFO: Waiting for pod downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e to disappear
    Aug 24 12:01:02.889: INFO: Pod downwardapi-volume-6581b6c8-4558-4fe3-88fd-e2b48291623e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:02.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6968" for this suite. 08/24/23 12:01:02.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:02.909
Aug 24 12:01:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:01:02.91
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:02.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:02.946
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-f1d56545-0c57-4b1e-bcbf-2cbec782b343 08/24/23 12:01:02.953
STEP: Creating a pod to test consume configMaps 08/24/23 12:01:02.961
Aug 24 12:01:02.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735" in namespace "configmap-5390" to be "Succeeded or Failed"
Aug 24 12:01:02.981: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701309ms
Aug 24 12:01:04.986: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011578699s
Aug 24 12:01:06.987: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011942594s
STEP: Saw pod success 08/24/23 12:01:06.987
Aug 24 12:01:06.987: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735" satisfied condition "Succeeded or Failed"
Aug 24 12:01:06.991: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:01:07.005
Aug 24 12:01:07.041: INFO: Waiting for pod pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735 to disappear
Aug 24 12:01:07.048: INFO: Pod pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:07.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5390" for this suite. 08/24/23 12:01:07.053
------------------------------
â€¢ [4.159 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:02.909
    Aug 24 12:01:02.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:01:02.91
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:02.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:02.946
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-f1d56545-0c57-4b1e-bcbf-2cbec782b343 08/24/23 12:01:02.953
    STEP: Creating a pod to test consume configMaps 08/24/23 12:01:02.961
    Aug 24 12:01:02.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735" in namespace "configmap-5390" to be "Succeeded or Failed"
    Aug 24 12:01:02.981: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701309ms
    Aug 24 12:01:04.986: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011578699s
    Aug 24 12:01:06.987: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011942594s
    STEP: Saw pod success 08/24/23 12:01:06.987
    Aug 24 12:01:06.987: INFO: Pod "pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735" satisfied condition "Succeeded or Failed"
    Aug 24 12:01:06.991: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:01:07.005
    Aug 24 12:01:07.041: INFO: Waiting for pod pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735 to disappear
    Aug 24 12:01:07.048: INFO: Pod pod-configmaps-1db62331-1ad0-4850-a034-376d9d293735 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:07.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5390" for this suite. 08/24/23 12:01:07.053
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:07.068
Aug 24 12:01:07.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:01:07.069
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:07.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:07.103
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:01:07.11
Aug 24 12:01:07.128: INFO: Waiting up to 5m0s for pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc" in namespace "emptydir-5458" to be "Succeeded or Failed"
Aug 24 12:01:07.131: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724831ms
Aug 24 12:01:09.143: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015445674s
Aug 24 12:01:11.137: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008837418s
STEP: Saw pod success 08/24/23 12:01:11.137
Aug 24 12:01:11.137: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc" satisfied condition "Succeeded or Failed"
Aug 24 12:01:11.140: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-bd5a0094-16a4-4640-b806-bf87c5113fbc container test-container: <nil>
STEP: delete the pod 08/24/23 12:01:11.147
Aug 24 12:01:11.168: INFO: Waiting for pod pod-bd5a0094-16a4-4640-b806-bf87c5113fbc to disappear
Aug 24 12:01:11.174: INFO: Pod pod-bd5a0094-16a4-4640-b806-bf87c5113fbc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:11.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5458" for this suite. 08/24/23 12:01:11.179
------------------------------
â€¢ [4.124 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:07.068
    Aug 24 12:01:07.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:01:07.069
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:07.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:07.103
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:01:07.11
    Aug 24 12:01:07.128: INFO: Waiting up to 5m0s for pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc" in namespace "emptydir-5458" to be "Succeeded or Failed"
    Aug 24 12:01:07.131: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724831ms
    Aug 24 12:01:09.143: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015445674s
    Aug 24 12:01:11.137: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008837418s
    STEP: Saw pod success 08/24/23 12:01:11.137
    Aug 24 12:01:11.137: INFO: Pod "pod-bd5a0094-16a4-4640-b806-bf87c5113fbc" satisfied condition "Succeeded or Failed"
    Aug 24 12:01:11.140: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-bd5a0094-16a4-4640-b806-bf87c5113fbc container test-container: <nil>
    STEP: delete the pod 08/24/23 12:01:11.147
    Aug 24 12:01:11.168: INFO: Waiting for pod pod-bd5a0094-16a4-4640-b806-bf87c5113fbc to disappear
    Aug 24 12:01:11.174: INFO: Pod pod-bd5a0094-16a4-4640-b806-bf87c5113fbc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:11.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5458" for this suite. 08/24/23 12:01:11.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:11.195
Aug 24 12:01:11.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 12:01:11.196
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:11.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:11.228
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:01:11.235
Aug 24 12:01:11.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a" in namespace "projected-7203" to be "Succeeded or Failed"
Aug 24 12:01:11.261: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.441267ms
Aug 24 12:01:13.266: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018088133s
Aug 24 12:01:15.266: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018438378s
STEP: Saw pod success 08/24/23 12:01:15.266
Aug 24 12:01:15.266: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a" satisfied condition "Succeeded or Failed"
Aug 24 12:01:15.273: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a container client-container: <nil>
STEP: delete the pod 08/24/23 12:01:15.281
Aug 24 12:01:15.318: INFO: Waiting for pod downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a to disappear
Aug 24 12:01:15.323: INFO: Pod downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:01:15.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7203" for this suite. 08/24/23 12:01:15.329
------------------------------
â€¢ [4.146 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:11.195
    Aug 24 12:01:11.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 12:01:11.196
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:11.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:11.228
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:01:11.235
    Aug 24 12:01:11.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a" in namespace "projected-7203" to be "Succeeded or Failed"
    Aug 24 12:01:11.261: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.441267ms
    Aug 24 12:01:13.266: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018088133s
    Aug 24 12:01:15.266: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018438378s
    STEP: Saw pod success 08/24/23 12:01:15.266
    Aug 24 12:01:15.266: INFO: Pod "downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a" satisfied condition "Succeeded or Failed"
    Aug 24 12:01:15.273: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a container client-container: <nil>
    STEP: delete the pod 08/24/23 12:01:15.281
    Aug 24 12:01:15.318: INFO: Waiting for pod downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a to disappear
    Aug 24 12:01:15.323: INFO: Pod downwardapi-volume-7dc9bd9d-a5d0-4520-9c33-48aa0540762a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:01:15.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7203" for this suite. 08/24/23 12:01:15.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:01:15.343
Aug 24 12:01:15.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename cronjob 08/24/23 12:01:15.344
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:15.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:15.381
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/24/23 12:01:15.387
STEP: Ensuring a job is scheduled 08/24/23 12:01:15.396
STEP: Ensuring exactly one is scheduled 08/24/23 12:02:01.4
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 12:02:01.402
STEP: Ensuring the job is replaced with a new one 08/24/23 12:02:01.407
STEP: Removing cronjob 08/24/23 12:03:01.411
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:01.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2972" for this suite. 08/24/23 12:03:01.429
------------------------------
â€¢ [SLOW TEST] [106.117 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:01:15.343
    Aug 24 12:01:15.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename cronjob 08/24/23 12:01:15.344
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:01:15.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:01:15.381
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/24/23 12:01:15.387
    STEP: Ensuring a job is scheduled 08/24/23 12:01:15.396
    STEP: Ensuring exactly one is scheduled 08/24/23 12:02:01.4
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/24/23 12:02:01.402
    STEP: Ensuring the job is replaced with a new one 08/24/23 12:02:01.407
    STEP: Removing cronjob 08/24/23 12:03:01.411
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:01.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2972" for this suite. 08/24/23 12:03:01.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:01.465
Aug 24 12:03:01.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename ephemeral-containers-test 08/24/23 12:03:01.466
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:01.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:01.56
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/24/23 12:03:01.567
Aug 24 12:03:01.592: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2928" to be "running and ready"
Aug 24 12:03:01.602: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.817542ms
Aug 24 12:03:01.602: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:03:03.606: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014246447s
Aug 24 12:03:03.606: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 24 12:03:03.606: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/24/23 12:03:03.61
Aug 24 12:03:03.631: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2928" to be "container debugger running"
Aug 24 12:03:03.634: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.731258ms
Aug 24 12:03:05.640: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008741173s
Aug 24 12:03:07.640: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009126406s
Aug 24 12:03:07.640: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/24/23 12:03:07.64
Aug 24 12:03:07.640: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2928 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:03:07.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:03:07.641: INFO: ExecWithOptions: Clientset creation
Aug 24 12:03:07.641: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-2928/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 24 12:03:07.791: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:07.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2928" for this suite. 08/24/23 12:03:07.856
------------------------------
â€¢ [SLOW TEST] [6.401 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:01.465
    Aug 24 12:03:01.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/24/23 12:03:01.466
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:01.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:01.56
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/24/23 12:03:01.567
    Aug 24 12:03:01.592: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2928" to be "running and ready"
    Aug 24 12:03:01.602: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.817542ms
    Aug 24 12:03:01.602: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:03:03.606: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014246447s
    Aug 24 12:03:03.606: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 24 12:03:03.606: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/24/23 12:03:03.61
    Aug 24 12:03:03.631: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2928" to be "container debugger running"
    Aug 24 12:03:03.634: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.731258ms
    Aug 24 12:03:05.640: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008741173s
    Aug 24 12:03:07.640: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009126406s
    Aug 24 12:03:07.640: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/24/23 12:03:07.64
    Aug 24 12:03:07.640: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2928 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:03:07.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:03:07.641: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:03:07.641: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/ephemeral-containers-test-2928/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 24 12:03:07.791: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:07.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2928" for this suite. 08/24/23 12:03:07.856
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:07.866
Aug 24 12:03:07.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename watch 08/24/23 12:03:07.867
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:07.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:07.892
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/24/23 12:03:07.899
STEP: creating a new configmap 08/24/23 12:03:07.901
STEP: modifying the configmap once 08/24/23 12:03:07.909
STEP: closing the watch once it receives two notifications 08/24/23 12:03:07.922
Aug 24 12:03:07.922: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37753 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:03:07.922: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37754 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/24/23 12:03:07.922
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/24/23 12:03:07.932
STEP: deleting the configmap 08/24/23 12:03:07.934
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/24/23 12:03:07.952
Aug 24 12:03:07.953: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37755 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:03:07.953: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37756 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:03:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6196" for this suite. 08/24/23 12:03:07.957
------------------------------
â€¢ [0.110 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:07.866
    Aug 24 12:03:07.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename watch 08/24/23 12:03:07.867
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:07.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:07.892
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/24/23 12:03:07.899
    STEP: creating a new configmap 08/24/23 12:03:07.901
    STEP: modifying the configmap once 08/24/23 12:03:07.909
    STEP: closing the watch once it receives two notifications 08/24/23 12:03:07.922
    Aug 24 12:03:07.922: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37753 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:03:07.922: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37754 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/24/23 12:03:07.922
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/24/23 12:03:07.932
    STEP: deleting the configmap 08/24/23 12:03:07.934
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/24/23 12:03:07.952
    Aug 24 12:03:07.953: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37755 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:03:07.953: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6196  12fc7380-bd06-4764-873c-7d55eea64a1c 37756 0 2023-08-24 12:03:07 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 12:03:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:03:07.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6196" for this suite. 08/24/23 12:03:07.957
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:03:07.977
Aug 24 12:03:07.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-probe 08/24/23 12:03:07.978
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:08.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:08.026
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627 in namespace container-probe-8023 08/24/23 12:03:08.034
Aug 24 12:03:08.058: INFO: Waiting up to 5m0s for pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627" in namespace "container-probe-8023" to be "not pending"
Aug 24 12:03:08.076: INFO: Pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627": Phase="Pending", Reason="", readiness=false. Elapsed: 18.261921ms
Aug 24 12:03:10.081: INFO: Pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627": Phase="Running", Reason="", readiness=true. Elapsed: 2.023269822s
Aug 24 12:03:10.081: INFO: Pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627" satisfied condition "not pending"
Aug 24 12:03:10.082: INFO: Started pod busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627 in namespace container-probe-8023
STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 12:03:10.082
Aug 24 12:03:10.085: INFO: Initial restart count of pod busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627 is 0
STEP: deleting the pod 08/24/23 12:07:10.732
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:10.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8023" for this suite. 08/24/23 12:07:10.774
------------------------------
â€¢ [SLOW TEST] [242.818 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:03:07.977
    Aug 24 12:03:07.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-probe 08/24/23 12:03:07.978
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:03:08.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:03:08.026
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627 in namespace container-probe-8023 08/24/23 12:03:08.034
    Aug 24 12:03:08.058: INFO: Waiting up to 5m0s for pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627" in namespace "container-probe-8023" to be "not pending"
    Aug 24 12:03:08.076: INFO: Pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627": Phase="Pending", Reason="", readiness=false. Elapsed: 18.261921ms
    Aug 24 12:03:10.081: INFO: Pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627": Phase="Running", Reason="", readiness=true. Elapsed: 2.023269822s
    Aug 24 12:03:10.081: INFO: Pod "busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627" satisfied condition "not pending"
    Aug 24 12:03:10.082: INFO: Started pod busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627 in namespace container-probe-8023
    STEP: checking the pod's current state and verifying that restartCount is present 08/24/23 12:03:10.082
    Aug 24 12:03:10.085: INFO: Initial restart count of pod busybox-da309ed6-a3b8-44aa-bb8a-2f210778d627 is 0
    STEP: deleting the pod 08/24/23 12:07:10.732
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:10.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8023" for this suite. 08/24/23 12:07:10.774
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:10.795
Aug 24 12:07:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename security-context-test 08/24/23 12:07:10.797
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:10.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:10.829
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Aug 24 12:07:10.851: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a" in namespace "security-context-test-7886" to be "Succeeded or Failed"
Aug 24 12:07:10.855: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139404ms
Aug 24 12:07:12.871: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020570727s
Aug 24 12:07:14.859: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008059695s
Aug 24 12:07:14.859: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:14.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7886" for this suite. 08/24/23 12:07:14.866
------------------------------
â€¢ [4.088 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:10.795
    Aug 24 12:07:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename security-context-test 08/24/23 12:07:10.797
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:10.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:10.829
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Aug 24 12:07:10.851: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a" in namespace "security-context-test-7886" to be "Succeeded or Failed"
    Aug 24 12:07:10.855: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139404ms
    Aug 24 12:07:12.871: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020570727s
    Aug 24 12:07:14.859: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008059695s
    Aug 24 12:07:14.859: INFO: Pod "busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:14.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7886" for this suite. 08/24/23 12:07:14.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:14.885
Aug 24 12:07:14.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-pred 08/24/23 12:07:14.886
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:14.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:14.922
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 12:07:14.931: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 12:07:14.941: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 12:07:14.946: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
Aug 24 12:07:14.965: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 12:07:14.966: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 12:07:14.966: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 12:07:14.966: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container autoscaler ready: true, restart count 0
Aug 24 12:07:14.966: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container grafana ready: true, restart count 0
Aug 24 12:07:14.966: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Aug 24 12:07:14.966: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Aug 24 12:07:14.966: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Aug 24 12:07:14.966: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 24 12:07:14.966: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container metrics-server ready: true, restart count 0
Aug 24 12:07:14.966: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 12:07:14.966: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 12:07:14.966: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container config-reloader ready: true, restart count 0
Aug 24 12:07:14.966: INFO: 	Container prometheus ready: true, restart count 0
Aug 24 12:07:14.966: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:07:14.966: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:07:14.966: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container busybox ready: true, restart count 1
Aug 24 12:07:14.966: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container nginx ready: true, restart count 0
Aug 24 12:07:14.966: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.966: INFO: 	Container nginx ready: true, restart count 0
Aug 24 12:07:14.966: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
Aug 24 12:07:14.976: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.976: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 12:07:14.976: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.976: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 12:07:14.976: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 12:07:14.976: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.976: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 12:07:14.976: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.976: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 12:07:14.976: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.976: INFO: 	Container e2e ready: true, restart count 0
Aug 24 12:07:14.976: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:07:14.976: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.976: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:07:14.976: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:07:14.976: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
Aug 24 12:07:14.985: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.985: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 12:07:14.985: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.985: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 12:07:14.985: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 12:07:14.985: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.985: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 12:07:14.985: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.985: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 12:07:14.985: INFO: busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a from security-context-test-7886 started at 2023-08-24 12:07:10 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.985: INFO: 	Container busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a ready: false, restart count 0
Aug 24 12:07:14.985: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
Aug 24 12:07:14.985: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 12:07:14.985: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:07:14.985: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:07:14.985: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node gitlab-1-26-36460-guscsyka22xa-node-0 08/24/23 12:07:15.021
STEP: verifying the node has the label node gitlab-1-26-36460-guscsyka22xa-node-1 08/24/23 12:07:15.049
STEP: verifying the node has the label node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 12:07:15.073
Aug 24 12:07:15.096: INFO: Pod calico-node-j7kf4 requesting resource cpu=250m on Node gitlab-1-26-36460-guscsyka22xa-node-1
Aug 24 12:07:15.096: INFO: Pod calico-node-mvq8r requesting resource cpu=250m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.096: INFO: Pod calico-node-n4d54 requesting resource cpu=250m on Node gitlab-1-26-36460-guscsyka22xa-node-2
Aug 24 12:07:15.096: INFO: Pod csi-cinder-nodeplugin-95jsr requesting resource cpu=50m on Node gitlab-1-26-36460-guscsyka22xa-node-1
Aug 24 12:07:15.096: INFO: Pod csi-cinder-nodeplugin-vd9c7 requesting resource cpu=50m on Node gitlab-1-26-36460-guscsyka22xa-node-2
Aug 24 12:07:15.096: INFO: Pod csi-cinder-nodeplugin-xjfv7 requesting resource cpu=50m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.096: INFO: Pod kube-dns-autoscaler-5f4bb48647-vmmzg requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.096: INFO: Pod magnum-grafana-867fcd9667-cr4gs requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.097: INFO: Pod magnum-kube-prometheus-sta-operator-66b867f676-jjxrh requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.097: INFO: Pod magnum-kube-state-metrics-79d5d4dd8f-mbg6t requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.097: INFO: Pod magnum-metrics-server-5d9f484f5-f9v5m requesting resource cpu=100m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.097: INFO: Pod magnum-prometheus-node-exporter-hmfcv requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-2
Aug 24 12:07:15.097: INFO: Pod magnum-prometheus-node-exporter-mjbts requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.097: INFO: Pod magnum-prometheus-node-exporter-rl449 requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-1
Aug 24 12:07:15.097: INFO: Pod npd-btqvw requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.097: INFO: Pod npd-j8d7p requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-2
Aug 24 12:07:15.097: INFO: Pod npd-msw52 requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-1
Aug 24 12:07:15.097: INFO: Pod prometheus-magnum-kube-prometheus-sta-prometheus-0 requesting resource cpu=342m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.097: INFO: Pod sonobuoy requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-2
Aug 24 12:07:15.097: INFO: Pod sonobuoy-e2e-job-232e54a70299452d requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-1
Aug 24 12:07:15.098: INFO: Pod sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-2
Aug 24 12:07:15.098: INFO: Pod sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.098: INFO: Pod sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-1
Aug 24 12:07:15.098: INFO: Pod busybox requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.098: INFO: Pod nginx-7f456874f4-992ld requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.098: INFO: Pod nginx-7f456874f4-b57r9 requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
STEP: Starting Pods to consume most of the cluster CPU. 08/24/23 12:07:15.098
Aug 24 12:07:15.098: INFO: Creating a pod which consumes cpu=2252m on Node gitlab-1-26-36460-guscsyka22xa-node-0
Aug 24 12:07:15.113: INFO: Creating a pod which consumes cpu=2576m on Node gitlab-1-26-36460-guscsyka22xa-node-1
Aug 24 12:07:15.132: INFO: Creating a pod which consumes cpu=2576m on Node gitlab-1-26-36460-guscsyka22xa-node-2
Aug 24 12:07:15.156: INFO: Waiting up to 5m0s for pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3" in namespace "sched-pred-783" to be "running"
Aug 24 12:07:15.183: INFO: Pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 27.170507ms
Aug 24 12:07:17.189: INFO: Pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.03236751s
Aug 24 12:07:17.189: INFO: Pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3" satisfied condition "running"
Aug 24 12:07:17.189: INFO: Waiting up to 5m0s for pod "filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792" in namespace "sched-pred-783" to be "running"
Aug 24 12:07:17.193: INFO: Pod "filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792": Phase="Running", Reason="", readiness=true. Elapsed: 4.367876ms
Aug 24 12:07:17.193: INFO: Pod "filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792" satisfied condition "running"
Aug 24 12:07:17.193: INFO: Waiting up to 5m0s for pod "filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73" in namespace "sched-pred-783" to be "running"
Aug 24 12:07:17.197: INFO: Pod "filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73": Phase="Running", Reason="", readiness=true. Elapsed: 3.72959ms
Aug 24 12:07:17.197: INFO: Pod "filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/24/23 12:07:17.197
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e5054cfca3a86], Reason = [Scheduled], Message = [Successfully assigned sched-pred-783/filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3 to gitlab-1-26-36460-guscsyka22xa-node-0] 08/24/23 12:07:17.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e505504a3543b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:07:17.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e50550770bf43], Reason = [Created], Message = [Created container filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e50550efb8fca], Reason = [Started], Message = [Started container filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e5054d04d9028], Reason = [Scheduled], Message = [Successfully assigned sched-pred-783/filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792 to gitlab-1-26-36460-guscsyka22xa-node-1] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e50550e750198], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e505510e4de7c], Reason = [Created], Message = [Created container filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e505518be396c], Reason = [Started], Message = [Started container filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e5054d3325099], Reason = [Scheduled], Message = [Successfully assigned sched-pred-783/filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73 to gitlab-1-26-36460-guscsyka22xa-node-2] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e505507cac673], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e50550af5bf9b], Reason = [Created], Message = [Created container filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e505511c2c8a5], Reason = [Started], Message = [Started container filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73] 08/24/23 12:07:17.202
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177e50554c074cdd], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 08/24/23 12:07:17.227
STEP: removing the label node off the node gitlab-1-26-36460-guscsyka22xa-node-0 08/24/23 12:07:18.226
STEP: verifying the node doesn't have the label node 08/24/23 12:07:18.246
STEP: removing the label node off the node gitlab-1-26-36460-guscsyka22xa-node-1 08/24/23 12:07:18.253
STEP: verifying the node doesn't have the label node 08/24/23 12:07:18.273
STEP: removing the label node off the node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 12:07:18.28
STEP: verifying the node doesn't have the label node 08/24/23 12:07:18.303
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:18.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-783" for this suite. 08/24/23 12:07:18.312
------------------------------
â€¢ [3.444 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:14.885
    Aug 24 12:07:14.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-pred 08/24/23 12:07:14.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:14.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:14.922
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 12:07:14.931: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 12:07:14.941: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 12:07:14.946: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
    Aug 24 12:07:14.965: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container grafana ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: 	Container prometheus ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container busybox ready: true, restart count 1
    Aug 24 12:07:14.966: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.966: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 12:07:14.966: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
    Aug 24 12:07:14.976: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.976: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.976: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.976: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.976: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.976: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.976: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:07:14.976: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
    Aug 24 12:07:14.985: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.985: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 12:07:14.985: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.985: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 12:07:14.985: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 12:07:14.985: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.985: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 12:07:14.985: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.985: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 12:07:14.985: INFO: busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a from security-context-test-7886 started at 2023-08-24 12:07:10 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.985: INFO: 	Container busybox-readonly-false-e9b4ff93-053a-4b09-8560-61262047d08a ready: false, restart count 0
    Aug 24 12:07:14.985: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
    Aug 24 12:07:14.985: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 12:07:14.985: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:07:14.985: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:07:14.985: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node gitlab-1-26-36460-guscsyka22xa-node-0 08/24/23 12:07:15.021
    STEP: verifying the node has the label node gitlab-1-26-36460-guscsyka22xa-node-1 08/24/23 12:07:15.049
    STEP: verifying the node has the label node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 12:07:15.073
    Aug 24 12:07:15.096: INFO: Pod calico-node-j7kf4 requesting resource cpu=250m on Node gitlab-1-26-36460-guscsyka22xa-node-1
    Aug 24 12:07:15.096: INFO: Pod calico-node-mvq8r requesting resource cpu=250m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.096: INFO: Pod calico-node-n4d54 requesting resource cpu=250m on Node gitlab-1-26-36460-guscsyka22xa-node-2
    Aug 24 12:07:15.096: INFO: Pod csi-cinder-nodeplugin-95jsr requesting resource cpu=50m on Node gitlab-1-26-36460-guscsyka22xa-node-1
    Aug 24 12:07:15.096: INFO: Pod csi-cinder-nodeplugin-vd9c7 requesting resource cpu=50m on Node gitlab-1-26-36460-guscsyka22xa-node-2
    Aug 24 12:07:15.096: INFO: Pod csi-cinder-nodeplugin-xjfv7 requesting resource cpu=50m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.096: INFO: Pod kube-dns-autoscaler-5f4bb48647-vmmzg requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.096: INFO: Pod magnum-grafana-867fcd9667-cr4gs requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.097: INFO: Pod magnum-kube-prometheus-sta-operator-66b867f676-jjxrh requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.097: INFO: Pod magnum-kube-state-metrics-79d5d4dd8f-mbg6t requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.097: INFO: Pod magnum-metrics-server-5d9f484f5-f9v5m requesting resource cpu=100m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.097: INFO: Pod magnum-prometheus-node-exporter-hmfcv requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-2
    Aug 24 12:07:15.097: INFO: Pod magnum-prometheus-node-exporter-mjbts requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.097: INFO: Pod magnum-prometheus-node-exporter-rl449 requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-1
    Aug 24 12:07:15.097: INFO: Pod npd-btqvw requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.097: INFO: Pod npd-j8d7p requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-2
    Aug 24 12:07:15.097: INFO: Pod npd-msw52 requesting resource cpu=20m on Node gitlab-1-26-36460-guscsyka22xa-node-1
    Aug 24 12:07:15.097: INFO: Pod prometheus-magnum-kube-prometheus-sta-prometheus-0 requesting resource cpu=342m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.097: INFO: Pod sonobuoy requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-2
    Aug 24 12:07:15.097: INFO: Pod sonobuoy-e2e-job-232e54a70299452d requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-1
    Aug 24 12:07:15.098: INFO: Pod sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-2
    Aug 24 12:07:15.098: INFO: Pod sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.098: INFO: Pod sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-1
    Aug 24 12:07:15.098: INFO: Pod busybox requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.098: INFO: Pod nginx-7f456874f4-992ld requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.098: INFO: Pod nginx-7f456874f4-b57r9 requesting resource cpu=0m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    STEP: Starting Pods to consume most of the cluster CPU. 08/24/23 12:07:15.098
    Aug 24 12:07:15.098: INFO: Creating a pod which consumes cpu=2252m on Node gitlab-1-26-36460-guscsyka22xa-node-0
    Aug 24 12:07:15.113: INFO: Creating a pod which consumes cpu=2576m on Node gitlab-1-26-36460-guscsyka22xa-node-1
    Aug 24 12:07:15.132: INFO: Creating a pod which consumes cpu=2576m on Node gitlab-1-26-36460-guscsyka22xa-node-2
    Aug 24 12:07:15.156: INFO: Waiting up to 5m0s for pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3" in namespace "sched-pred-783" to be "running"
    Aug 24 12:07:15.183: INFO: Pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 27.170507ms
    Aug 24 12:07:17.189: INFO: Pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.03236751s
    Aug 24 12:07:17.189: INFO: Pod "filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3" satisfied condition "running"
    Aug 24 12:07:17.189: INFO: Waiting up to 5m0s for pod "filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792" in namespace "sched-pred-783" to be "running"
    Aug 24 12:07:17.193: INFO: Pod "filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792": Phase="Running", Reason="", readiness=true. Elapsed: 4.367876ms
    Aug 24 12:07:17.193: INFO: Pod "filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792" satisfied condition "running"
    Aug 24 12:07:17.193: INFO: Waiting up to 5m0s for pod "filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73" in namespace "sched-pred-783" to be "running"
    Aug 24 12:07:17.197: INFO: Pod "filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73": Phase="Running", Reason="", readiness=true. Elapsed: 3.72959ms
    Aug 24 12:07:17.197: INFO: Pod "filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/24/23 12:07:17.197
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e5054cfca3a86], Reason = [Scheduled], Message = [Successfully assigned sched-pred-783/filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3 to gitlab-1-26-36460-guscsyka22xa-node-0] 08/24/23 12:07:17.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e505504a3543b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:07:17.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e50550770bf43], Reason = [Created], Message = [Created container filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3.177e50550efb8fca], Reason = [Started], Message = [Started container filler-pod-2e7b1f05-a468-4b11-be22-ddc2d19ea1a3] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e5054d04d9028], Reason = [Scheduled], Message = [Successfully assigned sched-pred-783/filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792 to gitlab-1-26-36460-guscsyka22xa-node-1] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e50550e750198], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e505510e4de7c], Reason = [Created], Message = [Created container filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792.177e505518be396c], Reason = [Started], Message = [Started container filler-pod-7aff0b64-0ead-4ea0-8060-989a5d9e4792] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e5054d3325099], Reason = [Scheduled], Message = [Successfully assigned sched-pred-783/filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73 to gitlab-1-26-36460-guscsyka22xa-node-2] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e505507cac673], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e50550af5bf9b], Reason = [Created], Message = [Created container filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73.177e505511c2c8a5], Reason = [Started], Message = [Started container filler-pod-dad60b82-a74e-4b93-b4d0-4a25a8b96e73] 08/24/23 12:07:17.202
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177e50554c074cdd], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..] 08/24/23 12:07:17.227
    STEP: removing the label node off the node gitlab-1-26-36460-guscsyka22xa-node-0 08/24/23 12:07:18.226
    STEP: verifying the node doesn't have the label node 08/24/23 12:07:18.246
    STEP: removing the label node off the node gitlab-1-26-36460-guscsyka22xa-node-1 08/24/23 12:07:18.253
    STEP: verifying the node doesn't have the label node 08/24/23 12:07:18.273
    STEP: removing the label node off the node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 12:07:18.28
    STEP: verifying the node doesn't have the label node 08/24/23 12:07:18.303
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:18.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-783" for this suite. 08/24/23 12:07:18.312
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:18.329
Aug 24 12:07:18.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 12:07:18.33
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:18.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:18.356
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/24/23 12:07:18.363
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8915.svc.cluster.local;sleep 1; done
 08/24/23 12:07:18.371
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local;sleep 1; done
 08/24/23 12:07:18.371
STEP: creating a pod to probe DNS 08/24/23 12:07:18.372
STEP: submitting the pod to kubernetes 08/24/23 12:07:18.372
Aug 24 12:07:18.400: INFO: Waiting up to 15m0s for pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96" in namespace "dns-8915" to be "running"
Aug 24 12:07:18.420: INFO: Pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96": Phase="Pending", Reason="", readiness=false. Elapsed: 19.309767ms
Aug 24 12:07:20.425: INFO: Pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96": Phase="Running", Reason="", readiness=true. Elapsed: 2.024533525s
Aug 24 12:07:20.425: INFO: Pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:07:20.425
STEP: looking for the results for each expected name from probers 08/24/23 12:07:20.43
Aug 24 12:07:20.439: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.443: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.446: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.449: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.453: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.457: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.461: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.465: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
Aug 24 12:07:20.465: INFO: Lookups using dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8915.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local jessie_udp@dns-test-service-2.dns-8915.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local]

Aug 24 12:07:25.493: INFO: DNS probes using dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96 succeeded

STEP: deleting the pod 08/24/23 12:07:25.494
STEP: deleting the test headless service 08/24/23 12:07:25.535
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:25.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8915" for this suite. 08/24/23 12:07:25.622
------------------------------
â€¢ [SLOW TEST] [7.322 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:18.329
    Aug 24 12:07:18.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 12:07:18.33
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:18.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:18.356
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/24/23 12:07:18.363
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8915.svc.cluster.local;sleep 1; done
     08/24/23 12:07:18.371
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local;sleep 1; done
     08/24/23 12:07:18.371
    STEP: creating a pod to probe DNS 08/24/23 12:07:18.372
    STEP: submitting the pod to kubernetes 08/24/23 12:07:18.372
    Aug 24 12:07:18.400: INFO: Waiting up to 15m0s for pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96" in namespace "dns-8915" to be "running"
    Aug 24 12:07:18.420: INFO: Pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96": Phase="Pending", Reason="", readiness=false. Elapsed: 19.309767ms
    Aug 24 12:07:20.425: INFO: Pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96": Phase="Running", Reason="", readiness=true. Elapsed: 2.024533525s
    Aug 24 12:07:20.425: INFO: Pod "dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:07:20.425
    STEP: looking for the results for each expected name from probers 08/24/23 12:07:20.43
    Aug 24 12:07:20.439: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.443: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.446: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.449: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.453: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.457: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.461: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.465: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96: the server could not find the requested resource (get pods dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96)
    Aug 24 12:07:20.465: INFO: Lookups using dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8915.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local jessie_udp@dns-test-service-2.dns-8915.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local]

    Aug 24 12:07:25.493: INFO: DNS probes using dns-8915/dns-test-10a7a17c-159c-445e-bda1-2f0d505b5f96 succeeded

    STEP: deleting the pod 08/24/23 12:07:25.494
    STEP: deleting the test headless service 08/24/23 12:07:25.535
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:25.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8915" for this suite. 08/24/23 12:07:25.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:25.658
Aug 24 12:07:25.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 12:07:25.66
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:25.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:25.692
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/24/23 12:07:25.699
Aug 24 12:07:25.709: INFO: Waiting up to 5m0s for pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87" in namespace "downward-api-412" to be "Succeeded or Failed"
Aug 24 12:07:25.716: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.349212ms
Aug 24 12:07:27.721: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011966257s
Aug 24 12:07:29.721: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011933338s
STEP: Saw pod success 08/24/23 12:07:29.721
Aug 24 12:07:29.721: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87" satisfied condition "Succeeded or Failed"
Aug 24 12:07:29.724: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87 container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:07:29.782
Aug 24 12:07:29.802: INFO: Waiting for pod downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87 to disappear
Aug 24 12:07:29.809: INFO: Pod downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:29.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-412" for this suite. 08/24/23 12:07:29.815
------------------------------
â€¢ [4.167 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:25.658
    Aug 24 12:07:25.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:07:25.66
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:25.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:25.692
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/24/23 12:07:25.699
    Aug 24 12:07:25.709: INFO: Waiting up to 5m0s for pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87" in namespace "downward-api-412" to be "Succeeded or Failed"
    Aug 24 12:07:25.716: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87": Phase="Pending", Reason="", readiness=false. Elapsed: 7.349212ms
    Aug 24 12:07:27.721: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011966257s
    Aug 24 12:07:29.721: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011933338s
    STEP: Saw pod success 08/24/23 12:07:29.721
    Aug 24 12:07:29.721: INFO: Pod "downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87" satisfied condition "Succeeded or Failed"
    Aug 24 12:07:29.724: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:07:29.782
    Aug 24 12:07:29.802: INFO: Waiting for pod downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87 to disappear
    Aug 24 12:07:29.809: INFO: Pod downward-api-40bfa2dc-aa74-40eb-9e22-7e979088bf87 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:29.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-412" for this suite. 08/24/23 12:07:29.815
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:29.826
Aug 24 12:07:29.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename podtemplate 08/24/23 12:07:29.827
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:29.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:29.857
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/24/23 12:07:29.864
Aug 24 12:07:29.873: INFO: created test-podtemplate-1
Aug 24 12:07:29.888: INFO: created test-podtemplate-2
Aug 24 12:07:29.896: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/24/23 12:07:29.897
STEP: delete collection of pod templates 08/24/23 12:07:29.902
Aug 24 12:07:29.902: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/24/23 12:07:29.919
Aug 24 12:07:29.919: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:29.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8834" for this suite. 08/24/23 12:07:29.928
------------------------------
â€¢ [0.113 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:29.826
    Aug 24 12:07:29.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename podtemplate 08/24/23 12:07:29.827
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:29.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:29.857
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/24/23 12:07:29.864
    Aug 24 12:07:29.873: INFO: created test-podtemplate-1
    Aug 24 12:07:29.888: INFO: created test-podtemplate-2
    Aug 24 12:07:29.896: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/24/23 12:07:29.897
    STEP: delete collection of pod templates 08/24/23 12:07:29.902
    Aug 24 12:07:29.902: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/24/23 12:07:29.919
    Aug 24 12:07:29.919: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:29.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8834" for this suite. 08/24/23 12:07:29.928
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:29.94
Aug 24 12:07:29.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:07:29.941
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:29.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:29.971
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/24/23 12:07:29.978
STEP: fetching the ConfigMap 08/24/23 12:07:29.987
STEP: patching the ConfigMap 08/24/23 12:07:29.995
STEP: listing all ConfigMaps in all namespaces with a label selector 08/24/23 12:07:30.003
STEP: deleting the ConfigMap by collection with a label selector 08/24/23 12:07:30.02
STEP: listing all ConfigMaps in test namespace 08/24/23 12:07:30.032
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:30.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4207" for this suite. 08/24/23 12:07:30.043
------------------------------
â€¢ [0.114 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:29.94
    Aug 24 12:07:29.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:07:29.941
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:29.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:29.971
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/24/23 12:07:29.978
    STEP: fetching the ConfigMap 08/24/23 12:07:29.987
    STEP: patching the ConfigMap 08/24/23 12:07:29.995
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/24/23 12:07:30.003
    STEP: deleting the ConfigMap by collection with a label selector 08/24/23 12:07:30.02
    STEP: listing all ConfigMaps in test namespace 08/24/23 12:07:30.032
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:30.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4207" for this suite. 08/24/23 12:07:30.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:30.056
Aug 24 12:07:30.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:07:30.057
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:30.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:30.134
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Aug 24 12:07:30.173: INFO: Create a RollingUpdate DaemonSet
Aug 24 12:07:30.182: INFO: Check that daemon pods launch on every node of the cluster
Aug 24 12:07:30.187: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:30.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:07:30.194: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:07:31.199: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:31.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:07:31.203: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:07:32.199: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:32.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:07:32.203: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 24 12:07:32.203: INFO: Update the DaemonSet to trigger a rollout
Aug 24 12:07:32.223: INFO: Updating DaemonSet daemon-set
Aug 24 12:07:35.260: INFO: Roll back the DaemonSet before rollout is complete
Aug 24 12:07:35.279: INFO: Updating DaemonSet daemon-set
Aug 24 12:07:35.279: INFO: Make sure DaemonSet rollback is complete
Aug 24 12:07:35.320: INFO: Wrong image for pod: daemon-set-9wcq4. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 24 12:07:35.320: INFO: Pod daemon-set-9wcq4 is not available
Aug 24 12:07:35.405: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:36.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:37.414: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:38.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:39.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:40.416: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:41.417: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:42.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:43.414: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:44.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:45.414: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:46.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:47.417: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:48.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:49.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:07:50.410: INFO: Pod daemon-set-gd786 is not available
Aug 24 12:07:50.418: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:07:50.427
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1297, will wait for the garbage collector to delete the pods 08/24/23 12:07:50.428
Aug 24 12:07:50.498: INFO: Deleting DaemonSet.extensions daemon-set took: 16.168123ms
Aug 24 12:07:50.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.285526ms
Aug 24 12:07:53.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:07:53.002: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 12:07:53.005: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38806"},"items":null}

Aug 24 12:07:53.008: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38806"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:07:53.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1297" for this suite. 08/24/23 12:07:53.03
------------------------------
â€¢ [SLOW TEST] [22.986 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:30.056
    Aug 24 12:07:30.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:07:30.057
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:30.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:30.134
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Aug 24 12:07:30.173: INFO: Create a RollingUpdate DaemonSet
    Aug 24 12:07:30.182: INFO: Check that daemon pods launch on every node of the cluster
    Aug 24 12:07:30.187: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:30.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:07:30.194: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:07:31.199: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:31.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:07:31.203: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:07:32.199: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:32.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:07:32.203: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Aug 24 12:07:32.203: INFO: Update the DaemonSet to trigger a rollout
    Aug 24 12:07:32.223: INFO: Updating DaemonSet daemon-set
    Aug 24 12:07:35.260: INFO: Roll back the DaemonSet before rollout is complete
    Aug 24 12:07:35.279: INFO: Updating DaemonSet daemon-set
    Aug 24 12:07:35.279: INFO: Make sure DaemonSet rollback is complete
    Aug 24 12:07:35.320: INFO: Wrong image for pod: daemon-set-9wcq4. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 24 12:07:35.320: INFO: Pod daemon-set-9wcq4 is not available
    Aug 24 12:07:35.405: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:36.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:37.414: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:38.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:39.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:40.416: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:41.417: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:42.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:43.414: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:44.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:45.414: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:46.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:47.417: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:48.413: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:49.412: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:07:50.410: INFO: Pod daemon-set-gd786 is not available
    Aug 24 12:07:50.418: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:07:50.427
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1297, will wait for the garbage collector to delete the pods 08/24/23 12:07:50.428
    Aug 24 12:07:50.498: INFO: Deleting DaemonSet.extensions daemon-set took: 16.168123ms
    Aug 24 12:07:50.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.285526ms
    Aug 24 12:07:53.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:07:53.002: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 12:07:53.005: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38806"},"items":null}

    Aug 24 12:07:53.008: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38806"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:07:53.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1297" for this suite. 08/24/23 12:07:53.03
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:07:53.045
Aug 24 12:07:53.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:07:53.047
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:53.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:53.081
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:07:53.095
Aug 24 12:07:53.113: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9704" to be "running and ready"
Aug 24 12:07:53.125: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.356261ms
Aug 24 12:07:53.125: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:07:55.129: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01606558s
Aug 24 12:07:55.129: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 12:07:55.129: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/24/23 12:07:55.132
Aug 24 12:07:55.142: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9704" to be "running and ready"
Aug 24 12:07:55.146: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.621562ms
Aug 24 12:07:55.146: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:07:57.152: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01040826s
Aug 24 12:07:57.152: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 24 12:07:57.152: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/24/23 12:07:57.156
STEP: delete the pod with lifecycle hook 08/24/23 12:07:57.164
Aug 24 12:07:57.180: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 24 12:07:57.187: INFO: Pod pod-with-poststart-http-hook still exists
Aug 24 12:07:59.189: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 24 12:07:59.193: INFO: Pod pod-with-poststart-http-hook still exists
Aug 24 12:08:01.188: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 24 12:08:01.193: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9704" for this suite. 08/24/23 12:08:01.199
------------------------------
â€¢ [SLOW TEST] [8.163 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:07:53.045
    Aug 24 12:07:53.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:07:53.047
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:07:53.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:07:53.081
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:07:53.095
    Aug 24 12:07:53.113: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9704" to be "running and ready"
    Aug 24 12:07:53.125: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.356261ms
    Aug 24 12:07:53.125: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:07:55.129: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.01606558s
    Aug 24 12:07:55.129: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 12:07:55.129: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/24/23 12:07:55.132
    Aug 24 12:07:55.142: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9704" to be "running and ready"
    Aug 24 12:07:55.146: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.621562ms
    Aug 24 12:07:55.146: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:07:57.152: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01040826s
    Aug 24 12:07:57.152: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 24 12:07:57.152: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/24/23 12:07:57.156
    STEP: delete the pod with lifecycle hook 08/24/23 12:07:57.164
    Aug 24 12:07:57.180: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 24 12:07:57.187: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 24 12:07:59.189: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 24 12:07:59.193: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 24 12:08:01.188: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 24 12:08:01.193: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9704" for this suite. 08/24/23 12:08:01.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:01.214
Aug 24 12:08:01.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:08:01.216
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:01.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:01.246
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 12:08:01.252
Aug 24 12:08:01.267: INFO: Waiting up to 5m0s for pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42" in namespace "emptydir-7063" to be "Succeeded or Failed"
Aug 24 12:08:01.284: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42": Phase="Pending", Reason="", readiness=false. Elapsed: 16.834584ms
Aug 24 12:08:03.292: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024357054s
Aug 24 12:08:05.289: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021683362s
STEP: Saw pod success 08/24/23 12:08:05.289
Aug 24 12:08:05.290: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42" satisfied condition "Succeeded or Failed"
Aug 24 12:08:05.294: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-4b6605fe-1076-43c2-8af1-fff438646a42 container test-container: <nil>
STEP: delete the pod 08/24/23 12:08:05.361
Aug 24 12:08:05.382: INFO: Waiting for pod pod-4b6605fe-1076-43c2-8af1-fff438646a42 to disappear
Aug 24 12:08:05.386: INFO: Pod pod-4b6605fe-1076-43c2-8af1-fff438646a42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:05.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7063" for this suite. 08/24/23 12:08:05.392
------------------------------
â€¢ [4.189 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:01.214
    Aug 24 12:08:01.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:08:01.216
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:01.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:01.246
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/24/23 12:08:01.252
    Aug 24 12:08:01.267: INFO: Waiting up to 5m0s for pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42" in namespace "emptydir-7063" to be "Succeeded or Failed"
    Aug 24 12:08:01.284: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42": Phase="Pending", Reason="", readiness=false. Elapsed: 16.834584ms
    Aug 24 12:08:03.292: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024357054s
    Aug 24 12:08:05.289: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021683362s
    STEP: Saw pod success 08/24/23 12:08:05.289
    Aug 24 12:08:05.290: INFO: Pod "pod-4b6605fe-1076-43c2-8af1-fff438646a42" satisfied condition "Succeeded or Failed"
    Aug 24 12:08:05.294: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-4b6605fe-1076-43c2-8af1-fff438646a42 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:08:05.361
    Aug 24 12:08:05.382: INFO: Waiting for pod pod-4b6605fe-1076-43c2-8af1-fff438646a42 to disappear
    Aug 24 12:08:05.386: INFO: Pod pod-4b6605fe-1076-43c2-8af1-fff438646a42 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:05.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7063" for this suite. 08/24/23 12:08:05.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:05.407
Aug 24 12:08:05.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename init-container 08/24/23 12:08:05.409
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:05.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:05.439
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/24/23 12:08:05.445
Aug 24 12:08:05.446: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:10.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1031" for this suite. 08/24/23 12:08:10.631
------------------------------
â€¢ [SLOW TEST] [5.237 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:05.407
    Aug 24 12:08:05.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename init-container 08/24/23 12:08:05.409
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:05.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:05.439
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/24/23 12:08:05.445
    Aug 24 12:08:05.446: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:10.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1031" for this suite. 08/24/23 12:08:10.631
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:10.645
Aug 24 12:08:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 12:08:10.647
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:10.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:10.683
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-563 08/24/23 12:08:10.691
STEP: creating service affinity-clusterip in namespace services-563 08/24/23 12:08:10.691
STEP: creating replication controller affinity-clusterip in namespace services-563 08/24/23 12:08:10.72
I0824 12:08:10.753417      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-563, replica count: 3
I0824 12:08:13.809398      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:08:13.821: INFO: Creating new exec pod
Aug 24 12:08:13.835: INFO: Waiting up to 5m0s for pod "execpod-affinitywqndb" in namespace "services-563" to be "running"
Aug 24 12:08:13.857: INFO: Pod "execpod-affinitywqndb": Phase="Pending", Reason="", readiness=false. Elapsed: 21.153028ms
Aug 24 12:08:15.871: INFO: Pod "execpod-affinitywqndb": Phase="Running", Reason="", readiness=true. Elapsed: 2.035438331s
Aug 24 12:08:15.871: INFO: Pod "execpod-affinitywqndb" satisfied condition "running"
Aug 24 12:08:16.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-563 exec execpod-affinitywqndb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 24 12:08:17.148: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 24 12:08:17.148: INFO: stdout: ""
Aug 24 12:08:17.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-563 exec execpod-affinitywqndb -- /bin/sh -x -c nc -v -z -w 2 10.254.191.208 80'
Aug 24 12:08:17.410: INFO: stderr: "+ nc -v -z -w 2 10.254.191.208 80\nConnection to 10.254.191.208 80 port [tcp/http] succeeded!\n"
Aug 24 12:08:17.410: INFO: stdout: ""
Aug 24 12:08:17.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-563 exec execpod-affinitywqndb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.191.208:80/ ; done'
Aug 24 12:08:17.751: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n"
Aug 24 12:08:17.751: INFO: stdout: "\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962"
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
Aug 24 12:08:17.751: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-563, will wait for the garbage collector to delete the pods 08/24/23 12:08:17.776
Aug 24 12:08:17.852: INFO: Deleting ReplicationController affinity-clusterip took: 11.85608ms
Aug 24 12:08:17.953: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.296456ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:20.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-563" for this suite. 08/24/23 12:08:20.199
------------------------------
â€¢ [SLOW TEST] [9.566 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:10.645
    Aug 24 12:08:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 12:08:10.647
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:10.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:10.683
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-563 08/24/23 12:08:10.691
    STEP: creating service affinity-clusterip in namespace services-563 08/24/23 12:08:10.691
    STEP: creating replication controller affinity-clusterip in namespace services-563 08/24/23 12:08:10.72
    I0824 12:08:10.753417      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-563, replica count: 3
    I0824 12:08:13.809398      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:08:13.821: INFO: Creating new exec pod
    Aug 24 12:08:13.835: INFO: Waiting up to 5m0s for pod "execpod-affinitywqndb" in namespace "services-563" to be "running"
    Aug 24 12:08:13.857: INFO: Pod "execpod-affinitywqndb": Phase="Pending", Reason="", readiness=false. Elapsed: 21.153028ms
    Aug 24 12:08:15.871: INFO: Pod "execpod-affinitywqndb": Phase="Running", Reason="", readiness=true. Elapsed: 2.035438331s
    Aug 24 12:08:15.871: INFO: Pod "execpod-affinitywqndb" satisfied condition "running"
    Aug 24 12:08:16.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-563 exec execpod-affinitywqndb -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 24 12:08:17.148: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 24 12:08:17.148: INFO: stdout: ""
    Aug 24 12:08:17.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-563 exec execpod-affinitywqndb -- /bin/sh -x -c nc -v -z -w 2 10.254.191.208 80'
    Aug 24 12:08:17.410: INFO: stderr: "+ nc -v -z -w 2 10.254.191.208 80\nConnection to 10.254.191.208 80 port [tcp/http] succeeded!\n"
    Aug 24 12:08:17.410: INFO: stdout: ""
    Aug 24 12:08:17.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-563 exec execpod-affinitywqndb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.191.208:80/ ; done'
    Aug 24 12:08:17.751: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.191.208:80/\n"
    Aug 24 12:08:17.751: INFO: stdout: "\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962\naffinity-clusterip-46962"
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Received response from host: affinity-clusterip-46962
    Aug 24 12:08:17.751: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-563, will wait for the garbage collector to delete the pods 08/24/23 12:08:17.776
    Aug 24 12:08:17.852: INFO: Deleting ReplicationController affinity-clusterip took: 11.85608ms
    Aug 24 12:08:17.953: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.296456ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:20.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-563" for this suite. 08/24/23 12:08:20.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:20.212
Aug 24 12:08:20.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:08:20.214
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:20.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:20.288
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/24/23 12:08:20.296
STEP: waiting for pod running 08/24/23 12:08:20.309
Aug 24 12:08:20.310: INFO: Waiting up to 2m0s for pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" in namespace "var-expansion-277" to be "running"
Aug 24 12:08:20.315: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670908ms
Aug 24 12:08:22.319: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371046s
Aug 24 12:08:22.319: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" satisfied condition "running"
STEP: creating a file in subpath 08/24/23 12:08:22.319
Aug 24 12:08:22.323: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-277 PodName:var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:22.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:08:22.324: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:22.324: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-277/pods/var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/24/23 12:08:22.46
Aug 24 12:08:22.465: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-277 PodName:var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:08:22.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:08:22.465: INFO: ExecWithOptions: Clientset creation
Aug 24 12:08:22.466: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-277/pods/var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/24/23 12:08:22.584
Aug 24 12:08:23.109: INFO: Successfully updated pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e"
STEP: waiting for annotated pod running 08/24/23 12:08:23.11
Aug 24 12:08:23.110: INFO: Waiting up to 2m0s for pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" in namespace "var-expansion-277" to be "running"
Aug 24 12:08:23.114: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e": Phase="Running", Reason="", readiness=true. Elapsed: 3.679804ms
Aug 24 12:08:23.114: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" satisfied condition "running"
STEP: deleting the pod gracefully 08/24/23 12:08:23.114
Aug 24 12:08:23.114: INFO: Deleting pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" in namespace "var-expansion-277"
Aug 24 12:08:23.131: INFO: Wait up to 5m0s for pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:08:57.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-277" for this suite. 08/24/23 12:08:57.147
------------------------------
â€¢ [SLOW TEST] [36.944 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:20.212
    Aug 24 12:08:20.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:08:20.214
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:20.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:20.288
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/24/23 12:08:20.296
    STEP: waiting for pod running 08/24/23 12:08:20.309
    Aug 24 12:08:20.310: INFO: Waiting up to 2m0s for pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" in namespace "var-expansion-277" to be "running"
    Aug 24 12:08:20.315: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.670908ms
    Aug 24 12:08:22.319: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371046s
    Aug 24 12:08:22.319: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" satisfied condition "running"
    STEP: creating a file in subpath 08/24/23 12:08:22.319
    Aug 24 12:08:22.323: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-277 PodName:var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:22.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:08:22.324: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:22.324: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-277/pods/var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/24/23 12:08:22.46
    Aug 24 12:08:22.465: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-277 PodName:var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:08:22.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:08:22.465: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:08:22.466: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-277/pods/var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/24/23 12:08:22.584
    Aug 24 12:08:23.109: INFO: Successfully updated pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e"
    STEP: waiting for annotated pod running 08/24/23 12:08:23.11
    Aug 24 12:08:23.110: INFO: Waiting up to 2m0s for pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" in namespace "var-expansion-277" to be "running"
    Aug 24 12:08:23.114: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e": Phase="Running", Reason="", readiness=true. Elapsed: 3.679804ms
    Aug 24 12:08:23.114: INFO: Pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" satisfied condition "running"
    STEP: deleting the pod gracefully 08/24/23 12:08:23.114
    Aug 24 12:08:23.114: INFO: Deleting pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" in namespace "var-expansion-277"
    Aug 24 12:08:23.131: INFO: Wait up to 5m0s for pod "var-expansion-20fa6437-b787-4e09-93ad-85022e286b6e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:08:57.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-277" for this suite. 08/24/23 12:08:57.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:08:57.157
Aug 24 12:08:57.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename cronjob 08/24/23 12:08:57.158
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:57.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:57.198
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/24/23 12:08:57.208
STEP: Ensuring more than one job is running at a time 08/24/23 12:08:57.218
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/24/23 12:10:01.221
STEP: Removing cronjob 08/24/23 12:10:01.225
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:01.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2164" for this suite. 08/24/23 12:10:01.257
------------------------------
â€¢ [SLOW TEST] [64.120 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:08:57.157
    Aug 24 12:08:57.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename cronjob 08/24/23 12:08:57.158
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:08:57.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:08:57.198
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/24/23 12:08:57.208
    STEP: Ensuring more than one job is running at a time 08/24/23 12:08:57.218
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/24/23 12:10:01.221
    STEP: Removing cronjob 08/24/23 12:10:01.225
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:01.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2164" for this suite. 08/24/23 12:10:01.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:01.286
Aug 24 12:10:01.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 12:10:01.287
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:01.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:01.358
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9542.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/24/23 12:10:01.364
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9542.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/24/23 12:10:01.364
STEP: creating a pod to probe /etc/hosts 08/24/23 12:10:01.365
STEP: submitting the pod to kubernetes 08/24/23 12:10:01.365
Aug 24 12:10:01.374: INFO: Waiting up to 15m0s for pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc" in namespace "dns-9542" to be "running"
Aug 24 12:10:01.380: INFO: Pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383792ms
Aug 24 12:10:03.384: INFO: Pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010059086s
Aug 24 12:10:03.384: INFO: Pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:10:03.384
STEP: looking for the results for each expected name from probers 08/24/23 12:10:03.389
Aug 24 12:10:03.406: INFO: DNS probes using dns-9542/dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc succeeded

STEP: deleting the pod 08/24/23 12:10:03.406
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:03.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9542" for this suite. 08/24/23 12:10:03.436
------------------------------
â€¢ [2.163 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:01.286
    Aug 24 12:10:01.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 12:10:01.287
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:01.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:01.358
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9542.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/24/23 12:10:01.364
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9542.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/24/23 12:10:01.364
    STEP: creating a pod to probe /etc/hosts 08/24/23 12:10:01.365
    STEP: submitting the pod to kubernetes 08/24/23 12:10:01.365
    Aug 24 12:10:01.374: INFO: Waiting up to 15m0s for pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc" in namespace "dns-9542" to be "running"
    Aug 24 12:10:01.380: INFO: Pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383792ms
    Aug 24 12:10:03.384: INFO: Pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.010059086s
    Aug 24 12:10:03.384: INFO: Pod "dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:10:03.384
    STEP: looking for the results for each expected name from probers 08/24/23 12:10:03.389
    Aug 24 12:10:03.406: INFO: DNS probes using dns-9542/dns-test-830f3e28-227e-47c1-8504-8bcc17f42bfc succeeded

    STEP: deleting the pod 08/24/23 12:10:03.406
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:03.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9542" for this suite. 08/24/23 12:10:03.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:03.451
Aug 24 12:10:03.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-pred 08/24/23 12:10:03.452
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:03.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:03.496
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 24 12:10:03.502: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 24 12:10:03.509: INFO: Waiting for terminating namespaces to be deleted...
Aug 24 12:10:03.512: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
Aug 24 12:10:03.527: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 12:10:03.527: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 12:10:03.527: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 12:10:03.527: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container autoscaler ready: true, restart count 0
Aug 24 12:10:03.527: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container grafana ready: true, restart count 0
Aug 24 12:10:03.527: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Aug 24 12:10:03.527: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Aug 24 12:10:03.527: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Aug 24 12:10:03.527: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 24 12:10:03.527: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container metrics-server ready: true, restart count 0
Aug 24 12:10:03.527: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 12:10:03.527: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 12:10:03.527: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container config-reloader ready: true, restart count 0
Aug 24 12:10:03.527: INFO: 	Container prometheus ready: true, restart count 0
Aug 24 12:10:03.527: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:10:03.527: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:10:03.527: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container busybox ready: true, restart count 1
Aug 24 12:10:03.527: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container nginx ready: true, restart count 0
Aug 24 12:10:03.527: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.527: INFO: 	Container nginx ready: true, restart count 0
Aug 24 12:10:03.527: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
Aug 24 12:10:03.536: INFO: concurrent-28214650-tbzl5 from cronjob-2164 started at 2023-08-24 12:10:00 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.536: INFO: 	Container c ready: true, restart count 0
Aug 24 12:10:03.536: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.536: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 12:10:03.536: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.536: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 12:10:03.537: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 12:10:03.537: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.537: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 12:10:03.537: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.537: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 12:10:03.537: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.537: INFO: 	Container e2e ready: true, restart count 0
Aug 24 12:10:03.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:10:03.537: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:10:03.537: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 24 12:10:03.537: INFO: 
Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
Aug 24 12:10:03.544: INFO: concurrent-28214649-hnhdm from cronjob-2164 started at 2023-08-24 12:09:00 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.544: INFO: 	Container c ready: true, restart count 0
Aug 24 12:10:03.544: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.544: INFO: 	Container calico-node ready: true, restart count 0
Aug 24 12:10:03.544: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.544: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Aug 24 12:10:03.544: INFO: 	Container node-driver-registrar ready: true, restart count 0
Aug 24 12:10:03.544: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.544: INFO: 	Container node-exporter ready: true, restart count 0
Aug 24 12:10:03.544: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.544: INFO: 	Container node-problem-detector ready: true, restart count 0
Aug 24 12:10:03.544: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
Aug 24 12:10:03.544: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 24 12:10:03.544: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
Aug 24 12:10:03.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 24 12:10:03.544: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 12:10:03.544
Aug 24 12:10:03.554: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-553" to be "running"
Aug 24 12:10:03.560: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.690232ms
Aug 24 12:10:05.567: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012104383s
Aug 24 12:10:05.567: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 12:10:05.57
STEP: Trying to apply a random label on the found node. 08/24/23 12:10:05.588
STEP: verifying the node has the label kubernetes.io/e2e-6c694751-83e4-4126-83d2-7309ee2d7956 42 08/24/23 12:10:05.614
STEP: Trying to relaunch the pod, now with labels. 08/24/23 12:10:05.629
Aug 24 12:10:05.647: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-553" to be "not pending"
Aug 24 12:10:05.663: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 15.622553ms
Aug 24 12:10:07.668: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.020561546s
Aug 24 12:10:07.668: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-6c694751-83e4-4126-83d2-7309ee2d7956 off the node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 12:10:07.671
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6c694751-83e4-4126-83d2-7309ee2d7956 08/24/23 12:10:07.692
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:07.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-553" for this suite. 08/24/23 12:10:07.706
------------------------------
â€¢ [4.264 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:03.451
    Aug 24 12:10:03.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-pred 08/24/23 12:10:03.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:03.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:03.496
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 24 12:10:03.502: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 24 12:10:03.509: INFO: Waiting for terminating namespaces to be deleted...
    Aug 24 12:10:03.512: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-0 before test
    Aug 24 12:10:03.527: INFO: calico-node-mvq8r from kube-system started at 2023-08-24 10:12:10 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: csi-cinder-nodeplugin-xjfv7 from kube-system started at 2023-08-24 10:12:24 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: kube-dns-autoscaler-5f4bb48647-vmmzg from kube-system started at 2023-08-24 10:12:23 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container autoscaler ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: magnum-grafana-867fcd9667-cr4gs from kube-system started at 2023-08-24 10:12:46 +0000 UTC (3 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container grafana ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: magnum-kube-prometheus-sta-operator-66b867f676-jjxrh from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: magnum-kube-state-metrics-79d5d4dd8f-mbg6t from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: magnum-metrics-server-5d9f484f5-f9v5m from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container metrics-server ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: magnum-prometheus-node-exporter-mjbts from kube-system started at 2023-08-24 10:12:46 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: npd-btqvw from kube-system started at 2023-08-24 10:12:24 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-08-24 10:13:09 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: 	Container prometheus ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-lrtdk from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: busybox from test-k8s started at 2023-08-24 10:13:52 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container busybox ready: true, restart count 1
    Aug 24 12:10:03.527: INFO: nginx-7f456874f4-992ld from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: nginx-7f456874f4-b57r9 from test-k8s started at 2023-08-24 10:19:32 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.527: INFO: 	Container nginx ready: true, restart count 0
    Aug 24 12:10:03.527: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-1 before test
    Aug 24 12:10:03.536: INFO: concurrent-28214650-tbzl5 from cronjob-2164 started at 2023-08-24 12:10:00 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.536: INFO: 	Container c ready: true, restart count 0
    Aug 24 12:10:03.536: INFO: calico-node-j7kf4 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.536: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 12:10:03.536: INFO: csi-cinder-nodeplugin-95jsr from kube-system started at 2023-08-24 10:37:50 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.536: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: magnum-prometheus-node-exporter-rl449 from kube-system started at 2023-08-24 10:37:35 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.537: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: npd-msw52 from kube-system started at 2023-08-24 10:37:50 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.537: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: sonobuoy-e2e-job-232e54a70299452d from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.537: INFO: 	Container e2e ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-vxdck from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 24 12:10:03.537: INFO: 
    Logging pods the apiserver thinks is on node gitlab-1-26-36460-guscsyka22xa-node-2 before test
    Aug 24 12:10:03.544: INFO: concurrent-28214649-hnhdm from cronjob-2164 started at 2023-08-24 12:09:00 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.544: INFO: 	Container c ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: calico-node-n4d54 from kube-system started at 2023-08-24 10:36:44 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.544: INFO: 	Container calico-node ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: csi-cinder-nodeplugin-vd9c7 from kube-system started at 2023-08-24 10:56:05 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.544: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: magnum-prometheus-node-exporter-hmfcv from kube-system started at 2023-08-24 10:56:05 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.544: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: npd-j8d7p from kube-system started at 2023-08-24 10:36:59 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.544: INFO: 	Container node-problem-detector ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: sonobuoy from sonobuoy started at 2023-08-24 10:39:07 +0000 UTC (1 container statuses recorded)
    Aug 24 12:10:03.544: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: sonobuoy-systemd-logs-daemon-set-f3f986dd84374c0f-fw2zf from sonobuoy started at 2023-08-24 10:39:14 +0000 UTC (2 container statuses recorded)
    Aug 24 12:10:03.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 24 12:10:03.544: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/24/23 12:10:03.544
    Aug 24 12:10:03.554: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-553" to be "running"
    Aug 24 12:10:03.560: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.690232ms
    Aug 24 12:10:05.567: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.012104383s
    Aug 24 12:10:05.567: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/24/23 12:10:05.57
    STEP: Trying to apply a random label on the found node. 08/24/23 12:10:05.588
    STEP: verifying the node has the label kubernetes.io/e2e-6c694751-83e4-4126-83d2-7309ee2d7956 42 08/24/23 12:10:05.614
    STEP: Trying to relaunch the pod, now with labels. 08/24/23 12:10:05.629
    Aug 24 12:10:05.647: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-553" to be "not pending"
    Aug 24 12:10:05.663: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 15.622553ms
    Aug 24 12:10:07.668: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.020561546s
    Aug 24 12:10:07.668: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-6c694751-83e4-4126-83d2-7309ee2d7956 off the node gitlab-1-26-36460-guscsyka22xa-node-2 08/24/23 12:10:07.671
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-6c694751-83e4-4126-83d2-7309ee2d7956 08/24/23 12:10:07.692
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:07.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-553" for this suite. 08/24/23 12:10:07.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:07.719
Aug 24 12:10:07.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 12:10:07.721
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:07.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:07.764
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/24/23 12:10:07.772
Aug 24 12:10:07.787: INFO: Waiting up to 5m0s for pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543" in namespace "projected-5782" to be "running and ready"
Aug 24 12:10:07.798: INFO: Pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543": Phase="Pending", Reason="", readiness=false. Elapsed: 11.274774ms
Aug 24 12:10:07.798: INFO: The phase of Pod labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:10:09.801: INFO: Pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543": Phase="Running", Reason="", readiness=true. Elapsed: 2.014642835s
Aug 24 12:10:09.801: INFO: The phase of Pod labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543 is Running (Ready = true)
Aug 24 12:10:09.802: INFO: Pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543" satisfied condition "running and ready"
Aug 24 12:10:10.372: INFO: Successfully updated pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:12.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5782" for this suite. 08/24/23 12:10:12.394
------------------------------
â€¢ [4.687 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:07.719
    Aug 24 12:10:07.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 12:10:07.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:07.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:07.764
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/24/23 12:10:07.772
    Aug 24 12:10:07.787: INFO: Waiting up to 5m0s for pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543" in namespace "projected-5782" to be "running and ready"
    Aug 24 12:10:07.798: INFO: Pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543": Phase="Pending", Reason="", readiness=false. Elapsed: 11.274774ms
    Aug 24 12:10:07.798: INFO: The phase of Pod labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:10:09.801: INFO: Pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543": Phase="Running", Reason="", readiness=true. Elapsed: 2.014642835s
    Aug 24 12:10:09.801: INFO: The phase of Pod labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543 is Running (Ready = true)
    Aug 24 12:10:09.802: INFO: Pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543" satisfied condition "running and ready"
    Aug 24 12:10:10.372: INFO: Successfully updated pod "labelsupdate5b150ca0-7a22-4eb1-8c7f-a91b4d860543"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:12.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5782" for this suite. 08/24/23 12:10:12.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:12.407
Aug 24 12:10:12.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename job 08/24/23 12:10:12.409
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:12.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:12.44
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/24/23 12:10:12.446
STEP: Ensuring active pods == parallelism 08/24/23 12:10:12.457
STEP: Orphaning one of the Job's Pods 08/24/23 12:10:14.462
Aug 24 12:10:14.993: INFO: Successfully updated pod "adopt-release-dlvkd"
STEP: Checking that the Job readopts the Pod 08/24/23 12:10:14.993
Aug 24 12:10:14.994: INFO: Waiting up to 15m0s for pod "adopt-release-dlvkd" in namespace "job-2632" to be "adopted"
Aug 24 12:10:15.002: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 8.605208ms
Aug 24 12:10:17.007: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013361872s
Aug 24 12:10:17.007: INFO: Pod "adopt-release-dlvkd" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/24/23 12:10:17.007
Aug 24 12:10:17.531: INFO: Successfully updated pod "adopt-release-dlvkd"
STEP: Checking that the Job releases the Pod 08/24/23 12:10:17.531
Aug 24 12:10:17.531: INFO: Waiting up to 15m0s for pod "adopt-release-dlvkd" in namespace "job-2632" to be "released"
Aug 24 12:10:17.541: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 9.846473ms
Aug 24 12:10:19.546: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.014837166s
Aug 24 12:10:19.546: INFO: Pod "adopt-release-dlvkd" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2632" for this suite. 08/24/23 12:10:19.553
------------------------------
â€¢ [SLOW TEST] [7.160 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:12.407
    Aug 24 12:10:12.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename job 08/24/23 12:10:12.409
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:12.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:12.44
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/24/23 12:10:12.446
    STEP: Ensuring active pods == parallelism 08/24/23 12:10:12.457
    STEP: Orphaning one of the Job's Pods 08/24/23 12:10:14.462
    Aug 24 12:10:14.993: INFO: Successfully updated pod "adopt-release-dlvkd"
    STEP: Checking that the Job readopts the Pod 08/24/23 12:10:14.993
    Aug 24 12:10:14.994: INFO: Waiting up to 15m0s for pod "adopt-release-dlvkd" in namespace "job-2632" to be "adopted"
    Aug 24 12:10:15.002: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 8.605208ms
    Aug 24 12:10:17.007: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.013361872s
    Aug 24 12:10:17.007: INFO: Pod "adopt-release-dlvkd" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/24/23 12:10:17.007
    Aug 24 12:10:17.531: INFO: Successfully updated pod "adopt-release-dlvkd"
    STEP: Checking that the Job releases the Pod 08/24/23 12:10:17.531
    Aug 24 12:10:17.531: INFO: Waiting up to 15m0s for pod "adopt-release-dlvkd" in namespace "job-2632" to be "released"
    Aug 24 12:10:17.541: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 9.846473ms
    Aug 24 12:10:19.546: INFO: Pod "adopt-release-dlvkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.014837166s
    Aug 24 12:10:19.546: INFO: Pod "adopt-release-dlvkd" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:19.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2632" for this suite. 08/24/23 12:10:19.553
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:19.568
Aug 24 12:10:19.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 12:10:19.57
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:19.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:19.603
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/24/23 12:10:19.608
STEP: getting 08/24/23 12:10:19.641
STEP: listing in namespace 08/24/23 12:10:19.661
STEP: patching 08/24/23 12:10:19.666
STEP: deleting 08/24/23 12:10:19.68
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:19.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9874" for this suite. 08/24/23 12:10:19.705
------------------------------
â€¢ [0.146 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:19.568
    Aug 24 12:10:19.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 12:10:19.57
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:19.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:19.603
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/24/23 12:10:19.608
    STEP: getting 08/24/23 12:10:19.641
    STEP: listing in namespace 08/24/23 12:10:19.661
    STEP: patching 08/24/23 12:10:19.666
    STEP: deleting 08/24/23 12:10:19.68
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:19.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9874" for this suite. 08/24/23 12:10:19.705
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:19.714
Aug 24 12:10:19.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename gc 08/24/23 12:10:19.716
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:19.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:19.752
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 24 12:10:19.850: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"eda99c94-10e9-4a5d-8789-b6ba0bc210fc", Controller:(*bool)(0xc003c4e59e), BlockOwnerDeletion:(*bool)(0xc003c4e59f)}}
Aug 24 12:10:19.866: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3e3216a6-0a5a-46f2-8ded-8c0361f1a1df", Controller:(*bool)(0xc003c4e816), BlockOwnerDeletion:(*bool)(0xc003c4e817)}}
Aug 24 12:10:19.881: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b0297f9a-f94a-42a7-ad49-098d9343b573", Controller:(*bool)(0xc003c4ea96), BlockOwnerDeletion:(*bool)(0xc003c4ea97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:24.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9006" for this suite. 08/24/23 12:10:24.915
------------------------------
â€¢ [SLOW TEST] [5.214 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:19.714
    Aug 24 12:10:19.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename gc 08/24/23 12:10:19.716
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:19.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:19.752
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 24 12:10:19.850: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"eda99c94-10e9-4a5d-8789-b6ba0bc210fc", Controller:(*bool)(0xc003c4e59e), BlockOwnerDeletion:(*bool)(0xc003c4e59f)}}
    Aug 24 12:10:19.866: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3e3216a6-0a5a-46f2-8ded-8c0361f1a1df", Controller:(*bool)(0xc003c4e816), BlockOwnerDeletion:(*bool)(0xc003c4e817)}}
    Aug 24 12:10:19.881: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b0297f9a-f94a-42a7-ad49-098d9343b573", Controller:(*bool)(0xc003c4ea96), BlockOwnerDeletion:(*bool)(0xc003c4ea97)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:24.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9006" for this suite. 08/24/23 12:10:24.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:24.932
Aug 24 12:10:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 12:10:24.933
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:24.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:24.99
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/24/23 12:10:25.001
Aug 24 12:10:25.001: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8507 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/24/23 12:10:25.083
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:10:25.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8507" for this suite. 08/24/23 12:10:25.138
------------------------------
â€¢ [0.234 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:24.932
    Aug 24 12:10:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:10:24.933
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:24.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:24.99
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/24/23 12:10:25.001
    Aug 24 12:10:25.001: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-8507 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/24/23 12:10:25.083
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:10:25.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8507" for this suite. 08/24/23 12:10:25.138
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:10:25.167
Aug 24 12:10:25.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 12:10:25.168
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:25.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:25.213
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/24/23 12:10:25.218
STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:10:25.726
Aug 24 12:10:25.749: INFO: Pod name wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed: Found 0 pods out of 5
Aug 24 12:10:30.760: INFO: Pod name wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/24/23 12:10:30.76
Aug 24 12:10:30.760: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:30.768: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.740834ms
Aug 24 12:10:32.773: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013231345s
Aug 24 12:10:34.774: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014020854s
Aug 24 12:10:36.772: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011891372s
Aug 24 12:10:38.774: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01407323s
Aug 24 12:10:40.773: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Running", Reason="", readiness=true. Elapsed: 10.012478766s
Aug 24 12:10:40.773: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9" satisfied condition "running"
Aug 24 12:10:40.773: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-cvqjr" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:40.777: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-cvqjr": Phase="Running", Reason="", readiness=true. Elapsed: 4.317065ms
Aug 24 12:10:40.777: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-cvqjr" satisfied condition "running"
Aug 24 12:10:40.777: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-kcmvc" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:40.781: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-kcmvc": Phase="Running", Reason="", readiness=true. Elapsed: 3.7279ms
Aug 24 12:10:40.781: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-kcmvc" satisfied condition "running"
Aug 24 12:10:40.781: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-r5f6h" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:40.784: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-r5f6h": Phase="Running", Reason="", readiness=true. Elapsed: 3.507425ms
Aug 24 12:10:40.784: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-r5f6h" satisfied condition "running"
Aug 24 12:10:40.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-tf2gs" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:40.788: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-tf2gs": Phase="Running", Reason="", readiness=true. Elapsed: 3.636127ms
Aug 24 12:10:40.788: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-tf2gs" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed in namespace emptydir-wrapper-3985, will wait for the garbage collector to delete the pods 08/24/23 12:10:40.788
Aug 24 12:10:40.852: INFO: Deleting ReplicationController wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed took: 10.148517ms
Aug 24 12:10:41.053: INFO: Terminating ReplicationController wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed pods took: 200.697861ms
STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:10:44.263
Aug 24 12:10:44.291: INFO: Pod name wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7: Found 0 pods out of 5
Aug 24 12:10:49.313: INFO: Pod name wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/24/23 12:10:49.313
Aug 24 12:10:49.314: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:49.370: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 55.76539ms
Aug 24 12:10:51.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061453645s
Aug 24 12:10:53.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061864075s
Aug 24 12:10:55.384: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069804922s
Aug 24 12:10:57.379: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065001898s
Aug 24 12:10:59.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Running", Reason="", readiness=true. Elapsed: 10.062325585s
Aug 24 12:10:59.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c" satisfied condition "running"
Aug 24 12:10:59.377: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-fs4kh" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:59.385: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-fs4kh": Phase="Running", Reason="", readiness=true. Elapsed: 8.201073ms
Aug 24 12:10:59.385: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-fs4kh" satisfied condition "running"
Aug 24 12:10:59.385: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-k259f" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:59.391: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-k259f": Phase="Running", Reason="", readiness=true. Elapsed: 5.846964ms
Aug 24 12:10:59.391: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-k259f" satisfied condition "running"
Aug 24 12:10:59.391: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-nv9hb" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:59.395: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-nv9hb": Phase="Running", Reason="", readiness=true. Elapsed: 4.316419ms
Aug 24 12:10:59.395: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-nv9hb" satisfied condition "running"
Aug 24 12:10:59.395: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-xddhq" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:10:59.403: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-xddhq": Phase="Running", Reason="", readiness=true. Elapsed: 7.886677ms
Aug 24 12:10:59.403: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-xddhq" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7 in namespace emptydir-wrapper-3985, will wait for the garbage collector to delete the pods 08/24/23 12:10:59.403
Aug 24 12:10:59.473: INFO: Deleting ReplicationController wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7 took: 12.338797ms
Aug 24 12:10:59.673: INFO: Terminating ReplicationController wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7 pods took: 200.617197ms
STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:11:03.483
Aug 24 12:11:03.513: INFO: Pod name wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a: Found 0 pods out of 5
Aug 24 12:11:08.522: INFO: Pod name wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/24/23 12:11:08.522
Aug 24 12:11:08.522: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:11:08.527: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010184ms
Aug 24 12:11:10.533: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010372513s
Aug 24 12:11:12.532: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009958455s
Aug 24 12:11:14.531: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00923709s
Aug 24 12:11:16.542: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Running", Reason="", readiness=true. Elapsed: 8.019360294s
Aug 24 12:11:16.542: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb" satisfied condition "running"
Aug 24 12:11:16.542: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:11:16.547: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs": Phase="Pending", Reason="", readiness=false. Elapsed: 5.271251ms
Aug 24 12:11:18.559: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs": Phase="Running", Reason="", readiness=true. Elapsed: 2.017230244s
Aug 24 12:11:18.559: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs" satisfied condition "running"
Aug 24 12:11:18.559: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-86s9n" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:11:18.565: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-86s9n": Phase="Running", Reason="", readiness=true. Elapsed: 5.541769ms
Aug 24 12:11:18.565: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-86s9n" satisfied condition "running"
Aug 24 12:11:18.565: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-pkxsj" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:11:18.572: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-pkxsj": Phase="Running", Reason="", readiness=true. Elapsed: 7.575797ms
Aug 24 12:11:18.572: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-pkxsj" satisfied condition "running"
Aug 24 12:11:18.572: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-smz4b" in namespace "emptydir-wrapper-3985" to be "running"
Aug 24 12:11:18.577: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-smz4b": Phase="Running", Reason="", readiness=true. Elapsed: 4.96084ms
Aug 24 12:11:18.577: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-smz4b" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a in namespace emptydir-wrapper-3985, will wait for the garbage collector to delete the pods 08/24/23 12:11:18.577
Aug 24 12:11:18.647: INFO: Deleting ReplicationController wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a took: 13.642978ms
Aug 24 12:11:18.848: INFO: Terminating ReplicationController wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a pods took: 200.836546ms
STEP: Cleaning up the configMaps 08/24/23 12:11:22.348
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:22.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3985" for this suite. 08/24/23 12:11:22.777
------------------------------
â€¢ [SLOW TEST] [57.619 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:10:25.167
    Aug 24 12:10:25.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir-wrapper 08/24/23 12:10:25.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:10:25.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:10:25.213
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/24/23 12:10:25.218
    STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:10:25.726
    Aug 24 12:10:25.749: INFO: Pod name wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed: Found 0 pods out of 5
    Aug 24 12:10:30.760: INFO: Pod name wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/24/23 12:10:30.76
    Aug 24 12:10:30.760: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:30.768: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.740834ms
    Aug 24 12:10:32.773: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013231345s
    Aug 24 12:10:34.774: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014020854s
    Aug 24 12:10:36.772: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011891372s
    Aug 24 12:10:38.774: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01407323s
    Aug 24 12:10:40.773: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9": Phase="Running", Reason="", readiness=true. Elapsed: 10.012478766s
    Aug 24 12:10:40.773: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-bzrs9" satisfied condition "running"
    Aug 24 12:10:40.773: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-cvqjr" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:40.777: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-cvqjr": Phase="Running", Reason="", readiness=true. Elapsed: 4.317065ms
    Aug 24 12:10:40.777: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-cvqjr" satisfied condition "running"
    Aug 24 12:10:40.777: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-kcmvc" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:40.781: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-kcmvc": Phase="Running", Reason="", readiness=true. Elapsed: 3.7279ms
    Aug 24 12:10:40.781: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-kcmvc" satisfied condition "running"
    Aug 24 12:10:40.781: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-r5f6h" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:40.784: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-r5f6h": Phase="Running", Reason="", readiness=true. Elapsed: 3.507425ms
    Aug 24 12:10:40.784: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-r5f6h" satisfied condition "running"
    Aug 24 12:10:40.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-tf2gs" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:40.788: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-tf2gs": Phase="Running", Reason="", readiness=true. Elapsed: 3.636127ms
    Aug 24 12:10:40.788: INFO: Pod "wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed-tf2gs" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed in namespace emptydir-wrapper-3985, will wait for the garbage collector to delete the pods 08/24/23 12:10:40.788
    Aug 24 12:10:40.852: INFO: Deleting ReplicationController wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed took: 10.148517ms
    Aug 24 12:10:41.053: INFO: Terminating ReplicationController wrapped-volume-race-a47509bd-f6f5-46a6-a07c-047320c45fed pods took: 200.697861ms
    STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:10:44.263
    Aug 24 12:10:44.291: INFO: Pod name wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7: Found 0 pods out of 5
    Aug 24 12:10:49.313: INFO: Pod name wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/24/23 12:10:49.313
    Aug 24 12:10:49.314: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:49.370: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 55.76539ms
    Aug 24 12:10:51.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061453645s
    Aug 24 12:10:53.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061864075s
    Aug 24 12:10:55.384: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069804922s
    Aug 24 12:10:57.379: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065001898s
    Aug 24 12:10:59.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c": Phase="Running", Reason="", readiness=true. Elapsed: 10.062325585s
    Aug 24 12:10:59.376: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-c5f4c" satisfied condition "running"
    Aug 24 12:10:59.377: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-fs4kh" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:59.385: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-fs4kh": Phase="Running", Reason="", readiness=true. Elapsed: 8.201073ms
    Aug 24 12:10:59.385: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-fs4kh" satisfied condition "running"
    Aug 24 12:10:59.385: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-k259f" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:59.391: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-k259f": Phase="Running", Reason="", readiness=true. Elapsed: 5.846964ms
    Aug 24 12:10:59.391: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-k259f" satisfied condition "running"
    Aug 24 12:10:59.391: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-nv9hb" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:59.395: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-nv9hb": Phase="Running", Reason="", readiness=true. Elapsed: 4.316419ms
    Aug 24 12:10:59.395: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-nv9hb" satisfied condition "running"
    Aug 24 12:10:59.395: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-xddhq" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:10:59.403: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-xddhq": Phase="Running", Reason="", readiness=true. Elapsed: 7.886677ms
    Aug 24 12:10:59.403: INFO: Pod "wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7-xddhq" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7 in namespace emptydir-wrapper-3985, will wait for the garbage collector to delete the pods 08/24/23 12:10:59.403
    Aug 24 12:10:59.473: INFO: Deleting ReplicationController wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7 took: 12.338797ms
    Aug 24 12:10:59.673: INFO: Terminating ReplicationController wrapped-volume-race-49a0bf7e-762d-4f40-a847-009ec34993e7 pods took: 200.617197ms
    STEP: Creating RC which spawns configmap-volume pods 08/24/23 12:11:03.483
    Aug 24 12:11:03.513: INFO: Pod name wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a: Found 0 pods out of 5
    Aug 24 12:11:08.522: INFO: Pod name wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/24/23 12:11:08.522
    Aug 24 12:11:08.522: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:11:08.527: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010184ms
    Aug 24 12:11:10.533: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010372513s
    Aug 24 12:11:12.532: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009958455s
    Aug 24 12:11:14.531: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00923709s
    Aug 24 12:11:16.542: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb": Phase="Running", Reason="", readiness=true. Elapsed: 8.019360294s
    Aug 24 12:11:16.542: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-2jkkb" satisfied condition "running"
    Aug 24 12:11:16.542: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:11:16.547: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs": Phase="Pending", Reason="", readiness=false. Elapsed: 5.271251ms
    Aug 24 12:11:18.559: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs": Phase="Running", Reason="", readiness=true. Elapsed: 2.017230244s
    Aug 24 12:11:18.559: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-68rxs" satisfied condition "running"
    Aug 24 12:11:18.559: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-86s9n" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:11:18.565: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-86s9n": Phase="Running", Reason="", readiness=true. Elapsed: 5.541769ms
    Aug 24 12:11:18.565: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-86s9n" satisfied condition "running"
    Aug 24 12:11:18.565: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-pkxsj" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:11:18.572: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-pkxsj": Phase="Running", Reason="", readiness=true. Elapsed: 7.575797ms
    Aug 24 12:11:18.572: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-pkxsj" satisfied condition "running"
    Aug 24 12:11:18.572: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-smz4b" in namespace "emptydir-wrapper-3985" to be "running"
    Aug 24 12:11:18.577: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-smz4b": Phase="Running", Reason="", readiness=true. Elapsed: 4.96084ms
    Aug 24 12:11:18.577: INFO: Pod "wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a-smz4b" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a in namespace emptydir-wrapper-3985, will wait for the garbage collector to delete the pods 08/24/23 12:11:18.577
    Aug 24 12:11:18.647: INFO: Deleting ReplicationController wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a took: 13.642978ms
    Aug 24 12:11:18.848: INFO: Terminating ReplicationController wrapped-volume-race-8cd8ab5e-44ae-435d-9e6a-a28db8bc475a pods took: 200.836546ms
    STEP: Cleaning up the configMaps 08/24/23 12:11:22.348
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:22.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3985" for this suite. 08/24/23 12:11:22.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:22.79
Aug 24 12:11:22.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename namespaces 08/24/23 12:11:22.792
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:22.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:22.826
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/24/23 12:11:22.831
STEP: patching the Namespace 08/24/23 12:11:22.898
STEP: get the Namespace and ensuring it has the label 08/24/23 12:11:22.912
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:22.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9809" for this suite. 08/24/23 12:11:22.922
STEP: Destroying namespace "nspatchtest-d9cffb41-21d9-479c-bb48-78be216a084f-9664" for this suite. 08/24/23 12:11:22.933
------------------------------
â€¢ [0.152 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:22.79
    Aug 24 12:11:22.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename namespaces 08/24/23 12:11:22.792
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:22.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:22.826
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/24/23 12:11:22.831
    STEP: patching the Namespace 08/24/23 12:11:22.898
    STEP: get the Namespace and ensuring it has the label 08/24/23 12:11:22.912
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:22.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9809" for this suite. 08/24/23 12:11:22.922
    STEP: Destroying namespace "nspatchtest-d9cffb41-21d9-479c-bb48-78be216a084f-9664" for this suite. 08/24/23 12:11:22.933
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:22.946
Aug 24 12:11:22.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:11:22.948
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:22.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:22.983
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-b9bb9e8d-f694-4bb3-9725-b7d847481b2b 08/24/23 12:11:22.992
STEP: Creating the pod 08/24/23 12:11:22.999
Aug 24 12:11:23.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59" in namespace "configmap-2307" to be "running and ready"
Aug 24 12:11:23.021: INFO: Pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59": Phase="Pending", Reason="", readiness=false. Elapsed: 7.415194ms
Aug 24 12:11:23.021: INFO: The phase of Pod pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:11:25.026: INFO: Pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59": Phase="Running", Reason="", readiness=true. Elapsed: 2.01307742s
Aug 24 12:11:25.026: INFO: The phase of Pod pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59 is Running (Ready = true)
Aug 24 12:11:25.027: INFO: Pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-b9bb9e8d-f694-4bb3-9725-b7d847481b2b 08/24/23 12:11:25.091
STEP: waiting to observe update in volume 08/24/23 12:11:25.1
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:27.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2307" for this suite. 08/24/23 12:11:27.125
------------------------------
â€¢ [4.188 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:22.946
    Aug 24 12:11:22.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:11:22.948
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:22.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:22.983
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-b9bb9e8d-f694-4bb3-9725-b7d847481b2b 08/24/23 12:11:22.992
    STEP: Creating the pod 08/24/23 12:11:22.999
    Aug 24 12:11:23.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59" in namespace "configmap-2307" to be "running and ready"
    Aug 24 12:11:23.021: INFO: Pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59": Phase="Pending", Reason="", readiness=false. Elapsed: 7.415194ms
    Aug 24 12:11:23.021: INFO: The phase of Pod pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:11:25.026: INFO: Pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59": Phase="Running", Reason="", readiness=true. Elapsed: 2.01307742s
    Aug 24 12:11:25.026: INFO: The phase of Pod pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59 is Running (Ready = true)
    Aug 24 12:11:25.027: INFO: Pod "pod-configmaps-6aabadf4-9f81-42b0-8747-4d7fbdd0ab59" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-b9bb9e8d-f694-4bb3-9725-b7d847481b2b 08/24/23 12:11:25.091
    STEP: waiting to observe update in volume 08/24/23 12:11:25.1
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:27.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2307" for this suite. 08/24/23 12:11:27.125
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:27.134
Aug 24 12:11:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 12:11:27.135
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:27.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:27.167
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/24/23 12:11:27.173
STEP: getting 08/24/23 12:11:27.199
STEP: listing 08/24/23 12:11:27.206
STEP: deleting 08/24/23 12:11:27.21
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:27.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-9415" for this suite. 08/24/23 12:11:27.246
------------------------------
â€¢ [0.128 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:27.134
    Aug 24 12:11:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename csiinlinevolumes 08/24/23 12:11:27.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:27.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:27.167
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/24/23 12:11:27.173
    STEP: getting 08/24/23 12:11:27.199
    STEP: listing 08/24/23 12:11:27.206
    STEP: deleting 08/24/23 12:11:27.21
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:27.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-9415" for this suite. 08/24/23 12:11:27.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:27.266
Aug 24 12:11:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename job 08/24/23 12:11:27.267
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:27.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:27.305
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/24/23 12:11:27.311
STEP: Ensuring job reaches completions 08/24/23 12:11:27.324
STEP: Ensuring pods with index for job exist 08/24/23 12:11:37.329
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:37.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6077" for this suite. 08/24/23 12:11:37.339
------------------------------
â€¢ [SLOW TEST] [10.089 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:27.266
    Aug 24 12:11:27.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename job 08/24/23 12:11:27.267
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:27.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:27.305
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/24/23 12:11:27.311
    STEP: Ensuring job reaches completions 08/24/23 12:11:27.324
    STEP: Ensuring pods with index for job exist 08/24/23 12:11:37.329
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:37.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6077" for this suite. 08/24/23 12:11:37.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:37.359
Aug 24 12:11:37.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename podtemplate 08/24/23 12:11:37.36
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:37.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:37.388
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8276" for this suite. 08/24/23 12:11:37.433
------------------------------
â€¢ [0.083 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:37.359
    Aug 24 12:11:37.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename podtemplate 08/24/23 12:11:37.36
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:37.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:37.388
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8276" for this suite. 08/24/23 12:11:37.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:37.444
Aug 24 12:11:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 12:11:37.446
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:37.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:37.471
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-bfa4ca3e-c48e-4abc-9dc8-058980cbbdc5 08/24/23 12:11:37.478
STEP: Creating a pod to test consume secrets 08/24/23 12:11:37.486
Aug 24 12:11:37.500: INFO: Waiting up to 5m0s for pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba" in namespace "secrets-7581" to be "Succeeded or Failed"
Aug 24 12:11:37.521: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba": Phase="Pending", Reason="", readiness=false. Elapsed: 21.39593ms
Aug 24 12:11:39.536: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035961499s
Aug 24 12:11:41.527: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026709439s
STEP: Saw pod success 08/24/23 12:11:41.527
Aug 24 12:11:41.527: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba" satisfied condition "Succeeded or Failed"
Aug 24 12:11:41.530: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:11:41.538
Aug 24 12:11:41.563: INFO: Waiting for pod pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba to disappear
Aug 24 12:11:41.577: INFO: Pod pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:11:41.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7581" for this suite. 08/24/23 12:11:41.584
------------------------------
â€¢ [4.157 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:37.444
    Aug 24 12:11:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 12:11:37.446
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:37.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:37.471
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-bfa4ca3e-c48e-4abc-9dc8-058980cbbdc5 08/24/23 12:11:37.478
    STEP: Creating a pod to test consume secrets 08/24/23 12:11:37.486
    Aug 24 12:11:37.500: INFO: Waiting up to 5m0s for pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba" in namespace "secrets-7581" to be "Succeeded or Failed"
    Aug 24 12:11:37.521: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba": Phase="Pending", Reason="", readiness=false. Elapsed: 21.39593ms
    Aug 24 12:11:39.536: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035961499s
    Aug 24 12:11:41.527: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026709439s
    STEP: Saw pod success 08/24/23 12:11:41.527
    Aug 24 12:11:41.527: INFO: Pod "pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba" satisfied condition "Succeeded or Failed"
    Aug 24 12:11:41.530: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:11:41.538
    Aug 24 12:11:41.563: INFO: Waiting for pod pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba to disappear
    Aug 24 12:11:41.577: INFO: Pod pod-secrets-9354ece4-f6c3-4c90-9e62-47dd38f05fba no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:11:41.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7581" for this suite. 08/24/23 12:11:41.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:11:41.601
Aug 24 12:11:41.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 12:11:41.603
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:41.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:41.651
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 24 12:11:41.682: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 24 12:11:46.687: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:11:46.687
Aug 24 12:11:46.687: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 24 12:11:48.692: INFO: Creating deployment "test-rollover-deployment"
Aug 24 12:11:48.720: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 24 12:11:50.730: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 24 12:11:50.738: INFO: Ensure that both replica sets have 1 created replica
Aug 24 12:11:50.744: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 24 12:11:50.760: INFO: Updating deployment test-rollover-deployment
Aug 24 12:11:50.760: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 24 12:11:52.769: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 24 12:11:52.776: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 24 12:11:52.782: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:11:52.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:11:54.790: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:11:54.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:11:56.790: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:11:56.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:11:58.794: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:11:58.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:12:00.791: INFO: all replica sets need to contain the pod-template-hash label
Aug 24 12:12:00.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 24 12:12:02.791: INFO: 
Aug 24 12:12:02.791: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:12:02.804: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8730  c1efedd4-7c40-4331-89ce-bcdd3b036b16 41324 2 2023-08-24 12:11:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b4a208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:11:48 +0000 UTC,LastTransitionTime:2023-08-24 12:11:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-24 12:12:02 +0000 UTC,LastTransitionTime:2023-08-24 12:11:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 24 12:12:02.808: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8730  d675c493-397a-492e-ad83-bf6a6659172a 41313 2 2023-08-24 12:11:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c1efedd4-7c40-4331-89ce-bcdd3b036b16 0xc005865477 0xc005865478}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1efedd4-7c40-4331-89ce-bcdd3b036b16\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005865528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:12:02.808: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 24 12:12:02.808: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8730  a6de770c-ffa7-4bfc-a640-c22ddfc8a62a 41323 2 2023-08-24 12:11:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c1efedd4-7c40-4331-89ce-bcdd3b036b16 0xc005865347 0xc005865348}] [] [{e2e.test Update apps/v1 2023-08-24 12:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1efedd4-7c40-4331-89ce-bcdd3b036b16\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005865408 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:12:02.808: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8730  30185aa6-8d54-49c9-8c74-744594f2429f 41270 2 2023-08-24 12:11:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c1efedd4-7c40-4331-89ce-bcdd3b036b16 0xc005865597 0xc005865598}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1efedd4-7c40-4331-89ce-bcdd3b036b16\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005865648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:12:02.812: INFO: Pod "test-rollover-deployment-6c6df9974f-pvrfw" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-pvrfw test-rollover-deployment-6c6df9974f- deployment-8730  71210c75-b53e-4b5d-89f4-87a0cba56f62 41284 0 2023-08-24 12:11:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:648d2964adf4591438c2c398373e72b4f8c9638362eae01809074b6b3ac20260 cni.projectcalico.org/podIP:10.100.181.153/32 cni.projectcalico.org/podIPs:10.100.181.153/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f d675c493-397a-492e-ad83-bf6a6659172a 0xc005b4a5a7 0xc005b4a5a8}] [] [{kube-controller-manager Update v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d675c493-397a-492e-ad83-bf6a6659172a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 12:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 12:11:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fz9mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fz9mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.153,StartTime:2023-08-24 12:11:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:11:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://b23f8d7eea9c422d8edb519c26a5d9f4010260bcaa35e097cd87fb417fa38f15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:02.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8730" for this suite. 08/24/23 12:12:02.818
------------------------------
â€¢ [SLOW TEST] [21.232 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:11:41.601
    Aug 24 12:11:41.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 12:11:41.603
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:11:41.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:11:41.651
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 24 12:11:41.682: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 24 12:11:46.687: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:11:46.687
    Aug 24 12:11:46.687: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 24 12:11:48.692: INFO: Creating deployment "test-rollover-deployment"
    Aug 24 12:11:48.720: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 24 12:11:50.730: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 24 12:11:50.738: INFO: Ensure that both replica sets have 1 created replica
    Aug 24 12:11:50.744: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 24 12:11:50.760: INFO: Updating deployment test-rollover-deployment
    Aug 24 12:11:50.760: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 24 12:11:52.769: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 24 12:11:52.776: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 24 12:11:52.782: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:11:52.782: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:11:54.790: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:11:54.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:11:56.790: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:11:56.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:11:58.794: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:11:58.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:12:00.791: INFO: all replica sets need to contain the pod-template-hash label
    Aug 24 12:12:00.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 11, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 11, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 24 12:12:02.791: INFO: 
    Aug 24 12:12:02.791: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:12:02.804: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8730  c1efedd4-7c40-4331-89ce-bcdd3b036b16 41324 2 2023-08-24 12:11:48 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b4a208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:11:48 +0000 UTC,LastTransitionTime:2023-08-24 12:11:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-24 12:12:02 +0000 UTC,LastTransitionTime:2023-08-24 12:11:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 24 12:12:02.808: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8730  d675c493-397a-492e-ad83-bf6a6659172a 41313 2 2023-08-24 12:11:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c1efedd4-7c40-4331-89ce-bcdd3b036b16 0xc005865477 0xc005865478}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1efedd4-7c40-4331-89ce-bcdd3b036b16\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005865528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:12:02.808: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 24 12:12:02.808: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8730  a6de770c-ffa7-4bfc-a640-c22ddfc8a62a 41323 2 2023-08-24 12:11:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c1efedd4-7c40-4331-89ce-bcdd3b036b16 0xc005865347 0xc005865348}] [] [{e2e.test Update apps/v1 2023-08-24 12:11:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1efedd4-7c40-4331-89ce-bcdd3b036b16\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:12:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005865408 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:12:02.808: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8730  30185aa6-8d54-49c9-8c74-744594f2429f 41270 2 2023-08-24 12:11:48 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c1efedd4-7c40-4331-89ce-bcdd3b036b16 0xc005865597 0xc005865598}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1efedd4-7c40-4331-89ce-bcdd3b036b16\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005865648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:12:02.812: INFO: Pod "test-rollover-deployment-6c6df9974f-pvrfw" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-pvrfw test-rollover-deployment-6c6df9974f- deployment-8730  71210c75-b53e-4b5d-89f4-87a0cba56f62 41284 0 2023-08-24 12:11:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:648d2964adf4591438c2c398373e72b4f8c9638362eae01809074b6b3ac20260 cni.projectcalico.org/podIP:10.100.181.153/32 cni.projectcalico.org/podIPs:10.100.181.153/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f d675c493-397a-492e-ad83-bf6a6659172a 0xc005b4a5a7 0xc005b4a5a8}] [] [{kube-controller-manager Update v1 2023-08-24 12:11:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d675c493-397a-492e-ad83-bf6a6659172a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-08-24 12:11:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-24 12:11:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.181.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fz9mk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fz9mk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:11:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:10.100.181.153,StartTime:2023-08-24 12:11:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:11:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://b23f8d7eea9c422d8edb519c26a5d9f4010260bcaa35e097cd87fb417fa38f15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.181.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:02.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8730" for this suite. 08/24/23 12:12:02.818
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:02.834
Aug 24 12:12:02.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:12:02.835
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:02.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:02.869
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/24/23 12:12:02.876
Aug 24 12:12:02.891: INFO: Waiting up to 5m0s for pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829" in namespace "var-expansion-1777" to be "Succeeded or Failed"
Aug 24 12:12:02.895: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.294607ms
Aug 24 12:12:04.899: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008231429s
Aug 24 12:12:06.899: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007618265s
STEP: Saw pod success 08/24/23 12:12:06.899
Aug 24 12:12:06.899: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829" satisfied condition "Succeeded or Failed"
Aug 24 12:12:06.902: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829 container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:12:06.912
Aug 24 12:12:06.933: INFO: Waiting for pod var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829 to disappear
Aug 24 12:12:06.938: INFO: Pod var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:06.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1777" for this suite. 08/24/23 12:12:06.943
------------------------------
â€¢ [4.118 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:02.834
    Aug 24 12:12:02.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:12:02.835
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:02.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:02.869
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/24/23 12:12:02.876
    Aug 24 12:12:02.891: INFO: Waiting up to 5m0s for pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829" in namespace "var-expansion-1777" to be "Succeeded or Failed"
    Aug 24 12:12:02.895: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829": Phase="Pending", Reason="", readiness=false. Elapsed: 4.294607ms
    Aug 24 12:12:04.899: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008231429s
    Aug 24 12:12:06.899: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007618265s
    STEP: Saw pod success 08/24/23 12:12:06.899
    Aug 24 12:12:06.899: INFO: Pod "var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829" satisfied condition "Succeeded or Failed"
    Aug 24 12:12:06.902: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:12:06.912
    Aug 24 12:12:06.933: INFO: Waiting for pod var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829 to disappear
    Aug 24 12:12:06.938: INFO: Pod var-expansion-82475b15-2bf8-4f04-af2f-59a9384d4829 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:06.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1777" for this suite. 08/24/23 12:12:06.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:06.964
Aug 24 12:12:06.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:12:06.967
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:06.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:06.999
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/24/23 12:12:07.008
STEP: Creating a ResourceQuota 08/24/23 12:12:12.011
STEP: Ensuring resource quota status is calculated 08/24/23 12:12:12.041
STEP: Creating a ReplicaSet 08/24/23 12:12:14.048
STEP: Ensuring resource quota status captures replicaset creation 08/24/23 12:12:14.07
STEP: Deleting a ReplicaSet 08/24/23 12:12:16.074
STEP: Ensuring resource quota status released usage 08/24/23 12:12:16.084
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:18.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2443" for this suite. 08/24/23 12:12:18.092
------------------------------
â€¢ [SLOW TEST] [11.142 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:06.964
    Aug 24 12:12:06.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:12:06.967
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:06.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:06.999
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/24/23 12:12:07.008
    STEP: Creating a ResourceQuota 08/24/23 12:12:12.011
    STEP: Ensuring resource quota status is calculated 08/24/23 12:12:12.041
    STEP: Creating a ReplicaSet 08/24/23 12:12:14.048
    STEP: Ensuring resource quota status captures replicaset creation 08/24/23 12:12:14.07
    STEP: Deleting a ReplicaSet 08/24/23 12:12:16.074
    STEP: Ensuring resource quota status released usage 08/24/23 12:12:16.084
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:18.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2443" for this suite. 08/24/23 12:12:18.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:18.108
Aug 24 12:12:18.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 12:12:18.109
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:18.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:18.176
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:12:18.207
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:12:19.756
STEP: Deploying the webhook pod 08/24/23 12:12:19.769
STEP: Wait for the deployment to be ready 08/24/23 12:12:19.795
Aug 24 12:12:19.830: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:12:21.842
STEP: Verifying the service has paired with the endpoint 08/24/23 12:12:21.873
Aug 24 12:12:22.874: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 24 12:12:22.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-514-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 12:12:23.423
STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 12:12:23.459
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:26.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6768" for this suite. 08/24/23 12:12:26.11
STEP: Destroying namespace "webhook-6768-markers" for this suite. 08/24/23 12:12:26.16
------------------------------
â€¢ [SLOW TEST] [8.076 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:18.108
    Aug 24 12:12:18.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 12:12:18.109
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:18.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:18.176
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:12:18.207
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:12:19.756
    STEP: Deploying the webhook pod 08/24/23 12:12:19.769
    STEP: Wait for the deployment to be ready 08/24/23 12:12:19.795
    Aug 24 12:12:19.830: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:12:21.842
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:12:21.873
    Aug 24 12:12:22.874: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 24 12:12:22.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-514-crds.webhook.example.com via the AdmissionRegistration API 08/24/23 12:12:23.423
    STEP: Creating a custom resource that should be mutated by the webhook 08/24/23 12:12:23.459
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:26.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6768" for this suite. 08/24/23 12:12:26.11
    STEP: Destroying namespace "webhook-6768-markers" for this suite. 08/24/23 12:12:26.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:26.184
Aug 24 12:12:26.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:12:26.185
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:26.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:26.29
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-5481/configmap-test-dba2ab36-6cb6-4b49-a67f-bed0c0df8ca2 08/24/23 12:12:26.318
STEP: Creating a pod to test consume configMaps 08/24/23 12:12:26.35
Aug 24 12:12:26.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a" in namespace "configmap-5481" to be "Succeeded or Failed"
Aug 24 12:12:26.378: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.412197ms
Aug 24 12:12:28.383: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017269095s
Aug 24 12:12:30.384: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01835516s
STEP: Saw pod success 08/24/23 12:12:30.384
Aug 24 12:12:30.385: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a" satisfied condition "Succeeded or Failed"
Aug 24 12:12:30.387: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a container env-test: <nil>
STEP: delete the pod 08/24/23 12:12:30.402
Aug 24 12:12:30.427: INFO: Waiting for pod pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a to disappear
Aug 24 12:12:30.431: INFO: Pod pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:30.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5481" for this suite. 08/24/23 12:12:30.437
------------------------------
â€¢ [4.272 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:26.184
    Aug 24 12:12:26.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:12:26.185
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:26.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:26.29
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-5481/configmap-test-dba2ab36-6cb6-4b49-a67f-bed0c0df8ca2 08/24/23 12:12:26.318
    STEP: Creating a pod to test consume configMaps 08/24/23 12:12:26.35
    Aug 24 12:12:26.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a" in namespace "configmap-5481" to be "Succeeded or Failed"
    Aug 24 12:12:26.378: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.412197ms
    Aug 24 12:12:28.383: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017269095s
    Aug 24 12:12:30.384: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01835516s
    STEP: Saw pod success 08/24/23 12:12:30.384
    Aug 24 12:12:30.385: INFO: Pod "pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a" satisfied condition "Succeeded or Failed"
    Aug 24 12:12:30.387: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a container env-test: <nil>
    STEP: delete the pod 08/24/23 12:12:30.402
    Aug 24 12:12:30.427: INFO: Waiting for pod pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a to disappear
    Aug 24 12:12:30.431: INFO: Pod pod-configmaps-90382a43-e225-437d-9647-924f0eeb6c8a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:30.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5481" for this suite. 08/24/23 12:12:30.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:30.457
Aug 24 12:12:30.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:12:30.459
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:30.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:30.487
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 12:12:30.495
Aug 24 12:12:30.512: INFO: Waiting up to 5m0s for pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93" in namespace "emptydir-4543" to be "Succeeded or Failed"
Aug 24 12:12:30.528: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93": Phase="Pending", Reason="", readiness=false. Elapsed: 15.886504ms
Aug 24 12:12:32.532: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020182378s
Aug 24 12:12:34.533: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020979998s
STEP: Saw pod success 08/24/23 12:12:34.533
Aug 24 12:12:34.533: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93" satisfied condition "Succeeded or Failed"
Aug 24 12:12:34.536: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-9163a62c-cf38-4a12-b015-e9c1b2495d93 container test-container: <nil>
STEP: delete the pod 08/24/23 12:12:34.542
Aug 24 12:12:34.568: INFO: Waiting for pod pod-9163a62c-cf38-4a12-b015-e9c1b2495d93 to disappear
Aug 24 12:12:34.574: INFO: Pod pod-9163a62c-cf38-4a12-b015-e9c1b2495d93 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:34.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4543" for this suite. 08/24/23 12:12:34.58
------------------------------
â€¢ [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:30.457
    Aug 24 12:12:30.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:12:30.459
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:30.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:30.487
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/24/23 12:12:30.495
    Aug 24 12:12:30.512: INFO: Waiting up to 5m0s for pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93" in namespace "emptydir-4543" to be "Succeeded or Failed"
    Aug 24 12:12:30.528: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93": Phase="Pending", Reason="", readiness=false. Elapsed: 15.886504ms
    Aug 24 12:12:32.532: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020182378s
    Aug 24 12:12:34.533: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020979998s
    STEP: Saw pod success 08/24/23 12:12:34.533
    Aug 24 12:12:34.533: INFO: Pod "pod-9163a62c-cf38-4a12-b015-e9c1b2495d93" satisfied condition "Succeeded or Failed"
    Aug 24 12:12:34.536: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-9163a62c-cf38-4a12-b015-e9c1b2495d93 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:12:34.542
    Aug 24 12:12:34.568: INFO: Waiting for pod pod-9163a62c-cf38-4a12-b015-e9c1b2495d93 to disappear
    Aug 24 12:12:34.574: INFO: Pod pod-9163a62c-cf38-4a12-b015-e9c1b2495d93 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:34.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4543" for this suite. 08/24/23 12:12:34.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:34.595
Aug 24 12:12:34.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename lease-test 08/24/23 12:12:34.596
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:34.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:34.622
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:34.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6816" for this suite. 08/24/23 12:12:34.711
------------------------------
â€¢ [0.124 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:34.595
    Aug 24 12:12:34.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename lease-test 08/24/23 12:12:34.596
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:34.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:34.622
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:34.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6816" for this suite. 08/24/23 12:12:34.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:34.728
Aug 24 12:12:34.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replication-controller 08/24/23 12:12:34.729
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:34.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:34.76
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 24 12:12:34.766: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/24/23 12:12:35.78
STEP: Checking rc "condition-test" has the desired failure condition set 08/24/23 12:12:35.791
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/24/23 12:12:36.814
Aug 24 12:12:36.875: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/24/23 12:12:36.875
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:36.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2414" for this suite. 08/24/23 12:12:36.886
------------------------------
â€¢ [2.168 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:34.728
    Aug 24 12:12:34.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replication-controller 08/24/23 12:12:34.729
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:34.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:34.76
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 24 12:12:34.766: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/24/23 12:12:35.78
    STEP: Checking rc "condition-test" has the desired failure condition set 08/24/23 12:12:35.791
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/24/23 12:12:36.814
    Aug 24 12:12:36.875: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/24/23 12:12:36.875
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:36.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2414" for this suite. 08/24/23 12:12:36.886
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:36.894
Aug 24 12:12:36.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:12:36.896
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:36.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:36.93
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 24 12:12:36.951: INFO: Waiting up to 2m0s for pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" in namespace "var-expansion-2273" to be "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:12:36.971: INFO: Pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.27401ms
Aug 24 12:12:38.975: INFO: Pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024476993s
Aug 24 12:12:38.975: INFO: Pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 24 12:12:38.975: INFO: Deleting pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" in namespace "var-expansion-2273"
Aug 24 12:12:38.988: INFO: Wait up to 5m0s for pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:40.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2273" for this suite. 08/24/23 12:12:41
------------------------------
â€¢ [4.116 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:36.894
    Aug 24 12:12:36.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:12:36.896
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:36.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:36.93
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 24 12:12:36.951: INFO: Waiting up to 2m0s for pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" in namespace "var-expansion-2273" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:12:36.971: INFO: Pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.27401ms
    Aug 24 12:12:38.975: INFO: Pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024476993s
    Aug 24 12:12:38.975: INFO: Pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 24 12:12:38.975: INFO: Deleting pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" in namespace "var-expansion-2273"
    Aug 24 12:12:38.988: INFO: Wait up to 5m0s for pod "var-expansion-4730f102-76c1-4467-b1f9-e3bb9c87f76d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:40.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2273" for this suite. 08/24/23 12:12:41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:41.012
Aug 24 12:12:41.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replicaset 08/24/23 12:12:41.014
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:41.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:41.043
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/24/23 12:12:41.049
Aug 24 12:12:41.061: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 24 12:12:46.065: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/24/23 12:12:46.066
STEP: getting scale subresource 08/24/23 12:12:46.066
STEP: updating a scale subresource 08/24/23 12:12:46.07
STEP: verifying the replicaset Spec.Replicas was modified 08/24/23 12:12:46.087
STEP: Patch a scale subresource 08/24/23 12:12:46.099
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:12:46.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4368" for this suite. 08/24/23 12:12:46.157
------------------------------
â€¢ [SLOW TEST] [5.161 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:41.012
    Aug 24 12:12:41.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replicaset 08/24/23 12:12:41.014
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:41.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:41.043
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/24/23 12:12:41.049
    Aug 24 12:12:41.061: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 24 12:12:46.065: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/24/23 12:12:46.066
    STEP: getting scale subresource 08/24/23 12:12:46.066
    STEP: updating a scale subresource 08/24/23 12:12:46.07
    STEP: verifying the replicaset Spec.Replicas was modified 08/24/23 12:12:46.087
    STEP: Patch a scale subresource 08/24/23 12:12:46.099
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:12:46.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4368" for this suite. 08/24/23 12:12:46.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:12:46.176
Aug 24 12:12:46.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename taint-single-pod 08/24/23 12:12:46.177
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:46.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:46.222
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 24 12:12:46.229: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 12:13:46.275: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 24 12:13:46.280: INFO: Starting informer...
STEP: Starting pod... 08/24/23 12:13:46.28
Aug 24 12:13:46.516: INFO: Pod is running on gitlab-1-26-36460-guscsyka22xa-node-2. Tainting Node
STEP: Trying to apply a taint on the Node 08/24/23 12:13:46.517
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 12:13:46.547
STEP: Waiting short time to make sure Pod is queued for deletion 08/24/23 12:13:46.551
Aug 24 12:13:46.552: INFO: Pod wasn't evicted. Proceeding
Aug 24 12:13:46.552: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 12:13:46.621
STEP: Waiting some time to make sure that toleration time passed. 08/24/23 12:13:46.727
Aug 24 12:15:01.728: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:01.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3845" for this suite. 08/24/23 12:15:01.745
------------------------------
â€¢ [SLOW TEST] [135.584 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:12:46.176
    Aug 24 12:12:46.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename taint-single-pod 08/24/23 12:12:46.177
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:12:46.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:12:46.222
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 24 12:12:46.229: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 12:13:46.275: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 24 12:13:46.280: INFO: Starting informer...
    STEP: Starting pod... 08/24/23 12:13:46.28
    Aug 24 12:13:46.516: INFO: Pod is running on gitlab-1-26-36460-guscsyka22xa-node-2. Tainting Node
    STEP: Trying to apply a taint on the Node 08/24/23 12:13:46.517
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 12:13:46.547
    STEP: Waiting short time to make sure Pod is queued for deletion 08/24/23 12:13:46.551
    Aug 24 12:13:46.552: INFO: Pod wasn't evicted. Proceeding
    Aug 24 12:13:46.552: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/24/23 12:13:46.621
    STEP: Waiting some time to make sure that toleration time passed. 08/24/23 12:13:46.727
    Aug 24 12:15:01.728: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:01.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3845" for this suite. 08/24/23 12:15:01.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:01.768
Aug 24 12:15:01.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replication-controller 08/24/23 12:15:01.769
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:01.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:01.8
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/24/23 12:15:01.807
Aug 24 12:15:01.825: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6189" to be "running and ready"
Aug 24 12:15:01.830: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.716838ms
Aug 24 12:15:01.830: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:15:03.833: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007921912s
Aug 24 12:15:03.833: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 24 12:15:03.833: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/24/23 12:15:03.835
STEP: Then the orphan pod is adopted 08/24/23 12:15:03.848
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:04.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6189" for this suite. 08/24/23 12:15:04.861
------------------------------
â€¢ [3.103 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:01.768
    Aug 24 12:15:01.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replication-controller 08/24/23 12:15:01.769
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:01.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:01.8
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/24/23 12:15:01.807
    Aug 24 12:15:01.825: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6189" to be "running and ready"
    Aug 24 12:15:01.830: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.716838ms
    Aug 24 12:15:01.830: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:15:03.833: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007921912s
    Aug 24 12:15:03.833: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 24 12:15:03.833: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/24/23 12:15:03.835
    STEP: Then the orphan pod is adopted 08/24/23 12:15:03.848
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:04.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6189" for this suite. 08/24/23 12:15:04.861
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:04.871
Aug 24 12:15:04.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:15:04.872
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:04.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:04.903
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-a9050810-eecb-4e51-8cc9-fe51bc578f7a 08/24/23 12:15:04.91
STEP: Creating a pod to test consume configMaps 08/24/23 12:15:04.918
Aug 24 12:15:04.935: INFO: Waiting up to 5m0s for pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1" in namespace "configmap-2844" to be "Succeeded or Failed"
Aug 24 12:15:04.943: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.342964ms
Aug 24 12:15:06.962: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027139361s
Aug 24 12:15:08.948: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013110811s
STEP: Saw pod success 08/24/23 12:15:08.948
Aug 24 12:15:08.949: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1" satisfied condition "Succeeded or Failed"
Aug 24 12:15:08.953: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:15:09.015
Aug 24 12:15:09.039: INFO: Waiting for pod pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1 to disappear
Aug 24 12:15:09.046: INFO: Pod pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:09.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2844" for this suite. 08/24/23 12:15:09.052
------------------------------
â€¢ [4.190 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:04.871
    Aug 24 12:15:04.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:15:04.872
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:04.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:04.903
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-a9050810-eecb-4e51-8cc9-fe51bc578f7a 08/24/23 12:15:04.91
    STEP: Creating a pod to test consume configMaps 08/24/23 12:15:04.918
    Aug 24 12:15:04.935: INFO: Waiting up to 5m0s for pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1" in namespace "configmap-2844" to be "Succeeded or Failed"
    Aug 24 12:15:04.943: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.342964ms
    Aug 24 12:15:06.962: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027139361s
    Aug 24 12:15:08.948: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013110811s
    STEP: Saw pod success 08/24/23 12:15:08.948
    Aug 24 12:15:08.949: INFO: Pod "pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1" satisfied condition "Succeeded or Failed"
    Aug 24 12:15:08.953: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:15:09.015
    Aug 24 12:15:09.039: INFO: Waiting for pod pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1 to disappear
    Aug 24 12:15:09.046: INFO: Pod pod-configmaps-2628e165-72dc-47dd-b671-bed6f898cdb1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:09.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2844" for this suite. 08/24/23 12:15:09.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:09.067
Aug 24 12:15:09.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 12:15:09.068
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:09.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:09.105
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 24 12:15:09.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:09.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9564" for this suite. 08/24/23 12:15:09.931
------------------------------
â€¢ [0.919 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:09.067
    Aug 24 12:15:09.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename custom-resource-definition 08/24/23 12:15:09.068
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:09.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:09.105
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 24 12:15:09.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:09.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9564" for this suite. 08/24/23 12:15:09.931
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:09.987
Aug 24 12:15:09.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 12:15:09.988
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:10.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:10.039
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:15:10.048
Aug 24 12:15:10.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f" in namespace "downward-api-6402" to be "Succeeded or Failed"
Aug 24 12:15:10.075: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.901333ms
Aug 24 12:15:12.080: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011727527s
Aug 24 12:15:14.098: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029867708s
STEP: Saw pod success 08/24/23 12:15:14.098
Aug 24 12:15:14.099: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f" satisfied condition "Succeeded or Failed"
Aug 24 12:15:14.105: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f container client-container: <nil>
STEP: delete the pod 08/24/23 12:15:14.174
Aug 24 12:15:14.205: INFO: Waiting for pod downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f to disappear
Aug 24 12:15:14.216: INFO: Pod downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:14.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6402" for this suite. 08/24/23 12:15:14.221
------------------------------
â€¢ [4.246 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:09.987
    Aug 24 12:15:09.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:15:09.988
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:10.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:10.039
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:15:10.048
    Aug 24 12:15:10.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f" in namespace "downward-api-6402" to be "Succeeded or Failed"
    Aug 24 12:15:10.075: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.901333ms
    Aug 24 12:15:12.080: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011727527s
    Aug 24 12:15:14.098: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029867708s
    STEP: Saw pod success 08/24/23 12:15:14.098
    Aug 24 12:15:14.099: INFO: Pod "downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f" satisfied condition "Succeeded or Failed"
    Aug 24 12:15:14.105: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f container client-container: <nil>
    STEP: delete the pod 08/24/23 12:15:14.174
    Aug 24 12:15:14.205: INFO: Waiting for pod downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f to disappear
    Aug 24 12:15:14.216: INFO: Pod downwardapi-volume-55900859-e71d-4b7b-9aa0-48390903549f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:14.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6402" for this suite. 08/24/23 12:15:14.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:14.235
Aug 24 12:15:14.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename endpointslice 08/24/23 12:15:14.236
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:14.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:14.27
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 24 12:15:14.297: INFO: Endpoints addresses: [10.0.0.15] , ports: [6443]
Aug 24 12:15:14.297: INFO: EndpointSlices addresses: [10.0.0.15] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:14.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1683" for this suite. 08/24/23 12:15:14.316
------------------------------
â€¢ [0.093 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:14.235
    Aug 24 12:15:14.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename endpointslice 08/24/23 12:15:14.236
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:14.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:14.27
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 24 12:15:14.297: INFO: Endpoints addresses: [10.0.0.15] , ports: [6443]
    Aug 24 12:15:14.297: INFO: EndpointSlices addresses: [10.0.0.15] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:14.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1683" for this suite. 08/24/23 12:15:14.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:14.329
Aug 24 12:15:14.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename watch 08/24/23 12:15:14.33
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:14.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:14.368
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/24/23 12:15:14.375
STEP: modifying the configmap once 08/24/23 12:15:14.383
STEP: modifying the configmap a second time 08/24/23 12:15:14.399
STEP: deleting the configmap 08/24/23 12:15:14.408
STEP: creating a watch on configmaps from the resource version returned by the first update 08/24/23 12:15:14.429
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/24/23 12:15:14.434
Aug 24 12:15:14.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-637  65f4cdbf-03c5-474a-a105-d13044308f66 42380 0 2023-08-24 12:15:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 12:15:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:15:14.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-637  65f4cdbf-03c5-474a-a105-d13044308f66 42382 0 2023-08-24 12:15:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 12:15:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:14.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-637" for this suite. 08/24/23 12:15:14.44
------------------------------
â€¢ [0.122 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:14.329
    Aug 24 12:15:14.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename watch 08/24/23 12:15:14.33
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:14.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:14.368
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/24/23 12:15:14.375
    STEP: modifying the configmap once 08/24/23 12:15:14.383
    STEP: modifying the configmap a second time 08/24/23 12:15:14.399
    STEP: deleting the configmap 08/24/23 12:15:14.408
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/24/23 12:15:14.429
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/24/23 12:15:14.434
    Aug 24 12:15:14.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-637  65f4cdbf-03c5-474a-a105-d13044308f66 42380 0 2023-08-24 12:15:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 12:15:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:15:14.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-637  65f4cdbf-03c5-474a-a105-d13044308f66 42382 0 2023-08-24 12:15:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 12:15:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:14.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-637" for this suite. 08/24/23 12:15:14.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:14.459
Aug 24 12:15:14.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename dns 08/24/23 12:15:14.46
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:14.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:14.497
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/24/23 12:15:14.503
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/24/23 12:15:14.503
STEP: creating a pod to probe DNS 08/24/23 12:15:14.503
STEP: submitting the pod to kubernetes 08/24/23 12:15:14.504
Aug 24 12:15:14.514: INFO: Waiting up to 15m0s for pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691" in namespace "dns-9245" to be "running"
Aug 24 12:15:14.523: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691": Phase="Pending", Reason="", readiness=false. Elapsed: 8.653883ms
Aug 24 12:15:16.528: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013584766s
Aug 24 12:15:18.527: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691": Phase="Running", Reason="", readiness=true. Elapsed: 4.012846894s
Aug 24 12:15:18.527: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691" satisfied condition "running"
STEP: retrieving the pod 08/24/23 12:15:18.527
STEP: looking for the results for each expected name from probers 08/24/23 12:15:18.53
Aug 24 12:15:18.547: INFO: DNS probes using dns-9245/dns-test-75110cd1-a47a-42aa-8b36-0c375f186691 succeeded

STEP: deleting the pod 08/24/23 12:15:18.547
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:18.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9245" for this suite. 08/24/23 12:15:18.58
------------------------------
â€¢ [4.130 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:14.459
    Aug 24 12:15:14.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename dns 08/24/23 12:15:14.46
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:14.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:14.497
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/24/23 12:15:14.503
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/24/23 12:15:14.503
    STEP: creating a pod to probe DNS 08/24/23 12:15:14.503
    STEP: submitting the pod to kubernetes 08/24/23 12:15:14.504
    Aug 24 12:15:14.514: INFO: Waiting up to 15m0s for pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691" in namespace "dns-9245" to be "running"
    Aug 24 12:15:14.523: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691": Phase="Pending", Reason="", readiness=false. Elapsed: 8.653883ms
    Aug 24 12:15:16.528: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013584766s
    Aug 24 12:15:18.527: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691": Phase="Running", Reason="", readiness=true. Elapsed: 4.012846894s
    Aug 24 12:15:18.527: INFO: Pod "dns-test-75110cd1-a47a-42aa-8b36-0c375f186691" satisfied condition "running"
    STEP: retrieving the pod 08/24/23 12:15:18.527
    STEP: looking for the results for each expected name from probers 08/24/23 12:15:18.53
    Aug 24 12:15:18.547: INFO: DNS probes using dns-9245/dns-test-75110cd1-a47a-42aa-8b36-0c375f186691 succeeded

    STEP: deleting the pod 08/24/23 12:15:18.547
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:18.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9245" for this suite. 08/24/23 12:15:18.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:18.593
Aug 24 12:15:18.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 12:15:18.594
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:18.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:18.622
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/24/23 12:15:18.628
STEP: submitting the pod to kubernetes 08/24/23 12:15:18.628
Aug 24 12:15:18.644: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" in namespace "pods-6322" to be "running and ready"
Aug 24 12:15:18.647: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707866ms
Aug 24 12:15:18.647: INFO: The phase of Pod pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:15:20.653: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=true. Elapsed: 2.009456111s
Aug 24 12:15:20.653: INFO: The phase of Pod pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380 is Running (Ready = true)
Aug 24 12:15:20.653: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/24/23 12:15:20.656
STEP: updating the pod 08/24/23 12:15:20.662
Aug 24 12:15:21.196: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380"
Aug 24 12:15:21.196: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" in namespace "pods-6322" to be "terminated with reason DeadlineExceeded"
Aug 24 12:15:21.201: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=true. Elapsed: 4.718052ms
Aug 24 12:15:23.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=true. Elapsed: 2.009783382s
Aug 24 12:15:25.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=false. Elapsed: 4.009653798s
Aug 24 12:15:27.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009941404s
Aug 24 12:15:27.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:27.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6322" for this suite. 08/24/23 12:15:27.212
------------------------------
â€¢ [SLOW TEST] [8.637 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:18.593
    Aug 24 12:15:18.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 12:15:18.594
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:18.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:18.622
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/24/23 12:15:18.628
    STEP: submitting the pod to kubernetes 08/24/23 12:15:18.628
    Aug 24 12:15:18.644: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" in namespace "pods-6322" to be "running and ready"
    Aug 24 12:15:18.647: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707866ms
    Aug 24 12:15:18.647: INFO: The phase of Pod pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:15:20.653: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=true. Elapsed: 2.009456111s
    Aug 24 12:15:20.653: INFO: The phase of Pod pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380 is Running (Ready = true)
    Aug 24 12:15:20.653: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/24/23 12:15:20.656
    STEP: updating the pod 08/24/23 12:15:20.662
    Aug 24 12:15:21.196: INFO: Successfully updated pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380"
    Aug 24 12:15:21.196: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" in namespace "pods-6322" to be "terminated with reason DeadlineExceeded"
    Aug 24 12:15:21.201: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=true. Elapsed: 4.718052ms
    Aug 24 12:15:23.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=true. Elapsed: 2.009783382s
    Aug 24 12:15:25.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Running", Reason="", readiness=false. Elapsed: 4.009653798s
    Aug 24 12:15:27.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009941404s
    Aug 24 12:15:27.206: INFO: Pod "pod-update-activedeadlineseconds-cf41d136-5131-4630-9faf-ec32e6a73380" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:27.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6322" for this suite. 08/24/23 12:15:27.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:27.23
Aug 24 12:15:27.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:15:27.232
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:27.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:27.259
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/24/23 12:15:27.266
STEP: Getting a ResourceQuota 08/24/23 12:15:27.277
STEP: Listing all ResourceQuotas with LabelSelector 08/24/23 12:15:27.283
STEP: Patching the ResourceQuota 08/24/23 12:15:27.287
STEP: Deleting a Collection of ResourceQuotas 08/24/23 12:15:27.297
STEP: Verifying the deleted ResourceQuota 08/24/23 12:15:27.307
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-902" for this suite. 08/24/23 12:15:27.317
------------------------------
â€¢ [0.098 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:27.23
    Aug 24 12:15:27.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:15:27.232
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:27.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:27.259
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/24/23 12:15:27.266
    STEP: Getting a ResourceQuota 08/24/23 12:15:27.277
    STEP: Listing all ResourceQuotas with LabelSelector 08/24/23 12:15:27.283
    STEP: Patching the ResourceQuota 08/24/23 12:15:27.287
    STEP: Deleting a Collection of ResourceQuotas 08/24/23 12:15:27.297
    STEP: Verifying the deleted ResourceQuota 08/24/23 12:15:27.307
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-902" for this suite. 08/24/23 12:15:27.317
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:27.329
Aug 24 12:15:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 12:15:27.331
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:27.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:27.367
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-1414 08/24/23 12:15:27.375
STEP: creating service affinity-nodeport in namespace services-1414 08/24/23 12:15:27.376
STEP: creating replication controller affinity-nodeport in namespace services-1414 08/24/23 12:15:27.416
I0824 12:15:27.439641      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1414, replica count: 3
I0824 12:15:30.490971      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:15:30.503: INFO: Creating new exec pod
Aug 24 12:15:30.514: INFO: Waiting up to 5m0s for pod "execpod-affinitym6x7v" in namespace "services-1414" to be "running"
Aug 24 12:15:30.523: INFO: Pod "execpod-affinitym6x7v": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996251ms
Aug 24 12:15:32.530: INFO: Pod "execpod-affinitym6x7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.016084883s
Aug 24 12:15:32.530: INFO: Pod "execpod-affinitym6x7v" satisfied condition "running"
Aug 24 12:15:33.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 24 12:15:33.815: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 24 12:15:33.815: INFO: stdout: ""
Aug 24 12:15:33.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 10.254.2.66 80'
Aug 24 12:15:34.067: INFO: stderr: "+ nc -v -z -w 2 10.254.2.66 80\nConnection to 10.254.2.66 80 port [tcp/http] succeeded!\n"
Aug 24 12:15:34.067: INFO: stdout: ""
Aug 24 12:15:34.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 30222'
Aug 24 12:15:34.361: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 30222\nConnection to 10.0.0.4 30222 port [tcp/*] succeeded!\n"
Aug 24 12:15:34.361: INFO: stdout: ""
Aug 24 12:15:34.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 10.0.0.17 30222'
Aug 24 12:15:34.602: INFO: stderr: "+ nc -v -z -w 2 10.0.0.17 30222\nConnection to 10.0.0.17 30222 port [tcp/*] succeeded!\n"
Aug 24 12:15:34.602: INFO: stdout: ""
Aug 24 12:15:34.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:30222/ ; done'
Aug 24 12:15:34.960: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n"
Aug 24 12:15:34.960: INFO: stdout: "\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv"
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
Aug 24 12:15:34.960: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1414, will wait for the garbage collector to delete the pods 08/24/23 12:15:34.983
Aug 24 12:15:35.056: INFO: Deleting ReplicationController affinity-nodeport took: 12.988317ms
Aug 24 12:15:35.157: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.717177ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:37.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1414" for this suite. 08/24/23 12:15:37.6
------------------------------
â€¢ [SLOW TEST] [10.285 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:27.329
    Aug 24 12:15:27.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 12:15:27.331
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:27.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:27.367
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-1414 08/24/23 12:15:27.375
    STEP: creating service affinity-nodeport in namespace services-1414 08/24/23 12:15:27.376
    STEP: creating replication controller affinity-nodeport in namespace services-1414 08/24/23 12:15:27.416
    I0824 12:15:27.439641      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1414, replica count: 3
    I0824 12:15:30.490971      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:15:30.503: INFO: Creating new exec pod
    Aug 24 12:15:30.514: INFO: Waiting up to 5m0s for pod "execpod-affinitym6x7v" in namespace "services-1414" to be "running"
    Aug 24 12:15:30.523: INFO: Pod "execpod-affinitym6x7v": Phase="Pending", Reason="", readiness=false. Elapsed: 8.996251ms
    Aug 24 12:15:32.530: INFO: Pod "execpod-affinitym6x7v": Phase="Running", Reason="", readiness=true. Elapsed: 2.016084883s
    Aug 24 12:15:32.530: INFO: Pod "execpod-affinitym6x7v" satisfied condition "running"
    Aug 24 12:15:33.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 24 12:15:33.815: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 24 12:15:33.815: INFO: stdout: ""
    Aug 24 12:15:33.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 10.254.2.66 80'
    Aug 24 12:15:34.067: INFO: stderr: "+ nc -v -z -w 2 10.254.2.66 80\nConnection to 10.254.2.66 80 port [tcp/http] succeeded!\n"
    Aug 24 12:15:34.067: INFO: stdout: ""
    Aug 24 12:15:34.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 30222'
    Aug 24 12:15:34.361: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 30222\nConnection to 10.0.0.4 30222 port [tcp/*] succeeded!\n"
    Aug 24 12:15:34.361: INFO: stdout: ""
    Aug 24 12:15:34.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c nc -v -z -w 2 10.0.0.17 30222'
    Aug 24 12:15:34.602: INFO: stderr: "+ nc -v -z -w 2 10.0.0.17 30222\nConnection to 10.0.0.17 30222 port [tcp/*] succeeded!\n"
    Aug 24 12:15:34.602: INFO: stdout: ""
    Aug 24 12:15:34.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-1414 exec execpod-affinitym6x7v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:30222/ ; done'
    Aug 24 12:15:34.960: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30222/\n"
    Aug 24 12:15:34.960: INFO: stdout: "\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv\naffinity-nodeport-gmgqv"
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Received response from host: affinity-nodeport-gmgqv
    Aug 24 12:15:34.960: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-1414, will wait for the garbage collector to delete the pods 08/24/23 12:15:34.983
    Aug 24 12:15:35.056: INFO: Deleting ReplicationController affinity-nodeport took: 12.988317ms
    Aug 24 12:15:35.157: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.717177ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:37.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1414" for this suite. 08/24/23 12:15:37.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:37.616
Aug 24 12:15:37.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:15:37.617
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:37.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:37.647
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/24/23 12:15:37.653
Aug 24 12:15:37.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/24/23 12:15:46.977
Aug 24 12:15:46.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:15:49.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:15:58.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-572" for this suite. 08/24/23 12:15:58.156
------------------------------
â€¢ [SLOW TEST] [20.551 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:37.616
    Aug 24 12:15:37.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename crd-publish-openapi 08/24/23 12:15:37.617
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:37.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:37.647
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/24/23 12:15:37.653
    Aug 24 12:15:37.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/24/23 12:15:46.977
    Aug 24 12:15:46.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:15:49.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:15:58.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-572" for this suite. 08/24/23 12:15:58.156
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:15:58.167
Aug 24 12:15:58.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:15:58.168
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:58.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:58.191
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/24/23 12:15:58.199
Aug 24 12:15:58.209: INFO: Waiting up to 5m0s for pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2" in namespace "emptydir-3178" to be "Succeeded or Failed"
Aug 24 12:15:58.215: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81569ms
Aug 24 12:16:00.219: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010537184s
Aug 24 12:16:02.219: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010398593s
STEP: Saw pod success 08/24/23 12:16:02.219
Aug 24 12:16:02.219: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2" satisfied condition "Succeeded or Failed"
Aug 24 12:16:02.224: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-2121f4ba-e932-460b-9574-cb86a7920fd2 container test-container: <nil>
STEP: delete the pod 08/24/23 12:16:02.235
Aug 24 12:16:02.258: INFO: Waiting for pod pod-2121f4ba-e932-460b-9574-cb86a7920fd2 to disappear
Aug 24 12:16:02.264: INFO: Pod pod-2121f4ba-e932-460b-9574-cb86a7920fd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:02.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3178" for this suite. 08/24/23 12:16:02.27
------------------------------
â€¢ [4.116 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:15:58.167
    Aug 24 12:15:58.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:15:58.168
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:15:58.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:15:58.191
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/24/23 12:15:58.199
    Aug 24 12:15:58.209: INFO: Waiting up to 5m0s for pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2" in namespace "emptydir-3178" to be "Succeeded or Failed"
    Aug 24 12:15:58.215: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.81569ms
    Aug 24 12:16:00.219: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010537184s
    Aug 24 12:16:02.219: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010398593s
    STEP: Saw pod success 08/24/23 12:16:02.219
    Aug 24 12:16:02.219: INFO: Pod "pod-2121f4ba-e932-460b-9574-cb86a7920fd2" satisfied condition "Succeeded or Failed"
    Aug 24 12:16:02.224: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-2121f4ba-e932-460b-9574-cb86a7920fd2 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:16:02.235
    Aug 24 12:16:02.258: INFO: Waiting for pod pod-2121f4ba-e932-460b-9574-cb86a7920fd2 to disappear
    Aug 24 12:16:02.264: INFO: Pod pod-2121f4ba-e932-460b-9574-cb86a7920fd2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:02.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3178" for this suite. 08/24/23 12:16:02.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:02.289
Aug 24 12:16:02.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 12:16:02.29
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:02.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:02.323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:16:02.353
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:16:02.768
STEP: Deploying the webhook pod 08/24/23 12:16:02.78
STEP: Wait for the deployment to be ready 08/24/23 12:16:02.802
Aug 24 12:16:02.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:16:04.846
STEP: Verifying the service has paired with the endpoint 08/24/23 12:16:04.863
Aug 24 12:16:05.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 12:16:05.869
STEP: create a pod 08/24/23 12:16:05.894
Aug 24 12:16:05.902: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6820" to be "running"
Aug 24 12:16:05.911: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.651245ms
Aug 24 12:16:07.915: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013353724s
Aug 24 12:16:07.915: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/24/23 12:16:07.915
Aug 24 12:16:07.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=webhook-6820 attach --namespace=webhook-6820 to-be-attached-pod -i -c=container1'
Aug 24 12:16:08.085: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:08.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6820" for this suite. 08/24/23 12:16:08.212
STEP: Destroying namespace "webhook-6820-markers" for this suite. 08/24/23 12:16:08.233
------------------------------
â€¢ [SLOW TEST] [5.972 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:02.289
    Aug 24 12:16:02.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 12:16:02.29
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:02.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:02.323
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:16:02.353
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:16:02.768
    STEP: Deploying the webhook pod 08/24/23 12:16:02.78
    STEP: Wait for the deployment to be ready 08/24/23 12:16:02.802
    Aug 24 12:16:02.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:16:04.846
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:16:04.863
    Aug 24 12:16:05.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/24/23 12:16:05.869
    STEP: create a pod 08/24/23 12:16:05.894
    Aug 24 12:16:05.902: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-6820" to be "running"
    Aug 24 12:16:05.911: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.651245ms
    Aug 24 12:16:07.915: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013353724s
    Aug 24 12:16:07.915: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/24/23 12:16:07.915
    Aug 24 12:16:07.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=webhook-6820 attach --namespace=webhook-6820 to-be-attached-pod -i -c=container1'
    Aug 24 12:16:08.085: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:08.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6820" for this suite. 08/24/23 12:16:08.212
    STEP: Destroying namespace "webhook-6820-markers" for this suite. 08/24/23 12:16:08.233
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:08.261
Aug 24 12:16:08.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename projected 08/24/23 12:16:08.263
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:08.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:08.362
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-5e051166-0a20-478a-b7f0-b11281c190f0 08/24/23 12:16:08.374
STEP: Creating a pod to test consume secrets 08/24/23 12:16:08.394
Aug 24 12:16:08.418: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf" in namespace "projected-6233" to be "Succeeded or Failed"
Aug 24 12:16:08.445: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf": Phase="Pending", Reason="", readiness=false. Elapsed: 27.275729ms
Aug 24 12:16:10.450: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032348094s
Aug 24 12:16:12.450: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031585212s
STEP: Saw pod success 08/24/23 12:16:12.45
Aug 24 12:16:12.463: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf" satisfied condition "Succeeded or Failed"
Aug 24 12:16:12.467: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf container projected-secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:16:12.481
Aug 24 12:16:12.504: INFO: Waiting for pod pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf to disappear
Aug 24 12:16:12.517: INFO: Pod pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:12.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6233" for this suite. 08/24/23 12:16:12.523
------------------------------
â€¢ [4.273 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:08.261
    Aug 24 12:16:08.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename projected 08/24/23 12:16:08.263
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:08.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:08.362
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-5e051166-0a20-478a-b7f0-b11281c190f0 08/24/23 12:16:08.374
    STEP: Creating a pod to test consume secrets 08/24/23 12:16:08.394
    Aug 24 12:16:08.418: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf" in namespace "projected-6233" to be "Succeeded or Failed"
    Aug 24 12:16:08.445: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf": Phase="Pending", Reason="", readiness=false. Elapsed: 27.275729ms
    Aug 24 12:16:10.450: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032348094s
    Aug 24 12:16:12.450: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031585212s
    STEP: Saw pod success 08/24/23 12:16:12.45
    Aug 24 12:16:12.463: INFO: Pod "pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf" satisfied condition "Succeeded or Failed"
    Aug 24 12:16:12.467: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:16:12.481
    Aug 24 12:16:12.504: INFO: Waiting for pod pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf to disappear
    Aug 24 12:16:12.517: INFO: Pod pod-projected-secrets-7c5ed841-3d4d-485c-9bfb-ccf0fc5921bf no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:12.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6233" for this suite. 08/24/23 12:16:12.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:12.538
Aug 24 12:16:12.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 12:16:12.54
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:12.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:12.578
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 24 12:16:12.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3178 version'
Aug 24 12:16:12.706: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 24 12:16:12.706: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:27Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:16:45Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:12.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3178" for this suite. 08/24/23 12:16:12.713
------------------------------
â€¢ [0.185 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:12.538
    Aug 24 12:16:12.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:16:12.54
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:12.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:12.578
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 24 12:16:12.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3178 version'
    Aug 24 12:16:12.706: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 24 12:16:12.706: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:23:27Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.7\", GitCommit:\"84e1fc493a47446df2e155e70fca768d2653a398\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:16:45Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:12.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3178" for this suite. 08/24/23 12:16:12.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:12.724
Aug 24 12:16:12.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:16:12.726
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:12.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:12.768
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:16:12.775
Aug 24 12:16:12.793: INFO: Waiting up to 5m0s for pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465" in namespace "emptydir-1745" to be "Succeeded or Failed"
Aug 24 12:16:12.800: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465": Phase="Pending", Reason="", readiness=false. Elapsed: 7.78988ms
Aug 24 12:16:14.805: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012730855s
Aug 24 12:16:16.804: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011771918s
STEP: Saw pod success 08/24/23 12:16:16.804
Aug 24 12:16:16.805: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465" satisfied condition "Succeeded or Failed"
Aug 24 12:16:16.807: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-2abf0cec-dfd4-4303-a0b7-37292fc78465 container test-container: <nil>
STEP: delete the pod 08/24/23 12:16:16.814
Aug 24 12:16:16.835: INFO: Waiting for pod pod-2abf0cec-dfd4-4303-a0b7-37292fc78465 to disappear
Aug 24 12:16:16.840: INFO: Pod pod-2abf0cec-dfd4-4303-a0b7-37292fc78465 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:16.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1745" for this suite. 08/24/23 12:16:16.845
------------------------------
â€¢ [4.129 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:12.724
    Aug 24 12:16:12.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:16:12.726
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:12.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:12.768
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/24/23 12:16:12.775
    Aug 24 12:16:12.793: INFO: Waiting up to 5m0s for pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465" in namespace "emptydir-1745" to be "Succeeded or Failed"
    Aug 24 12:16:12.800: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465": Phase="Pending", Reason="", readiness=false. Elapsed: 7.78988ms
    Aug 24 12:16:14.805: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012730855s
    Aug 24 12:16:16.804: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011771918s
    STEP: Saw pod success 08/24/23 12:16:16.804
    Aug 24 12:16:16.805: INFO: Pod "pod-2abf0cec-dfd4-4303-a0b7-37292fc78465" satisfied condition "Succeeded or Failed"
    Aug 24 12:16:16.807: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-2abf0cec-dfd4-4303-a0b7-37292fc78465 container test-container: <nil>
    STEP: delete the pod 08/24/23 12:16:16.814
    Aug 24 12:16:16.835: INFO: Waiting for pod pod-2abf0cec-dfd4-4303-a0b7-37292fc78465 to disappear
    Aug 24 12:16:16.840: INFO: Pod pod-2abf0cec-dfd4-4303-a0b7-37292fc78465 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:16.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1745" for this suite. 08/24/23 12:16:16.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:16.854
Aug 24 12:16:16.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:16:16.855
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:16.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:16.881
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-d22e2014-e747-49ad-971c-49d6bc29b491 08/24/23 12:16:16.888
STEP: Creating a pod to test consume configMaps 08/24/23 12:16:16.896
Aug 24 12:16:16.911: INFO: Waiting up to 5m0s for pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3" in namespace "configmap-4465" to be "Succeeded or Failed"
Aug 24 12:16:16.923: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.657934ms
Aug 24 12:16:18.927: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016488325s
Aug 24 12:16:20.927: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016078025s
STEP: Saw pod success 08/24/23 12:16:20.927
Aug 24 12:16:20.927: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3" satisfied condition "Succeeded or Failed"
Aug 24 12:16:20.930: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3 container configmap-volume-test: <nil>
STEP: delete the pod 08/24/23 12:16:20.938
Aug 24 12:16:20.959: INFO: Waiting for pod pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3 to disappear
Aug 24 12:16:20.970: INFO: Pod pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:20.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4465" for this suite. 08/24/23 12:16:20.977
------------------------------
â€¢ [4.141 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:16.854
    Aug 24 12:16:16.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:16:16.855
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:16.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:16.881
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-d22e2014-e747-49ad-971c-49d6bc29b491 08/24/23 12:16:16.888
    STEP: Creating a pod to test consume configMaps 08/24/23 12:16:16.896
    Aug 24 12:16:16.911: INFO: Waiting up to 5m0s for pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3" in namespace "configmap-4465" to be "Succeeded or Failed"
    Aug 24 12:16:16.923: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.657934ms
    Aug 24 12:16:18.927: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016488325s
    Aug 24 12:16:20.927: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016078025s
    STEP: Saw pod success 08/24/23 12:16:20.927
    Aug 24 12:16:20.927: INFO: Pod "pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3" satisfied condition "Succeeded or Failed"
    Aug 24 12:16:20.930: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3 container configmap-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:16:20.938
    Aug 24 12:16:20.959: INFO: Waiting for pod pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3 to disappear
    Aug 24 12:16:20.970: INFO: Pod pod-configmaps-04a6f4a5-3f64-4950-b8e8-503deb13afc3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:20.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4465" for this suite. 08/24/23 12:16:20.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:21
Aug 24 12:16:21.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:16:21.002
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:21.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:21.033
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:16:21.051
Aug 24 12:16:21.070: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8055" to be "running and ready"
Aug 24 12:16:21.074: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.633511ms
Aug 24 12:16:21.074: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:16:23.079: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009753408s
Aug 24 12:16:23.080: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 24 12:16:23.080: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/24/23 12:16:23.083
Aug 24 12:16:23.093: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8055" to be "running and ready"
Aug 24 12:16:23.098: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.971278ms
Aug 24 12:16:23.098: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:16:25.104: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011242347s
Aug 24 12:16:25.104: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 24 12:16:25.104: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/24/23 12:16:25.108
Aug 24 12:16:25.137: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 24 12:16:25.143: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 24 12:16:27.144: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 24 12:16:27.149: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 24 12:16:29.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 24 12:16:29.149: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/24/23 12:16:29.149
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:29.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8055" for this suite. 08/24/23 12:16:29.162
------------------------------
â€¢ [SLOW TEST] [8.171 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:21
    Aug 24 12:16:21.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/24/23 12:16:21.002
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:21.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:21.033
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/24/23 12:16:21.051
    Aug 24 12:16:21.070: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8055" to be "running and ready"
    Aug 24 12:16:21.074: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.633511ms
    Aug 24 12:16:21.074: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:16:23.079: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009753408s
    Aug 24 12:16:23.080: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 24 12:16:23.080: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/24/23 12:16:23.083
    Aug 24 12:16:23.093: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8055" to be "running and ready"
    Aug 24 12:16:23.098: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.971278ms
    Aug 24 12:16:23.098: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:16:25.104: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011242347s
    Aug 24 12:16:25.104: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 24 12:16:25.104: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/24/23 12:16:25.108
    Aug 24 12:16:25.137: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 24 12:16:25.143: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 24 12:16:27.144: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 24 12:16:27.149: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 24 12:16:29.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 24 12:16:29.149: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/24/23 12:16:29.149
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:29.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8055" for this suite. 08/24/23 12:16:29.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:29.174
Aug 24 12:16:29.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 12:16:29.176
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:29.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:29.207
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:16:29.236
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:16:30.51
STEP: Deploying the webhook pod 08/24/23 12:16:30.522
STEP: Wait for the deployment to be ready 08/24/23 12:16:30.543
Aug 24 12:16:30.561: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:16:32.572
STEP: Verifying the service has paired with the endpoint 08/24/23 12:16:32.592
Aug 24 12:16:33.593: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/24/23 12:16:33.598
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:16:33.624
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/24/23 12:16:33.636
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:16:33.652
STEP: Patching a validating webhook configuration's rules to include the create operation 08/24/23 12:16:33.669
STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:16:33.687
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9720" for this suite. 08/24/23 12:16:33.894
STEP: Destroying namespace "webhook-9720-markers" for this suite. 08/24/23 12:16:33.915
------------------------------
â€¢ [4.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:29.174
    Aug 24 12:16:29.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 12:16:29.176
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:29.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:29.207
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:16:29.236
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:16:30.51
    STEP: Deploying the webhook pod 08/24/23 12:16:30.522
    STEP: Wait for the deployment to be ready 08/24/23 12:16:30.543
    Aug 24 12:16:30.561: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:16:32.572
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:16:32.592
    Aug 24 12:16:33.593: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/24/23 12:16:33.598
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:16:33.624
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/24/23 12:16:33.636
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:16:33.652
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/24/23 12:16:33.669
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/24/23 12:16:33.687
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:33.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9720" for this suite. 08/24/23 12:16:33.894
    STEP: Destroying namespace "webhook-9720-markers" for this suite. 08/24/23 12:16:33.915
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:33.933
Aug 24 12:16:33.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 12:16:33.934
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:33.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:33.966
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/24/23 12:16:33.977
Aug 24 12:16:33.998: INFO: Waiting up to 5m0s for pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff" in namespace "downward-api-5698" to be "running and ready"
Aug 24 12:16:34.012: INFO: Pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.993776ms
Aug 24 12:16:34.012: INFO: The phase of Pod labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:16:36.017: INFO: Pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff": Phase="Running", Reason="", readiness=true. Elapsed: 2.019003307s
Aug 24 12:16:36.017: INFO: The phase of Pod labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff is Running (Ready = true)
Aug 24 12:16:36.017: INFO: Pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff" satisfied condition "running and ready"
Aug 24 12:16:36.548: INFO: Successfully updated pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5698" for this suite. 08/24/23 12:16:40.578
------------------------------
â€¢ [SLOW TEST] [6.653 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:33.933
    Aug 24 12:16:33.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:16:33.934
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:33.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:33.966
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/24/23 12:16:33.977
    Aug 24 12:16:33.998: INFO: Waiting up to 5m0s for pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff" in namespace "downward-api-5698" to be "running and ready"
    Aug 24 12:16:34.012: INFO: Pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.993776ms
    Aug 24 12:16:34.012: INFO: The phase of Pod labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:16:36.017: INFO: Pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff": Phase="Running", Reason="", readiness=true. Elapsed: 2.019003307s
    Aug 24 12:16:36.017: INFO: The phase of Pod labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff is Running (Ready = true)
    Aug 24 12:16:36.017: INFO: Pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff" satisfied condition "running and ready"
    Aug 24 12:16:36.548: INFO: Successfully updated pod "labelsupdate4bbf57f6-18e6-42d6-9c85-07221743eaff"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5698" for this suite. 08/24/23 12:16:40.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:40.589
Aug 24 12:16:40.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:16:40.59
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:40.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:40.614
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/24/23 12:16:40.637
Aug 24 12:16:40.637: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe" in namespace "kubelet-test-8758" to be "completed"
Aug 24 12:16:40.643: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944007ms
Aug 24 12:16:42.647: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010544065s
Aug 24 12:16:44.648: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011117091s
Aug 24 12:16:44.648: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:44.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8758" for this suite. 08/24/23 12:16:44.666
------------------------------
â€¢ [4.086 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:40.589
    Aug 24 12:16:40.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:16:40.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:40.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:40.614
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/24/23 12:16:40.637
    Aug 24 12:16:40.637: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe" in namespace "kubelet-test-8758" to be "completed"
    Aug 24 12:16:40.643: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944007ms
    Aug 24 12:16:42.647: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010544065s
    Aug 24 12:16:44.648: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011117091s
    Aug 24 12:16:44.648: INFO: Pod "agnhost-host-aliases4e700957-fb02-494a-8c0f-264dc9ca8dbe" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:44.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8758" for this suite. 08/24/23 12:16:44.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:44.676
Aug 24 12:16:44.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 12:16:44.677
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:44.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:44.706
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/24/23 12:16:44.713
Aug 24 12:16:44.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7" in namespace "downward-api-9403" to be "Succeeded or Failed"
Aug 24 12:16:44.749: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.977891ms
Aug 24 12:16:46.769: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037375786s
Aug 24 12:16:48.754: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022454987s
STEP: Saw pod success 08/24/23 12:16:48.754
Aug 24 12:16:48.754: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7" satisfied condition "Succeeded or Failed"
Aug 24 12:16:48.758: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7 container client-container: <nil>
STEP: delete the pod 08/24/23 12:16:48.768
Aug 24 12:16:48.792: INFO: Waiting for pod downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7 to disappear
Aug 24 12:16:48.798: INFO: Pod downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:48.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9403" for this suite. 08/24/23 12:16:48.804
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:44.676
    Aug 24 12:16:44.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:16:44.677
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:44.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:44.706
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/24/23 12:16:44.713
    Aug 24 12:16:44.732: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7" in namespace "downward-api-9403" to be "Succeeded or Failed"
    Aug 24 12:16:44.749: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.977891ms
    Aug 24 12:16:46.769: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037375786s
    Aug 24 12:16:48.754: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022454987s
    STEP: Saw pod success 08/24/23 12:16:48.754
    Aug 24 12:16:48.754: INFO: Pod "downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7" satisfied condition "Succeeded or Failed"
    Aug 24 12:16:48.758: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7 container client-container: <nil>
    STEP: delete the pod 08/24/23 12:16:48.768
    Aug 24 12:16:48.792: INFO: Waiting for pod downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7 to disappear
    Aug 24 12:16:48.798: INFO: Pod downwardapi-volume-f85f474a-3d25-46fe-9558-51561271cec7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:48.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9403" for this suite. 08/24/23 12:16:48.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:48.819
Aug 24 12:16:48.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 12:16:48.82
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:48.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:48.849
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-eb42230c-05da-486e-be7d-0639cc9c8f1f 08/24/23 12:16:48.86
STEP: Creating secret with name s-test-opt-upd-2a5f480f-0403-4c2e-a4dd-83a2297795b8 08/24/23 12:16:48.868
STEP: Creating the pod 08/24/23 12:16:48.875
Aug 24 12:16:48.891: INFO: Waiting up to 5m0s for pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350" in namespace "secrets-4539" to be "running and ready"
Aug 24 12:16:48.900: INFO: Pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350": Phase="Pending", Reason="", readiness=false. Elapsed: 9.355362ms
Aug 24 12:16:48.900: INFO: The phase of Pod pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:16:50.904: INFO: Pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350": Phase="Running", Reason="", readiness=true. Elapsed: 2.013761493s
Aug 24 12:16:50.905: INFO: The phase of Pod pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350 is Running (Ready = true)
Aug 24 12:16:50.905: INFO: Pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-eb42230c-05da-486e-be7d-0639cc9c8f1f 08/24/23 12:16:50.929
STEP: Updating secret s-test-opt-upd-2a5f480f-0403-4c2e-a4dd-83a2297795b8 08/24/23 12:16:50.942
STEP: Creating secret with name s-test-opt-create-bbbe6414-0300-4792-85c8-224486caa9c8 08/24/23 12:16:50.957
STEP: waiting to observe update in volume 08/24/23 12:16:50.973
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:53.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4539" for this suite. 08/24/23 12:16:53.012
------------------------------
â€¢ [4.202 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:48.819
    Aug 24 12:16:48.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 12:16:48.82
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:48.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:48.849
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-eb42230c-05da-486e-be7d-0639cc9c8f1f 08/24/23 12:16:48.86
    STEP: Creating secret with name s-test-opt-upd-2a5f480f-0403-4c2e-a4dd-83a2297795b8 08/24/23 12:16:48.868
    STEP: Creating the pod 08/24/23 12:16:48.875
    Aug 24 12:16:48.891: INFO: Waiting up to 5m0s for pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350" in namespace "secrets-4539" to be "running and ready"
    Aug 24 12:16:48.900: INFO: Pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350": Phase="Pending", Reason="", readiness=false. Elapsed: 9.355362ms
    Aug 24 12:16:48.900: INFO: The phase of Pod pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:16:50.904: INFO: Pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350": Phase="Running", Reason="", readiness=true. Elapsed: 2.013761493s
    Aug 24 12:16:50.905: INFO: The phase of Pod pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350 is Running (Ready = true)
    Aug 24 12:16:50.905: INFO: Pod "pod-secrets-97080980-e545-4b50-a562-d2ae5ba71350" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-eb42230c-05da-486e-be7d-0639cc9c8f1f 08/24/23 12:16:50.929
    STEP: Updating secret s-test-opt-upd-2a5f480f-0403-4c2e-a4dd-83a2297795b8 08/24/23 12:16:50.942
    STEP: Creating secret with name s-test-opt-create-bbbe6414-0300-4792-85c8-224486caa9c8 08/24/23 12:16:50.957
    STEP: waiting to observe update in volume 08/24/23 12:16:50.973
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:53.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4539" for this suite. 08/24/23 12:16:53.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:53.024
Aug 24 12:16:53.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 12:16:53.025
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:53.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:53.108
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:16:53.148
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:16:53.864
STEP: Deploying the webhook pod 08/24/23 12:16:53.883
STEP: Wait for the deployment to be ready 08/24/23 12:16:53.96
Aug 24 12:16:54.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:16:56.06
STEP: Verifying the service has paired with the endpoint 08/24/23 12:16:56.086
Aug 24 12:16:57.087: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/24/23 12:16:57.203
STEP: Creating a configMap that should be mutated 08/24/23 12:16:57.228
STEP: Deleting the collection of validation webhooks 08/24/23 12:16:57.273
STEP: Creating a configMap that should not be mutated 08/24/23 12:16:57.337
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:16:57.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8341" for this suite. 08/24/23 12:16:57.465
STEP: Destroying namespace "webhook-8341-markers" for this suite. 08/24/23 12:16:57.488
------------------------------
â€¢ [4.508 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:53.024
    Aug 24 12:16:53.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 12:16:53.025
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:53.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:53.108
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:16:53.148
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:16:53.864
    STEP: Deploying the webhook pod 08/24/23 12:16:53.883
    STEP: Wait for the deployment to be ready 08/24/23 12:16:53.96
    Aug 24 12:16:54.029: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:16:56.06
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:16:56.086
    Aug 24 12:16:57.087: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/24/23 12:16:57.203
    STEP: Creating a configMap that should be mutated 08/24/23 12:16:57.228
    STEP: Deleting the collection of validation webhooks 08/24/23 12:16:57.273
    STEP: Creating a configMap that should not be mutated 08/24/23 12:16:57.337
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:16:57.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8341" for this suite. 08/24/23 12:16:57.465
    STEP: Destroying namespace "webhook-8341-markers" for this suite. 08/24/23 12:16:57.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:16:57.537
Aug 24 12:16:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 12:16:57.538
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:57.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:57.588
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-41626421-01ab-430a-b882-831f927ac26d 08/24/23 12:16:57.594
STEP: Creating a pod to test consume secrets 08/24/23 12:16:57.605
Aug 24 12:16:57.619: INFO: Waiting up to 5m0s for pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82" in namespace "secrets-6452" to be "Succeeded or Failed"
Aug 24 12:16:57.650: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82": Phase="Pending", Reason="", readiness=false. Elapsed: 30.114644ms
Aug 24 12:16:59.672: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052434479s
Aug 24 12:17:01.655: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035416458s
STEP: Saw pod success 08/24/23 12:17:01.655
Aug 24 12:17:01.655: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82" satisfied condition "Succeeded or Failed"
Aug 24 12:17:01.659: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82 container secret-volume-test: <nil>
STEP: delete the pod 08/24/23 12:17:01.668
Aug 24 12:17:01.695: INFO: Waiting for pod pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82 to disappear
Aug 24 12:17:01.701: INFO: Pod pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6452" for this suite. 08/24/23 12:17:01.711
------------------------------
â€¢ [4.184 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:16:57.537
    Aug 24 12:16:57.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 12:16:57.538
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:16:57.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:16:57.588
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-41626421-01ab-430a-b882-831f927ac26d 08/24/23 12:16:57.594
    STEP: Creating a pod to test consume secrets 08/24/23 12:16:57.605
    Aug 24 12:16:57.619: INFO: Waiting up to 5m0s for pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82" in namespace "secrets-6452" to be "Succeeded or Failed"
    Aug 24 12:16:57.650: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82": Phase="Pending", Reason="", readiness=false. Elapsed: 30.114644ms
    Aug 24 12:16:59.672: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052434479s
    Aug 24 12:17:01.655: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035416458s
    STEP: Saw pod success 08/24/23 12:17:01.655
    Aug 24 12:17:01.655: INFO: Pod "pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82" satisfied condition "Succeeded or Failed"
    Aug 24 12:17:01.659: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82 container secret-volume-test: <nil>
    STEP: delete the pod 08/24/23 12:17:01.668
    Aug 24 12:17:01.695: INFO: Waiting for pod pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82 to disappear
    Aug 24 12:17:01.701: INFO: Pod pod-secrets-666c5b36-0b48-437e-b57e-788543f3ae82 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6452" for this suite. 08/24/23 12:17:01.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:01.722
Aug 24 12:17:01.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:17:01.723
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:01.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:01.752
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Aug 24 12:17:01.784: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:17:01.799
Aug 24 12:17:01.806: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:01.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:17:01.816: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:17:02.877: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:02.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:17:02.891: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:17:03.834: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:03.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:17:03.843: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 08/24/23 12:17:03.916
STEP: Check that daemon pods images are updated. 08/24/23 12:17:03.945
Aug 24 12:17:03.948: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:03.948: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:03.948: INFO: Wrong image for pod: daemon-set-rt66x. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:03.959: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:04.968: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:04.968: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:04.973: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:05.963: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:05.964: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:05.967: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:06.965: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:06.965: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:06.965: INFO: Pod daemon-set-vhr29 is not available
Aug 24 12:17:06.971: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:07.964: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:07.964: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:07.964: INFO: Pod daemon-set-vhr29 is not available
Aug 24 12:17:07.968: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:08.965: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:08.972: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:09.964: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 24 12:17:09.964: INFO: Pod daemon-set-vnjf6 is not available
Aug 24 12:17:09.968: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:10.969: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:11.965: INFO: Pod daemon-set-wb6dz is not available
Aug 24 12:17:11.970: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 08/24/23 12:17:11.97
Aug 24 12:17:11.976: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:11.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:17:11.982: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
Aug 24 12:17:12.988: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:17:12.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:17:12.999: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:17:13.021
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9547, will wait for the garbage collector to delete the pods 08/24/23 12:17:13.021
Aug 24 12:17:13.085: INFO: Deleting DaemonSet.extensions daemon-set took: 10.693575ms
Aug 24 12:17:13.286: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.017867ms
Aug 24 12:17:15.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:17:15.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 12:17:15.295: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43728"},"items":null}

Aug 24 12:17:15.297: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43728"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:15.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9547" for this suite. 08/24/23 12:17:15.318
------------------------------
â€¢ [SLOW TEST] [13.609 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:01.722
    Aug 24 12:17:01.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:17:01.723
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:01.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:01.752
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Aug 24 12:17:01.784: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:17:01.799
    Aug 24 12:17:01.806: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:01.816: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:17:01.816: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:17:02.877: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:02.891: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:17:02.891: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:17:03.834: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:03.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:17:03.843: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 08/24/23 12:17:03.916
    STEP: Check that daemon pods images are updated. 08/24/23 12:17:03.945
    Aug 24 12:17:03.948: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:03.948: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:03.948: INFO: Wrong image for pod: daemon-set-rt66x. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:03.959: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:04.968: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:04.968: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:04.973: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:05.963: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:05.964: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:05.967: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:06.965: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:06.965: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:06.965: INFO: Pod daemon-set-vhr29 is not available
    Aug 24 12:17:06.971: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:07.964: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:07.964: INFO: Wrong image for pod: daemon-set-mwmp6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:07.964: INFO: Pod daemon-set-vhr29 is not available
    Aug 24 12:17:07.968: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:08.965: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:08.972: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:09.964: INFO: Wrong image for pod: daemon-set-k8x42. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 24 12:17:09.964: INFO: Pod daemon-set-vnjf6 is not available
    Aug 24 12:17:09.968: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:10.969: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:11.965: INFO: Pod daemon-set-wb6dz is not available
    Aug 24 12:17:11.970: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 08/24/23 12:17:11.97
    Aug 24 12:17:11.976: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:11.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:17:11.982: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-1 is running 0 daemon pod, expected 1
    Aug 24 12:17:12.988: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:17:12.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:17:12.999: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:17:13.021
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9547, will wait for the garbage collector to delete the pods 08/24/23 12:17:13.021
    Aug 24 12:17:13.085: INFO: Deleting DaemonSet.extensions daemon-set took: 10.693575ms
    Aug 24 12:17:13.286: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.017867ms
    Aug 24 12:17:15.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:17:15.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 12:17:15.295: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43728"},"items":null}

    Aug 24 12:17:15.297: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43728"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:15.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9547" for this suite. 08/24/23 12:17:15.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:15.334
Aug 24 12:17:15.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 12:17:15.335
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:15.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:15.376
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/24/23 12:17:15.382
Aug 24 12:17:15.399: INFO: Waiting up to 5m0s for pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f" in namespace "downward-api-9878" to be "Succeeded or Failed"
Aug 24 12:17:15.403: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868549ms
Aug 24 12:17:17.407: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008653218s
Aug 24 12:17:19.411: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012827413s
STEP: Saw pod success 08/24/23 12:17:19.412
Aug 24 12:17:19.414: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f" satisfied condition "Succeeded or Failed"
Aug 24 12:17:19.418: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:17:19.426
Aug 24 12:17:19.445: INFO: Waiting for pod downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f to disappear
Aug 24 12:17:19.451: INFO: Pod downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:19.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9878" for this suite. 08/24/23 12:17:19.456
------------------------------
â€¢ [4.132 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:15.334
    Aug 24 12:17:15.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:17:15.335
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:15.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:15.376
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/24/23 12:17:15.382
    Aug 24 12:17:15.399: INFO: Waiting up to 5m0s for pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f" in namespace "downward-api-9878" to be "Succeeded or Failed"
    Aug 24 12:17:15.403: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.868549ms
    Aug 24 12:17:17.407: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008653218s
    Aug 24 12:17:19.411: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012827413s
    STEP: Saw pod success 08/24/23 12:17:19.412
    Aug 24 12:17:19.414: INFO: Pod "downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f" satisfied condition "Succeeded or Failed"
    Aug 24 12:17:19.418: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:17:19.426
    Aug 24 12:17:19.445: INFO: Waiting for pod downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f to disappear
    Aug 24 12:17:19.451: INFO: Pod downward-api-6afe4b5d-4487-4fe1-b9ac-00e4d523ed2f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:19.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9878" for this suite. 08/24/23 12:17:19.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:19.471
Aug 24 12:17:19.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename discovery 08/24/23 12:17:19.473
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:19.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:19.496
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/24/23 12:17:19.509
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 24 12:17:20.331: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 24 12:17:20.334: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 24 12:17:20.334: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 24 12:17:20.334: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 24 12:17:20.334: INFO: Checking APIGroup: apps
Aug 24 12:17:20.337: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 24 12:17:20.337: INFO: Versions found [{apps/v1 v1}]
Aug 24 12:17:20.337: INFO: apps/v1 matches apps/v1
Aug 24 12:17:20.337: INFO: Checking APIGroup: events.k8s.io
Aug 24 12:17:20.339: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 24 12:17:20.339: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 24 12:17:20.339: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 24 12:17:20.339: INFO: Checking APIGroup: authentication.k8s.io
Aug 24 12:17:20.348: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 24 12:17:20.348: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 24 12:17:20.348: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 24 12:17:20.348: INFO: Checking APIGroup: authorization.k8s.io
Aug 24 12:17:20.351: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 24 12:17:20.351: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 24 12:17:20.351: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 24 12:17:20.351: INFO: Checking APIGroup: autoscaling
Aug 24 12:17:20.365: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 24 12:17:20.365: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 24 12:17:20.366: INFO: autoscaling/v2 matches autoscaling/v2
Aug 24 12:17:20.366: INFO: Checking APIGroup: batch
Aug 24 12:17:20.368: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 24 12:17:20.368: INFO: Versions found [{batch/v1 v1}]
Aug 24 12:17:20.368: INFO: batch/v1 matches batch/v1
Aug 24 12:17:20.368: INFO: Checking APIGroup: certificates.k8s.io
Aug 24 12:17:20.372: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 24 12:17:20.373: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 24 12:17:20.373: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 24 12:17:20.373: INFO: Checking APIGroup: networking.k8s.io
Aug 24 12:17:20.376: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 24 12:17:20.376: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
Aug 24 12:17:20.376: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 24 12:17:20.376: INFO: Checking APIGroup: policy
Aug 24 12:17:20.382: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 24 12:17:20.382: INFO: Versions found [{policy/v1 v1}]
Aug 24 12:17:20.382: INFO: policy/v1 matches policy/v1
Aug 24 12:17:20.383: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 24 12:17:20.385: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 24 12:17:20.385: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 24 12:17:20.385: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 24 12:17:20.385: INFO: Checking APIGroup: storage.k8s.io
Aug 24 12:17:20.388: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 24 12:17:20.388: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 24 12:17:20.388: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 24 12:17:20.388: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 24 12:17:20.392: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 24 12:17:20.392: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1alpha1 v1alpha1}]
Aug 24 12:17:20.392: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 24 12:17:20.392: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 24 12:17:20.395: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 24 12:17:20.395: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 24 12:17:20.396: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 24 12:17:20.396: INFO: Checking APIGroup: scheduling.k8s.io
Aug 24 12:17:20.398: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 24 12:17:20.398: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 24 12:17:20.398: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 24 12:17:20.398: INFO: Checking APIGroup: coordination.k8s.io
Aug 24 12:17:20.400: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 24 12:17:20.400: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 24 12:17:20.400: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 24 12:17:20.400: INFO: Checking APIGroup: node.k8s.io
Aug 24 12:17:20.403: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 24 12:17:20.403: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 24 12:17:20.403: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 24 12:17:20.403: INFO: Checking APIGroup: discovery.k8s.io
Aug 24 12:17:20.405: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 24 12:17:20.405: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 24 12:17:20.406: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 24 12:17:20.406: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 24 12:17:20.409: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 24 12:17:20.409: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 24 12:17:20.409: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 24 12:17:20.409: INFO: Checking APIGroup: internal.apiserver.k8s.io
Aug 24 12:17:20.411: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Aug 24 12:17:20.411: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Aug 24 12:17:20.411: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Aug 24 12:17:20.411: INFO: Checking APIGroup: resource.k8s.io
Aug 24 12:17:20.414: INFO: PreferredVersion.GroupVersion: resource.k8s.io/v1alpha1
Aug 24 12:17:20.414: INFO: Versions found [{resource.k8s.io/v1alpha1 v1alpha1}]
Aug 24 12:17:20.414: INFO: resource.k8s.io/v1alpha1 matches resource.k8s.io/v1alpha1
Aug 24 12:17:20.414: INFO: Checking APIGroup: crd.projectcalico.org
Aug 24 12:17:20.416: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 24 12:17:20.416: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 24 12:17:20.416: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 24 12:17:20.416: INFO: Checking APIGroup: monitoring.coreos.com
Aug 24 12:17:20.419: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 24 12:17:20.419: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 24 12:17:20.419: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 24 12:17:20.419: INFO: Checking APIGroup: metrics.k8s.io
Aug 24 12:17:20.421: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 24 12:17:20.421: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 24 12:17:20.421: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:20.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-342" for this suite. 08/24/23 12:17:20.426
------------------------------
â€¢ [0.968 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:19.471
    Aug 24 12:17:19.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename discovery 08/24/23 12:17:19.473
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:19.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:19.496
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/24/23 12:17:19.509
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 24 12:17:20.331: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 24 12:17:20.334: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 24 12:17:20.334: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 24 12:17:20.334: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 24 12:17:20.334: INFO: Checking APIGroup: apps
    Aug 24 12:17:20.337: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 24 12:17:20.337: INFO: Versions found [{apps/v1 v1}]
    Aug 24 12:17:20.337: INFO: apps/v1 matches apps/v1
    Aug 24 12:17:20.337: INFO: Checking APIGroup: events.k8s.io
    Aug 24 12:17:20.339: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 24 12:17:20.339: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 24 12:17:20.339: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 24 12:17:20.339: INFO: Checking APIGroup: authentication.k8s.io
    Aug 24 12:17:20.348: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 24 12:17:20.348: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 24 12:17:20.348: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 24 12:17:20.348: INFO: Checking APIGroup: authorization.k8s.io
    Aug 24 12:17:20.351: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 24 12:17:20.351: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 24 12:17:20.351: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 24 12:17:20.351: INFO: Checking APIGroup: autoscaling
    Aug 24 12:17:20.365: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 24 12:17:20.365: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 24 12:17:20.366: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 24 12:17:20.366: INFO: Checking APIGroup: batch
    Aug 24 12:17:20.368: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 24 12:17:20.368: INFO: Versions found [{batch/v1 v1}]
    Aug 24 12:17:20.368: INFO: batch/v1 matches batch/v1
    Aug 24 12:17:20.368: INFO: Checking APIGroup: certificates.k8s.io
    Aug 24 12:17:20.372: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 24 12:17:20.373: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 24 12:17:20.373: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 24 12:17:20.373: INFO: Checking APIGroup: networking.k8s.io
    Aug 24 12:17:20.376: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 24 12:17:20.376: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
    Aug 24 12:17:20.376: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 24 12:17:20.376: INFO: Checking APIGroup: policy
    Aug 24 12:17:20.382: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 24 12:17:20.382: INFO: Versions found [{policy/v1 v1}]
    Aug 24 12:17:20.382: INFO: policy/v1 matches policy/v1
    Aug 24 12:17:20.383: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 24 12:17:20.385: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 24 12:17:20.385: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 24 12:17:20.385: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 24 12:17:20.385: INFO: Checking APIGroup: storage.k8s.io
    Aug 24 12:17:20.388: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 24 12:17:20.388: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 24 12:17:20.388: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 24 12:17:20.388: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 24 12:17:20.392: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 24 12:17:20.392: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1alpha1 v1alpha1}]
    Aug 24 12:17:20.392: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 24 12:17:20.392: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 24 12:17:20.395: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 24 12:17:20.395: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 24 12:17:20.396: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 24 12:17:20.396: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 24 12:17:20.398: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 24 12:17:20.398: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 24 12:17:20.398: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 24 12:17:20.398: INFO: Checking APIGroup: coordination.k8s.io
    Aug 24 12:17:20.400: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 24 12:17:20.400: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 24 12:17:20.400: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 24 12:17:20.400: INFO: Checking APIGroup: node.k8s.io
    Aug 24 12:17:20.403: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 24 12:17:20.403: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 24 12:17:20.403: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 24 12:17:20.403: INFO: Checking APIGroup: discovery.k8s.io
    Aug 24 12:17:20.405: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 24 12:17:20.405: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 24 12:17:20.406: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 24 12:17:20.406: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 24 12:17:20.409: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 24 12:17:20.409: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 24 12:17:20.409: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 24 12:17:20.409: INFO: Checking APIGroup: internal.apiserver.k8s.io
    Aug 24 12:17:20.411: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
    Aug 24 12:17:20.411: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
    Aug 24 12:17:20.411: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
    Aug 24 12:17:20.411: INFO: Checking APIGroup: resource.k8s.io
    Aug 24 12:17:20.414: INFO: PreferredVersion.GroupVersion: resource.k8s.io/v1alpha1
    Aug 24 12:17:20.414: INFO: Versions found [{resource.k8s.io/v1alpha1 v1alpha1}]
    Aug 24 12:17:20.414: INFO: resource.k8s.io/v1alpha1 matches resource.k8s.io/v1alpha1
    Aug 24 12:17:20.414: INFO: Checking APIGroup: crd.projectcalico.org
    Aug 24 12:17:20.416: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Aug 24 12:17:20.416: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Aug 24 12:17:20.416: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Aug 24 12:17:20.416: INFO: Checking APIGroup: monitoring.coreos.com
    Aug 24 12:17:20.419: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Aug 24 12:17:20.419: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Aug 24 12:17:20.419: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Aug 24 12:17:20.419: INFO: Checking APIGroup: metrics.k8s.io
    Aug 24 12:17:20.421: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Aug 24 12:17:20.421: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Aug 24 12:17:20.421: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:20.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-342" for this suite. 08/24/23 12:17:20.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:20.445
Aug 24 12:17:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:17:20.446
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:20.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:20.485
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-442/configmap-test-ed4cee2e-0616-41ea-b8b2-fd5c1cd6e854 08/24/23 12:17:20.491
STEP: Creating a pod to test consume configMaps 08/24/23 12:17:20.505
Aug 24 12:17:20.521: INFO: Waiting up to 5m0s for pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b" in namespace "configmap-442" to be "Succeeded or Failed"
Aug 24 12:17:20.532: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.318591ms
Aug 24 12:17:22.537: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01498056s
Aug 24 12:17:24.547: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025115738s
STEP: Saw pod success 08/24/23 12:17:24.547
Aug 24 12:17:24.547: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b" satisfied condition "Succeeded or Failed"
Aug 24 12:17:24.552: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b container env-test: <nil>
STEP: delete the pod 08/24/23 12:17:24.559
Aug 24 12:17:24.581: INFO: Waiting for pod pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b to disappear
Aug 24 12:17:24.588: INFO: Pod pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:24.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-442" for this suite. 08/24/23 12:17:24.592
------------------------------
â€¢ [4.158 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:20.445
    Aug 24 12:17:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:17:20.446
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:20.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:20.485
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-442/configmap-test-ed4cee2e-0616-41ea-b8b2-fd5c1cd6e854 08/24/23 12:17:20.491
    STEP: Creating a pod to test consume configMaps 08/24/23 12:17:20.505
    Aug 24 12:17:20.521: INFO: Waiting up to 5m0s for pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b" in namespace "configmap-442" to be "Succeeded or Failed"
    Aug 24 12:17:20.532: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.318591ms
    Aug 24 12:17:22.537: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01498056s
    Aug 24 12:17:24.547: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025115738s
    STEP: Saw pod success 08/24/23 12:17:24.547
    Aug 24 12:17:24.547: INFO: Pod "pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b" satisfied condition "Succeeded or Failed"
    Aug 24 12:17:24.552: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b container env-test: <nil>
    STEP: delete the pod 08/24/23 12:17:24.559
    Aug 24 12:17:24.581: INFO: Waiting for pod pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b to disappear
    Aug 24 12:17:24.588: INFO: Pod pod-configmaps-29e05b28-90b7-4465-bf5d-b21ec2f7cc7b no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:24.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-442" for this suite. 08/24/23 12:17:24.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:24.619
Aug 24 12:17:24.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename configmap 08/24/23 12:17:24.62
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:24.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:24.702
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-9293de3f-d3e4-4abf-8847-b7340d15d29b 08/24/23 12:17:24.712
STEP: Creating a pod to test consume configMaps 08/24/23 12:17:24.723
Aug 24 12:17:24.746: INFO: Waiting up to 5m0s for pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1" in namespace "configmap-5052" to be "Succeeded or Failed"
Aug 24 12:17:24.753: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.409634ms
Aug 24 12:17:26.756: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010224098s
Aug 24 12:17:28.758: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012030883s
STEP: Saw pod success 08/24/23 12:17:28.758
Aug 24 12:17:28.758: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1" satisfied condition "Succeeded or Failed"
Aug 24 12:17:28.761: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:17:28.773
Aug 24 12:17:28.802: INFO: Waiting for pod pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1 to disappear
Aug 24 12:17:28.807: INFO: Pod pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:28.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5052" for this suite. 08/24/23 12:17:28.814
------------------------------
â€¢ [4.206 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:24.619
    Aug 24 12:17:24.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename configmap 08/24/23 12:17:24.62
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:24.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:24.702
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-9293de3f-d3e4-4abf-8847-b7340d15d29b 08/24/23 12:17:24.712
    STEP: Creating a pod to test consume configMaps 08/24/23 12:17:24.723
    Aug 24 12:17:24.746: INFO: Waiting up to 5m0s for pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1" in namespace "configmap-5052" to be "Succeeded or Failed"
    Aug 24 12:17:24.753: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.409634ms
    Aug 24 12:17:26.756: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1": Phase="Running", Reason="", readiness=false. Elapsed: 2.010224098s
    Aug 24 12:17:28.758: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012030883s
    STEP: Saw pod success 08/24/23 12:17:28.758
    Aug 24 12:17:28.758: INFO: Pod "pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1" satisfied condition "Succeeded or Failed"
    Aug 24 12:17:28.761: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:17:28.773
    Aug 24 12:17:28.802: INFO: Waiting for pod pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1 to disappear
    Aug 24 12:17:28.807: INFO: Pod pod-configmaps-45d926fe-b410-4a73-b79c-544a4571add1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:28.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5052" for this suite. 08/24/23 12:17:28.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:28.827
Aug 24 12:17:28.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename subpath 08/24/23 12:17:28.828
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:28.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:28.9
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/24/23 12:17:28.912
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-r85g 08/24/23 12:17:28.94
STEP: Creating a pod to test atomic-volume-subpath 08/24/23 12:17:28.94
Aug 24 12:17:28.950: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r85g" in namespace "subpath-7584" to be "Succeeded or Failed"
Aug 24 12:17:28.959: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058745ms
Aug 24 12:17:30.965: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 2.014192243s
Aug 24 12:17:32.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 4.012533873s
Aug 24 12:17:34.966: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 6.015316651s
Aug 24 12:17:36.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 8.012120175s
Aug 24 12:17:38.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 10.012560917s
Aug 24 12:17:40.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 12.012518236s
Aug 24 12:17:42.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 14.013051187s
Aug 24 12:17:44.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 16.013390787s
Aug 24 12:17:46.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 18.01389881s
Aug 24 12:17:48.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 20.013948308s
Aug 24 12:17:50.965: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=false. Elapsed: 22.014734606s
Aug 24 12:17:52.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012846205s
STEP: Saw pod success 08/24/23 12:17:52.963
Aug 24 12:17:52.963: INFO: Pod "pod-subpath-test-configmap-r85g" satisfied condition "Succeeded or Failed"
Aug 24 12:17:52.966: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-subpath-test-configmap-r85g container test-container-subpath-configmap-r85g: <nil>
STEP: delete the pod 08/24/23 12:17:52.978
Aug 24 12:17:52.996: INFO: Waiting for pod pod-subpath-test-configmap-r85g to disappear
Aug 24 12:17:52.999: INFO: Pod pod-subpath-test-configmap-r85g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r85g 08/24/23 12:17:52.999
Aug 24 12:17:53.000: INFO: Deleting pod "pod-subpath-test-configmap-r85g" in namespace "subpath-7584"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:53.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7584" for this suite. 08/24/23 12:17:53.008
------------------------------
â€¢ [SLOW TEST] [24.190 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:28.827
    Aug 24 12:17:28.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename subpath 08/24/23 12:17:28.828
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:28.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:28.9
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/24/23 12:17:28.912
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-r85g 08/24/23 12:17:28.94
    STEP: Creating a pod to test atomic-volume-subpath 08/24/23 12:17:28.94
    Aug 24 12:17:28.950: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r85g" in namespace "subpath-7584" to be "Succeeded or Failed"
    Aug 24 12:17:28.959: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058745ms
    Aug 24 12:17:30.965: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 2.014192243s
    Aug 24 12:17:32.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 4.012533873s
    Aug 24 12:17:34.966: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 6.015316651s
    Aug 24 12:17:36.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 8.012120175s
    Aug 24 12:17:38.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 10.012560917s
    Aug 24 12:17:40.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 12.012518236s
    Aug 24 12:17:42.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 14.013051187s
    Aug 24 12:17:44.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 16.013390787s
    Aug 24 12:17:46.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 18.01389881s
    Aug 24 12:17:48.964: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=true. Elapsed: 20.013948308s
    Aug 24 12:17:50.965: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Running", Reason="", readiness=false. Elapsed: 22.014734606s
    Aug 24 12:17:52.963: INFO: Pod "pod-subpath-test-configmap-r85g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012846205s
    STEP: Saw pod success 08/24/23 12:17:52.963
    Aug 24 12:17:52.963: INFO: Pod "pod-subpath-test-configmap-r85g" satisfied condition "Succeeded or Failed"
    Aug 24 12:17:52.966: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-subpath-test-configmap-r85g container test-container-subpath-configmap-r85g: <nil>
    STEP: delete the pod 08/24/23 12:17:52.978
    Aug 24 12:17:52.996: INFO: Waiting for pod pod-subpath-test-configmap-r85g to disappear
    Aug 24 12:17:52.999: INFO: Pod pod-subpath-test-configmap-r85g no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-r85g 08/24/23 12:17:52.999
    Aug 24 12:17:53.000: INFO: Deleting pod "pod-subpath-test-configmap-r85g" in namespace "subpath-7584"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:53.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7584" for this suite. 08/24/23 12:17:53.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:53.018
Aug 24 12:17:53.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename containers 08/24/23 12:17:53.02
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:53.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:53.045
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/24/23 12:17:53.056
Aug 24 12:17:53.068: INFO: Waiting up to 5m0s for pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a" in namespace "containers-9748" to be "Succeeded or Failed"
Aug 24 12:17:53.080: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.700675ms
Aug 24 12:17:55.087: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018064854s
Aug 24 12:17:57.085: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016588886s
STEP: Saw pod success 08/24/23 12:17:57.085
Aug 24 12:17:57.085: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a" satisfied condition "Succeeded or Failed"
Aug 24 12:17:57.088: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod client-containers-40082671-7c97-48de-b4ef-f6231b74097a container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:17:57.095
Aug 24 12:17:57.111: INFO: Waiting for pod client-containers-40082671-7c97-48de-b4ef-f6231b74097a to disappear
Aug 24 12:17:57.117: INFO: Pod client-containers-40082671-7c97-48de-b4ef-f6231b74097a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:17:57.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9748" for this suite. 08/24/23 12:17:57.121
------------------------------
â€¢ [4.110 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:53.018
    Aug 24 12:17:53.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename containers 08/24/23 12:17:53.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:53.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:53.045
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/24/23 12:17:53.056
    Aug 24 12:17:53.068: INFO: Waiting up to 5m0s for pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a" in namespace "containers-9748" to be "Succeeded or Failed"
    Aug 24 12:17:53.080: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.700675ms
    Aug 24 12:17:55.087: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018064854s
    Aug 24 12:17:57.085: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016588886s
    STEP: Saw pod success 08/24/23 12:17:57.085
    Aug 24 12:17:57.085: INFO: Pod "client-containers-40082671-7c97-48de-b4ef-f6231b74097a" satisfied condition "Succeeded or Failed"
    Aug 24 12:17:57.088: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod client-containers-40082671-7c97-48de-b4ef-f6231b74097a container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:17:57.095
    Aug 24 12:17:57.111: INFO: Waiting for pod client-containers-40082671-7c97-48de-b4ef-f6231b74097a to disappear
    Aug 24 12:17:57.117: INFO: Pod client-containers-40082671-7c97-48de-b4ef-f6231b74097a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:17:57.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9748" for this suite. 08/24/23 12:17:57.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:17:57.135
Aug 24 12:17:57.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:17:57.137
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:57.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:57.169
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 12:17:57.200: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 12:18:57.247: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/24/23 12:18:57.252
Aug 24 12:18:57.292: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 24 12:18:57.311: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 24 12:18:57.373: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 24 12:18:57.387: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 24 12:18:57.453: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 24 12:18:57.471: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/24/23 12:18:57.471
Aug 24 12:18:57.471: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9939" to be "running"
Aug 24 12:18:57.479: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008393ms
Aug 24 12:18:59.495: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.024085528s
Aug 24 12:18:59.495: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 24 12:18:59.495: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
Aug 24 12:18:59.501: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.194087ms
Aug 24 12:18:59.501: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:18:59.502: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
Aug 24 12:18:59.507: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61617ms
Aug 24 12:19:01.512: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009881982s
Aug 24 12:19:01.512: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:19:01.512: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
Aug 24 12:19:01.516: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.257214ms
Aug 24 12:19:01.516: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:19:01.516: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
Aug 24 12:19:01.520: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.7818ms
Aug 24 12:19:01.520: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:19:01.520: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
Aug 24 12:19:01.525: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.192687ms
Aug 24 12:19:01.525: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/24/23 12:19:01.525
Aug 24 12:19:01.532: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9939" to be "running"
Aug 24 12:19:01.538: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.307131ms
Aug 24 12:19:03.544: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01173978s
Aug 24 12:19:05.544: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011880836s
Aug 24 12:19:05.544: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:19:05.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9939" for this suite. 08/24/23 12:19:05.634
------------------------------
â€¢ [SLOW TEST] [68.512 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:17:57.135
    Aug 24 12:17:57.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:17:57.137
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:17:57.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:17:57.169
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 12:17:57.200: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 12:18:57.247: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/24/23 12:18:57.252
    Aug 24 12:18:57.292: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 24 12:18:57.311: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 24 12:18:57.373: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 24 12:18:57.387: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 24 12:18:57.453: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 24 12:18:57.471: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/24/23 12:18:57.471
    Aug 24 12:18:57.471: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9939" to be "running"
    Aug 24 12:18:57.479: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008393ms
    Aug 24 12:18:59.495: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.024085528s
    Aug 24 12:18:59.495: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 24 12:18:59.495: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
    Aug 24 12:18:59.501: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.194087ms
    Aug 24 12:18:59.501: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:18:59.502: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
    Aug 24 12:18:59.507: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61617ms
    Aug 24 12:19:01.512: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009881982s
    Aug 24 12:19:01.512: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:19:01.512: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
    Aug 24 12:19:01.516: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.257214ms
    Aug 24 12:19:01.516: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:19:01.516: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
    Aug 24 12:19:01.520: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.7818ms
    Aug 24 12:19:01.520: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:19:01.520: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9939" to be "running"
    Aug 24 12:19:01.525: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.192687ms
    Aug 24 12:19:01.525: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/24/23 12:19:01.525
    Aug 24 12:19:01.532: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9939" to be "running"
    Aug 24 12:19:01.538: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.307131ms
    Aug 24 12:19:03.544: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01173978s
    Aug 24 12:19:05.544: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011880836s
    Aug 24 12:19:05.544: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:19:05.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9939" for this suite. 08/24/23 12:19:05.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:19:05.65
Aug 24 12:19:05.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:19:05.652
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:19:05.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:19:05.694
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 12:19:05.7
Aug 24 12:19:05.718: INFO: Waiting up to 5m0s for pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a" in namespace "emptydir-3683" to be "Succeeded or Failed"
Aug 24 12:19:05.729: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.726832ms
Aug 24 12:19:07.734: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015979555s
Aug 24 12:19:09.735: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016763329s
STEP: Saw pod success 08/24/23 12:19:09.735
Aug 24 12:19:09.735: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a" satisfied condition "Succeeded or Failed"
Aug 24 12:19:09.738: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a container test-container: <nil>
STEP: delete the pod 08/24/23 12:19:09.746
Aug 24 12:19:09.766: INFO: Waiting for pod pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a to disappear
Aug 24 12:19:09.771: INFO: Pod pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:19:09.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3683" for this suite. 08/24/23 12:19:09.775
------------------------------
â€¢ [4.135 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:19:05.65
    Aug 24 12:19:05.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:19:05.652
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:19:05.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:19:05.694
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/24/23 12:19:05.7
    Aug 24 12:19:05.718: INFO: Waiting up to 5m0s for pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a" in namespace "emptydir-3683" to be "Succeeded or Failed"
    Aug 24 12:19:05.729: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.726832ms
    Aug 24 12:19:07.734: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015979555s
    Aug 24 12:19:09.735: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016763329s
    STEP: Saw pod success 08/24/23 12:19:09.735
    Aug 24 12:19:09.735: INFO: Pod "pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a" satisfied condition "Succeeded or Failed"
    Aug 24 12:19:09.738: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a container test-container: <nil>
    STEP: delete the pod 08/24/23 12:19:09.746
    Aug 24 12:19:09.766: INFO: Waiting for pod pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a to disappear
    Aug 24 12:19:09.771: INFO: Pod pod-e5e5d67d-c332-4a6a-b0eb-05581d29ca9a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:19:09.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3683" for this suite. 08/24/23 12:19:09.775
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:19:09.787
Aug 24 12:19:09.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:19:09.788
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:19:09.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:19:09.819
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 24 12:19:09.849: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 24 12:20:09.902: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/24/23 12:20:09.906
Aug 24 12:20:09.934: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 24 12:20:09.947: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 24 12:20:09.983: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 24 12:20:10.002: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 24 12:20:10.044: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 24 12:20:10.063: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/24/23 12:20:10.063
Aug 24 12:20:10.063: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5909" to be "running"
Aug 24 12:20:10.092: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.191648ms
Aug 24 12:20:12.105: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.041124013s
Aug 24 12:20:12.105: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 24 12:20:12.105: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
Aug 24 12:20:12.110: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.784051ms
Aug 24 12:20:12.110: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:20:12.110: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
Aug 24 12:20:12.114: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.441427ms
Aug 24 12:20:12.114: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:20:12.114: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
Aug 24 12:20:12.119: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.28085ms
Aug 24 12:20:12.119: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:20:12.119: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
Aug 24 12:20:12.123: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.309353ms
Aug 24 12:20:12.123: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 24 12:20:12.123: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
Aug 24 12:20:12.127: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.755156ms
Aug 24 12:20:12.127: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/24/23 12:20:12.127
Aug 24 12:20:12.141: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 24 12:20:12.157: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.661189ms
Aug 24 12:20:14.162: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021001223s
Aug 24 12:20:16.161: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020517281s
Aug 24 12:20:18.162: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02081103s
Aug 24 12:20:20.164: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.023029552s
Aug 24 12:20:20.164: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:20:20.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5909" for this suite. 08/24/23 12:20:20.354
------------------------------
â€¢ [SLOW TEST] [70.583 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:19:09.787
    Aug 24 12:19:09.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename sched-preemption 08/24/23 12:19:09.788
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:19:09.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:19:09.819
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 24 12:19:09.849: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 24 12:20:09.902: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/24/23 12:20:09.906
    Aug 24 12:20:09.934: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 24 12:20:09.947: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 24 12:20:09.983: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 24 12:20:10.002: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 24 12:20:10.044: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 24 12:20:10.063: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/24/23 12:20:10.063
    Aug 24 12:20:10.063: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5909" to be "running"
    Aug 24 12:20:10.092: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.191648ms
    Aug 24 12:20:12.105: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.041124013s
    Aug 24 12:20:12.105: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 24 12:20:12.105: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
    Aug 24 12:20:12.110: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.784051ms
    Aug 24 12:20:12.110: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:20:12.110: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
    Aug 24 12:20:12.114: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.441427ms
    Aug 24 12:20:12.114: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:20:12.114: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
    Aug 24 12:20:12.119: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.28085ms
    Aug 24 12:20:12.119: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:20:12.119: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
    Aug 24 12:20:12.123: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.309353ms
    Aug 24 12:20:12.123: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 24 12:20:12.123: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5909" to be "running"
    Aug 24 12:20:12.127: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.755156ms
    Aug 24 12:20:12.127: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/24/23 12:20:12.127
    Aug 24 12:20:12.141: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 24 12:20:12.157: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 16.661189ms
    Aug 24 12:20:14.162: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021001223s
    Aug 24 12:20:16.161: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020517281s
    Aug 24 12:20:18.162: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02081103s
    Aug 24 12:20:20.164: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.023029552s
    Aug 24 12:20:20.164: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:20:20.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5909" for this suite. 08/24/23 12:20:20.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:20:20.373
Aug 24 12:20:20.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:20:20.375
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:20.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:20.412
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/24/23 12:20:20.419
STEP: Creating a ResourceQuota 08/24/23 12:20:25.423
STEP: Ensuring resource quota status is calculated 08/24/23 12:20:25.436
STEP: Creating a ReplicationController 08/24/23 12:20:27.442
STEP: Ensuring resource quota status captures replication controller creation 08/24/23 12:20:27.463
STEP: Deleting a ReplicationController 08/24/23 12:20:29.467
STEP: Ensuring resource quota status released usage 08/24/23 12:20:29.479
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:20:31.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7836" for this suite. 08/24/23 12:20:31.49
------------------------------
â€¢ [SLOW TEST] [11.124 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:20:20.373
    Aug 24 12:20:20.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:20:20.375
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:20.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:20.412
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/24/23 12:20:20.419
    STEP: Creating a ResourceQuota 08/24/23 12:20:25.423
    STEP: Ensuring resource quota status is calculated 08/24/23 12:20:25.436
    STEP: Creating a ReplicationController 08/24/23 12:20:27.442
    STEP: Ensuring resource quota status captures replication controller creation 08/24/23 12:20:27.463
    STEP: Deleting a ReplicationController 08/24/23 12:20:29.467
    STEP: Ensuring resource quota status released usage 08/24/23 12:20:29.479
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:20:31.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7836" for this suite. 08/24/23 12:20:31.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:20:31.499
Aug 24 12:20:31.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 12:20:31.501
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:31.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:31.534
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/24/23 12:20:31.542
Aug 24 12:20:31.560: INFO: created test-pod-1
Aug 24 12:20:31.578: INFO: created test-pod-2
Aug 24 12:20:31.597: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/24/23 12:20:31.597
Aug 24 12:20:31.597: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2649' to be running and ready
Aug 24 12:20:31.652: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:20:31.652: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:20:31.652: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:20:31.652: INFO: 0 / 3 pods in namespace 'pods-2649' are running and ready (0 seconds elapsed)
Aug 24 12:20:31.652: INFO: expected 0 pod replicas in namespace 'pods-2649', 0 are Running and Ready.
Aug 24 12:20:31.652: INFO: POD         NODE                                   PHASE    GRACE  CONDITIONS
Aug 24 12:20:31.652: INFO: test-pod-1  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
Aug 24 12:20:31.652: INFO: test-pod-2  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
Aug 24 12:20:31.652: INFO: test-pod-3  gitlab-1-26-36460-guscsyka22xa-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
Aug 24 12:20:31.652: INFO: 
Aug 24 12:20:33.663: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:20:33.663: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 24 12:20:33.663: INFO: 1 / 3 pods in namespace 'pods-2649' are running and ready (2 seconds elapsed)
Aug 24 12:20:33.663: INFO: expected 0 pod replicas in namespace 'pods-2649', 0 are Running and Ready.
Aug 24 12:20:33.663: INFO: POD         NODE                                   PHASE    GRACE  CONDITIONS
Aug 24 12:20:33.663: INFO: test-pod-1  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
Aug 24 12:20:33.663: INFO: test-pod-2  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
Aug 24 12:20:33.663: INFO: 
Aug 24 12:20:35.663: INFO: 3 / 3 pods in namespace 'pods-2649' are running and ready (4 seconds elapsed)
Aug 24 12:20:35.663: INFO: expected 0 pod replicas in namespace 'pods-2649', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/24/23 12:20:35.694
Aug 24 12:20:35.707: INFO: Pod quantity 3 is different from expected quantity 0
Aug 24 12:20:36.721: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:20:37.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2649" for this suite. 08/24/23 12:20:37.716
------------------------------
â€¢ [SLOW TEST] [6.226 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:20:31.499
    Aug 24 12:20:31.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 12:20:31.501
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:31.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:31.534
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/24/23 12:20:31.542
    Aug 24 12:20:31.560: INFO: created test-pod-1
    Aug 24 12:20:31.578: INFO: created test-pod-2
    Aug 24 12:20:31.597: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/24/23 12:20:31.597
    Aug 24 12:20:31.597: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-2649' to be running and ready
    Aug 24 12:20:31.652: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:20:31.652: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:20:31.652: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:20:31.652: INFO: 0 / 3 pods in namespace 'pods-2649' are running and ready (0 seconds elapsed)
    Aug 24 12:20:31.652: INFO: expected 0 pod replicas in namespace 'pods-2649', 0 are Running and Ready.
    Aug 24 12:20:31.652: INFO: POD         NODE                                   PHASE    GRACE  CONDITIONS
    Aug 24 12:20:31.652: INFO: test-pod-1  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
    Aug 24 12:20:31.652: INFO: test-pod-2  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
    Aug 24 12:20:31.652: INFO: test-pod-3  gitlab-1-26-36460-guscsyka22xa-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
    Aug 24 12:20:31.652: INFO: 
    Aug 24 12:20:33.663: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:20:33.663: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 24 12:20:33.663: INFO: 1 / 3 pods in namespace 'pods-2649' are running and ready (2 seconds elapsed)
    Aug 24 12:20:33.663: INFO: expected 0 pod replicas in namespace 'pods-2649', 0 are Running and Ready.
    Aug 24 12:20:33.663: INFO: POD         NODE                                   PHASE    GRACE  CONDITIONS
    Aug 24 12:20:33.663: INFO: test-pod-1  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
    Aug 24 12:20:33.663: INFO: test-pod-2  gitlab-1-26-36460-guscsyka22xa-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:20:31 +0000 UTC  }]
    Aug 24 12:20:33.663: INFO: 
    Aug 24 12:20:35.663: INFO: 3 / 3 pods in namespace 'pods-2649' are running and ready (4 seconds elapsed)
    Aug 24 12:20:35.663: INFO: expected 0 pod replicas in namespace 'pods-2649', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/24/23 12:20:35.694
    Aug 24 12:20:35.707: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 24 12:20:36.721: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:20:37.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2649" for this suite. 08/24/23 12:20:37.716
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:20:37.726
Aug 24 12:20:37.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 12:20:37.727
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:37.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:37.753
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/24/23 12:20:37.759
Aug 24 12:20:37.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 create -f -'
Aug 24 12:20:38.827: INFO: stderr: ""
Aug 24 12:20:38.827: INFO: stdout: "pod/pause created\n"
Aug 24 12:20:38.827: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 24 12:20:38.827: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2390" to be "running and ready"
Aug 24 12:20:38.834: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.269767ms
Aug 24 12:20:38.834: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'gitlab-1-26-36460-guscsyka22xa-node-2' to be 'Running' but was 'Pending'
Aug 24 12:20:40.838: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010857783s
Aug 24 12:20:40.838: INFO: Pod "pause" satisfied condition "running and ready"
Aug 24 12:20:40.838: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/24/23 12:20:40.838
Aug 24 12:20:40.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 label pods pause testing-label=testing-label-value'
Aug 24 12:20:40.986: INFO: stderr: ""
Aug 24 12:20:40.986: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/24/23 12:20:40.986
Aug 24 12:20:40.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get pod pause -L testing-label'
Aug 24 12:20:41.117: INFO: stderr: ""
Aug 24 12:20:41.117: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/24/23 12:20:41.117
Aug 24 12:20:41.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 label pods pause testing-label-'
Aug 24 12:20:41.260: INFO: stderr: ""
Aug 24 12:20:41.260: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/24/23 12:20:41.26
Aug 24 12:20:41.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get pod pause -L testing-label'
Aug 24 12:20:41.379: INFO: stderr: ""
Aug 24 12:20:41.379: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/24/23 12:20:41.379
Aug 24 12:20:41.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 delete --grace-period=0 --force -f -'
Aug 24 12:20:41.515: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 24 12:20:41.515: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 24 12:20:41.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get rc,svc -l name=pause --no-headers'
Aug 24 12:20:41.630: INFO: stderr: "No resources found in kubectl-2390 namespace.\n"
Aug 24 12:20:41.630: INFO: stdout: ""
Aug 24 12:20:41.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 24 12:20:41.738: INFO: stderr: ""
Aug 24 12:20:41.738: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:20:41.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2390" for this suite. 08/24/23 12:20:41.743
------------------------------
â€¢ [4.030 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:20:37.726
    Aug 24 12:20:37.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:20:37.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:37.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:37.753
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/24/23 12:20:37.759
    Aug 24 12:20:37.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 create -f -'
    Aug 24 12:20:38.827: INFO: stderr: ""
    Aug 24 12:20:38.827: INFO: stdout: "pod/pause created\n"
    Aug 24 12:20:38.827: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 24 12:20:38.827: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2390" to be "running and ready"
    Aug 24 12:20:38.834: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.269767ms
    Aug 24 12:20:38.834: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'gitlab-1-26-36460-guscsyka22xa-node-2' to be 'Running' but was 'Pending'
    Aug 24 12:20:40.838: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010857783s
    Aug 24 12:20:40.838: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 24 12:20:40.838: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/24/23 12:20:40.838
    Aug 24 12:20:40.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 label pods pause testing-label=testing-label-value'
    Aug 24 12:20:40.986: INFO: stderr: ""
    Aug 24 12:20:40.986: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/24/23 12:20:40.986
    Aug 24 12:20:40.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get pod pause -L testing-label'
    Aug 24 12:20:41.117: INFO: stderr: ""
    Aug 24 12:20:41.117: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/24/23 12:20:41.117
    Aug 24 12:20:41.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 label pods pause testing-label-'
    Aug 24 12:20:41.260: INFO: stderr: ""
    Aug 24 12:20:41.260: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/24/23 12:20:41.26
    Aug 24 12:20:41.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get pod pause -L testing-label'
    Aug 24 12:20:41.379: INFO: stderr: ""
    Aug 24 12:20:41.379: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/24/23 12:20:41.379
    Aug 24 12:20:41.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 delete --grace-period=0 --force -f -'
    Aug 24 12:20:41.515: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 24 12:20:41.515: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 24 12:20:41.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get rc,svc -l name=pause --no-headers'
    Aug 24 12:20:41.630: INFO: stderr: "No resources found in kubectl-2390 namespace.\n"
    Aug 24 12:20:41.630: INFO: stdout: ""
    Aug 24 12:20:41.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-2390 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 24 12:20:41.738: INFO: stderr: ""
    Aug 24 12:20:41.738: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:20:41.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2390" for this suite. 08/24/23 12:20:41.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:20:41.756
Aug 24 12:20:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename services 08/24/23 12:20:41.757
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:41.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:41.794
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7152 08/24/23 12:20:41.8
STEP: changing the ExternalName service to type=ClusterIP 08/24/23 12:20:41.828
STEP: creating replication controller externalname-service in namespace services-7152 08/24/23 12:20:41.887
I0824 12:20:41.918828      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7152, replica count: 2
I0824 12:20:44.971084      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 24 12:20:44.971: INFO: Creating new exec pod
Aug 24 12:20:44.983: INFO: Waiting up to 5m0s for pod "execpodx4ldz" in namespace "services-7152" to be "running"
Aug 24 12:20:45.007: INFO: Pod "execpodx4ldz": Phase="Pending", Reason="", readiness=false. Elapsed: 23.865298ms
Aug 24 12:20:47.011: INFO: Pod "execpodx4ldz": Phase="Running", Reason="", readiness=true. Elapsed: 2.028156252s
Aug 24 12:20:47.011: INFO: Pod "execpodx4ldz" satisfied condition "running"
Aug 24 12:20:48.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7152 exec execpodx4ldz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 24 12:20:48.261: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 24 12:20:48.261: INFO: stdout: ""
Aug 24 12:20:48.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7152 exec execpodx4ldz -- /bin/sh -x -c nc -v -z -w 2 10.254.62.19 80'
Aug 24 12:20:48.548: INFO: stderr: "+ nc -v -z -w 2 10.254.62.19 80\nConnection to 10.254.62.19 80 port [tcp/http] succeeded!\n"
Aug 24 12:20:48.548: INFO: stdout: ""
Aug 24 12:20:48.548: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 24 12:20:48.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7152" for this suite. 08/24/23 12:20:48.624
------------------------------
â€¢ [SLOW TEST] [6.879 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:20:41.756
    Aug 24 12:20:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename services 08/24/23 12:20:41.757
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:41.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:41.794
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7152 08/24/23 12:20:41.8
    STEP: changing the ExternalName service to type=ClusterIP 08/24/23 12:20:41.828
    STEP: creating replication controller externalname-service in namespace services-7152 08/24/23 12:20:41.887
    I0824 12:20:41.918828      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7152, replica count: 2
    I0824 12:20:44.971084      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 24 12:20:44.971: INFO: Creating new exec pod
    Aug 24 12:20:44.983: INFO: Waiting up to 5m0s for pod "execpodx4ldz" in namespace "services-7152" to be "running"
    Aug 24 12:20:45.007: INFO: Pod "execpodx4ldz": Phase="Pending", Reason="", readiness=false. Elapsed: 23.865298ms
    Aug 24 12:20:47.011: INFO: Pod "execpodx4ldz": Phase="Running", Reason="", readiness=true. Elapsed: 2.028156252s
    Aug 24 12:20:47.011: INFO: Pod "execpodx4ldz" satisfied condition "running"
    Aug 24 12:20:48.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7152 exec execpodx4ldz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 24 12:20:48.261: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 24 12:20:48.261: INFO: stdout: ""
    Aug 24 12:20:48.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=services-7152 exec execpodx4ldz -- /bin/sh -x -c nc -v -z -w 2 10.254.62.19 80'
    Aug 24 12:20:48.548: INFO: stderr: "+ nc -v -z -w 2 10.254.62.19 80\nConnection to 10.254.62.19 80 port [tcp/http] succeeded!\n"
    Aug 24 12:20:48.548: INFO: stdout: ""
    Aug 24 12:20:48.548: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:20:48.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7152" for this suite. 08/24/23 12:20:48.624
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:20:48.638
Aug 24 12:20:48.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 12:20:48.639
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:48.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:48.676
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:20:48.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1004" for this suite. 08/24/23 12:20:48.781
------------------------------
â€¢ [0.177 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:20:48.638
    Aug 24 12:20:48.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 12:20:48.639
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:48.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:48.676
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:20:48.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1004" for this suite. 08/24/23 12:20:48.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:20:48.817
Aug 24 12:20:48.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename deployment 08/24/23 12:20:48.819
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:49.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:49.033
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 24 12:20:49.085: INFO: Creating deployment "test-recreate-deployment"
Aug 24 12:20:49.098: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 24 12:20:49.129: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 24 12:20:51.137: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 24 12:20:51.140: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 24 12:20:51.154: INFO: Updating deployment test-recreate-deployment
Aug 24 12:20:51.154: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 24 12:20:51.308: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4557  35db9285-c1ed-4dde-ba1a-6c988e550206 45063 2 2023-08-24 12:20:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005480718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 12:20:51 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-24 12:20:51 +0000 UTC,LastTransitionTime:2023-08-24 12:20:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 24 12:20:51.311: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4557  743e86a3-40a0-4f6f-a796-ea66366f5298 45061 1 2023-08-24 12:20:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 35db9285-c1ed-4dde-ba1a-6c988e550206 0xc005480be0 0xc005480be1}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35db9285-c1ed-4dde-ba1a-6c988e550206\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005480c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:20:51.311: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 24 12:20:51.311: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4557  c66fddc9-0513-4d24-a979-b5fa4af99857 45052 2 2023-08-24 12:20:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 35db9285-c1ed-4dde-ba1a-6c988e550206 0xc005480ac7 0xc005480ac8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35db9285-c1ed-4dde-ba1a-6c988e550206\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005480b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 24 12:20:51.322: INFO: Pod "test-recreate-deployment-cff6dc657-zzbz9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-zzbz9 test-recreate-deployment-cff6dc657- deployment-4557  861b54b4-7f19-4060-99a0-ef9a14ebbc3b 45064 0 2023-08-24 12:20:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 743e86a3-40a0-4f6f-a796-ea66366f5298 0xc005481100 0xc005481101}] [] [{kube-controller-manager Update v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743e86a3-40a0-4f6f-a796-ea66366f5298\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5fqn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5fqn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 12:20:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 24 12:20:51.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4557" for this suite. 08/24/23 12:20:51.326
------------------------------
â€¢ [2.522 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:20:48.817
    Aug 24 12:20:48.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename deployment 08/24/23 12:20:48.819
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:49.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:49.033
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 24 12:20:49.085: INFO: Creating deployment "test-recreate-deployment"
    Aug 24 12:20:49.098: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 24 12:20:49.129: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 24 12:20:51.137: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 24 12:20:51.140: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 24 12:20:51.154: INFO: Updating deployment test-recreate-deployment
    Aug 24 12:20:51.154: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 24 12:20:51.308: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4557  35db9285-c1ed-4dde-ba1a-6c988e550206 45063 2 2023-08-24 12:20:49 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005480718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 12:20:51 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-24 12:20:51 +0000 UTC,LastTransitionTime:2023-08-24 12:20:49 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 24 12:20:51.311: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4557  743e86a3-40a0-4f6f-a796-ea66366f5298 45061 1 2023-08-24 12:20:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 35db9285-c1ed-4dde-ba1a-6c988e550206 0xc005480be0 0xc005480be1}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35db9285-c1ed-4dde-ba1a-6c988e550206\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005480c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:20:51.311: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 24 12:20:51.311: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4557  c66fddc9-0513-4d24-a979-b5fa4af99857 45052 2 2023-08-24 12:20:49 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 35db9285-c1ed-4dde-ba1a-6c988e550206 0xc005480ac7 0xc005480ac8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35db9285-c1ed-4dde-ba1a-6c988e550206\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005480b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 24 12:20:51.322: INFO: Pod "test-recreate-deployment-cff6dc657-zzbz9" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-zzbz9 test-recreate-deployment-cff6dc657- deployment-4557  861b54b4-7f19-4060-99a0-ef9a14ebbc3b 45064 0 2023-08-24 12:20:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 743e86a3-40a0-4f6f-a796-ea66366f5298 0xc005481100 0xc005481101}] [] [{kube-controller-manager Update v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"743e86a3-40a0-4f6f-a796-ea66366f5298\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:20:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5fqn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5fqn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gitlab-1-26-36460-guscsyka22xa-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:20:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.18,PodIP:,StartTime:2023-08-24 12:20:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:20:51.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4557" for this suite. 08/24/23 12:20:51.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:20:51.343
Aug 24 12:20:51.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename replication-controller 08/24/23 12:20:51.345
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:51.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:51.376
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020 08/24/23 12:20:51.382
Aug 24 12:20:51.394: INFO: Pod name my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020: Found 0 pods out of 1
Aug 24 12:20:56.398: INFO: Pod name my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020: Found 1 pods out of 1
Aug 24 12:20:56.398: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020" are running
Aug 24 12:20:56.398: INFO: Waiting up to 5m0s for pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c" in namespace "replication-controller-3697" to be "running"
Aug 24 12:20:56.403: INFO: Pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c": Phase="Running", Reason="", readiness=true. Elapsed: 4.556404ms
Aug 24 12:20:56.403: INFO: Pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c" satisfied condition "running"
Aug 24 12:20:56.403: INFO: Pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:51 +0000 UTC Reason: Message:}])
Aug 24 12:20:56.404: INFO: Trying to dial the pod
Aug 24 12:21:01.417: INFO: Controller my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020: Got expected result from replica 1 [my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c]: "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:01.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3697" for this suite. 08/24/23 12:21:01.421
------------------------------
â€¢ [SLOW TEST] [10.090 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:20:51.343
    Aug 24 12:20:51.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename replication-controller 08/24/23 12:20:51.345
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:20:51.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:20:51.376
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020 08/24/23 12:20:51.382
    Aug 24 12:20:51.394: INFO: Pod name my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020: Found 0 pods out of 1
    Aug 24 12:20:56.398: INFO: Pod name my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020: Found 1 pods out of 1
    Aug 24 12:20:56.398: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020" are running
    Aug 24 12:20:56.398: INFO: Waiting up to 5m0s for pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c" in namespace "replication-controller-3697" to be "running"
    Aug 24 12:20:56.403: INFO: Pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c": Phase="Running", Reason="", readiness=true. Elapsed: 4.556404ms
    Aug 24 12:20:56.403: INFO: Pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c" satisfied condition "running"
    Aug 24 12:20:56.403: INFO: Pod "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:20:51 +0000 UTC Reason: Message:}])
    Aug 24 12:20:56.404: INFO: Trying to dial the pod
    Aug 24 12:21:01.417: INFO: Controller my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020: Got expected result from replica 1 [my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c]: "my-hostname-basic-48ef2243-4cc1-4dc4-bafb-3fb087676020-ng48c", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:01.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3697" for this suite. 08/24/23 12:21:01.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:01.434
Aug 24 12:21:01.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename disruption 08/24/23 12:21:01.436
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:01.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:01.459
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/24/23 12:21:01.473
STEP: Updating PodDisruptionBudget status 08/24/23 12:21:01.48
STEP: Waiting for all pods to be running 08/24/23 12:21:01.498
Aug 24 12:21:01.513: INFO: running pods: 0 < 1
STEP: locating a running pod 08/24/23 12:21:03.518
STEP: Waiting for the pdb to be processed 08/24/23 12:21:03.535
STEP: Patching PodDisruptionBudget status 08/24/23 12:21:03.541
STEP: Waiting for the pdb to be processed 08/24/23 12:21:03.564
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:03.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4266" for this suite. 08/24/23 12:21:03.574
------------------------------
â€¢ [2.165 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:01.434
    Aug 24 12:21:01.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename disruption 08/24/23 12:21:01.436
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:01.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:01.459
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/24/23 12:21:01.473
    STEP: Updating PodDisruptionBudget status 08/24/23 12:21:01.48
    STEP: Waiting for all pods to be running 08/24/23 12:21:01.498
    Aug 24 12:21:01.513: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/24/23 12:21:03.518
    STEP: Waiting for the pdb to be processed 08/24/23 12:21:03.535
    STEP: Patching PodDisruptionBudget status 08/24/23 12:21:03.541
    STEP: Waiting for the pdb to be processed 08/24/23 12:21:03.564
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:03.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4266" for this suite. 08/24/23 12:21:03.574
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:03.6
Aug 24 12:21:03.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename var-expansion 08/24/23 12:21:03.601
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:03.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:03.631
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/24/23 12:21:03.636
Aug 24 12:21:03.647: INFO: Waiting up to 5m0s for pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d" in namespace "var-expansion-7981" to be "Succeeded or Failed"
Aug 24 12:21:03.651: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.298067ms
Aug 24 12:21:05.657: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010052749s
Aug 24 12:21:07.655: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008150824s
STEP: Saw pod success 08/24/23 12:21:07.655
Aug 24 12:21:07.655: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d" satisfied condition "Succeeded or Failed"
Aug 24 12:21:07.660: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:21:07.742
Aug 24 12:21:07.775: INFO: Waiting for pod var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d to disappear
Aug 24 12:21:07.783: INFO: Pod var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:07.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7981" for this suite. 08/24/23 12:21:07.787
------------------------------
â€¢ [4.197 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:03.6
    Aug 24 12:21:03.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename var-expansion 08/24/23 12:21:03.601
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:03.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:03.631
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/24/23 12:21:03.636
    Aug 24 12:21:03.647: INFO: Waiting up to 5m0s for pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d" in namespace "var-expansion-7981" to be "Succeeded or Failed"
    Aug 24 12:21:03.651: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.298067ms
    Aug 24 12:21:05.657: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010052749s
    Aug 24 12:21:07.655: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008150824s
    STEP: Saw pod success 08/24/23 12:21:07.655
    Aug 24 12:21:07.655: INFO: Pod "var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d" satisfied condition "Succeeded or Failed"
    Aug 24 12:21:07.660: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:21:07.742
    Aug 24 12:21:07.775: INFO: Waiting for pod var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d to disappear
    Aug 24 12:21:07.783: INFO: Pod var-expansion-f6d236a6-89e4-4220-b24b-85c924f0ca2d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:07.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7981" for this suite. 08/24/23 12:21:07.787
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:07.799
Aug 24 12:21:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:21:07.8
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:07.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:07.841
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 24 12:21:07.863: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b" in namespace "kubelet-test-9488" to be "running and ready"
Aug 24 12:21:07.882: INFO: Pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.212551ms
Aug 24 12:21:07.882: INFO: The phase of Pod busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:21:09.901: INFO: Pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.037678287s
Aug 24 12:21:09.901: INFO: The phase of Pod busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b is Running (Ready = true)
Aug 24 12:21:09.901: INFO: Pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:09.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9488" for this suite. 08/24/23 12:21:09.991
------------------------------
â€¢ [2.201 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:07.799
    Aug 24 12:21:07.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubelet-test 08/24/23 12:21:07.8
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:07.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:07.841
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 24 12:21:07.863: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b" in namespace "kubelet-test-9488" to be "running and ready"
    Aug 24 12:21:07.882: INFO: Pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.212551ms
    Aug 24 12:21:07.882: INFO: The phase of Pod busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:21:09.901: INFO: Pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.037678287s
    Aug 24 12:21:09.901: INFO: The phase of Pod busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b is Running (Ready = true)
    Aug 24 12:21:09.901: INFO: Pod "busybox-readonly-fsf1dddefa-3a50-4799-8523-9ac730821e8b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:09.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9488" for this suite. 08/24/23 12:21:09.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:10.005
Aug 24 12:21:10.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename containers 08/24/23 12:21:10.006
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:10.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:10.041
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/24/23 12:21:10.048
Aug 24 12:21:10.069: INFO: Waiting up to 5m0s for pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944" in namespace "containers-7945" to be "Succeeded or Failed"
Aug 24 12:21:10.076: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944": Phase="Pending", Reason="", readiness=false. Elapsed: 6.300387ms
Aug 24 12:21:12.080: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010395102s
Aug 24 12:21:14.080: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010530703s
STEP: Saw pod success 08/24/23 12:21:14.08
Aug 24 12:21:14.080: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944" satisfied condition "Succeeded or Failed"
Aug 24 12:21:14.087: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod client-containers-9e1046c0-3901-4139-9012-621b68b8c944 container agnhost-container: <nil>
STEP: delete the pod 08/24/23 12:21:14.098
Aug 24 12:21:14.128: INFO: Waiting for pod client-containers-9e1046c0-3901-4139-9012-621b68b8c944 to disappear
Aug 24 12:21:14.150: INFO: Pod client-containers-9e1046c0-3901-4139-9012-621b68b8c944 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:14.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7945" for this suite. 08/24/23 12:21:14.157
------------------------------
â€¢ [4.179 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:10.005
    Aug 24 12:21:10.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename containers 08/24/23 12:21:10.006
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:10.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:10.041
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/24/23 12:21:10.048
    Aug 24 12:21:10.069: INFO: Waiting up to 5m0s for pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944" in namespace "containers-7945" to be "Succeeded or Failed"
    Aug 24 12:21:10.076: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944": Phase="Pending", Reason="", readiness=false. Elapsed: 6.300387ms
    Aug 24 12:21:12.080: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010395102s
    Aug 24 12:21:14.080: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010530703s
    STEP: Saw pod success 08/24/23 12:21:14.08
    Aug 24 12:21:14.080: INFO: Pod "client-containers-9e1046c0-3901-4139-9012-621b68b8c944" satisfied condition "Succeeded or Failed"
    Aug 24 12:21:14.087: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod client-containers-9e1046c0-3901-4139-9012-621b68b8c944 container agnhost-container: <nil>
    STEP: delete the pod 08/24/23 12:21:14.098
    Aug 24 12:21:14.128: INFO: Waiting for pod client-containers-9e1046c0-3901-4139-9012-621b68b8c944 to disappear
    Aug 24 12:21:14.150: INFO: Pod client-containers-9e1046c0-3901-4139-9012-621b68b8c944 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:14.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7945" for this suite. 08/24/23 12:21:14.157
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:14.184
Aug 24 12:21:14.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename controllerrevisions 08/24/23 12:21:14.186
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:14.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:14.298
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-98hgs-daemon-set" 08/24/23 12:21:14.338
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:21:14.347
Aug 24 12:21:14.351: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:21:14.360: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 0
Aug 24 12:21:14.360: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:21:15.365: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:21:15.371: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 0
Aug 24 12:21:15.371: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:21:16.372: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:21:16.379: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 3
Aug 24 12:21:16.380: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-98hgs-daemon-set
STEP: Confirm DaemonSet "e2e-98hgs-daemon-set" successfully created with "daemonset-name=e2e-98hgs-daemon-set" label 08/24/23 12:21:16.386
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-98hgs-daemon-set" 08/24/23 12:21:16.401
Aug 24 12:21:16.409: INFO: Located ControllerRevision: "e2e-98hgs-daemon-set-67857bd874"
STEP: Patching ControllerRevision "e2e-98hgs-daemon-set-67857bd874" 08/24/23 12:21:16.416
Aug 24 12:21:16.429: INFO: e2e-98hgs-daemon-set-67857bd874 has been patched
STEP: Create a new ControllerRevision 08/24/23 12:21:16.429
Aug 24 12:21:16.442: INFO: Created ControllerRevision: e2e-98hgs-daemon-set-59b5d5f648
STEP: Confirm that there are two ControllerRevisions 08/24/23 12:21:16.442
Aug 24 12:21:16.442: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 12:21:16.446: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-98hgs-daemon-set-67857bd874" 08/24/23 12:21:16.446
STEP: Confirm that there is only one ControllerRevision 08/24/23 12:21:16.453
Aug 24 12:21:16.454: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 12:21:16.456: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-98hgs-daemon-set-59b5d5f648" 08/24/23 12:21:16.459
Aug 24 12:21:16.476: INFO: e2e-98hgs-daemon-set-59b5d5f648 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/24/23 12:21:16.476
W0824 12:21:16.490045      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/24/23 12:21:16.49
Aug 24 12:21:16.490: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 12:21:17.493: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 12:21:17.501: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-98hgs-daemon-set-59b5d5f648=updated" 08/24/23 12:21:17.501
STEP: Confirm that there is only one ControllerRevision 08/24/23 12:21:17.514
Aug 24 12:21:17.514: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 24 12:21:17.518: INFO: Found 1 ControllerRevisions
Aug 24 12:21:17.520: INFO: ControllerRevision "e2e-98hgs-daemon-set-58dfd7c46f" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-98hgs-daemon-set" 08/24/23 12:21:17.525
STEP: deleting DaemonSet.extensions e2e-98hgs-daemon-set in namespace controllerrevisions-4416, will wait for the garbage collector to delete the pods 08/24/23 12:21:17.525
Aug 24 12:21:17.589: INFO: Deleting DaemonSet.extensions e2e-98hgs-daemon-set took: 10.629267ms
Aug 24 12:21:17.689: INFO: Terminating DaemonSet.extensions e2e-98hgs-daemon-set pods took: 100.182545ms
Aug 24 12:21:19.498: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 0
Aug 24 12:21:19.498: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-98hgs-daemon-set
Aug 24 12:21:19.503: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45445"},"items":null}

Aug 24 12:21:19.508: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45445"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:19.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-4416" for this suite. 08/24/23 12:21:19.561
------------------------------
â€¢ [SLOW TEST] [5.391 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:14.184
    Aug 24 12:21:14.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename controllerrevisions 08/24/23 12:21:14.186
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:14.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:14.298
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-98hgs-daemon-set" 08/24/23 12:21:14.338
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:21:14.347
    Aug 24 12:21:14.351: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:21:14.360: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 0
    Aug 24 12:21:14.360: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:21:15.365: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:21:15.371: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 0
    Aug 24 12:21:15.371: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:21:16.372: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:21:16.379: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 3
    Aug 24 12:21:16.380: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-98hgs-daemon-set
    STEP: Confirm DaemonSet "e2e-98hgs-daemon-set" successfully created with "daemonset-name=e2e-98hgs-daemon-set" label 08/24/23 12:21:16.386
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-98hgs-daemon-set" 08/24/23 12:21:16.401
    Aug 24 12:21:16.409: INFO: Located ControllerRevision: "e2e-98hgs-daemon-set-67857bd874"
    STEP: Patching ControllerRevision "e2e-98hgs-daemon-set-67857bd874" 08/24/23 12:21:16.416
    Aug 24 12:21:16.429: INFO: e2e-98hgs-daemon-set-67857bd874 has been patched
    STEP: Create a new ControllerRevision 08/24/23 12:21:16.429
    Aug 24 12:21:16.442: INFO: Created ControllerRevision: e2e-98hgs-daemon-set-59b5d5f648
    STEP: Confirm that there are two ControllerRevisions 08/24/23 12:21:16.442
    Aug 24 12:21:16.442: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 12:21:16.446: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-98hgs-daemon-set-67857bd874" 08/24/23 12:21:16.446
    STEP: Confirm that there is only one ControllerRevision 08/24/23 12:21:16.453
    Aug 24 12:21:16.454: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 12:21:16.456: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-98hgs-daemon-set-59b5d5f648" 08/24/23 12:21:16.459
    Aug 24 12:21:16.476: INFO: e2e-98hgs-daemon-set-59b5d5f648 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/24/23 12:21:16.476
    W0824 12:21:16.490045      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/24/23 12:21:16.49
    Aug 24 12:21:16.490: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 12:21:17.493: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 12:21:17.501: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-98hgs-daemon-set-59b5d5f648=updated" 08/24/23 12:21:17.501
    STEP: Confirm that there is only one ControllerRevision 08/24/23 12:21:17.514
    Aug 24 12:21:17.514: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 24 12:21:17.518: INFO: Found 1 ControllerRevisions
    Aug 24 12:21:17.520: INFO: ControllerRevision "e2e-98hgs-daemon-set-58dfd7c46f" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-98hgs-daemon-set" 08/24/23 12:21:17.525
    STEP: deleting DaemonSet.extensions e2e-98hgs-daemon-set in namespace controllerrevisions-4416, will wait for the garbage collector to delete the pods 08/24/23 12:21:17.525
    Aug 24 12:21:17.589: INFO: Deleting DaemonSet.extensions e2e-98hgs-daemon-set took: 10.629267ms
    Aug 24 12:21:17.689: INFO: Terminating DaemonSet.extensions e2e-98hgs-daemon-set pods took: 100.182545ms
    Aug 24 12:21:19.498: INFO: Number of nodes with available pods controlled by daemonset e2e-98hgs-daemon-set: 0
    Aug 24 12:21:19.498: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-98hgs-daemon-set
    Aug 24 12:21:19.503: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45445"},"items":null}

    Aug 24 12:21:19.508: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45445"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:19.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-4416" for this suite. 08/24/23 12:21:19.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:19.581
Aug 24 12:21:19.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pods 08/24/23 12:21:19.582
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:19.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:19.62
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 24 12:21:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: creating the pod 08/24/23 12:21:19.629
STEP: submitting the pod to kubernetes 08/24/23 12:21:19.629
Aug 24 12:21:19.647: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393" in namespace "pods-2265" to be "running and ready"
Aug 24 12:21:19.662: INFO: Pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393": Phase="Pending", Reason="", readiness=false. Elapsed: 14.993279ms
Aug 24 12:21:19.663: INFO: The phase of Pod pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:21:21.666: INFO: Pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393": Phase="Running", Reason="", readiness=true. Elapsed: 2.018982252s
Aug 24 12:21:21.667: INFO: The phase of Pod pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393 is Running (Ready = true)
Aug 24 12:21:21.667: INFO: Pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:21.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2265" for this suite. 08/24/23 12:21:21.842
------------------------------
â€¢ [2.271 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:19.581
    Aug 24 12:21:19.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pods 08/24/23 12:21:19.582
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:19.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:19.62
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 24 12:21:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: creating the pod 08/24/23 12:21:19.629
    STEP: submitting the pod to kubernetes 08/24/23 12:21:19.629
    Aug 24 12:21:19.647: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393" in namespace "pods-2265" to be "running and ready"
    Aug 24 12:21:19.662: INFO: Pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393": Phase="Pending", Reason="", readiness=false. Elapsed: 14.993279ms
    Aug 24 12:21:19.663: INFO: The phase of Pod pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:21:21.666: INFO: Pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393": Phase="Running", Reason="", readiness=true. Elapsed: 2.018982252s
    Aug 24 12:21:21.667: INFO: The phase of Pod pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393 is Running (Ready = true)
    Aug 24 12:21:21.667: INFO: Pod "pod-exec-websocket-81942a41-5d51-434e-bdf3-f8bf1ab5f393" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:21.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2265" for this suite. 08/24/23 12:21:21.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:21.854
Aug 24 12:21:21.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename namespaces 08/24/23 12:21:21.855
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:21.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:21.887
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/24/23 12:21:21.893
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:21.923
STEP: Creating a pod in the namespace 08/24/23 12:21:21.929
STEP: Waiting for the pod to have running status 08/24/23 12:21:21.941
Aug 24 12:21:21.941: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7148" to be "running"
Aug 24 12:21:21.948: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.859892ms
Aug 24 12:21:23.952: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010766412s
Aug 24 12:21:23.952: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/24/23 12:21:23.952
STEP: Waiting for the namespace to be removed. 08/24/23 12:21:23.96
STEP: Recreating the namespace 08/24/23 12:21:34.964
STEP: Verifying there are no pods in the namespace 08/24/23 12:21:34.992
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:34.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6699" for this suite. 08/24/23 12:21:35.006
STEP: Destroying namespace "nsdeletetest-7148" for this suite. 08/24/23 12:21:35.017
Aug 24 12:21:35.019: INFO: Namespace nsdeletetest-7148 was already deleted
STEP: Destroying namespace "nsdeletetest-9571" for this suite. 08/24/23 12:21:35.019
------------------------------
â€¢ [SLOW TEST] [13.175 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:21.854
    Aug 24 12:21:21.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename namespaces 08/24/23 12:21:21.855
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:21.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:21.887
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/24/23 12:21:21.893
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:21.923
    STEP: Creating a pod in the namespace 08/24/23 12:21:21.929
    STEP: Waiting for the pod to have running status 08/24/23 12:21:21.941
    Aug 24 12:21:21.941: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7148" to be "running"
    Aug 24 12:21:21.948: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.859892ms
    Aug 24 12:21:23.952: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010766412s
    Aug 24 12:21:23.952: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/24/23 12:21:23.952
    STEP: Waiting for the namespace to be removed. 08/24/23 12:21:23.96
    STEP: Recreating the namespace 08/24/23 12:21:34.964
    STEP: Verifying there are no pods in the namespace 08/24/23 12:21:34.992
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:34.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6699" for this suite. 08/24/23 12:21:35.006
    STEP: Destroying namespace "nsdeletetest-7148" for this suite. 08/24/23 12:21:35.017
    Aug 24 12:21:35.019: INFO: Namespace nsdeletetest-7148 was already deleted
    STEP: Destroying namespace "nsdeletetest-9571" for this suite. 08/24/23 12:21:35.019
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:35.031
Aug 24 12:21:35.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename resourcequota 08/24/23 12:21:35.033
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:35.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:35.064
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/24/23 12:21:35.071
STEP: Creating a ResourceQuota 08/24/23 12:21:40.076
STEP: Ensuring resource quota status is calculated 08/24/23 12:21:40.089
STEP: Creating a Service 08/24/23 12:21:42.096
STEP: Creating a NodePort Service 08/24/23 12:21:42.125
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/24/23 12:21:42.184
STEP: Ensuring resource quota status captures service creation 08/24/23 12:21:42.215
STEP: Deleting Services 08/24/23 12:21:44.22
STEP: Ensuring resource quota status released usage 08/24/23 12:21:44.302
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:46.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5124" for this suite. 08/24/23 12:21:46.313
------------------------------
â€¢ [SLOW TEST] [11.293 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:35.031
    Aug 24 12:21:35.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename resourcequota 08/24/23 12:21:35.033
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:35.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:35.064
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/24/23 12:21:35.071
    STEP: Creating a ResourceQuota 08/24/23 12:21:40.076
    STEP: Ensuring resource quota status is calculated 08/24/23 12:21:40.089
    STEP: Creating a Service 08/24/23 12:21:42.096
    STEP: Creating a NodePort Service 08/24/23 12:21:42.125
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/24/23 12:21:42.184
    STEP: Ensuring resource quota status captures service creation 08/24/23 12:21:42.215
    STEP: Deleting Services 08/24/23 12:21:44.22
    STEP: Ensuring resource quota status released usage 08/24/23 12:21:44.302
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:46.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5124" for this suite. 08/24/23 12:21:46.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:46.325
Aug 24 12:21:46.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 12:21:46.326
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:46.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:46.357
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/24/23 12:21:46.364
Aug 24 12:21:46.365: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-1507 proxy --unix-socket=/tmp/kubectl-proxy-unix4001753859/test'
STEP: retrieving proxy /api/ output 08/24/23 12:21:46.463
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:46.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1507" for this suite. 08/24/23 12:21:46.47
------------------------------
â€¢ [0.156 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:46.325
    Aug 24 12:21:46.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:21:46.326
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:46.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:46.357
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/24/23 12:21:46.364
    Aug 24 12:21:46.365: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-1507 proxy --unix-socket=/tmp/kubectl-proxy-unix4001753859/test'
    STEP: retrieving proxy /api/ output 08/24/23 12:21:46.463
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:46.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1507" for this suite. 08/24/23 12:21:46.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:46.482
Aug 24 12:21:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename watch 08/24/23 12:21:46.483
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:46.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:46.518
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/24/23 12:21:46.525
STEP: creating a new configmap 08/24/23 12:21:46.529
STEP: modifying the configmap once 08/24/23 12:21:46.537
STEP: changing the label value of the configmap 08/24/23 12:21:46.551
STEP: Expecting to observe a delete notification for the watched object 08/24/23 12:21:46.562
Aug 24 12:21:46.562: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45643 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:21:46.562: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45644 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:21:46.563: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45645 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/24/23 12:21:46.563
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/24/23 12:21:46.576
STEP: changing the label value of the configmap back 08/24/23 12:21:56.577
STEP: modifying the configmap a third time 08/24/23 12:21:56.587
STEP: deleting the configmap 08/24/23 12:21:56.596
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/24/23 12:21:56.604
Aug 24 12:21:56.604: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45681 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:21:56.605: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45682 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 24 12:21:56.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45683 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 24 12:21:56.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9624" for this suite. 08/24/23 12:21:56.61
------------------------------
â€¢ [SLOW TEST] [10.136 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:46.482
    Aug 24 12:21:46.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename watch 08/24/23 12:21:46.483
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:46.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:46.518
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/24/23 12:21:46.525
    STEP: creating a new configmap 08/24/23 12:21:46.529
    STEP: modifying the configmap once 08/24/23 12:21:46.537
    STEP: changing the label value of the configmap 08/24/23 12:21:46.551
    STEP: Expecting to observe a delete notification for the watched object 08/24/23 12:21:46.562
    Aug 24 12:21:46.562: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45643 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:21:46.562: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45644 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:21:46.563: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45645 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/24/23 12:21:46.563
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/24/23 12:21:46.576
    STEP: changing the label value of the configmap back 08/24/23 12:21:56.577
    STEP: modifying the configmap a third time 08/24/23 12:21:56.587
    STEP: deleting the configmap 08/24/23 12:21:56.596
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/24/23 12:21:56.604
    Aug 24 12:21:56.604: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45681 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:21:56.605: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45682 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 24 12:21:56.605: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9624  5d74e5ff-56de-45ba-a22d-0a400287124c 45683 0 2023-08-24 12:21:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:21:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:21:56.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9624" for this suite. 08/24/23 12:21:56.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:21:56.623
Aug 24 12:21:56.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename job 08/24/23 12:21:56.624
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:56.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:56.655
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/24/23 12:21:56.671
STEP: Patching the Job 08/24/23 12:21:56.684
STEP: Watching for Job to be patched 08/24/23 12:21:56.71
Aug 24 12:21:56.714: INFO: Event ADDED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 24 12:21:56.714: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 24 12:21:56.714: INFO: Event MODIFIED found for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/24/23 12:21:56.714
STEP: Watching for Job to be updated 08/24/23 12:21:56.73
Aug 24 12:21:56.732: INFO: Event MODIFIED found for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 12:21:56.732: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/24/23 12:21:56.732
Aug 24 12:21:56.737: INFO: Job: e2e-nb629 as labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched]
STEP: Waiting for job to complete 08/24/23 12:21:56.737
STEP: Delete a job collection with a labelselector 08/24/23 12:22:04.742
STEP: Watching for Job to be deleted 08/24/23 12:22:04.75
Aug 24 12:22:04.754: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 12:22:04.754: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 12:22:04.755: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 12:22:04.755: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 12:22:04.755: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 24 12:22:04.755: INFO: Event DELETED found for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/24/23 12:22:04.755
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 24 12:22:04.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9667" for this suite. 08/24/23 12:22:04.785
------------------------------
â€¢ [SLOW TEST] [8.191 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:21:56.623
    Aug 24 12:21:56.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename job 08/24/23 12:21:56.624
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:21:56.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:21:56.655
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/24/23 12:21:56.671
    STEP: Patching the Job 08/24/23 12:21:56.684
    STEP: Watching for Job to be patched 08/24/23 12:21:56.71
    Aug 24 12:21:56.714: INFO: Event ADDED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 24 12:21:56.714: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 24 12:21:56.714: INFO: Event MODIFIED found for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/24/23 12:21:56.714
    STEP: Watching for Job to be updated 08/24/23 12:21:56.73
    Aug 24 12:21:56.732: INFO: Event MODIFIED found for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 12:21:56.732: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/24/23 12:21:56.732
    Aug 24 12:21:56.737: INFO: Job: e2e-nb629 as labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched]
    STEP: Waiting for job to complete 08/24/23 12:21:56.737
    STEP: Delete a job collection with a labelselector 08/24/23 12:22:04.742
    STEP: Watching for Job to be deleted 08/24/23 12:22:04.75
    Aug 24 12:22:04.754: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 12:22:04.754: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 12:22:04.755: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 12:22:04.755: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 12:22:04.755: INFO: Event MODIFIED observed for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 24 12:22:04.755: INFO: Event DELETED found for Job e2e-nb629 in namespace job-9667 with labels: map[e2e-job-label:e2e-nb629 e2e-nb629:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/24/23 12:22:04.755
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:22:04.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9667" for this suite. 08/24/23 12:22:04.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:22:04.814
Aug 24 12:22:04.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename statefulset 08/24/23 12:22:04.816
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:04.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:04.869
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-207 08/24/23 12:22:04.876
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-207 08/24/23 12:22:04.885
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-207 08/24/23 12:22:04.901
Aug 24 12:22:04.911: INFO: Found 0 stateful pods, waiting for 1
Aug 24 12:22:14.916: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/24/23 12:22:14.916
Aug 24 12:22:14.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:22:15.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:22:15.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:22:15.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:22:15.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 24 12:22:25.186: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:22:25.186: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:22:25.219: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Aug 24 12:22:25.219: INFO: ss-0  gitlab-1-26-36460-guscsyka22xa-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  }]
Aug 24 12:22:25.219: INFO: 
Aug 24 12:22:25.219: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 24 12:22:26.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995853317s
Aug 24 12:22:27.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982405655s
Aug 24 12:22:28.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97738849s
Aug 24 12:22:29.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972982873s
Aug 24 12:22:30.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967928382s
Aug 24 12:22:31.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963054604s
Aug 24 12:22:32.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958088103s
Aug 24 12:22:33.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950898615s
Aug 24 12:22:34.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.853638ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-207 08/24/23 12:22:35.275
Aug 24 12:22:35.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:22:35.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 24 12:22:35.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:22:35.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:22:35.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:22:35.798: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 24 12:22:35.798: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:22:35.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:22:35.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 24 12:22:36.065: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 24 12:22:36.065: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 24 12:22:36.065: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 24 12:22:36.069: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 24 12:22:46.076: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:22:46.076: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 24 12:22:46.076: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/24/23 12:22:46.076
Aug 24 12:22:46.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:22:46.325: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:22:46.325: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:22:46.325: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:22:46.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:22:46.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:22:46.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:22:46.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:22:46.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 24 12:22:46.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 24 12:22:46.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 24 12:22:46.846: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 24 12:22:46.846: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:22:46.853: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 24 12:22:56.863: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:22:56.864: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:22:56.864: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 24 12:22:56.885: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Aug 24 12:22:56.885: INFO: ss-0  gitlab-1-26-36460-guscsyka22xa-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  }]
Aug 24 12:22:56.885: INFO: ss-1  gitlab-1-26-36460-guscsyka22xa-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
Aug 24 12:22:56.885: INFO: ss-2  gitlab-1-26-36460-guscsyka22xa-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
Aug 24 12:22:56.885: INFO: 
Aug 24 12:22:56.885: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 24 12:22:57.888: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
Aug 24 12:22:57.888: INFO: ss-1  gitlab-1-26-36460-guscsyka22xa-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
Aug 24 12:22:57.888: INFO: ss-2  gitlab-1-26-36460-guscsyka22xa-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
Aug 24 12:22:57.889: INFO: 
Aug 24 12:22:57.889: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 24 12:22:58.893: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990624341s
Aug 24 12:22:59.900: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987218359s
Aug 24 12:23:00.903: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979973653s
Aug 24 12:23:01.907: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976331908s
Aug 24 12:23:02.911: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972636388s
Aug 24 12:23:03.914: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968703772s
Aug 24 12:23:04.918: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965445722s
Aug 24 12:23:05.925: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.006477ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-207 08/24/23 12:23:06.925
Aug 24 12:23:06.929: INFO: Scaling statefulset ss to 0
Aug 24 12:23:06.938: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 24 12:23:06.941: INFO: Deleting all statefulset in ns statefulset-207
Aug 24 12:23:06.945: INFO: Scaling statefulset ss to 0
Aug 24 12:23:06.955: INFO: Waiting for statefulset status.replicas updated to 0
Aug 24 12:23:06.958: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-207" for this suite. 08/24/23 12:23:07.01
------------------------------
â€¢ [SLOW TEST] [62.206 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:22:04.814
    Aug 24 12:22:04.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename statefulset 08/24/23 12:22:04.816
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:22:04.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:22:04.869
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-207 08/24/23 12:22:04.876
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-207 08/24/23 12:22:04.885
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-207 08/24/23 12:22:04.901
    Aug 24 12:22:04.911: INFO: Found 0 stateful pods, waiting for 1
    Aug 24 12:22:14.916: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/24/23 12:22:14.916
    Aug 24 12:22:14.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:22:15.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:22:15.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:22:15.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:22:15.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 24 12:22:25.186: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:22:25.186: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:22:25.219: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
    Aug 24 12:22:25.219: INFO: ss-0  gitlab-1-26-36460-guscsyka22xa-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  }]
    Aug 24 12:22:25.219: INFO: 
    Aug 24 12:22:25.219: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 24 12:22:26.233: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995853317s
    Aug 24 12:22:27.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982405655s
    Aug 24 12:22:28.242: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97738849s
    Aug 24 12:22:29.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972982873s
    Aug 24 12:22:30.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967928382s
    Aug 24 12:22:31.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963054604s
    Aug 24 12:22:32.264: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958088103s
    Aug 24 12:22:33.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.950898615s
    Aug 24 12:22:34.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.853638ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-207 08/24/23 12:22:35.275
    Aug 24 12:22:35.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:22:35.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 24 12:22:35.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:22:35.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:22:35.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:22:35.798: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 24 12:22:35.798: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:22:35.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:22:35.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 24 12:22:36.065: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 24 12:22:36.065: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 24 12:22:36.065: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 24 12:22:36.069: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 24 12:22:46.076: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:22:46.076: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 24 12:22:46.076: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/24/23 12:22:46.076
    Aug 24 12:22:46.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:22:46.325: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:22:46.325: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:22:46.325: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:22:46.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:22:46.570: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:22:46.570: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:22:46.570: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:22:46.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=statefulset-207 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 24 12:22:46.846: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 24 12:22:46.846: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 24 12:22:46.846: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 24 12:22:46.846: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:22:46.853: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Aug 24 12:22:56.863: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:22:56.864: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:22:56.864: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 24 12:22:56.885: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
    Aug 24 12:22:56.885: INFO: ss-0  gitlab-1-26-36460-guscsyka22xa-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:04 +0000 UTC  }]
    Aug 24 12:22:56.885: INFO: ss-1  gitlab-1-26-36460-guscsyka22xa-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
    Aug 24 12:22:56.885: INFO: ss-2  gitlab-1-26-36460-guscsyka22xa-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
    Aug 24 12:22:56.885: INFO: 
    Aug 24 12:22:56.885: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 24 12:22:57.888: INFO: POD   NODE                                   PHASE    GRACE  CONDITIONS
    Aug 24 12:22:57.888: INFO: ss-1  gitlab-1-26-36460-guscsyka22xa-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
    Aug 24 12:22:57.888: INFO: ss-2  gitlab-1-26-36460-guscsyka22xa-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:22:25 +0000 UTC  }]
    Aug 24 12:22:57.889: INFO: 
    Aug 24 12:22:57.889: INFO: StatefulSet ss has not reached scale 0, at 2
    Aug 24 12:22:58.893: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990624341s
    Aug 24 12:22:59.900: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987218359s
    Aug 24 12:23:00.903: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.979973653s
    Aug 24 12:23:01.907: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976331908s
    Aug 24 12:23:02.911: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972636388s
    Aug 24 12:23:03.914: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968703772s
    Aug 24 12:23:04.918: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965445722s
    Aug 24 12:23:05.925: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.006477ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-207 08/24/23 12:23:06.925
    Aug 24 12:23:06.929: INFO: Scaling statefulset ss to 0
    Aug 24 12:23:06.938: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 24 12:23:06.941: INFO: Deleting all statefulset in ns statefulset-207
    Aug 24 12:23:06.945: INFO: Scaling statefulset ss to 0
    Aug 24 12:23:06.955: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 24 12:23:06.958: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:07.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-207" for this suite. 08/24/23 12:23:07.01
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:07.023
Aug 24 12:23:07.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename webhook 08/24/23 12:23:07.024
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:07.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:07.049
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/24/23 12:23:07.084
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:23:07.65
STEP: Deploying the webhook pod 08/24/23 12:23:07.661
STEP: Wait for the deployment to be ready 08/24/23 12:23:07.686
Aug 24 12:23:07.699: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/24/23 12:23:09.71
STEP: Verifying the service has paired with the endpoint 08/24/23 12:23:09.728
Aug 24 12:23:10.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/24/23 12:23:10.733
STEP: Creating a custom resource definition that should be denied by the webhook 08/24/23 12:23:10.761
Aug 24 12:23:10.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:10.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5205" for this suite. 08/24/23 12:23:10.871
STEP: Destroying namespace "webhook-5205-markers" for this suite. 08/24/23 12:23:10.895
------------------------------
â€¢ [3.900 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:07.023
    Aug 24 12:23:07.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename webhook 08/24/23 12:23:07.024
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:07.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:07.049
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/24/23 12:23:07.084
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/24/23 12:23:07.65
    STEP: Deploying the webhook pod 08/24/23 12:23:07.661
    STEP: Wait for the deployment to be ready 08/24/23 12:23:07.686
    Aug 24 12:23:07.699: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/24/23 12:23:09.71
    STEP: Verifying the service has paired with the endpoint 08/24/23 12:23:09.728
    Aug 24 12:23:10.728: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/24/23 12:23:10.733
    STEP: Creating a custom resource definition that should be denied by the webhook 08/24/23 12:23:10.761
    Aug 24 12:23:10.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:10.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5205" for this suite. 08/24/23 12:23:10.871
    STEP: Destroying namespace "webhook-5205-markers" for this suite. 08/24/23 12:23:10.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:10.925
Aug 24 12:23:10.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename pod-network-test 08/24/23 12:23:10.927
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:10.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:10.953
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2258 08/24/23 12:23:10.963
STEP: creating a selector 08/24/23 12:23:10.963
STEP: Creating the service pods in kubernetes 08/24/23 12:23:10.963
Aug 24 12:23:10.963: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 24 12:23:11.035: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2258" to be "running and ready"
Aug 24 12:23:11.057: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.342879ms
Aug 24 12:23:11.057: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:23:13.065: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030056007s
Aug 24 12:23:13.065: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 24 12:23:15.062: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.026750867s
Aug 24 12:23:15.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:17.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025892748s
Aug 24 12:23:17.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:19.063: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027260946s
Aug 24 12:23:19.063: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:21.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025359394s
Aug 24 12:23:21.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:23.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025878022s
Aug 24 12:23:23.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:25.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.026130115s
Aug 24 12:23:25.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:27.063: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027288684s
Aug 24 12:23:27.063: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:29.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025778341s
Aug 24 12:23:29.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:31.065: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029854761s
Aug 24 12:23:31.065: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 24 12:23:33.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02621725s
Aug 24 12:23:33.062: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 24 12:23:33.062: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 24 12:23:33.066: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2258" to be "running and ready"
Aug 24 12:23:33.070: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.215269ms
Aug 24 12:23:33.070: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 24 12:23:33.070: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 24 12:23:33.074: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2258" to be "running and ready"
Aug 24 12:23:33.078: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.991951ms
Aug 24 12:23:33.078: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 24 12:23:33.078: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/24/23 12:23:33.082
Aug 24 12:23:33.094: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2258" to be "running"
Aug 24 12:23:33.102: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540641ms
Aug 24 12:23:35.106: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011680263s
Aug 24 12:23:35.106: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 24 12:23:35.109: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 24 12:23:35.109: INFO: Breadth first check of 10.100.148.241 on host 10.0.0.4...
Aug 24 12:23:35.112: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.134:9080/dial?request=hostname&protocol=udp&host=10.100.148.241&port=8081&tries=1'] Namespace:pod-network-test-2258 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:23:35.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:23:35.113: INFO: ExecWithOptions: Clientset creation
Aug 24 12:23:35.113: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2258/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.148.241%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 12:23:35.238: INFO: Waiting for responses: map[]
Aug 24 12:23:35.239: INFO: reached 10.100.148.241 after 0/1 tries
Aug 24 12:23:35.239: INFO: Breadth first check of 10.100.181.179 on host 10.0.0.18...
Aug 24 12:23:35.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.134:9080/dial?request=hostname&protocol=udp&host=10.100.181.179&port=8081&tries=1'] Namespace:pod-network-test-2258 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:23:35.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:23:35.243: INFO: ExecWithOptions: Clientset creation
Aug 24 12:23:35.243: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2258/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.181.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 12:23:35.377: INFO: Waiting for responses: map[]
Aug 24 12:23:35.377: INFO: reached 10.100.181.179 after 0/1 tries
Aug 24 12:23:35.377: INFO: Breadth first check of 10.100.45.189 on host 10.0.0.17...
Aug 24 12:23:35.381: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.134:9080/dial?request=hostname&protocol=udp&host=10.100.45.189&port=8081&tries=1'] Namespace:pod-network-test-2258 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:23:35.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:23:35.381: INFO: ExecWithOptions: Clientset creation
Aug 24 12:23:35.381: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2258/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.45.189%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 24 12:23:35.504: INFO: Waiting for responses: map[]
Aug 24 12:23:35.504: INFO: reached 10.100.45.189 after 0/1 tries
Aug 24 12:23:35.504: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:35.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2258" for this suite. 08/24/23 12:23:35.509
------------------------------
â€¢ [SLOW TEST] [24.595 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:10.925
    Aug 24 12:23:10.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename pod-network-test 08/24/23 12:23:10.927
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:10.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:10.953
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2258 08/24/23 12:23:10.963
    STEP: creating a selector 08/24/23 12:23:10.963
    STEP: Creating the service pods in kubernetes 08/24/23 12:23:10.963
    Aug 24 12:23:10.963: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 24 12:23:11.035: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2258" to be "running and ready"
    Aug 24 12:23:11.057: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.342879ms
    Aug 24 12:23:11.057: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:23:13.065: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030056007s
    Aug 24 12:23:13.065: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 24 12:23:15.062: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.026750867s
    Aug 24 12:23:15.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:17.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.025892748s
    Aug 24 12:23:17.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:19.063: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.027260946s
    Aug 24 12:23:19.063: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:21.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.025359394s
    Aug 24 12:23:21.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:23.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.025878022s
    Aug 24 12:23:23.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:25.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.026130115s
    Aug 24 12:23:25.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:27.063: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.027288684s
    Aug 24 12:23:27.063: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:29.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.025778341s
    Aug 24 12:23:29.061: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:31.065: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029854761s
    Aug 24 12:23:31.065: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 24 12:23:33.061: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.02621725s
    Aug 24 12:23:33.062: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 24 12:23:33.062: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 24 12:23:33.066: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2258" to be "running and ready"
    Aug 24 12:23:33.070: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.215269ms
    Aug 24 12:23:33.070: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 24 12:23:33.070: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 24 12:23:33.074: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2258" to be "running and ready"
    Aug 24 12:23:33.078: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.991951ms
    Aug 24 12:23:33.078: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 24 12:23:33.078: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/24/23 12:23:33.082
    Aug 24 12:23:33.094: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2258" to be "running"
    Aug 24 12:23:33.102: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540641ms
    Aug 24 12:23:35.106: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011680263s
    Aug 24 12:23:35.106: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 24 12:23:35.109: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 24 12:23:35.109: INFO: Breadth first check of 10.100.148.241 on host 10.0.0.4...
    Aug 24 12:23:35.112: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.134:9080/dial?request=hostname&protocol=udp&host=10.100.148.241&port=8081&tries=1'] Namespace:pod-network-test-2258 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:23:35.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:23:35.113: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:23:35.113: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2258/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.148.241%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 12:23:35.238: INFO: Waiting for responses: map[]
    Aug 24 12:23:35.239: INFO: reached 10.100.148.241 after 0/1 tries
    Aug 24 12:23:35.239: INFO: Breadth first check of 10.100.181.179 on host 10.0.0.18...
    Aug 24 12:23:35.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.134:9080/dial?request=hostname&protocol=udp&host=10.100.181.179&port=8081&tries=1'] Namespace:pod-network-test-2258 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:23:35.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:23:35.243: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:23:35.243: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2258/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.181.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 12:23:35.377: INFO: Waiting for responses: map[]
    Aug 24 12:23:35.377: INFO: reached 10.100.181.179 after 0/1 tries
    Aug 24 12:23:35.377: INFO: Breadth first check of 10.100.45.189 on host 10.0.0.17...
    Aug 24 12:23:35.381: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.45.134:9080/dial?request=hostname&protocol=udp&host=10.100.45.189&port=8081&tries=1'] Namespace:pod-network-test-2258 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:23:35.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:23:35.381: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:23:35.381: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2258/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.45.134%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.45.189%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 24 12:23:35.504: INFO: Waiting for responses: map[]
    Aug 24 12:23:35.504: INFO: reached 10.100.45.189 after 0/1 tries
    Aug 24 12:23:35.504: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:35.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2258" for this suite. 08/24/23 12:23:35.509
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:35.523
Aug 24 12:23:35.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename daemonsets 08/24/23 12:23:35.526
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:35.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:35.554
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:23:35.591
STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:23:35.602
Aug 24 12:23:35.616: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:23:35.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:23:35.621: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:23:36.628: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:23:36.632: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:23:36.632: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:23:37.627: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:23:37.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 24 12:23:37.630: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
Aug 24 12:23:38.627: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:23:38.632: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 24 12:23:38.632: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
Aug 24 12:23:39.626: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 24 12:23:39.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 24 12:23:39.630: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 08/24/23 12:23:39.633
Aug 24 12:23:39.638: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/24/23 12:23:39.638
Aug 24 12:23:39.653: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/24/23 12:23:39.653
Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: ADDED
Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.659: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.659: INFO: Found daemon set daemon-set in namespace daemonsets-5046 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 12:23:39.659: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/24/23 12:23:39.659
STEP: watching for the daemon set status to be patched 08/24/23 12:23:39.67
Aug 24 12:23:39.672: INFO: Observed &DaemonSet event: ADDED
Aug 24 12:23:39.672: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.673: INFO: Observed daemon set daemon-set in namespace daemonsets-5046 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
Aug 24 12:23:39.673: INFO: Found daemon set daemon-set in namespace daemonsets-5046 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 24 12:23:39.673: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:23:39.684
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5046, will wait for the garbage collector to delete the pods 08/24/23 12:23:39.684
Aug 24 12:23:39.750: INFO: Deleting DaemonSet.extensions daemon-set took: 11.897207ms
Aug 24 12:23:39.852: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.070036ms
Aug 24 12:23:41.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 24 12:23:41.763: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 24 12:23:41.766: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46502"},"items":null}

Aug 24 12:23:41.768: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46502"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:41.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5046" for this suite. 08/24/23 12:23:41.8
------------------------------
â€¢ [SLOW TEST] [6.292 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:35.523
    Aug 24 12:23:35.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename daemonsets 08/24/23 12:23:35.526
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:35.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:35.554
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 08/24/23 12:23:35.591
    STEP: Check that daemon pods launch on every node of the cluster. 08/24/23 12:23:35.602
    Aug 24 12:23:35.616: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:23:35.621: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:23:35.621: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:23:36.628: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:23:36.632: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:23:36.632: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:23:37.627: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:23:37.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 24 12:23:37.630: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-0 is running 0 daemon pod, expected 1
    Aug 24 12:23:38.627: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:23:38.632: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 24 12:23:38.632: INFO: Node gitlab-1-26-36460-guscsyka22xa-node-2 is running 0 daemon pod, expected 1
    Aug 24 12:23:39.626: INFO: DaemonSet pods can't tolerate node gitlab-1-26-36460-guscsyka22xa-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 24 12:23:39.630: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 24 12:23:39.630: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 08/24/23 12:23:39.633
    Aug 24 12:23:39.638: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/24/23 12:23:39.638
    Aug 24 12:23:39.653: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/24/23 12:23:39.653
    Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: ADDED
    Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.658: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.659: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.659: INFO: Found daemon set daemon-set in namespace daemonsets-5046 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 12:23:39.659: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/24/23 12:23:39.659
    STEP: watching for the daemon set status to be patched 08/24/23 12:23:39.67
    Aug 24 12:23:39.672: INFO: Observed &DaemonSet event: ADDED
    Aug 24 12:23:39.672: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.673: INFO: Observed daemon set daemon-set in namespace daemonsets-5046 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 24 12:23:39.673: INFO: Observed &DaemonSet event: MODIFIED
    Aug 24 12:23:39.673: INFO: Found daemon set daemon-set in namespace daemonsets-5046 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 24 12:23:39.673: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/24/23 12:23:39.684
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5046, will wait for the garbage collector to delete the pods 08/24/23 12:23:39.684
    Aug 24 12:23:39.750: INFO: Deleting DaemonSet.extensions daemon-set took: 11.897207ms
    Aug 24 12:23:39.852: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.070036ms
    Aug 24 12:23:41.763: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 24 12:23:41.763: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 24 12:23:41.766: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46502"},"items":null}

    Aug 24 12:23:41.768: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46502"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:41.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5046" for this suite. 08/24/23 12:23:41.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:41.82
Aug 24 12:23:41.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename downward-api 08/24/23 12:23:41.821
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:41.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:41.9
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/24/23 12:23:41.907
Aug 24 12:23:41.921: INFO: Waiting up to 5m0s for pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6" in namespace "downward-api-4588" to be "Succeeded or Failed"
Aug 24 12:23:41.942: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.017489ms
Aug 24 12:23:43.946: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025054589s
Aug 24 12:23:45.948: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026349531s
STEP: Saw pod success 08/24/23 12:23:45.948
Aug 24 12:23:45.948: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6" satisfied condition "Succeeded or Failed"
Aug 24 12:23:45.951: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6 container dapi-container: <nil>
STEP: delete the pod 08/24/23 12:23:46.009
Aug 24 12:23:46.033: INFO: Waiting for pod downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6 to disappear
Aug 24 12:23:46.039: INFO: Pod downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:46.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4588" for this suite. 08/24/23 12:23:46.044
------------------------------
â€¢ [4.236 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:41.82
    Aug 24 12:23:41.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename downward-api 08/24/23 12:23:41.821
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:41.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:41.9
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/24/23 12:23:41.907
    Aug 24 12:23:41.921: INFO: Waiting up to 5m0s for pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6" in namespace "downward-api-4588" to be "Succeeded or Failed"
    Aug 24 12:23:41.942: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.017489ms
    Aug 24 12:23:43.946: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025054589s
    Aug 24 12:23:45.948: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026349531s
    STEP: Saw pod success 08/24/23 12:23:45.948
    Aug 24 12:23:45.948: INFO: Pod "downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6" satisfied condition "Succeeded or Failed"
    Aug 24 12:23:45.951: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-2 pod downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6 container dapi-container: <nil>
    STEP: delete the pod 08/24/23 12:23:46.009
    Aug 24 12:23:46.033: INFO: Waiting for pod downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6 to disappear
    Aug 24 12:23:46.039: INFO: Pod downward-api-2752919b-d928-4bbf-9fcb-a66b16bd24c6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:46.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4588" for this suite. 08/24/23 12:23:46.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:46.076
Aug 24 12:23:46.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename emptydir 08/24/23 12:23:46.077
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:46.11
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:46.118
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/24/23 12:23:46.124
Aug 24 12:23:46.150: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c" in namespace "emptydir-6781" to be "running"
Aug 24 12:23:46.159: INFO: Pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.619703ms
Aug 24 12:23:48.164: INFO: Pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c": Phase="Running", Reason="", readiness=false. Elapsed: 2.013843625s
Aug 24 12:23:48.164: INFO: Pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/24/23 12:23:48.164
Aug 24 12:23:48.164: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6781 PodName:pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 24 12:23:48.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
Aug 24 12:23:48.165: INFO: ExecWithOptions: Clientset creation
Aug 24 12:23:48.165: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-6781/pods/pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 24 12:23:48.298: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:48.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6781" for this suite. 08/24/23 12:23:48.304
------------------------------
â€¢ [2.244 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:46.076
    Aug 24 12:23:46.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename emptydir 08/24/23 12:23:46.077
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:46.11
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:46.118
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/24/23 12:23:46.124
    Aug 24 12:23:46.150: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c" in namespace "emptydir-6781" to be "running"
    Aug 24 12:23:46.159: INFO: Pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.619703ms
    Aug 24 12:23:48.164: INFO: Pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c": Phase="Running", Reason="", readiness=false. Elapsed: 2.013843625s
    Aug 24 12:23:48.164: INFO: Pod "pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/24/23 12:23:48.164
    Aug 24 12:23:48.164: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6781 PodName:pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 24 12:23:48.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    Aug 24 12:23:48.165: INFO: ExecWithOptions: Clientset creation
    Aug 24 12:23:48.165: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-6781/pods/pod-sharedvolume-0d45a59e-0e24-43eb-bf3f-99ab58bb484c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 24 12:23:48.298: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:48.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6781" for this suite. 08/24/23 12:23:48.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:48.321
Aug 24 12:23:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename secrets 08/24/23 12:23:48.322
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:48.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:48.357
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-1085f5a9-19e4-49b0-940b-69b843b3d6e5 08/24/23 12:23:48.364
STEP: Creating a pod to test consume secrets 08/24/23 12:23:48.373
Aug 24 12:23:48.394: INFO: Waiting up to 5m0s for pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1" in namespace "secrets-8485" to be "Succeeded or Failed"
Aug 24 12:23:48.414: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.930818ms
Aug 24 12:23:50.419: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024427604s
Aug 24 12:23:52.418: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023897112s
Aug 24 12:23:54.419: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025075663s
STEP: Saw pod success 08/24/23 12:23:54.419
Aug 24 12:23:54.420: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1" satisfied condition "Succeeded or Failed"
Aug 24 12:23:54.423: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1 container secret-env-test: <nil>
STEP: delete the pod 08/24/23 12:23:54.481
Aug 24 12:23:54.504: INFO: Waiting for pod pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1 to disappear
Aug 24 12:23:54.510: INFO: Pod pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:54.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8485" for this suite. 08/24/23 12:23:54.539
------------------------------
â€¢ [SLOW TEST] [6.246 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:48.321
    Aug 24 12:23:48.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename secrets 08/24/23 12:23:48.322
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:48.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:48.357
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-1085f5a9-19e4-49b0-940b-69b843b3d6e5 08/24/23 12:23:48.364
    STEP: Creating a pod to test consume secrets 08/24/23 12:23:48.373
    Aug 24 12:23:48.394: INFO: Waiting up to 5m0s for pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1" in namespace "secrets-8485" to be "Succeeded or Failed"
    Aug 24 12:23:48.414: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.930818ms
    Aug 24 12:23:50.419: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024427604s
    Aug 24 12:23:52.418: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023897112s
    Aug 24 12:23:54.419: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025075663s
    STEP: Saw pod success 08/24/23 12:23:54.419
    Aug 24 12:23:54.420: INFO: Pod "pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1" satisfied condition "Succeeded or Failed"
    Aug 24 12:23:54.423: INFO: Trying to get logs from node gitlab-1-26-36460-guscsyka22xa-node-1 pod pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1 container secret-env-test: <nil>
    STEP: delete the pod 08/24/23 12:23:54.481
    Aug 24 12:23:54.504: INFO: Waiting for pod pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1 to disappear
    Aug 24 12:23:54.510: INFO: Pod pod-secrets-b25d8609-baec-4d62-b3b2-1d03c7ddcfd1 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:54.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8485" for this suite. 08/24/23 12:23:54.539
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:54.568
Aug 24 12:23:54.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename kubectl 08/24/23 12:23:54.569
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:54.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:54.612
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/24/23 12:23:54.62
Aug 24 12:23:54.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3948 create -f -'
Aug 24 12:23:55.074: INFO: stderr: ""
Aug 24 12:23:55.074: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/24/23 12:23:55.074
Aug 24 12:23:55.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3948 diff -f -'
Aug 24 12:23:55.572: INFO: rc: 1
Aug 24 12:23:55.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3948 delete -f -'
Aug 24 12:23:55.711: INFO: stderr: ""
Aug 24 12:23:55.711: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:55.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3948" for this suite. 08/24/23 12:23:55.721
------------------------------
â€¢ [1.170 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:54.568
    Aug 24 12:23:54.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename kubectl 08/24/23 12:23:54.569
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:54.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:54.612
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/24/23 12:23:54.62
    Aug 24 12:23:54.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3948 create -f -'
    Aug 24 12:23:55.074: INFO: stderr: ""
    Aug 24 12:23:55.074: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/24/23 12:23:55.074
    Aug 24 12:23:55.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3948 diff -f -'
    Aug 24 12:23:55.572: INFO: rc: 1
    Aug 24 12:23:55.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3520367747 --namespace=kubectl-3948 delete -f -'
    Aug 24 12:23:55.711: INFO: stderr: ""
    Aug 24 12:23:55.711: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:55.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3948" for this suite. 08/24/23 12:23:55.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:55.739
Aug 24 12:23:55.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename ingressclass 08/24/23 12:23:55.741
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:55.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:55.774
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/24/23 12:23:55.78
STEP: getting /apis/networking.k8s.io 08/24/23 12:23:55.785
STEP: getting /apis/networking.k8s.iov1 08/24/23 12:23:55.787
STEP: creating 08/24/23 12:23:55.789
STEP: getting 08/24/23 12:23:55.811
STEP: listing 08/24/23 12:23:55.814
STEP: watching 08/24/23 12:23:55.818
Aug 24 12:23:55.818: INFO: starting watch
STEP: patching 08/24/23 12:23:55.82
STEP: updating 08/24/23 12:23:55.828
Aug 24 12:23:55.836: INFO: waiting for watch events with expected annotations
Aug 24 12:23:55.836: INFO: saw patched and updated annotations
STEP: deleting 08/24/23 12:23:55.836
STEP: deleting a collection 08/24/23 12:23:55.849
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:55.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-873" for this suite. 08/24/23 12:23:55.875
------------------------------
â€¢ [0.155 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:55.739
    Aug 24 12:23:55.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename ingressclass 08/24/23 12:23:55.741
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:55.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:55.774
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/24/23 12:23:55.78
    STEP: getting /apis/networking.k8s.io 08/24/23 12:23:55.785
    STEP: getting /apis/networking.k8s.iov1 08/24/23 12:23:55.787
    STEP: creating 08/24/23 12:23:55.789
    STEP: getting 08/24/23 12:23:55.811
    STEP: listing 08/24/23 12:23:55.814
    STEP: watching 08/24/23 12:23:55.818
    Aug 24 12:23:55.818: INFO: starting watch
    STEP: patching 08/24/23 12:23:55.82
    STEP: updating 08/24/23 12:23:55.828
    Aug 24 12:23:55.836: INFO: waiting for watch events with expected annotations
    Aug 24 12:23:55.836: INFO: saw patched and updated annotations
    STEP: deleting 08/24/23 12:23:55.836
    STEP: deleting a collection 08/24/23 12:23:55.849
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:55.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-873" for this suite. 08/24/23 12:23:55.875
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/24/23 12:23:55.894
Aug 24 12:23:55.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:23:55.896
STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:55.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:55.932
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 24 12:23:55.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-743" for this suite. 08/24/23 12:23:55.975
------------------------------
â€¢ [0.098 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/24/23 12:23:55.894
    Aug 24 12:23:55.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3520367747
    STEP: Building a namespace api object, basename runtimeclass 08/24/23 12:23:55.896
    STEP: Waiting for a default service account to be provisioned in namespace 08/24/23 12:23:55.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/24/23 12:23:55.932
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 24 12:23:55.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-743" for this suite. 08/24/23 12:23:55.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 24 12:23:55.995: INFO: Running AfterSuite actions on node 1
Aug 24 12:23:55.995: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 24 12:23:55.995: INFO: Running AfterSuite actions on node 1
    Aug 24 12:23:55.995: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.150 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6193.527 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h43m14.054992875s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

